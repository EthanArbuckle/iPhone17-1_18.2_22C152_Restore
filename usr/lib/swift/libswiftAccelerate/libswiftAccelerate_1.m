vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, const uint8_t *__attribute__((__org_typedef(Pixel_8888))) *a6, vImage_Flags a7, double a8, double a9)
{
  return vImageVerticalShearD_ARGB8888(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShearD_ARGB8888(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

uint64_t vImage.PixelBuffer<>.shear<A>(direction:translate:slope:resamplingFilter:backgroundColor:useFloat16Accumulator:destination:)(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, int a7, uint64_t *a8, uint64_t a9)
{
  v25 = a8;
  uint64_t v23 = a3;
  v24 = a1;
  uint64_t v13 = *(void *)(a9 - 8);
  MEMORY[0x1F4188790](a1);
  v15 = (char *)&v21 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0);
  if (v16)
  {
    uint64_t v17 = 0;
  }
  else
  {
    uint64_t v22 = a4;
    HIDWORD(v21) = a7;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    uint64_t v17 = swift_allocObject();
    *(_OWORD *)(v17 + 16) = xmmword_1D2135FC0;
    *(_DWORD *)(v17 + 32) = a5;
    *(_WORD *)(v17 + 36) = WORD2(a5);
    a4 = v22;
    *(_WORD *)(v17 + 38) = HIWORD(a5);
    LOBYTE(a7) = BYTE4(v21);
  }
  v18 = *(void (**)(char *, uint64_t, uint64_t))(v13 + 16);
  v18(v15, a2, a9);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  v18(v15, v23, a9);
  BinaryFloatingPoint.init<A>(_:)();
  if (v17) {
    uint64_t v19 = v17 + 32;
  }
  else {
    uint64_t v19 = 0;
  }
  specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*v24, a4, *v25, v19, v17 == 0, (uint64_t)&unk_1F28D7AB8, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, a7 & 1, *v26);
  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.shear<A>(direction:translate:slope:resamplingFilter:backgroundColor:destination:)(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t *a7, uint64_t a8)
{
  v9 = v8;
  v26 = a1;
  v27 = a7;
  uint64_t v25 = a3;
  uint64_t v14 = *(void *)(a8 - 8);
  MEMORY[0x1F4188790](a1);
  char v16 = (char *)&v22 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  if (v17)
  {
    uint64_t v18 = 0;
  }
  else
  {
    uint64_t v23 = v9;
    uint64_t v24 = a4;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    uint64_t v18 = swift_allocObject();
    *(_OWORD *)(v18 + 16) = xmmword_1D2135FC0;
    *(void *)(v18 + 32) = a5;
    v9 = v23;
    a4 = v24;
  }
  uint64_t v19 = *(void (**)(char *, uint64_t, uint64_t))(v14 + 16);
  v19(v16, a2, a8);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  v19(v16, v25, a8);
  BinaryFloatingPoint.init<A>(_:)();
  if (v18) {
    uint64_t v20 = v18 + 32;
  }
  else {
    uint64_t v20 = 0;
  }
  specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*v26, a4, *v27, v20, v18 == 0, (uint64_t)&unk_1F28D7B58, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, 0, *v9);
  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.shear<A>(direction:translate:slope:resamplingFilter:backgroundColor:destination:)(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4, unint64_t a5, uint64_t a6, uint64_t a7)
{
  v17[0] = a6;
  uint64_t v18 = a4;
  uint64_t v11 = *(void *)(a7 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v13 = (char *)v17 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v14 = *(void (**)(char *))(v11 + 16);
  v14(v13);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  double v15 = v20;
  ((void (*)(char *, uint64_t, uint64_t))v14)(v13, a3, a7);
  BinaryFloatingPoint.init<A>(_:)();
  return specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*a1, v18, *(void *)v17[0], a5 | ((HIDWORD(a5) & 1) << 32), (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, float *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned Float, @unowned UInt32) -> (@unowned Int), v15, v19, 0.0, 0, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, float *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned Float, @unowned UInt32) -> (@unowned Int), 0, 0, *(void *)v17[1]);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned Float, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, Pixel_F *a6, vImage_Flags flags, double a8, double a9)
{
  return vImageVerticalShearD_PlanarF(a1, a2, a3, a4, a8, a9, a5, *a6, flags);
}

{
  return vImageHorizontalShearD_PlanarF(a1, a2, a3, a4, a8, a9, a5, *a6, flags);
}

uint64_t vImage.PixelBuffer<>.shear<A>(direction:translate:slope:resamplingFilter:backgroundColor:destination:)(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, unint64_t a6, uint64_t a7, uint64_t *a8, uint64_t a9)
{
  v10 = v9;
  v28 = a1;
  v29 = a8;
  uint64_t v27 = a3;
  uint64_t v15 = *(void *)(a9 - 8);
  MEMORY[0x1F4188790](a1);
  char v17 = (char *)&v25 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  if (v18)
  {
    uint64_t v19 = 0;
  }
  else
  {
    v26 = v10;
    unint64_t v25 = HIDWORD(a6);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
    uint64_t v19 = swift_allocObject();
    *(_OWORD *)(v19 + 16) = xmmword_1D2135FC0;
    *(void *)(v19 + 32) = a5;
    int v20 = v25;
    v10 = v26;
    *(_DWORD *)(v19 + 40) = a6;
    *(_DWORD *)(v19 + 44) = v20;
  }
  uint64_t v21 = *(void (**)(char *, uint64_t, uint64_t))(v15 + 16);
  v21(v17, a2, a9);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  v21(v17, v27, a9);
  BinaryFloatingPoint.init<A>(_:)();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135280;
  *(_DWORD *)(inited + 32) = 0;
  if (v19) {
    uint64_t v23 = v19 + 32;
  }
  else {
    uint64_t v23 = 0;
  }
  specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*v28, a4, *v29, v23, v19 == 0, inited + 32, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<Float>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<Float>?, @unowned UInt32) -> (@unowned Int), 0, 0, *v10);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<Float>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, const float *__attribute__((__org_typedef(Pixel_FFFF))) *a6, vImage_Flags a7, double a8, double a9)
{
  return vImageVerticalShearD_ARGBFFFF(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShearD_ARGBFFFF(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

uint64_t closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t a1, double a2, double a3, uint64_t a4, char a5, void (*a6)(uint64_t, uint64_t, void, void, uint64_t, char *, double, double), uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, void *a11, void (*a12)(uint64_t, uint64_t, void, void, uint64_t, char *, double, double), uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16)
{
  v29[4] = *MEMORY[0x1E4F143B8];
  int v18 = a5 & 1;
  type metadata accessor for vImage.PixelBuffer();
  v29[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v29[1] = v19;
  v29[2] = v20;
  v29[3] = v21;
  return closure #1 in closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)v29, v18, a6, a7, a1, a8, a9, a10, a2, a3, a11, a12, a13, a14, a15, a16);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, char a2, uint64_t (*a3)(uint64_t, uint64_t, void, void, uint64_t, char *), uint64_t a4, uint64_t a5, uint64_t a6, __int16 a7, char a8, void *a9, uint64_t (*a10)(uint64_t, uint64_t, void, void, uint64_t, char *))
{
  if ((a7 & 0x100) == 0) {
    a8 = a7;
  }
  if ((a2 & 1) == 0)
  {
    char v12 = a8;
    if ((*a9 & 0x8000000000000000) != 0)
    {
      __break(1u);
    }
    else if (!HIDWORD(*a9))
    {
      return a10(a5, result, 0, 0, a6, &v12);
    }
    __break(1u);
    goto LABEL_13;
  }
  char v11 = a8;
  if ((*a9 & 0x8000000000000000) != 0)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  if (HIDWORD(*a9))
  {
LABEL_14:
    __break(1u);
    return result;
  }
  return a3(a5, result, 0, 0, a6, &v11);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, char a2, uint64_t (*a3)(uint64_t, uint64_t, void, void, uint64_t, __int16 *), uint64_t a4, uint64_t a5, uint64_t a6, int a7, __int16 a8, void *a9, uint64_t (*a10)(uint64_t, uint64_t, void, void, uint64_t, __int16 *))
{
  if ((a7 & 0x10000) != 0) {
    __int16 v10 = a8;
  }
  else {
    __int16 v10 = a7;
  }
  if ((a2 & 1) == 0)
  {
    __int16 v12 = v10;
    if ((*a9 & 0x8000000000000000) != 0)
    {
      __break(1u);
    }
    else if (!HIDWORD(*a9))
    {
      return a10(a5, result, 0, 0, a6, &v12);
    }
    __break(1u);
    goto LABEL_14;
  }
  __int16 v11 = v10;
  if ((*a9 & 0x8000000000000000) != 0)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  if (HIDWORD(*a9))
  {
LABEL_15:
    __break(1u);
    return result;
  }
  return a3(a5, result, 0, 0, a6, &v11);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, char a2, uint64_t (*a3)(uint64_t, uint64_t, void, void, uint64_t, float *), double a4, double a5, float a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, void *a11, uint64_t (*a12)(uint64_t, uint64_t, void, void, uint64_t, float *))
{
  if ((a10 & 0x100000000) == 0) {
    a6 = *(float *)&a10;
  }
  if ((a2 & 1) == 0)
  {
    float v14 = a6;
    if ((*a11 & 0x8000000000000000) != 0)
    {
      __break(1u);
    }
    else if (!HIDWORD(*a11))
    {
      return a12(a8, result, 0, 0, a9, &v14);
    }
    __break(1u);
    goto LABEL_13;
  }
  float v13 = a6;
  if ((*a11 & 0x8000000000000000) != 0)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  if (HIDWORD(*a11))
  {
LABEL_14:
    __break(1u);
    return result;
  }
  return a3(a8, result, 0, 0, a9, &v13);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, char a2, uint64_t (*a3)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, char a8, uint64_t a9, void *a10, uint64_t (*a11)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *))
{
  if ((a8 & 1) == 0) {
    a9 = a7;
  }
  if ((a2 & 1) == 0)
  {
    uint64_t v12 = a9;
    if ((*a10 & 0x8000000000000000) != 0)
    {
      __break(1u);
    }
    else if (!HIDWORD(*a10))
    {
      return a11(a5, result, 0, 0, a6, &v12);
    }
    __break(1u);
    goto LABEL_13;
  }
  uint64_t v12 = a9;
  if ((*a10 & 0x8000000000000000) != 0)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  if (HIDWORD(*a10))
  {
LABEL_14:
    __break(1u);
    return result;
  }
  return a3(a5, result, 0, 0, a6, &v12);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t a1, int a2, void (*a3)(uint64_t, uint64_t, void, void, uint64_t, char *, double, double), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, double a10, void *a11, void (*a12)(uint64_t, uint64_t, void, void, uint64_t, char *, double, double), uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16)
{
  uint64_t v51 = a6;
  uint64_t v53 = a7;
  uint64_t v54 = a8;
  uint64_t v49 = a1;
  uint64_t v50 = a5;
  uint64_t v45 = a4;
  v46 = a3;
  int v52 = a2;
  v55 = a11;
  uint64_t v18 = type metadata accessor for Optional();
  uint64_t v19 = MEMORY[0x1F4188790](v18);
  uint64_t v21 = (char *)&v45 - ((v20 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v22 = MEMORY[0x1F4188790](v19);
  uint64_t v24 = (char *)&v45 - v23;
  uint64_t v25 = *(void *)(a16 - 8);
  uint64_t v26 = MEMORY[0x1F4188790](v22);
  v28 = (char *)&v45 - ((v27 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v29 = MEMORY[0x1F4188790](v26);
  v47 = (char *)&v45 - v30;
  uint64_t v31 = MEMORY[0x1F4188790](v29);
  v33 = (char *)&v45 - v32;
  MEMORY[0x1F4188790](v31);
  v36 = (char *)&v45 - v35;
  uint64_t v48 = v37;
  v38 = *(void (**)(char *, uint64_t, uint64_t))(v37 + 16);
  v39 = (void (**)(char *, uint64_t, uint64_t))(v25 + 16);
  v40 = (unsigned int (**)(char *, uint64_t, uint64_t))(v25 + 48);
  if ((v52 & 1) == 0)
  {
    uint64_t v41 = v34;
    v38(v21, v53, v34);
    (*v39)(v28, v54, a16);
    if ((*v40)(v21, 1, a16) == 1)
    {
      v42 = v47;
      (*(void (**)(char *, char *, uint64_t))(v25 + 32))(v47, v28, a16);
      uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v48 + 8))(v21, v41);
    }
    else
    {
      (*(void (**)(char *, uint64_t))(v25 + 8))(v28, a16);
      v42 = v47;
      uint64_t result = (*(uint64_t (**)(char *, char *, uint64_t))(v25 + 32))(v47, v21, a16);
    }
    if ((*v55 & 0x8000000000000000) != 0)
    {
      __break(1u);
    }
    else if (!HIDWORD(*v55))
    {
      a12(v50, v49, 0, 0, v51, v42, a9, a10);
      v36 = v42;
      return (*(uint64_t (**)(char *, uint64_t))(v25 + 8))(v36, a16);
    }
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v44 = v34;
  v38(v24, v53, v34);
  (*v39)(v33, v54, a16);
  if ((*v40)(v24, 1, a16) == 1)
  {
    (*(void (**)(char *, char *, uint64_t))(v25 + 32))(v36, v33, a16);
    uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v48 + 8))(v24, v44);
  }
  else
  {
    (*(void (**)(char *, uint64_t))(v25 + 8))(v33, a16);
    uint64_t result = (*(uint64_t (**)(char *, char *, uint64_t))(v25 + 32))(v36, v24, a16);
  }
  if ((*v55 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!HIDWORD(*v55))
  {
    v46(v50, v49, 0, 0, v51, v36, a9, a10);
    return (*(uint64_t (**)(char *, uint64_t))(v25 + 8))(v36, a16);
  }
LABEL_18:
  __break(1u);
  return result;
}

BOOL static vImage.ReflectionAxis.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void vImage.ReflectionAxis.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

uint64_t (*vImage.PixelBuffer<>.reflect(over:destination:)(uint64_t (*result)(uint64_t *, uint64_t *, void), uint64_t *a2))(uint64_t *, uint64_t *, void)
{
  uint64_t v3 = *v2;
  if (!*(void *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v4 = *(void *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v6 = *a2;
  if (!*(void *)(*a2 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v7 = *(void *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v8 = *(void *)(v6 + 40);
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    return result;
  }
  if (*(unsigned char *)result) {
    v9 = @nonobjc vImageHorizontalReflect_Planar8(_:_:_:);
  }
  else {
    v9 = @nonobjc vImageVerticalReflect_Planar8(_:_:_:);
  }
  return specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t (*)(uint64_t *, uint64_t *, void))v9, 0, v6, v3);
}

{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImage_Error (__cdecl *v9)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags);

  uint64_t v3 = *v2;
  if (!*(void *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v4 = *(void *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v6 = *a2;
  if (!*(void *)(*a2 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v7 = *(void *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v8 = *(void *)(v6 + 40);
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    return result;
  }
  if (*(unsigned char *)result) {
    v9 = (vImage_Error (__cdecl *)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags))@nonobjc vImageHorizontalReflect_Planar16F(_:_:_:);
  }
  else {
    v9 = @nonobjc vImageVerticalReflect_Planar16F(_:_:_:);
  }
  return specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t (*)(uint64_t *, uint64_t *, void))v9, 0, v6, v3);
}

{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImage_Error (__cdecl *v9)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags);

  uint64_t v3 = *v2;
  if (!*(void *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v4 = *(void *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v6 = *a2;
  if (!*(void *)(*a2 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v7 = *(void *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v8 = *(void *)(v6 + 40);
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    return result;
  }
  if (*(unsigned char *)result) {
    v9 = @nonobjc vImageHorizontalReflect_CbCr16F(_:_:_:);
  }
  else {
    v9 = @nonobjc vImageVerticalReflect_CbCr16F(_:_:_:);
  }
  return specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t (*)(uint64_t *, uint64_t *, void))v9, 0, v6, v3);
}

{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImage_Error (__cdecl *v9)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags);

  uint64_t v3 = *v2;
  if (!*(void *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v4 = *(void *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v6 = *a2;
  if (!*(void *)(*a2 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v7 = *(void *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v8 = *(void *)(v6 + 40);
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    return result;
  }
  if (*(unsigned char *)result) {
    v9 = @nonobjc vImageHorizontalReflect_ARGB16F(_:_:_:);
  }
  else {
    v9 = @nonobjc vImageVerticalReflect_ARGB16F(_:_:_:);
  }
  return specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t (*)(uint64_t *, uint64_t *, void))v9, 0, v6, v3);
}

{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImage_Error (__cdecl *v9)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags);

  uint64_t v3 = *v2;
  if (!*(void *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v4 = *(void *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v6 = *a2;
  if (!*(void *)(*a2 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v7 = *(void *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v8 = *(void *)(v6 + 40);
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    return result;
  }
  if (*(unsigned char *)result) {
    v9 = @nonobjc vImageHorizontalReflect_ARGB8888(_:_:_:);
  }
  else {
    v9 = @nonobjc vImageVerticalReflect_ARGB8888(_:_:_:);
  }
  return specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t (*)(uint64_t *, uint64_t *, void))v9, 0, v6, v3);
}

{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImage_Error (__cdecl *v9)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags);

  uint64_t v3 = *v2;
  if (!*(void *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v4 = *(void *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v6 = *a2;
  if (!*(void *)(*a2 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v7 = *(void *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v8 = *(void *)(v6 + 40);
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    return result;
  }
  if (*(unsigned char *)result) {
    v9 = @nonobjc vImageHorizontalReflect_ARGB16U(_:_:_:);
  }
  else {
    v9 = @nonobjc vImageVerticalReflect_ARGB16U(_:_:_:);
  }
  return specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t (*)(uint64_t *, uint64_t *, void))v9, 0, v6, v3);
}

{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImage_Error (__cdecl *v9)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags);

  uint64_t v3 = *v2;
  if (!*(void *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v4 = *(void *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v6 = *a2;
  if (!*(void *)(*a2 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v7 = *(void *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v8 = *(void *)(v6 + 40);
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    return result;
  }
  if (*(unsigned char *)result) {
    v9 = @nonobjc vImageHorizontalReflect_PlanarF(_:_:_:);
  }
  else {
    v9 = @nonobjc vImageVerticalReflect_PlanarF(_:_:_:);
  }
  return specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t (*)(uint64_t *, uint64_t *, void))v9, 0, v6, v3);
}

{
  uint64_t *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImage_Error (__cdecl *v9)(const vImage_Buffer *, const vImage_Buffer *, vImage_Flags);

  uint64_t v3 = *v2;
  if (!*(void *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v4 = *(void *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v6 = *a2;
  if (!*(void *)(*a2 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v7 = *(void *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v8 = *(void *)(v6 + 40);
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    return result;
  }
  if (*(unsigned char *)result) {
    v9 = @nonobjc vImageHorizontalReflect_ARGBFFFF(_:_:_:);
  }
  else {
    v9 = @nonobjc vImageVerticalReflect_ARGBFFFF(_:_:_:);
  }
  return specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t (*)(uint64_t *, uint64_t *, void))v9, 0, v6, v3);
}

uint64_t (*specialized vImage.PixelBuffer<>._reflect(reflectFunc:destination:)(uint64_t (*result)(uint64_t *, uint64_t *, void), uint64_t a2, uint64_t a3, uint64_t a4))(uint64_t *, uint64_t *, void)
{
  uint64_t v14 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a4 + 16))
  {
    __break(1u);
LABEL_10:
    __break(1u);
  }
  if (!*(void *)(a3 + 16)) {
    goto LABEL_10;
  }
  uint64_t v4 = *(void *)(a4 + 32);
  uint64_t v5 = *(void *)(a3 + 32);
  if (v4)
  {
    if (!v5 || v4 != v5) {
      goto LABEL_8;
    }
    __break(1u);
  }
  if (v5)
  {
LABEL_8:
    uint64_t v6 = *(void *)(a4 + 56);
    uint64_t v11 = v4;
    long long v12 = *(_OWORD *)(a4 + 40);
    uint64_t v13 = v6;
    uint64_t v7 = *(void *)(a3 + 56);
    uint64_t v8 = v5;
    long long v9 = *(_OWORD *)(a3 + 40);
    uint64_t v10 = v7;
    return (uint64_t (*)(uint64_t *, uint64_t *, void))result(&v11, &v8, 0);
  }
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>._reflect(reflectFunc:destination:)(uint64_t (*a1)(uint64_t, void *, void), uint64_t a2, uint64_t *a3)
{
  v11[4] = *MEMORY[0x1E4F143B8];
  uint64_t v4 = *a3;
  uint64_t v5 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v11[0] = v4;
  uint64_t result = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (v5)
  {
    if (result) {
      BOOL v7 = v5 == result;
    }
    else {
      BOOL v7 = 0;
    }
    if (!v7) {
      goto LABEL_9;
    }
    __break(1u);
  }
  if (result)
  {
LABEL_9:
    v11[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
    v11[1] = v8;
    v11[2] = v9;
    v11[3] = v10;
    return closure #1 in vImage.PixelBuffer<>._reflect(reflectFunc:destination:)((uint64_t)v11, v4, a1);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in vImage.PixelBuffer<>._reflect(reflectFunc:destination:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, void *, void))
{
  v9[4] = *MEMORY[0x1E4F143B8];
  type metadata accessor for vImage.PixelBuffer();
  v9[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v9[1] = v5;
  v9[2] = v6;
  v9[3] = v7;
  return a3(a1, v9, 0);
}

uint64_t vImage.PixelBuffer<>.scale(destination:)(uint64_t *a1)
{
  v1 = (uint64_t (*)(_OWORD *, _OWORD *, void, void))MEMORY[0x1E4F17130];

  return vImage.PixelBuffer<>.scale(destination:)(a1, v1);
}

{
  uint64_t (*v1)(_OWORD *, _OWORD *, void, void);
  uint64_t vars8;

  v1 = (uint64_t (*)(_OWORD *, _OWORD *, void, void))MEMORY[0x1E4F17118];

  return vImage.PixelBuffer<>.scale(destination:)(a1, v1);
}

{
  uint64_t (*v1)(_OWORD *, _OWORD *, void, void);
  uint64_t vars8;

  v1 = (uint64_t (*)(_OWORD *, _OWORD *, void, void))MEMORY[0x1E4F170F8];

  return vImage.PixelBuffer<>.scale(destination:)(a1, v1);
}

{
  uint64_t (*v1)(_OWORD *, _OWORD *, void, void);
  uint64_t vars8;

  v1 = (uint64_t (*)(_OWORD *, _OWORD *, void, void))MEMORY[0x1E4F17128];

  return vImage.PixelBuffer<>.scale(destination:)(a1, v1);
}

{
  uint64_t (*v1)(_OWORD *, _OWORD *, void, void);
  uint64_t vars8;

  v1 = (uint64_t (*)(_OWORD *, _OWORD *, void, void))MEMORY[0x1E4F17110];

  return vImage.PixelBuffer<>.scale(destination:)(a1, v1);
}

{
  uint64_t (*v1)(_OWORD *, _OWORD *, void, void);
  uint64_t vars8;

  v1 = (uint64_t (*)(_OWORD *, _OWORD *, void, void))MEMORY[0x1E4F170F0];

  return vImage.PixelBuffer<>.scale(destination:)(a1, v1);
}

{
  uint64_t (*v1)(_OWORD *, _OWORD *, void, void);
  uint64_t vars8;

  v1 = (uint64_t (*)(_OWORD *, _OWORD *, void, void))MEMORY[0x1E4F17138];

  return vImage.PixelBuffer<>.scale(destination:)(a1, v1);
}

{
  uint64_t (*v1)(_OWORD *, _OWORD *, void, void);
  uint64_t vars8;

  v1 = (uint64_t (*)(_OWORD *, _OWORD *, void, void))MEMORY[0x1E4F17100];

  return vImage.PixelBuffer<>.scale(destination:)(a1, v1);
}

uint64_t vImage.PixelBuffer<>.scale(useFloat16Accumulator:destination:)(char a1, uint64_t *a2)
{
  v2 = (uint64_t (*)(_OWORD *, _OWORD *, void, uint64_t))MEMORY[0x1E4F17120];

  return vImage.PixelBuffer<>.scale(useFloat16Accumulator:destination:)(a1, a2, v2);
}

{
  uint64_t (*v2)(_OWORD *, _OWORD *, void, uint64_t);
  uint64_t vars8;

  v2 = (uint64_t (*)(_OWORD *, _OWORD *, void, uint64_t))MEMORY[0x1E4F17108];

  return vImage.PixelBuffer<>.scale(useFloat16Accumulator:destination:)(a1, a2, v2);
}

{
  uint64_t (*v2)(_OWORD *, _OWORD *, void, uint64_t);
  uint64_t vars8;

  v2 = (uint64_t (*)(_OWORD *, _OWORD *, void, uint64_t))MEMORY[0x1E4F170E8];

  return vImage.PixelBuffer<>.scale(useFloat16Accumulator:destination:)(a1, a2, v2);
}

uint64_t vImage.PixelBuffer<>.scale(useFloat16Accumulator:destination:)(char a1, uint64_t *a2, uint64_t (*a3)(_OWORD *, _OWORD *, void, uint64_t))
{
  uint64_t v12 = *MEMORY[0x1E4F143B8];
  uint64_t v4 = *v3;
  if (!*(void *)(*v3 + 16))
  {
    __break(1u);
LABEL_8:
    __break(1u);
  }
  long long v5 = *(_OWORD *)(v4 + 48);
  v11[0] = *(_OWORD *)(v4 + 32);
  v11[1] = v5;
  uint64_t v6 = *a2;
  if (!*(void *)(*a2 + 16)) {
    goto LABEL_8;
  }
  if (a1) {
    uint64_t v7 = 4096;
  }
  else {
    uint64_t v7 = 0;
  }
  long long v8 = *(_OWORD *)(v6 + 48);
  v10[0] = *(_OWORD *)(v6 + 32);
  v10[1] = v8;
  return a3(v11, v10, 0, v7);
}

uint64_t vImage.PixelBuffer<>.scale(destination:)(uint64_t *a1, uint64_t (*a2)(_OWORD *, _OWORD *, void, void))
{
  uint64_t v10 = *MEMORY[0x1E4F143B8];
  uint64_t v3 = *v2;
  if (!*(void *)(*v2 + 16))
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  long long v4 = *(_OWORD *)(v3 + 48);
  v9[0] = *(_OWORD *)(v3 + 32);
  v9[1] = v4;
  uint64_t v5 = *a1;
  if (!*(void *)(*a1 + 16)) {
    goto LABEL_5;
  }
  long long v6 = *(_OWORD *)(v5 + 48);
  v8[0] = *(_OWORD *)(v5 + 32);
  v8[1] = v6;
  return a2(v9, v8, 0, 0);
}

uint64_t vImage.PixelBuffer<>.transform(_:backgroundColor:destination:)(_OWORD *a1, __int16 a2, uint64_t *a3)
{
  uint64_t v13 = *MEMORY[0x1E4F143B8];
  uint64_t v5 = *a3;
  uint64_t v6 = *v3;
  long long v7 = a1[1];
  v11[0] = *a1;
  v11[1] = v7;
  _OWORD v11[2] = a1[2];
  if (!*(void *)(v6 + 16)) {
    __break(1u);
  }
  if ((a2 & 0x100) != 0) {
    unint64_t v8 = 8;
  }
  else {
    unint64_t v8 = 4;
  }
  long long v9 = *(_OWORD *)(v6 + 48);
  v12[0] = *(_OWORD *)(v6 + 32);
  v12[1] = v9;
  return specialized closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)((uint64_t)v12, v5, (uint64_t)v11, a2 & 0x1FF, 0, v8);
}

uint64_t vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)(_OWORD *a1, uint64_t *a2, uint64_t a3, uint64_t a4, void (*a5)(uint64_t, uint64_t, void, uint64_t, char *, unint64_t), uint64_t a6, uint64_t a7, uint64_t a8)
{
  v22[4] = *MEMORY[0x1E4F143B8];
  uint64_t v14 = *a2;
  long long v15 = a1[1];
  v21[0] = *a1;
  v21[1] = v15;
  v21[2] = a1[2];
  if ((*(unsigned int (**)(uint64_t, uint64_t, uint64_t))(*(void *)(a8 - 8) + 48))(a3, 1, a8) == 1) {
    unint64_t v16 = 8;
  }
  else {
    unint64_t v16 = 4;
  }
  v22[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v22[1] = v17;
  v22[2] = v18;
  v22[3] = v19;
  return closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)((uint64_t)v22, v14, a5, a6, (uint64_t)v21, a3, a4, v16, *(void *)(a7 + 16), a8);
}

vImage_Error vImage.PixelBuffer<>.transform(_:backgroundColor:useFloat16Accumulator:destination:)(_OWORD *a1, int a2, char a3, uint64_t *a4)
{
  return specialized vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)(a1, *a4, a2 & 0x1FFFF, 0, a3, *v4);
}

vImage_Error specialized vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)(_OWORD *a1, uint64_t a2, int a3, Pixel_16F a4, char a5, uint64_t a6)
{
  uint64_t v16 = *MEMORY[0x1E4F143B8];
  long long v6 = a1[1];
  *(_OWORD *)&v13.a = *a1;
  *(_OWORD *)&v13.c = v6;
  *(_OWORD *)&v13.tx = a1[2];
  if (!*(void *)(a6 + 16))
  {
    __break(1u);
LABEL_14:
    __break(1u);
  }
  long long v7 = *(_OWORD *)(a6 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(a6 + 32);
  *(_OWORD *)&src.width = v7;
  if (!*(void *)(a2 + 16)) {
    goto LABEL_14;
  }
  long long v8 = *(_OWORD *)(a2 + 48);
  *(_OWORD *)&dest.data = *(_OWORD *)(a2 + 32);
  *(_OWORD *)&dest.width = v8;
  if ((a3 & 0x10000) != 0) {
    Pixel_16F v9 = a4;
  }
  else {
    Pixel_16F v9 = a3;
  }
  if ((a3 & 0x10000) != 0) {
    int v10 = 8;
  }
  else {
    int v10 = 4;
  }
  if (a5) {
    int v11 = 4096;
  }
  else {
    int v11 = 0;
  }
  return vImageAffineWarpD_Planar16F(&src, &dest, 0, &v13, v9, v10 | v11);
}

uint64_t specialized vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)(_OWORD *a1, uint64_t a2, uint64_t a3, char a4, uint64_t a5, char a6, uint64_t a7, uint64_t (*a8)(_OWORD *, _OWORD *, void, _OWORD *, uint64_t, void))
{
  uint64_t v17 = *MEMORY[0x1E4F143B8];
  long long v8 = a1[1];
  v14[0] = *a1;
  v14[1] = v8;
  v14[2] = a1[2];
  if (!*(void *)(a7 + 16))
  {
    __break(1u);
LABEL_11:
    __break(1u);
  }
  long long v9 = *(_OWORD *)(a7 + 48);
  v16[0] = *(_OWORD *)(a7 + 32);
  v16[1] = v9;
  if (!*(void *)(a2 + 16)) {
    goto LABEL_11;
  }
  long long v10 = *(_OWORD *)(a2 + 48);
  v15[0] = *(_OWORD *)(a2 + 32);
  v15[1] = v10;
  if (a4)
  {
    int v11 = 8;
  }
  else
  {
    a5 = a3;
    int v11 = 4;
  }
  if (a6) {
    int v12 = 4096;
  }
  else {
    int v12 = 0;
  }
  return a8(v16, v15, 0, v14, a5, v11 | v12);
}

uint64_t vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)(_OWORD *a1, uint64_t *a2, uint64_t a3, uint64_t a4, void (*a5)(uint64_t, uint64_t, void, uint64_t, char *), uint64_t a6, char a7, uint64_t a8, uint64_t a9)
{
  v26[4] = *MEMORY[0x1E4F143B8];
  uint64_t v15 = *a2;
  long long v16 = a1[1];
  v25[0] = *a1;
  v25[1] = v16;
  v25[2] = a1[2];
  int v17 = (*(uint64_t (**)(uint64_t, uint64_t, uint64_t))(*(void *)(a9 - 8) + 48))(a3, 1, a9);
  uint64_t v18 = 4;
  if (v17 == 1) {
    uint64_t v18 = 8;
  }
  uint64_t v19 = 4096;
  if ((a7 & 1) == 0) {
    uint64_t v19 = 0;
  }
  uint64_t v24 = v18 | v19;
  v26[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v26[1] = v20;
  v26[2] = v21;
  v26[3] = v22;
  return closure #1 in vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)((uint64_t)v26, v15, a5, a6, (uint64_t)v25, a3, a4, &v24, *(void *)(a8 + 16), a9);
}

uint64_t vImage.PixelBuffer<>.transform(_:backgroundColor:useFloat16Accumulator:destination:)(_OWORD *a1, unint64_t a2, char a3, uint64_t *a4)
{
  char v8 = BYTE4(a2) & 1;
  if ((a2 & 0x100000000) != 0)
  {
    uint64_t v12 = 0;
  }
  else
  {
    __int16 v9 = a2;
    unint64_t v10 = a2 >> 16;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    uint64_t v11 = swift_allocObject();
    *(_OWORD *)(v11 + 16) = xmmword_1D2135290;
    *(_WORD *)(v11 + 32) = v9;
    uint64_t v12 = v11 + 32;
    *(_WORD *)(v11 + 34) = v10;
  }
  specialized vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)(a1, *a4, v12, v8, (uint64_t)&unk_1F28D7A90, a3 & 1, *v4, MEMORY[0x1E4F16F78]);
  swift_bridgeObjectRelease();

  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.transform(_:backgroundColor:useFloat16Accumulator:destination:)(_OWORD *a1, unint64_t a2, char a3, char a4, uint64_t *a5)
{
  char v9 = a3 & 1;
  if (a3)
  {
    uint64_t v15 = 0;
  }
  else
  {
    __int16 v10 = a2;
    unint64_t v11 = a2 >> 16;
    unint64_t v12 = HIDWORD(a2);
    unint64_t v13 = HIWORD(a2);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    uint64_t v14 = swift_allocObject();
    *(_OWORD *)(v14 + 16) = xmmword_1D2135FC0;
    *(_WORD *)(v14 + 32) = v10;
    uint64_t v15 = v14 + 32;
    *(_WORD *)(v14 + 34) = v11;
    *(_WORD *)(v14 + 36) = v12;
    *(_WORD *)(v14 + 38) = v13;
  }
  specialized vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)(a1, *a5, v15, v9, (uint64_t)&unk_1F28D7B08, a4 & 1, *v5, MEMORY[0x1E4F16F70]);
  swift_bridgeObjectRelease();

  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.transform(_:backgroundColor:destination:)(_OWORD *a1, unint64_t a2, uint64_t *a3)
{
  uint64_t v21 = *MEMORY[0x1E4F143B8];
  if ((a2 & 0x100000000) != 0)
  {
    uint64_t v11 = 0;
  }
  else
  {
    unint64_t v7 = a2 >> 8;
    unint64_t v8 = a2 >> 16;
    unint64_t v9 = a2 >> 24;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    uint64_t v10 = swift_allocObject();
    *(_OWORD *)(v10 + 16) = xmmword_1D2135FC0;
    *(unsigned char *)(v10 + 32) = a2;
    uint64_t v11 = v10 + 32;
    *(unsigned char *)(v10 + 33) = v7;
    *(unsigned char *)(v10 + 34) = v8;
    *(unsigned char *)(v10 + 35) = v9;
  }
  uint64_t v12 = *a3;
  uint64_t v13 = *v3;
  long long v14 = a1[1];
  v19[0] = *a1;
  v19[1] = v14;
  v19[2] = a1[2];
  if (!*(void *)(v13 + 16)) {
    __break(1u);
  }
  long long v15 = *(_OWORD *)(v13 + 48);
  if ((a2 & 0x100000000) != 0) {
    unint64_t v16 = 8;
  }
  else {
    unint64_t v16 = 4;
  }
  v20[0] = *(_OWORD *)(v13 + 32);
  v20[1] = v15;
  specialized closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)((uint64_t)v20, v12, (uint64_t)v19, v11, (a2 & 0x100000000) >> 32, (uint64_t)&unk_1F28D78D0, v16, (uint64_t)v18, MEMORY[0x1E4F16F60]);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

{
  uint64_t *v3;
  uint64_t v5;
  uint64_t v6;
  long long v7;
  unint64_t v8;
  __n128 v9;
  long long v10;
  _OWORD v12[3];
  _OWORD v13[2];
  uint64_t v14;

  long long v14 = *MEMORY[0x1E4F143B8];
  uint64_t v5 = *a3;
  long long v6 = *v3;
  unint64_t v7 = a1[1];
  v12[0] = *a1;
  v12[1] = v7;
  _OWORD v12[2] = a1[2];
  if (!*(void *)(v6 + 16)) {
    __break(1u);
  }
  if ((a2 & 0x100000000) != 0) {
    unint64_t v8 = 8;
  }
  else {
    unint64_t v8 = 4;
  }
  uint64_t v10 = *(_OWORD *)(v6 + 48);
  v13[0] = *(_OWORD *)(v6 + 32);
  v9.n128_u64[1] = *((void *)&v13[0] + 1);
  v13[1] = v10;
  v9.n128_u64[0] = 0;
  return specialized closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)((uint64_t)v13, v5, (uint64_t)v12, a2 | ((HIDWORD(a2) & 1) << 32), v8, v9);
}

uint64_t vImage.PixelBuffer<>.transform(_:backgroundColor:destination:)(_OWORD *a1, unint64_t a2, char a3, uint64_t *a4)
{
  uint64_t v23 = *MEMORY[0x1E4F143B8];
  if (a3)
  {
    uint64_t v13 = 0;
  }
  else
  {
    __int16 v8 = a2;
    unint64_t v9 = a2 >> 16;
    unint64_t v10 = HIDWORD(a2);
    unint64_t v11 = HIWORD(a2);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    uint64_t v12 = swift_allocObject();
    *(_OWORD *)(v12 + 16) = xmmword_1D2135FC0;
    *(_WORD *)(v12 + 32) = v8;
    uint64_t v13 = v12 + 32;
    *(_WORD *)(v12 + 34) = v9;
    *(_WORD *)(v12 + 36) = v10;
    *(_WORD *)(v12 + 38) = v11;
  }
  uint64_t v14 = *a4;
  uint64_t v15 = *v4;
  long long v16 = a1[1];
  v21[0] = *a1;
  v21[1] = v16;
  v21[2] = a1[2];
  if (!*(void *)(v15 + 16)) {
    __break(1u);
  }
  long long v17 = *(_OWORD *)(v15 + 48);
  if (a3) {
    unint64_t v18 = 8;
  }
  else {
    unint64_t v18 = 4;
  }
  v22[0] = *(_OWORD *)(v15 + 32);
  v22[1] = v17;
  specialized closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)((uint64_t)v22, v14, (uint64_t)v21, v13, a3 & 1, (uint64_t)&unk_1F28D7BF8, v18, (uint64_t)v20, MEMORY[0x1E4F16F58]);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.transform(_:backgroundColor:destination:)(_OWORD *a1, unint64_t a2, unint64_t a3, char a4, uint64_t *a5)
{
  uint64_t v8 = 0;
  uint64_t v24 = *MEMORY[0x1E4F143B8];
  if ((a4 & 1) == 0)
  {
    int v9 = a3;
    int v10 = a2;
    unint64_t v11 = HIDWORD(a2);
    unint64_t v12 = HIDWORD(a3);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
    uint64_t v8 = swift_allocObject();
    *(_OWORD *)(v8 + 16) = xmmword_1D2135FC0;
    *(_DWORD *)(v8 + 32) = v10;
    *(_DWORD *)(v8 + 36) = v11;
    *(_DWORD *)(v8 + 40) = v9;
    *(_DWORD *)(v8 + 44) = v12;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135280;
  *(_DWORD *)(inited + 32) = 0;
  uint64_t v14 = *a5;
  uint64_t v15 = *v5;
  long long v16 = a1[1];
  v22[0] = *a1;
  v22[1] = v16;
  _OWORD v22[2] = a1[2];
  if (!*(void *)(v15 + 16)) {
    __break(1u);
  }
  if (v8) {
    unint64_t v17 = 4;
  }
  else {
    unint64_t v17 = 8;
  }
  long long v18 = *(_OWORD *)(v15 + 48);
  if (v8) {
    uint64_t v19 = v8 + 32;
  }
  else {
    uint64_t v19 = 0;
  }
  v23[0] = *(_OWORD *)(v15 + 32);
  v23[1] = v18;
  specialized closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)((uint64_t)v23, v14, (uint64_t)v22, v19, v8 == 0, inited + 32, v17, (uint64_t)v21, MEMORY[0x1E4F16F68]);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

void vImage_AffineTransform_Double.init(a:b:c:d:tx:ty:)(double *a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>, double a5@<D3>, double a6@<D4>, double a7@<D5>)
{
  *a1 = a2;
  a1[1] = a3;
  a1[2] = a4;
  a1[3] = a5;
  a1[4] = a6;
  a1[5] = a7;
}

uint64_t specialized closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, __int16 a4, unsigned __int8 a5, unint64_t a6)
{
  uint64_t v10 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a2 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  long long v6 = *(_OWORD *)(a2 + 48);
  v9[0] = *(_OWORD *)(a2 + 32);
  v9[1] = v6;
  if ((a6 & 0x8000000000000000) != 0)
  {
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
  }
  if (HIDWORD(a6)) {
    goto LABEL_10;
  }
  if ((a4 & 0x100) != 0) {
    unsigned __int8 v7 = a5;
  }
  else {
    unsigned __int8 v7 = a4;
  }
  return MEMORY[0x1D2600E50](a1, v9, 0, a3, v7);
}

uint64_t specialized closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, unint64_t a5, __n128 a6)
{
  uint64_t v9 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a2 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  long long v6 = *(_OWORD *)(a2 + 48);
  v8[0] = *(_OWORD *)(a2 + 32);
  v8[1] = v6;
  if ((a5 & 0x8000000000000000) != 0)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (HIDWORD(a5)) {
    goto LABEL_9;
  }
  if ((a4 & 0x100000000) == 0) {
    a6.n128_f32[0] = *(float *)&a4;
  }
  return MEMORY[0x1D2600E60](a1, v8, 0, a3, a6);
}

uint64_t specialized closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, char a5, uint64_t a6, unint64_t a7, uint64_t a8, uint64_t (*a9)(uint64_t, _OWORD *, void, uint64_t, uint64_t, unint64_t))
{
  uint64_t v13 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a2 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  long long v9 = *(_OWORD *)(a2 + 48);
  v12[0] = *(_OWORD *)(a2 + 32);
  v12[1] = v9;
  if ((a7 & 0x8000000000000000) != 0)
  {
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
  }
  if (HIDWORD(a7)) {
    goto LABEL_10;
  }
  if (a5) {
    a4 = a6;
  }
  return a9(a1, v12, 0, a3, a4, a7);
}

uint64_t closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)(uint64_t a1, uint64_t a2, void (*a3)(uint64_t, uint64_t, void, uint64_t, char *, unint64_t), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, unint64_t a8, uint64_t a9, uint64_t a10)
{
  v21[4] = *MEMORY[0x1E4F143B8];
  type metadata accessor for vImage.PixelBuffer();
  v21[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v21[1] = v13;
  v21[2] = v14;
  _OWORD v21[3] = v15;
  return closure #1 in closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)((uint64_t)v21, a3, a4, a1, a5, a6, a7, a8, a9, a10);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>._affineWarp<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:)(uint64_t a1, void (*a2)(uint64_t, uint64_t, void, uint64_t, char *, unint64_t), uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, unint64_t a8, uint64_t a9, uint64_t a10)
{
  v28 = a2;
  uint64_t v29 = a7;
  uint64_t v25 = a4;
  uint64_t v26 = a5;
  uint64_t v27 = a3;
  uint64_t v24 = a1;
  uint64_t v12 = type metadata accessor for Optional();
  uint64_t v13 = *(void *)(v12 - 8);
  uint64_t v14 = MEMORY[0x1F4188790](v12);
  long long v16 = (char *)&v24 - v15;
  uint64_t v17 = *(void *)(a10 - 8);
  uint64_t v18 = MEMORY[0x1F4188790](v14);
  uint64_t v20 = (char *)&v24 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1F4188790](v18);
  uint64_t v22 = (char *)&v24 - v21;
  (*(void (**)(char *, uint64_t, uint64_t))(v13 + 16))(v16, a6, v12);
  (*(void (**)(char *, uint64_t, uint64_t))(v17 + 16))(v20, v29, a10);
  if ((*(unsigned int (**)(char *, uint64_t, uint64_t))(v17 + 48))(v16, 1, a10) == 1)
  {
    (*(void (**)(char *, char *, uint64_t))(v17 + 32))(v22, v20, a10);
    uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v13 + 8))(v16, v12);
  }
  else
  {
    (*(void (**)(char *, uint64_t))(v17 + 8))(v20, a10);
    uint64_t result = (*(uint64_t (**)(char *, char *, uint64_t))(v17 + 32))(v22, v16, a10);
  }
  if ((a8 & 0x8000000000000000) != 0)
  {
    __break(1u);
  }
  else if (!HIDWORD(a8))
  {
    v28(v25, v24, 0, v26, v22, a8);
    return (*(uint64_t (**)(char *, uint64_t))(v17 + 8))(v22, a10);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)(uint64_t a1, uint64_t a2, void (*a3)(uint64_t, uint64_t, void, uint64_t, char *), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, void *a8, uint64_t a9, uint64_t a10)
{
  v21[4] = *MEMORY[0x1E4F143B8];
  type metadata accessor for vImage.PixelBuffer();
  v21[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v21[1] = v13;
  v21[2] = v14;
  _OWORD v21[3] = v15;
  return closure #1 in closure #1 in vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)((uint64_t)v21, a3, a4, a1, a5, a6, a7, a8, a9, a10);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>._affineWarpD<A>(_:destination:backgroundColor:nullBackgroundColor:affineWarpFunc:useFloat16Accumulator:)(uint64_t a1, void (*a2)(uint64_t, uint64_t, void, uint64_t, char *), uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, void *a8, uint64_t a9, uint64_t a10)
{
  v28 = a2;
  uint64_t v29 = a8;
  uint64_t v25 = a4;
  uint64_t v26 = a5;
  uint64_t v27 = a3;
  uint64_t v24 = a1;
  uint64_t v12 = type metadata accessor for Optional();
  uint64_t v13 = *(void *)(v12 - 8);
  uint64_t v14 = MEMORY[0x1F4188790](v12);
  long long v16 = (char *)&v24 - v15;
  uint64_t v17 = *(void *)(a10 - 8);
  uint64_t v18 = MEMORY[0x1F4188790](v14);
  uint64_t v20 = (char *)&v24 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1F4188790](v18);
  uint64_t v22 = (char *)&v24 - v21;
  (*(void (**)(char *, uint64_t, uint64_t))(v13 + 16))(v16, a6, v12);
  (*(void (**)(char *, uint64_t, uint64_t))(v17 + 16))(v20, a7, a10);
  if ((*(unsigned int (**)(char *, uint64_t, uint64_t))(v17 + 48))(v16, 1, a10) == 1)
  {
    (*(void (**)(char *, char *, uint64_t))(v17 + 32))(v22, v20, a10);
    uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v13 + 8))(v16, v12);
  }
  else
  {
    (*(void (**)(char *, uint64_t))(v17 + 8))(v20, a10);
    uint64_t result = (*(uint64_t (**)(char *, char *, uint64_t))(v17 + 32))(v22, v16, a10);
  }
  if ((*v29 & 0x8000000000000000) != 0)
  {
    __break(1u);
  }
  else if (!HIDWORD(*v29))
  {
    v28(v25, v24, 0, v26, v22);
    return (*(uint64_t (**)(char *, uint64_t))(v17 + 8))(v22, a10);
  }
  __break(1u);
  return result;
}

float vImage_AffineTransform.init(a:b:c:d:tx:ty:)@<S0>(float *a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>, double a5@<D3>, double a6@<D4>, double a7@<D5>)
{
  float v7 = a2;
  float v8 = a3;
  float v9 = a4;
  float v10 = a5;
  float v11 = a6;
  *a1 = v7;
  a1[1] = v8;
  a1[2] = v9;
  a1[3] = v10;
  float result = a7;
  a1[4] = v11;
  a1[5] = result;
  return result;
}

unint64_t lazy protocol witness table accessor for type vImage.ShearDirection and conformance vImage.ShearDirection()
{
  unint64_t result = lazy protocol witness table cache variable for type vImage.ShearDirection and conformance vImage.ShearDirection;
  if (!lazy protocol witness table cache variable for type vImage.ShearDirection and conformance vImage.ShearDirection)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.ShearDirection and conformance vImage.ShearDirection);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type vImage.ReflectionAxis and conformance vImage.ReflectionAxis()
{
  unint64_t result = lazy protocol witness table cache variable for type vImage.ReflectionAxis and conformance vImage.ReflectionAxis;
  if (!lazy protocol witness table cache variable for type vImage.ReflectionAxis and conformance vImage.ReflectionAxis)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.ReflectionAxis and conformance vImage.ReflectionAxis);
  }
  return result;
}

ValueMetadata *type metadata accessor for vImage.Rotation()
{
  return &type metadata for vImage.Rotation;
}

ValueMetadata *type metadata accessor for vImage.ShearDirection()
{
  return &type metadata for vImage.ShearDirection;
}

unsigned char *storeEnumTagSinglePayload for vImage.ShearDirection(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 1 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFF) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFE)
  {
    unsigned int v6 = ((a2 - 255) >> 8) + 1;
    *unint64_t result = a2 + 1;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D20F7050);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 1;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vImage.ReflectionAxis()
{
  return &type metadata for vImage.ReflectionAxis;
}

double BNNS.FusedBinaryArithmeticParameters.layerParameters(inputA:inputB:output:)@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, _OWORD *a3@<X2>, uint64_t *a4@<X8>)
{
  *(_OWORD *)&v21[116] = a2[7];
  *(_OWORD *)&v21[132] = a2[8];
  *(_OWORD *)&v21[148] = a2[9];
  *(_OWORD *)&v21[164] = a2[10];
  *(_OWORD *)&v21[52] = a2[3];
  *(_OWORD *)&v21[68] = a2[4];
  *(_OWORD *)&v21[84] = a2[5];
  *(_OWORD *)&v21[100] = a2[6];
  *(_OWORD *)&v21[4] = *a2;
  *(_OWORD *)&v21[20] = a2[1];
  *(_OWORD *)&v21[36] = a2[2];
  *(_OWORD *)&v20[116] = a3[7];
  *(_OWORD *)&v20[132] = a3[8];
  *(_OWORD *)&v20[148] = a3[9];
  *(_OWORD *)&v20[164] = a3[10];
  *(_OWORD *)&v20[52] = a3[3];
  *(_OWORD *)&v20[68] = a3[4];
  *(_OWORD *)&v20[84] = a3[5];
  *(_OWORD *)&v20[100] = a3[6];
  *(_OWORD *)&v20[4] = *a3;
  *(_OWORD *)&v20[20] = a3[1];
  int v7 = *(unsigned __int8 *)(v4 + 8);
  int v8 = *(unsigned __int8 *)(v4 + 9);
  int v9 = *(unsigned __int8 *)(v4 + 10);
  *(_OWORD *)&v20[36] = a3[2];
  uint64_t v10 = swift_slowAlloc();
  long long v11 = a1[9];
  *(_OWORD *)(v10 + 128) = a1[8];
  *(_OWORD *)(v10 + 144) = v11;
  *(_OWORD *)(v10 + 160) = a1[10];
  long long v12 = a1[5];
  *(_OWORD *)(v10 + 64) = a1[4];
  *(_OWORD *)(v10 + 80) = v12;
  long long v13 = a1[7];
  *(_OWORD *)(v10 + 96) = a1[6];
  *(_OWORD *)(v10 + 112) = v13;
  long long v14 = a1[1];
  *(_OWORD *)uint64_t v10 = *a1;
  *(_OWORD *)(v10 + 16) = v14;
  long long v15 = a1[3];
  *(_OWORD *)(v10 + 32) = a1[2];
  *(_OWORD *)(v10 + 48) = v15;
  *(_OWORD *)(v10 + 324) = *(_OWORD *)&v21[144];
  *(_OWORD *)(v10 + 340) = *(_OWORD *)&v21[160];
  *(_OWORD *)(v10 + 244) = *(_OWORD *)&v21[64];
  *(_OWORD *)(v10 + 260) = *(_OWORD *)&v21[80];
  *(_OWORD *)(v10 + 276) = *(_OWORD *)&v21[96];
  *(void *)uint64_t v4 = v10;
  *(_DWORD *)(v10 + 176) = v7;
  *(_DWORD *)(v10 + 356) = *(_DWORD *)&v21[176];
  *(_OWORD *)(v10 + 292) = *(_OWORD *)&v21[112];
  *(_OWORD *)(v10 + 308) = *(_OWORD *)&v21[128];
  *(_OWORD *)(v10 + 180) = *(_OWORD *)v21;
  *(_OWORD *)(v10 + 196) = *(_OWORD *)&v21[16];
  *(_OWORD *)(v10 + 212) = *(_OWORD *)&v21[32];
  *(_OWORD *)(v10 + 228) = *(_OWORD *)&v21[48];
  *(_DWORD *)(v10 + 360) = v8;
  *(_OWORD *)(v10 + 492) = *(_OWORD *)&v20[128];
  *(_OWORD *)(v10 + 508) = *(_OWORD *)&v20[144];
  *(_OWORD *)(v10 + 524) = *(_OWORD *)&v20[160];
  *(_DWORD *)(v10 + 540) = *(_DWORD *)&v20[176];
  *(_OWORD *)(v10 + 428) = *(_OWORD *)&v20[64];
  *(_OWORD *)(v10 + 444) = *(_OWORD *)&v20[80];
  *(_OWORD *)(v10 + 460) = *(_OWORD *)&v20[96];
  *(_OWORD *)(v10 + 476) = *(_OWORD *)&v20[112];
  *(_OWORD *)(v10 + 364) = *(_OWORD *)v20;
  *(_OWORD *)(v10 + 380) = *(_OWORD *)&v20[16];
  *(_OWORD *)(v10 + 396) = *(_OWORD *)&v20[32];
  *(_OWORD *)(v10 + 412) = *(_OWORD *)&v20[48];
  *(_DWORD *)(v10 + 544) = v9;
  int v16 = dword_1D2137EE4[*(char *)(v4 + 11)];
  type metadata accessor for BNNSLayerParametersArithmetic(0);
  a4[3] = v17;
  a4[4] = (uint64_t)&protocol witness table for BNNSLayerParametersArithmetic;
  uint64_t v18 = swift_allocObject();
  *a4 = v18;
  *(_DWORD *)(v18 + 16) = v16;
  *(void *)(v18 + 24) = v10;
  *(_DWORD *)(v18 + 32) = 0;
  *(int32x2_t *)(v18 + 36) = vdup_n_s32(0x7FC00000u);
  *(_DWORD *)(v18 + 44) = 1;
  double result = 0.0;
  *(_OWORD *)(v18 + 48) = 0u;
  *(_OWORD *)(v18 + 64) = 0u;
  return result;
}

void BNNS.FusedBinaryArithmeticParameters.inputADescriptorType.getter(unsigned char *a1@<X8>)
{
  *a1 = *(unsigned char *)(v1 + 8);
}

unsigned char *BNNS.FusedBinaryArithmeticParameters.inputADescriptorType.setter(unsigned char *result)
{
  *(unsigned char *)(v1 + 8) = *result;
  return result;
}

uint64_t (*BNNS.FusedBinaryArithmeticParameters.inputADescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedBinaryArithmeticParameters.inputBDescriptorType.getter(unsigned char *a1@<X8>)
{
  *a1 = *(unsigned char *)(v1 + 9);
}

unsigned char *BNNS.FusedBinaryArithmeticParameters.inputBDescriptorType.setter(unsigned char *result)
{
  *(unsigned char *)(v1 + 9) = *result;
  return result;
}

uint64_t (*BNNS.FusedBinaryArithmeticParameters.inputBDescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedBinaryArithmeticParameters.outputDescriptorType.getter(unsigned char *a1@<X8>)
{
  *a1 = *(unsigned char *)(v1 + 10);
}

unsigned char *BNNS.FusedBinaryArithmeticParameters.outputDescriptorType.setter(unsigned char *result)
{
  *(unsigned char *)(v1 + 10) = *result;
  return result;
}

uint64_t (*BNNS.FusedBinaryArithmeticParameters.outputDescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedBinaryArithmeticParameters.function.getter(unsigned char *a1@<X8>)
{
  *a1 = *(unsigned char *)(v1 + 11);
}

unsigned char *BNNS.FusedBinaryArithmeticParameters.function.setter(unsigned char *result)
{
  *(unsigned char *)(v1 + 11) = *result;
  return result;
}

uint64_t (*BNNS.FusedBinaryArithmeticParameters.function.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

char *BNNS.FusedBinaryArithmeticParameters.init(inputADescriptorType:inputBDescriptorType:outputDescriptorType:function:)@<X0>(char *result@<X0>, char *a2@<X1>, char *a3@<X2>, char *a4@<X3>, uint64_t a5@<X8>)
{
  char v5 = *result;
  char v6 = *a2;
  char v7 = *a3;
  char v8 = *a4;
  *(void *)a5 = 0;
  *(unsigned char *)(a5 + 8) = v5;
  *(unsigned char *)(a5 + 9) = v6;
  *(unsigned char *)(a5 + 10) = v7;
  *(unsigned char *)(a5 + 11) = v8;
  return result;
}

uint64_t BNNS.FusedParametersLayer.__allocating_init(inputA:inputB:output:fusedLayerParameters:filterParameters:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint32_t a5, size_t a6, int (__cdecl *a7)(void **, size_t, size_t), void (__cdecl *a8)(void *))
{
  uint64_t v49 = *MEMORY[0x1E4F143B8];
  if (*(void *)(a4 + 16) != 2)
  {
    __break(1u);
    goto LABEL_20;
  }
  outlined init with copy of BNNSOptimizer(a4 + 32, (uint64_t)&v33);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParameters);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableBinaryInputLayerParametersWrapper);
  if ((swift_dynamicCast() & 1) == 0)
  {
    long long v29 = 0u;
    memset(v30, 0, sizeof(v30));
    swift_bridgeObjectRelease();
    outlined destroy of FusableTernaryInputLayerParametersWrapper?((uint64_t)&v29, &demangling cache variable for type metadata for FusableBinaryInputLayerParametersWrapper?);
    return 0;
  }
  outlined init with take of FusableLayerParametersWrapper(&v29, (uint64_t)v46);
  if (*(void *)(a4 + 16) < 2uLL) {
LABEL_20:
  }
    __break(1u);
  outlined init with copy of BNNSOptimizer(a4 + 72, (uint64_t)&v33);
  swift_bridgeObjectRelease();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParametersWrapper);
  if ((swift_dynamicCast() & 1) == 0)
  {
    long long v29 = 0u;
    memset(v30, 0, sizeof(v30));
    outlined destroy of FusableTernaryInputLayerParametersWrapper?((uint64_t)&v29, &demangling cache variable for type metadata for FusableLayerParametersWrapper?);
LABEL_16:
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v46);
    return 0;
  }
  outlined init with take of FusableLayerParametersWrapper(&v29, (uint64_t)v43);
  uint64_t v17 = v47;
  uint64_t v16 = v48;
  __swift_mutable_project_boxed_opaque_existential_1((uint64_t)v46, v47);
  (*(void (**)(void *__return_ptr, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))(v16 + 8))(v42, a1, a2, a3, v17, v16);
  uint64_t v18 = v44;
  uint64_t v19 = v45;
  __swift_mutable_project_boxed_opaque_existential_1((uint64_t)v43, v44);
  (*(void (**)(void *__return_ptr, uint64_t, uint64_t, uint64_t, uint64_t))(v19 + 8))(v41, a3, a3, v18, v19);
  uint64_t v20 = v44;
  uint64_t v21 = v45;
  __swift_project_boxed_opaque_existential_1(v43, v44);
  int v22 = (*(uint64_t (**)(uint64_t, uint64_t))(v21 + 16))(v20, v21);
  if ((v22 - 2) > 3)
  {
LABEL_15:
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v41);
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v42);
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v43);
    goto LABEL_16;
  }
  int v23 = v22;
  outlined init with copy of BNNSOptimizer((uint64_t)v42, (uint64_t)v40);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
  type metadata accessor for BNNSLayerParametersArithmetic(0);
  swift_dynamicCast();
  LODWORD(v29) = v33;
  *((void *)&v29 + 1) = v34;
  v30[0] = v35;
  *(_OWORD *)&v30[1] = v36;
  v30[5] = v37;
  long long v31 = v38;
  uint64_t v32 = v39;
  uint64_t v24 = specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)((uint64_t)&v29, (uint64_t)v41, a5, a6, a7, a8, 8, v23);
  type metadata accessor for BNNS.FusedParametersLayer();
  uint64_t v25 = swift_allocObject();
  uint64_t v26 = v25;
  *(void *)(v25 + 24) = MEMORY[0x1E4FBC860];
  if (!v24)
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    goto LABEL_15;
  }
  *(void *)(v25 + 16) = v24;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<FusableLayerParametersWrapperDeallocatable?>);
  uint64_t v27 = swift_allocObject();
  *(_OWORD *)(v27 + 16) = xmmword_1D2135290;
  outlined init with copy of BNNSOptimizer((uint64_t)v46, (uint64_t)&v33);
  swift_retain();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParametersWrapperDeallocatable);
  if ((swift_dynamicCast() & 1) == 0)
  {
    *(void *)(v27 + 64) = 0;
    *(_OWORD *)(v27 + 32) = 0u;
    *(_OWORD *)(v27 + 48) = 0u;
  }
  outlined init with copy of BNNSOptimizer((uint64_t)v43, (uint64_t)&v29);
  if ((swift_dynamicCast() & 1) == 0)
  {
    *(void *)(v27 + 104) = 0;
    *(_OWORD *)(v27 + 72) = 0u;
    *(_OWORD *)(v27 + 88) = 0u;
  }
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v41);
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v42);
  *(void *)(v26 + 24) = v27;
  swift_release();
  swift_bridgeObjectRelease();
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v43);
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v46);
  return v26;
}

uint64_t BNNS.FusedParametersLayer.apply(batchSize:inputA:inputB:output:for:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, char a5)
{
  outlined init with take of UnsafeMutableRawPointer?(a2 + 136, (uint64_t)v57);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v57, (uint64_t)&v58);
  uint64_t v10 = v58;
  if (v58
    && (outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v56),
        outlined init with take of UnsafeMutableRawPointer?((uint64_t)v56, (uint64_t)&v59),
        (uint64_t v11 = v59) != 0)
    && (outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v55),
        outlined init with take of UnsafeMutableRawPointer?((uint64_t)v55, (uint64_t)&v60),
        v60))
  {
    char v36 = a5;
    out = v60;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
    uint64_t v12 = swift_allocObject();
    *(_OWORD *)(v12 + 16) = xmmword_1D2135290;
    *(void *)(v12 + 32) = v10;
    int v35 = (const void **)(v12 + 32);
    *(void *)(v12 + 40) = v11;
    uint64_t v34 = *(void **)(v5 + 16);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
    uint64_t v38 = swift_allocObject();
    *(_OWORD *)(v38 + 16) = xmmword_1D2135290;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v51);
    outlined init with take of BNNS.Shape((uint64_t)v51, (uint64_t)v52);
    outlined init with take of BNNS.Shape((uint64_t)v52, (uint64_t)v50);
    BNNS.Shape.size.getter((uint64_t)&v45);
    long long v13 = v45;
    long long v14 = v46;
    long long v15 = v47;
    unint64_t v33 = v48;
    unint64_t v30 = v49;
    outlined init with take of BNNS.Shape((uint64_t)v52, (uint64_t)v50);
    BNNS.Shape.stride.getter((uint64_t)&v45);
    *(void *)(v38 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v13, *((unint64_t *)&v13 + 1), v14, *((unint64_t *)&v14 + 1), v15, *((unint64_t *)&v15 + 1), v33, v30, v45, *((unint64_t *)&v45 + 1), v46, *((unint64_t *)&v46 + 1), v47, *((unint64_t *)&v47 + 1), v48, v49);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v50);
    outlined init with take of BNNS.Shape((uint64_t)v50, (uint64_t)v53);
    outlined init with take of BNNS.Shape((uint64_t)v53, (uint64_t)&v45);
    BNNS.Shape.size.getter((uint64_t)&v39);
    unint64_t v16 = v40;
    long long v17 = v41;
    long long v18 = v42;
    unint64_t v19 = v43;
    unint64_t v29 = v44;
    unint64_t v31 = v39;
    outlined init with take of BNNS.Shape((uint64_t)v53, (uint64_t)&v45);
    BNNS.Shape.stride.getter((uint64_t)&v39);
    *(void *)(v38 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v31, v16, v17, *((unint64_t *)&v17 + 1), v18, *((unint64_t *)&v18 + 1), v19, v29, v39, v40, v41, *((unint64_t *)&v41 + 1), v42, *((unint64_t *)&v42 + 1), v43, v44);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v51);
    outlined init with take of BNNS.Shape((uint64_t)v51, (uint64_t)v54);
    outlined init with take of BNNS.Shape((uint64_t)v54, (uint64_t)v50);
    BNNS.Shape.size.getter((uint64_t)&v45);
    long long v20 = v45;
    long long v21 = v46;
    long long v22 = v47;
    unint64_t v23 = v48;
    unint64_t v32 = v49;
    outlined init with take of BNNS.Shape((uint64_t)v54, (uint64_t)v50);
    BNNS.Shape.stride.getter((uint64_t)&v45);
    size_t v24 = specialized static BNNS.calculateBatchStride(size:stride:)(v20, *((unint64_t *)&v20 + 1), v21, *((unint64_t *)&v21 + 1), v22, *((unint64_t *)&v22 + 1), v23, v32, v45, *((unint64_t *)&v45 + 1), v46, *((unint64_t *)&v46 + 1), v47, *((unint64_t *)&v47 + 1), v48, v49);
    int v25 = BNNSFusedFilterApplyMultiInputBatch(v34, a1, 2uLL, v35, (const size_t *)(v38 + 32), out, v24, (v36 & 1) == 0);
    swift_bridgeObjectRelease();
    uint64_t result = swift_bridgeObjectRelease();
    if (!v25) {
      return result;
    }
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    unsigned char *v27 = 0;
  }
  else
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    unsigned char *v28 = 2;
  }
  return swift_willThrow();
}

uint64_t BNNS.FusedParametersLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingParameterGradients:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _OWORD *a5, long long *a6, _OWORD *a7, uint64_t a8)
{
  uint64_t v9 = v8;
  size_t v13 = a1;
  uint64_t v79 = *MEMORY[0x1E4F143B8];
  int64_t v14 = *(void *)(a8 + 16);
  uint64_t v15 = MEMORY[0x1E4FBC860];
  if (v14)
  {
    uint64_t v58 = a4;
    uint64_t v59 = a7;
    uint64_t v60 = a2;
    uint64_t v61 = a3;
    uint64_t v63 = v9;
    *(void *)&v67[0] = MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v14, 0);
    unint64_t v16 = (long long *)(a8 + 32);
    uint64_t v15 = *(void *)&v67[0];
    do
    {
      long long v17 = v16[9];
      long long v76 = v16[8];
      long long v77 = v17;
      long long v78 = v16[10];
      long long v18 = v16[5];
      long long v72 = v16[4];
      long long v73 = v18;
      long long v19 = v16[7];
      long long v74 = v16[6];
      long long v75 = v19;
      long long v20 = v16[1];
      long long v68 = *v16;
      long long v69 = v20;
      long long v21 = v16[3];
      long long v70 = v16[2];
      long long v71 = v21;
      long long v22 = (_OWORD *)swift_slowAlloc();
      long long v23 = v77;
      v22[8] = v76;
      v22[9] = v23;
      v22[10] = v78;
      long long v24 = v73;
      v22[4] = v72;
      void v22[5] = v24;
      long long v25 = v75;
      v22[6] = v74;
      v22[7] = v25;
      long long v26 = v69;
      *long long v22 = v68;
      v22[1] = v26;
      long long v27 = v71;
      _OWORD v22[2] = v70;
      _OWORD v22[3] = v27;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(void *)(v15 + 16) + 1, 1);
        uint64_t v15 = *(void *)&v67[0];
      }
      unint64_t v29 = *(void *)(v15 + 16);
      unint64_t v28 = *(void *)(v15 + 24);
      if (v29 >= v28 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v28 > 1), v29 + 1, 1);
        uint64_t v15 = *(void *)&v67[0];
      }
      *(void *)(v15 + 16) = v29 + 1;
      *(void *)(v15 + 8 * v29 + 32) = v22;
      v16 += 11;
      --v14;
    }
    while (v14);
    uint64_t v9 = v63;
    a3 = v61;
    size_t v13 = a1;
    a7 = v59;
    a2 = v60;
    a4 = v58;
  }
  long long v30 = a6[9];
  long long v76 = a6[8];
  long long v77 = v30;
  long long v78 = a6[10];
  long long v31 = a6[5];
  long long v72 = a6[4];
  long long v73 = v31;
  long long v32 = a6[7];
  long long v74 = a6[6];
  long long v75 = v32;
  long long v33 = a6[1];
  long long v68 = *a6;
  long long v69 = v33;
  long long v34 = a6[3];
  long long v70 = a6[2];
  long long v71 = v34;
  long long v35 = a7[8];
  long long v36 = a7[9];
  long long v37 = a7[6];
  v67[7] = a7[7];
  v67[8] = v35;
  long long v38 = a7[10];
  v67[9] = v36;
  v67[10] = v38;
  long long v39 = a7[4];
  long long v40 = a7[5];
  long long v41 = a7[2];
  v67[3] = a7[3];
  v67[4] = v39;
  v65 = (char *)v15;
  int v64 = 0;
  v67[5] = v40;
  v67[6] = v37;
  long long v42 = *a7;
  v67[1] = a7[1];
  v67[2] = v41;
  long long v43 = a5[9];
  *(_OWORD *)&v66.stride[7] = a5[8];
  *(_OWORD *)&v66.data_type = v43;
  *(_OWORD *)&v66.table_data_type = a5[10];
  v67[0] = v42;
  long long v44 = a5[5];
  *(_OWORD *)&v66.size[7] = a5[4];
  *(_OWORD *)&v66.stride[1] = v44;
  long long v45 = a5[7];
  *(_OWORD *)&v66.stride[3] = a5[6];
  *(_OWORD *)&v66.stride[5] = v45;
  long long v46 = a5[1];
  *(_OWORD *)&v66.flags = *a5;
  *(_OWORD *)&v66.size[1] = v46;
  long long v47 = a5[3];
  *(_OWORD *)&v66.size[3] = a5[2];
  *(_OWORD *)&v66.size[5] = v47;
  closure #1 in closure #1 in closure #2 in BNNS.FusedParametersLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingParameterGradients:)(&v66, a2, a3, (uint64_t)&v68, (uint64_t)v67, &v64, v9, v13, (uint64_t)a6, (uint64_t)a7, a4, (uint64_t)a5, &v65);
  if (v64)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *unint64_t v48 = 0;
    swift_willThrow();
    unint64_t v49 = v65;
    uint64_t v50 = *((void *)v65 + 2);
    if (v50)
    {
      swift_bridgeObjectRetain();
      for (uint64_t i = 0; i != v50; ++i)
      {
        uint64_t v52 = *(void *)&v49[8 * i + 32];
        if (v52) {
          MEMORY[0x1D26009C0](v52, -1, -1);
        }
      }
LABEL_20:
      swift_bridgeObjectRelease();
    }
  }
  else
  {
    uint64_t v53 = v65;
    uint64_t v54 = *((void *)v65 + 2);
    if (v54)
    {
      swift_bridgeObjectRetain();
      for (uint64_t j = 0; j != v54; ++j)
      {
        uint64_t v56 = *(void *)&v53[8 * j + 32];
        if (v56) {
          MEMORY[0x1D26009C0](v56, -1, -1);
        }
      }
      goto LABEL_20;
    }
  }
  return swift_bridgeObjectRelease();
}

uint64_t closure #1 in closure #1 in closure #2 in BNNS.FusedParametersLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingParameterGradients:)(BNNSNDArrayDescriptor *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, int *a6, uint64_t a7, size_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, char **a13)
{
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer?>);
  uint64_t v18 = swift_allocObject();
  *(_OWORD *)(v18 + 16) = xmmword_1D2135290;
  uint64_t v19 = v18;
  outlined init with take of UnsafeMutableRawPointer?(a2 + 136, (uint64_t)v114);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v114, (uint64_t)&v115);
  *(void *)(v19 + 32) = v115;
  in = (void **)(v19 + 32);
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v113);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v113, (uint64_t)&v116);
  *(void *)(v19 + 40) = v116;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>>);
  uint64_t v73 = swift_allocObject();
  *(_OWORD *)(v73 + 16) = xmmword_1D2135290;
  *(void *)(v73 + 32) = a4;
  *(void *)(v73 + 40) = a5;
  long long v74 = *(void **)(a7 + 16);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  uint64_t v118 = swift_allocObject();
  *(_OWORD *)(v118 + 16) = xmmword_1D2135290;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v111);
  outlined init with take of BNNS.Shape((uint64_t)v111, (uint64_t)v104);
  outlined init with take of BNNS.Shape((uint64_t)v104, (uint64_t)v110);
  BNNS.Shape.size.getter((uint64_t)&v105);
  long long v20 = v105;
  long long v21 = v106;
  long long v22 = v107;
  unint64_t v23 = v108;
  unint64_t v24 = v109;
  outlined init with take of BNNS.Shape((uint64_t)v104, (uint64_t)v110);
  BNNS.Shape.stride.getter((uint64_t)&v105);
  unint64_t v25 = specialized static BNNS.calculateBatchStride(size:stride:)(v20, *((unint64_t *)&v20 + 1), v21, *((unint64_t *)&v21 + 1), v22, *((unint64_t *)&v22 + 1), v23, v24, v105, *((unint64_t *)&v105 + 1), v106, *((unint64_t *)&v106 + 1), v107, *((unint64_t *)&v107 + 1), v108, v109);
  uint64_t v26 = v118;
  *(void *)(v118 + 32) = v25;
  long long v72 = (const size_t *)(v26 + 32);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v110);
  outlined init with take of BNNS.Shape((uint64_t)v110, (uint64_t)&v105);
  outlined init with take of BNNS.Shape((uint64_t)&v105, (uint64_t)v103);
  BNNS.Shape.size.getter((uint64_t)&v95);
  unint64_t v27 = v95;
  unint64_t v28 = v96;
  unint64_t v29 = v97;
  unint64_t v30 = v98;
  unint64_t v31 = v99;
  unint64_t v32 = v100;
  unint64_t v33 = v101;
  unint64_t v34 = v102;
  outlined init with take of BNNS.Shape((uint64_t)&v105, (uint64_t)v103);
  BNNS.Shape.stride.getter((uint64_t)&v95);
  unint64_t v35 = specialized static BNNS.calculateBatchStride(size:stride:)(v27, v28, v29, v30, v31, v32, v33, v34, v95, v96, v97, v98, v99, v100, v101, v102);
  *(void *)(v118 + 40) = v35;
  uint64_t v79 = swift_allocObject();
  *(_OWORD *)(v79 + 16) = xmmword_1D2135290;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v103);
  outlined init with take of BNNS.Shape((uint64_t)v103, (uint64_t)v110);
  outlined init with take of BNNS.Shape((uint64_t)v110, (uint64_t)v111);
  BNNS.Shape.size.getter((uint64_t)&v95);
  unint64_t v36 = v95;
  unint64_t v37 = v96;
  unint64_t v38 = v97;
  unint64_t v39 = v98;
  unint64_t v40 = v99;
  unint64_t v41 = v100;
  unint64_t v42 = v101;
  unint64_t v43 = v102;
  outlined init with take of BNNS.Shape((uint64_t)v110, (uint64_t)v111);
  BNNS.Shape.stride.getter((uint64_t)&v95);
  *(void *)(v79 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v36, v37, v38, v39, v40, v41, v42, v43, v95, v96, v97, v98, v99, v100, v101, v102);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v95);
  outlined init with take of BNNS.Shape((uint64_t)&v95, (uint64_t)v111);
  outlined init with take of BNNS.Shape((uint64_t)v111, (uint64_t)v94);
  BNNS.Shape.size.getter((uint64_t)&v86);
  unint64_t v44 = v86;
  unint64_t v45 = v87;
  unint64_t v46 = v88;
  unint64_t v47 = v89;
  unint64_t v48 = v90;
  unint64_t v49 = v91;
  unint64_t v51 = v92;
  unint64_t v50 = v93;
  outlined init with take of BNNS.Shape((uint64_t)v111, (uint64_t)v94);
  BNNS.Shape.stride.getter((uint64_t)&v86);
  *(void *)(v79 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v44, v45, v46, v47, v48, v49, v51, v50, v86, v87, v88, v89, v90, v91, v92, v93);
  outlined init with take of UnsafeMutableRawPointer?(a11 + 136, (uint64_t)v112);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v112, (uint64_t)&v117);
  out = v117;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v94);
  outlined init with take of BNNS.Shape((uint64_t)v94, (uint64_t)&v95);
  outlined init with take of BNNS.Shape((uint64_t)&v95, (uint64_t)v103);
  BNNS.Shape.size.getter((uint64_t)&v86);
  unint64_t v52 = v86;
  unint64_t v53 = v87;
  unint64_t v54 = v88;
  unint64_t v55 = v89;
  unint64_t v56 = v90;
  unint64_t v57 = v91;
  unint64_t v59 = v92;
  unint64_t v58 = v93;
  outlined init with take of BNNS.Shape((uint64_t)&v95, (uint64_t)v103);
  BNNS.Shape.stride.getter((uint64_t)&v86);
  size_t out_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v52, v53, v54, v55, v56, v57, v59, v58, v86, v87, v88, v89, v90, v91, v92, v93);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v86);
  outlined init with take of BNNS.Shape((uint64_t)&v86, (uint64_t)v103);
  outlined init with take of BNNS.Shape((uint64_t)v103, (uint64_t)v85);
  BNNS.Shape.size.getter((uint64_t)&v80);
  long long v60 = v80;
  long long v61 = v81;
  long long v62 = v82;
  unint64_t v63 = v83;
  unint64_t v64 = v84;
  outlined init with take of BNNS.Shape((uint64_t)v103, (uint64_t)v85);
  BNNS.Shape.stride.getter((uint64_t)&v80);
  size_t out_delta_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v60, *((unint64_t *)&v60 + 1), v61, *((unint64_t *)&v61 + 1), v62, *((unint64_t *)&v62 + 1), v63, v64, v80, *((unint64_t *)&v80 + 1), v81, *((unint64_t *)&v81 + 1), v82, *((unint64_t *)&v82 + 1), v83, v84);
  BNNSNDArrayDescriptor v66 = *a13;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  char isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
  *a13 = v66;
  if ((isUniquelyReferenced_nonNull_native & 1) == 0) {
    BNNSNDArrayDescriptor v66 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v66 + 2), 0, v66);
  }
  *a13 = v66;
  swift_bridgeObjectRetain();
  int v68 = BNNSFusedFilterApplyBackwardMultiInputBatch(v74, a8, 2uLL, (const void **)in, v72, (BNNSNDArrayDescriptor **)(v73 + 32), (const size_t *)(v79 + 32), out, out_stride, a1, out_delta_stride, (BNNSNDArrayDescriptor **)v66 + 4);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease_n();
  uint64_t result = swift_bridgeObjectRelease_n();
  *a6 = v68;
  return result;
}

uint64_t dispatch thunk of FusableBinaryInputLayerParametersWrapper.layerParameters(inputA:inputB:output:)(uint64_t *a1, uint64_t *a2, uint64_t *a3, uint64_t a4, uint64_t a5)
{
  uint64_t v5 = a1[17];
  int v6 = *((_DWORD *)a1 + 36);
  uint64_t v7 = a1[19];
  int v8 = *((_DWORD *)a1 + 40);
  uint64_t v9 = a2[17];
  int v10 = *((_DWORD *)a2 + 36);
  uint64_t v11 = a2[19];
  int v12 = *((_DWORD *)a2 + 40);
  uint64_t v13 = a3[17];
  int v14 = *((_DWORD *)a3 + 36);
  uint64_t v15 = a3[19];
  int v16 = *((_DWORD *)a3 + 40);
  long long v17 = *(uint64_t (**)(uint64_t *, uint64_t *, uint64_t *))(a5 + 8);
  uint64_t v53 = *a1;
  long long v54 = *(_OWORD *)(a1 + 1);
  long long v55 = *(_OWORD *)(a1 + 3);
  long long v56 = *(_OWORD *)(a1 + 5);
  long long v57 = *(_OWORD *)(a1 + 7);
  long long v58 = *(_OWORD *)(a1 + 9);
  long long v59 = *(_OWORD *)(a1 + 11);
  long long v60 = *(_OWORD *)(a1 + 13);
  long long v61 = *(_OWORD *)(a1 + 15);
  uint64_t v62 = v5;
  int v63 = v6;
  uint64_t v64 = v7;
  int v65 = v8;
  uint64_t v66 = *(uint64_t *)((char *)a1 + 164);
  uint64_t v39 = *a2;
  long long v40 = *(_OWORD *)(a2 + 1);
  long long v41 = *(_OWORD *)(a2 + 3);
  long long v42 = *(_OWORD *)(a2 + 5);
  long long v43 = *(_OWORD *)(a2 + 7);
  long long v44 = *(_OWORD *)(a2 + 9);
  long long v45 = *(_OWORD *)(a2 + 11);
  long long v46 = *(_OWORD *)(a2 + 13);
  long long v47 = *(_OWORD *)(a2 + 15);
  uint64_t v48 = v9;
  int v49 = v10;
  uint64_t v50 = v11;
  int v51 = v12;
  uint64_t v52 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v25 = *a3;
  long long v18 = *(_OWORD *)(a3 + 3);
  long long v26 = *(_OWORD *)(a3 + 1);
  long long v27 = v18;
  long long v19 = *(_OWORD *)(a3 + 7);
  long long v20 = *(_OWORD *)(a3 + 9);
  long long v21 = *(_OWORD *)(a3 + 11);
  long long v22 = *(_OWORD *)(a3 + 13);
  long long v23 = *(_OWORD *)(a3 + 15);
  long long v28 = *(_OWORD *)(a3 + 5);
  long long v29 = v19;
  long long v30 = v20;
  long long v31 = v21;
  long long v32 = v22;
  long long v33 = v23;
  uint64_t v34 = v13;
  int v35 = v14;
  uint64_t v36 = v15;
  int v37 = v16;
  uint64_t v38 = *(uint64_t *)((char *)a3 + 164);
  return v17(&v53, &v39, &v25);
}

uint64_t dispatch thunk of FusableBinaryInputLayerParametersWrapper.filterType.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

ValueMetadata *type metadata accessor for BNNS.FusedBinaryArithmeticParameters()
{
  return &type metadata for BNNS.FusedBinaryArithmeticParameters;
}

uint64_t sub_1D20F8748()
{
  return MEMORY[0x1F4186498](v0, 80, 7);
}

uint64_t *Quadrature.init(integrator:absoluteTolerance:relativeTolerance:)@<X0>(uint64_t *result@<X0>, uint64_t a2@<X8>, double a3@<D0>, double a4@<D1>)
{
  uint64_t v4 = *result;
  int v5 = *((unsigned __int8 *)result + 16);
  if (*((unsigned char *)result + 16))
  {
    BOOL v6 = v5 == 1;
    if (v5 == 1) {
      uint64_t v7 = *result;
    }
    else {
      uint64_t v7 = 0;
    }
    uint64_t v4 = 0;
    if (v6) {
      int v8 = 2;
    }
    else {
      int v8 = 0;
    }
  }
  else
  {
    uint64_t v7 = result[1];
    int v8 = 1;
  }
  *(_DWORD *)a2 = v8;
  *(double *)(a2 + 8) = a3;
  *(double *)(a2 + 16) = a4;
  *(void *)(a2 + 24) = v4;
  *(void *)(a2 + 32) = v7;
  return result;
}

double Quadrature.absoluteTolerance.getter()
{
  return *(double *)(v0 + 8);
}

void Quadrature.absoluteTolerance.setter(double a1)
{
  *(double *)(v1 + 8) = a1;
}

double (*Quadrature.absoluteTolerance.modify(void *a1))(uint64_t a1)
{
  a1[1] = v1;
  *a1 = *(void *)(v1 + 8);
  return Quadrature.absoluteTolerance.modify;
}

double Quadrature.absoluteTolerance.modify(uint64_t a1)
{
  double result = *(double *)a1;
  *(void *)(*(void *)(a1 + 8) + 8) = *(void *)a1;
  return result;
}

double Quadrature.relativeTolerance.getter()
{
  return *(double *)(v0 + 16);
}

void Quadrature.relativeTolerance.setter(double a1)
{
  *(double *)(v1 + 16) = a1;
}

double (*Quadrature.relativeTolerance.modify(void *a1))(uint64_t a1)
{
  a1[1] = v1;
  *a1 = *(void *)(v1 + 16);
  return Quadrature.relativeTolerance.modify;
}

double Quadrature.relativeTolerance.modify(uint64_t a1)
{
  double result = *(double *)a1;
  *(void *)(*(void *)(a1 + 8) + 16) = *(void *)a1;
  return result;
}

uint64_t Quadrature.integrate(over:integrand:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>, double a4@<D0>, double a5@<D1>)
{
  uint64_t v27 = *MEMORY[0x1E4F143B8];
  quadrature_integrator v11 = *(_DWORD *)v5;
  double v12 = *(double *)(v5 + 8);
  double v13 = *(double *)(v5 + 16);
  size_t v14 = *(void *)(v5 + 24);
  size_t v15 = *(void *)(v5 + 32);
  quadrature_status status = QUADRATURE_SUCCESS;
  double abs_error = 0.0;
  uint64_t v16 = swift_allocObject();
  *(void *)(v16 + 16) = a1;
  *(void *)(v16 + 24) = a2;
  uint64_t v17 = swift_allocObject();
  *(void *)(v17 + 16) = partial apply for thunk for @callee_guaranteed (@unowned UnsafeBufferPointer<Double>, @unowned UnsafeMutableBufferPointer<Double>) -> ();
  *(void *)(v17 + 24) = v16;
  v22[0] = partial apply for thunk for @escaping @callee_guaranteed (@unowned UnsafeBufferPointer<Double>, @unowned UnsafeMutableBufferPointer<Double>) -> ();
  v22[1] = v17;
  __f.fun = (quadrature_function_array)@objc closure #1 in closure #1 in closure #1 in Quadrature.integrate(over:integrand:);
  __f.fun_arg = v22;
  options.integrator = v11;
  options.abs_tolerance = v12;
  options.rel_tolerance = v13;
  options.qag_points_per_interval = v14;
  options.max_intervals = v15;
  swift_retain();
  double v18 = quadrature_integrate(&__f, a4, a5, &options, &status, &abs_error, 0, 0);
  swift_release();
  LOBYTE(a2) = swift_isEscapingClosureAtFileLocation();
  uint64_t result = swift_release();
  if (a2) {
    __break(1u);
  }
  if (status > QUADRATURE_ALLOC_ERROR)
  {
    switch(status)
    {
      case QUADRATURE_INVALID_ARG_ERROR:
        long long v21 = xmmword_1D2137F50;
        goto LABEL_14;
      case QUADRATURE_ERROR:
        *(void *)a3 = 0;
        *(void *)(a3 + 8) = 0;
        goto LABEL_15;
      case QUADRATURE_SUCCESS:
        double v20 = abs_error;
        *(double *)a3 = v18;
        *(double *)(a3 + 8) = v20;
        *(unsigned char *)(a3 + 16) = 0;
        return result;
    }
  }
  else
  {
    if (status == QUADRATURE_INTEGRATE_BAD_BEHAVIOUR_ERROR)
    {
      long long v21 = xmmword_1D2137F20;
      goto LABEL_14;
    }
    if (status == QUADRATURE_INTEGRATE_MAX_EVAL_ERROR)
    {
      long long v21 = xmmword_1D2137F30;
      goto LABEL_14;
    }
  }
  long long v21 = xmmword_1D2137F40;
LABEL_14:
  *(_OWORD *)a3 = v21;
LABEL_15:
  *(unsigned char *)(a3 + 16) = 1;
  return result;
}

{
  uint64_t v5;
  quadrature_integrator v11;
  double v12;
  double v13;
  size_t v14;
  size_t v15;
  uint64_t v16;
  uint64_t v17;
  double v18;
  uint64_t result;
  double v20;
  long long v21;
  void v22[2];
  double abs_error;
  quadrature_status status;
  quadrature_integrate_options options;
  quadrature_integrate_function __f;
  uint64_t v27;

  uint64_t v27 = *MEMORY[0x1E4F143B8];
  quadrature_integrator v11 = *(_DWORD *)v5;
  double v12 = *(double *)(v5 + 8);
  double v13 = *(double *)(v5 + 16);
  size_t v14 = *(void *)(v5 + 24);
  size_t v15 = *(void *)(v5 + 32);
  quadrature_status status = QUADRATURE_SUCCESS;
  double abs_error = 0.0;
  uint64_t v16 = swift_allocObject();
  *(void *)(v16 + 16) = a1;
  *(void *)(v16 + 24) = a2;
  uint64_t v17 = swift_allocObject();
  *(void *)(v17 + 16) = partial apply for thunk for @callee_guaranteed (@unowned Double) -> (@unowned Double);
  *(void *)(v17 + 24) = v16;
  v22[0] = partial apply for thunk for @escaping @callee_guaranteed (@unowned Double) -> (@unowned Double);
  v22[1] = v17;
  __f.fun = (quadrature_function_array)@objc closure #1 in closure #1 in closure #1 in Quadrature.integrate(over:integrand:);
  __f.fun_arg = v22;
  options.integrator = v11;
  options.abs_tolerance = v12;
  options.rel_tolerance = v13;
  options.qag_points_per_interval = v14;
  options.max_intervals = v15;
  swift_retain();
  double v18 = quadrature_integrate(&__f, a4, a5, &options, &status, &abs_error, 0, 0);
  swift_release();
  LOBYTE(a2) = swift_isEscapingClosureAtFileLocation();
  uint64_t result = swift_release();
  if (a2) {
    __break(1u);
  }
  if (status > QUADRATURE_ALLOC_ERROR)
  {
    switch(status)
    {
      case QUADRATURE_INVALID_ARG_ERROR:
        long long v21 = xmmword_1D2137F50;
        goto LABEL_14;
      case QUADRATURE_ERROR:
        *(void *)a3 = 0;
        *(void *)(a3 + 8) = 0;
        goto LABEL_15;
      case QUADRATURE_SUCCESS:
        double v20 = abs_error;
        *(double *)a3 = v18;
        *(double *)(a3 + 8) = v20;
        *(unsigned char *)(a3 + 16) = 0;
        return result;
    }
  }
  else
  {
    if (status == QUADRATURE_INTEGRATE_BAD_BEHAVIOUR_ERROR)
    {
      long long v21 = xmmword_1D2137F20;
      goto LABEL_14;
    }
    if (status == QUADRATURE_INTEGRATE_MAX_EVAL_ERROR)
    {
      long long v21 = xmmword_1D2137F30;
      goto LABEL_14;
    }
  }
  long long v21 = xmmword_1D2137F40;
LABEL_14:
  *(_OWORD *)a3 = v21;
LABEL_15:
  *(unsigned char *)(a3 + 16) = 1;
  return result;
}

void (**@objc closure #1 in closure #1 in closure #1 in Quadrature.integrate(over:integrand:)(void (**result)(void *, void *), uint64_t a2, uint64_t a3, uint64_t a4))(void *, void *)
{
  if (result)
  {
    uint64_t v4 = *result;
    v6[0] = a3;
    v6[1] = a2;
    v5[0] = a4;
    v5[1] = a2;
    swift_retain();
    v4(v6, v5);
    return (void (**)(void *, void *))swift_release();
  }
  return result;
}

Accelerate::Quadrature::Error __swiftcall Quadrature.Error.init(quadratureStatus:)(quadrature_status quadratureStatus)
{
  if (quadratureStatus <= -100)
  {
    if (quadratureStatus == QUADRATURE_INTEGRATE_BAD_BEHAVIOUR_ERROR)
    {
      *uint64_t v1 = 4;
      return (char)quadratureStatus;
    }
    if (quadratureStatus == QUADRATURE_INTEGRATE_MAX_EVAL_ERROR)
    {
      *uint64_t v1 = 3;
      return (char)quadratureStatus;
    }
LABEL_9:
    *uint64_t v1 = 2;
    return (char)quadratureStatus;
  }
  if (quadratureStatus == QUADRATURE_INTERNAL_ERROR) {
    goto LABEL_9;
  }
  if (quadratureStatus == QUADRATURE_ERROR)
  {
    *uint64_t v1 = 0;
    return (char)quadratureStatus;
  }
  if (quadratureStatus != QUADRATURE_INVALID_ARG_ERROR) {
    goto LABEL_9;
  }
  *uint64_t v1 = 1;
  return (char)quadratureStatus;
}

uint64_t sub_1D20F8B00()
{
  return MEMORY[0x1F4186498](v0, 32, 7);
}

uint64_t partial apply for thunk for @callee_guaranteed (@unowned UnsafeBufferPointer<Double>, @unowned UnsafeMutableBufferPointer<Double>) -> ()()
{
  return (*(uint64_t (**)(void))(v0 + 16))();
}

uint64_t sub_1D20F8B38()
{
  swift_release();

  return MEMORY[0x1F4186498](v0, 32, 7);
}

uint64_t partial apply for thunk for @escaping @callee_guaranteed (@unowned UnsafeBufferPointer<Double>, @unowned UnsafeMutableBufferPointer<Double>) -> ()(void *a1, void *a2)
{
  return (*(uint64_t (**)(void, void, void, void))(v2 + 16))(*a1, a1[1], *a2, a2[1]);
}

void (**closure #1 in closure #1 in closure #1 in Quadrature.integrate(over:integrand:)(void (**result)(uint64_t *__return_ptr, uint64_t *), uint64_t a2, uint64_t *a3, void *a4))(uint64_t *__return_ptr, uint64_t *)
{
  void (*v7)(uint64_t *__return_ptr, uint64_t *);
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t vars8;

  if (result)
  {
    uint64_t v4 = a2;
    if (a2 < 0)
    {
      __break(1u);
    }
    else if (a2)
    {
      uint64_t v7 = *result;
      swift_retain();
      do
      {
        int v8 = *a3++;
        int v10 = v8;
        v7(&v9, &v10);
        *a4++ = v9;
        --v4;
      }
      while (v4);
      return (void (**)(uint64_t *__return_ptr, uint64_t *))swift_release();
    }
  }
  return result;
}

void static Quadrature.Integrator.nonAdaptive.getter(uint64_t a1@<X8>)
{
  *(void *)a1 = 0;
  *(void *)(a1 + 8) = 0;
  *(unsigned char *)(a1 + 16) = 2;
}

void *static Quadrature.Integrator.adaptive(pointsPerInterval:maxIntervals:)@<X0>(void *result@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  *(void *)a3 = *result;
  *(void *)(a3 + 8) = a2;
  *(unsigned char *)(a3 + 16) = 0;
  return result;
}

uint64_t static Quadrature.Integrator.adaptiveWithSingularities(maxIntervals:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X8>)
{
  *(void *)a2 = result;
  *(void *)(a2 + 8) = 0;
  *(unsigned char *)(a2 + 16) = 1;
  return result;
}

uint64_t Quadrature.QAGPointsPerInterval.points.getter()
{
  return *(void *)v0;
}

void static Quadrature.QAGPointsPerInterval.fifteen.getter(void *a1@<X8>)
{
  *a1 = 15;
}

void static Quadrature.QAGPointsPerInterval.twentyOne.getter(void *a1@<X8>)
{
  *a1 = 21;
}

void static Quadrature.QAGPointsPerInterval.thirtyOne.getter(void *a1@<X8>)
{
  *a1 = 31;
}

void static Quadrature.QAGPointsPerInterval.fortyOne.getter(void *a1@<X8>)
{
  *a1 = 41;
}

void static Quadrature.QAGPointsPerInterval.fiftyOne.getter(void *a1@<X8>)
{
  *a1 = 51;
}

void static Quadrature.QAGPointsPerInterval.sixtyOne.getter(void *a1@<X8>)
{
  *a1 = 61;
}

unint64_t Quadrature.Error.errorDescription.getter()
{
  unint64_t result = 0x20636972656E6547;
  switch(*v0)
  {
    case 1:
      unint64_t result = 0xD000000000000011;
      break;
    case 2:
      unint64_t result = 0xD00000000000003FLL;
      break;
    case 3:
      unint64_t result = 0xD000000000000060;
      break;
    case 4:
      unint64_t result = 0xD000000000000071;
      break;
    default:
      return result;
  }
  return result;
}

BOOL static Quadrature.Error.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void Quadrature.Error.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int Quadrature.Error.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t sub_1D20F9040()
{
  return MEMORY[0x1F4186498](v0, 32, 7);
}

uint64_t partial apply for thunk for @callee_guaranteed (@unowned Double) -> (@unowned Double)()
{
  return (*(uint64_t (**)(void))(v0 + 16))();
}

void partial apply for thunk for @escaping @callee_guaranteed (@unowned Double) -> (@unowned Double)(double *a1@<X0>, double *a2@<X8>)
{
  *a2 = (*(double (**)(double))(v2 + 16))(*a1);
}

unint64_t lazy protocol witness table accessor for type Quadrature.Error and conformance Quadrature.Error()
{
  unint64_t result = lazy protocol witness table cache variable for type Quadrature.Error and conformance Quadrature.Error;
  if (!lazy protocol witness table cache variable for type Quadrature.Error and conformance Quadrature.Error)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Quadrature.Error and conformance Quadrature.Error);
  }
  return result;
}

double sub_1D20F9104@<D0>(uint64_t a1@<X0>, double *a2@<X8>)
{
  double result = *(double *)(a1 + 8);
  *a2 = result;
  return result;
}

double sub_1D20F9110(double *a1, uint64_t a2)
{
  double result = *a1;
  *(double *)(a2 + 8) = *a1;
  return result;
}

double sub_1D20F911C@<D0>(uint64_t a1@<X0>, double *a2@<X8>)
{
  double result = *(double *)(a1 + 16);
  *a2 = result;
  return result;
}

double sub_1D20F9128(double *a1, uint64_t a2)
{
  double result = *a1;
  *(double *)(a2 + 16) = *a1;
  return result;
}

ValueMetadata *type metadata accessor for Quadrature()
{
  return &type metadata for Quadrature;
}

__n128 __swift_memcpy17_8(__n128 *a1, __n128 *a2)
{
  __n128 result = *a2;
  a1[1].n128_u8[0] = a2[1].n128_u8[0];
  *a1 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for Quadrature.Integrator(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFE && *(unsigned char *)(a1 + 17)) {
    return (*(_DWORD *)a1 + 254);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 16);
  if (v3 <= 2) {
    int v4 = -1;
  }
  else {
    int v4 = v3 ^ 0xFF;
  }
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for Quadrature.Integrator(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFD)
  {
    *(unsigned char *)(result + 16) = 0;
    *(void *)__n128 result = a2 - 254;
    *(void *)(result + 8) = 0;
    if (a3 >= 0xFE) {
      *(unsigned char *)(result + 17) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFE) {
      *(unsigned char *)(result + 17) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 16) = -(char)a2;
    }
  }
  return result;
}

uint64_t getEnumTag for Quadrature.Integrator(uint64_t a1)
{
  if (*(unsigned __int8 *)(a1 + 16) <= 1u) {
    return *(unsigned __int8 *)(a1 + 16);
  }
  else {
    return (*(_DWORD *)a1 + 2);
  }
}

uint64_t destructiveInjectEnumTag for Quadrature.Integrator(uint64_t result, unsigned int a2)
{
  if (a2 >= 2)
  {
    *(void *)__n128 result = a2 - 2;
    *(void *)(result + 8) = 0;
    LOBYTE(a2) = 2;
  }
  *(unsigned char *)(result + 16) = a2;
  return result;
}

ValueMetadata *type metadata accessor for Quadrature.Integrator()
{
  return &type metadata for Quadrature.Integrator;
}

ValueMetadata *type metadata accessor for Quadrature.QAGPointsPerInterval()
{
  return &type metadata for Quadrature.QAGPointsPerInterval;
}

unsigned char *storeEnumTagSinglePayload for Quadrature.Error(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 4 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 4) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFC) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFB)
  {
    unsigned int v6 = ((a2 - 252) >> 8) + 1;
    *__n128 result = a2 + 4;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D20F9300);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *__n128 result = a2 + 4;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for Quadrature.Error()
{
  return &type metadata for Quadrature.Error;
}

uint64_t static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:)(a1, a2, v2[4], v2[5], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:)(a1, a2, v2[4], v2[5], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:));
}

uint64_t static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  return static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:));
}

{
  return static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:));
}

uint64_t static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t, void *))
{
  uint64_t v12 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  v14[2] = a3;
  _OWORD v14[3] = a4;
  v14[4] = a1;
  v14[5] = a2;
  return a6(v12, a5, v14);
}

uint64_t closure #1 in static vDSP.evaluatePolynomial<A>(usingCoefficients:withVariables:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t *a7, unint64_t *a8, void (*a9)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(a7);
  uint64_t v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a8, a7);
  a9(a3, a4, a1, a5, v16, a6, v17);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v12 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a7 + 8) + 16))(a5);
  if (result >= v12) {
    uint64_t v14 = v12;
  }
  else {
    uint64_t v14 = result;
  }
  if (v14 < 0)
  {
    __break(1u);
  }
  else if (*(void *)(a1 + 16))
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(uint64_t))(a7 + 16))(a8);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(uint64_t result, uint64_t a2, uint64_t a3, void *a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, uint64_t, uint64_t, void, uint64_t, uint64_t, uint64_t))
{
  if (result)
  {
    if (*a4) {
      return a7(a3 + 32, 1, result, 1, *a4, 1, a5, a6);
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:));
}

{
  return partial apply for closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:));
}

uint64_t partial apply for closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(v2 + 16);
  uint64_t v4 = *(void *)(v2 + 32);
  void v6[2] = *(void *)(v2 + 56);
  v6[3] = a1;
  long long v7 = *(_OWORD *)(v2 + 64);
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v4 + 24))(a2, v6, MEMORY[0x1E4FBC848] + 8, v3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(a1, a2, *(void *)(v2 + 16), *(void **)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16D80]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.evaluatePolynomial<A, B>(usingCoefficients:withVariables:result:)(a1, a2, *(void *)(v2 + 16), *(void **)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16D78]);
}

double BNNS.FusedConvolutionParameters.layerParameters(input:output:)@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, uint64_t *a3@<X8>)
{
  long long v7 = *(_OWORD *)(v3 + 120);
  long long v8 = *(_OWORD *)(v3 + 152);
  long long v74 = *(_OWORD *)(v3 + 136);
  long long v75 = v8;
  long long v76 = *(_OWORD *)(v3 + 168);
  long long v9 = *(_OWORD *)(v3 + 72);
  long long v10 = *(_OWORD *)(v3 + 88);
  long long v69 = *(_OWORD *)(v3 + 56);
  long long v70 = v9;
  long long v71 = v10;
  long long v11 = *(_OWORD *)(v3 + 104);
  long long v73 = v7;
  long long v72 = v11;
  long long v12 = *(_OWORD *)(v3 + 24);
  long long v66 = *(_OWORD *)(v3 + 8);
  long long v67 = v12;
  long long v68 = *(_OWORD *)(v3 + 40);
  outlined init with take of BNNSNDArrayDescriptor?(v3 + 184, (uint64_t)v77);
  uint64_t v13 = *(void *)(v3 + 408);
  uint64_t v14 = *(void *)(v3 + 416);
  if (*(unsigned char *)(v3 + 440) == 1)
  {
    uint64_t v59 = *(void *)(v3 + 424);
    uint64_t v60 = *(void *)(v3 + 432);
    uint64_t v57 = *(void *)(v3 + 408);
    uint64_t v58 = *(void *)(v3 + 416);
    uint64_t v14 = 0;
    uint64_t v13 = 0;
  }
  else
  {
    uint64_t v59 = 0;
    uint64_t v60 = 0;
    uint64_t v57 = 0;
    uint64_t v58 = 0;
  }
  uint64_t v54 = *(void *)(v3 + 376);
  uint64_t v55 = *(void *)(v3 + 368);
  uint64_t v52 = *(void *)(v3 + 392);
  uint64_t v53 = *(void *)(v3 + 384);
  uint64_t v56 = *(void *)(v3 + 400);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v77, (uint64_t)v62);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v62) == 1)
  {
    uint64_t v50 = 0;
    uint64_t v51 = 0;
    uint64_t v48 = 0;
    uint64_t v49 = 0;
    uint64_t v46 = 0;
    uint64_t v47 = 0;
    uint64_t v44 = 0;
    uint64_t v45 = 0;
    uint64_t v42 = 0;
    uint64_t v43 = 0;
    uint64_t v40 = 0;
    uint64_t v41 = 0;
    uint64_t v38 = 0;
    uint64_t v39 = 0;
    uint64_t v37 = 0;
    uint64_t v15 = 0;
    uint64_t v16 = 0;
    uint64_t v17 = 0;
    int v18 = 0;
    uint64_t v19 = 0;
    uint64_t v35 = 0;
    uint64_t v36 = 0;
  }
  else
  {
    uint64_t v36 = v62[0];
    uint64_t v50 = v62[1];
    uint64_t v51 = v62[2];
    uint64_t v48 = v62[4];
    uint64_t v49 = v62[3];
    uint64_t v46 = v62[6];
    uint64_t v47 = v62[5];
    uint64_t v44 = v62[8];
    uint64_t v45 = v62[7];
    uint64_t v42 = v62[10];
    uint64_t v43 = v62[9];
    uint64_t v40 = v62[12];
    uint64_t v41 = v62[11];
    uint64_t v38 = v62[14];
    uint64_t v39 = v62[13];
    uint64_t v37 = v62[15];
    uint64_t v15 = v62[16];
    uint64_t v16 = v62[17];
    uint64_t v35 = v62[18];
    uint64_t v17 = v62[19];
    int v18 = v63;
    uint64_t v19 = v64;
    int v34 = v65;
  }
  long long v20 = a1[9];
  __src[8] = a1[8];
  __src[9] = v20;
  long long v21 = a1[5];
  __src[4] = a1[4];
  __src[5] = v21;
  long long v22 = a1[7];
  __src[6] = a1[6];
  __src[7] = v22;
  long long v23 = a1[1];
  __src[0] = *a1;
  __src[1] = v23;
  long long v24 = a1[3];
  __src[2] = a1[2];
  __src[3] = v24;
  __src[18] = v73;
  __src[19] = v74;
  __src[20] = v75;
  __src[21] = v76;
  __src[14] = v69;
  __src[15] = v70;
  long long v25 = a1[10];
  __src[16] = v71;
  __src[17] = v72;
  __src[10] = v25;
  __src[11] = v66;
  __src[12] = v67;
  __src[13] = v68;
  long long v26 = a2[9];
  __src[30] = a2[8];
  __src[31] = v26;
  __src[32] = a2[10];
  long long v27 = a2[5];
  __src[26] = a2[4];
  __src[27] = v27;
  long long v28 = a2[7];
  __src[28] = a2[6];
  __src[29] = v28;
  long long v29 = a2[1];
  __src[22] = *a2;
  __src[23] = v29;
  long long v30 = a2[3];
  __src[24] = a2[2];
  __src[25] = v30;
  type metadata accessor for BNNSLayerParametersConvolution(0);
  a3[3] = v31;
  a3[4] = (uint64_t)&protocol witness table for BNNSLayerParametersConvolution;
  uint64_t v32 = swift_allocObject();
  *a3 = v32;
  memcpy((void *)(v32 + 16), __src, 0x210uLL);
  *(void *)(v32 + 552) = v50;
  *(void *)(v32 + 544) = v36;
  *(void *)(v32 + 560) = v51;
  *(void *)(v32 + 568) = v49;
  *(void *)(v32 + 576) = v48;
  *(void *)(v32 + 584) = v47;
  *(void *)(v32 + 592) = v46;
  *(void *)(v32 + 600) = v45;
  *(void *)(v32 + 608) = v44;
  *(void *)(v32 + 616) = v43;
  *(void *)(v32 + 624) = v42;
  *(void *)(v32 + 632) = v41;
  *(void *)(v32 + 640) = v40;
  *(void *)(v32 + 648) = v39;
  *(void *)(v32 + 656) = v38;
  *(void *)(v32 + 664) = v37;
  *(void *)(v32 + 672) = v15;
  *(void *)(v32 + 680) = v16;
  *(void *)(v32 + 688) = v35;
  *(void *)(v32 + 696) = v17;
  *(_DWORD *)(v32 + 704) = v18;
  *(void *)(v32 + 708) = v19;
  *(_DWORD *)(v32 + 716) = v34;
  *(void *)(v32 + 720) = 0x7FC0000000000000;
  *(void *)(v32 + 728) = 0x17FC00000;
  double result = 0.0;
  *(_OWORD *)(v32 + 736) = 0u;
  *(_OWORD *)(v32 + 752) = 0u;
  *(void *)(v32 + 768) = v55;
  *(void *)(v32 + 776) = v54;
  *(void *)(v32 + 784) = v53;
  *(void *)(v32 + 792) = v52;
  *(void *)(v32 + 800) = v13;
  *(void *)(v32 + 808) = v14;
  *(void *)(v32 + 816) = v56;
  *(void *)(v32 + 824) = v57;
  *(void *)(v32 + 832) = v58;
  *(void *)(v32 + 840) = v59;
  *(void *)(v32 + 848) = v60;
  return result;
}

void *BNNS.FusedConvolutionParameters.init(type:weights:bias:stride:dilationStride:groupSize:padding:)@<X0>(char *a1@<X0>, _OWORD *a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, void *a9@<X8>, long long *a10)
{
  outlined init with take of BNNSNDArrayDescriptor?(a3, (uint64_t)v29);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v29, (uint64_t)v30);
  long long v18 = a2[6];
  *(_OWORD *)&__src[15] = a2[7];
  long long v19 = a2[9];
  *(_OWORD *)&__src[17] = a2[8];
  *(_OWORD *)&__src[19] = v19;
  *(_OWORD *)&__src[21] = a2[10];
  long long v20 = a2[2];
  *(_OWORD *)&__src[7] = a2[3];
  long long v21 = a2[5];
  *(_OWORD *)&__src[9] = a2[4];
  *(_OWORD *)&__src[11] = v21;
  *(_OWORD *)&__src[13] = v18;
  long long v22 = a2[1];
  *(_OWORD *)&__src[1] = *a2;
  *(_OWORD *)&__src[3] = v22;
  *(_OWORD *)&__src[5] = v20;
  char v23 = *a1;
  long long v28 = *a10;
  uint64_t v24 = *((void *)a10 + 2);
  uint64_t v25 = *((void *)a10 + 3);
  LOBYTE(__src[0]) = v23;
  char v26 = *((unsigned char *)a10 + 32);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v30, (uint64_t)&__src[23]);
  __src[46] = a4;
  __src[47] = a5;
  __src[48] = a6;
  __src[49] = a7;
  *(_OWORD *)&__src[51] = v28;
  __src[50] = a8;
  __src[53] = v24;
  __src[54] = v25;
  LOBYTE(__src[55]) = v26;
  return memcpy(a9, __src, 0x1B9uLL);
}

void BNNS.FusedConvolutionParameters.type.getter(unsigned char *a1@<X8>)
{
  *a1 = *v1;
}

unsigned char *BNNS.FusedConvolutionParameters.type.setter(unsigned char *result)
{
  *Swift::UInt v1 = *result;
  return result;
}

uint64_t (*BNNS.FusedConvolutionParameters.type.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

__n128 BNNS.FusedConvolutionParameters.weights.getter@<Q0>(uint64_t a1@<X8>)
{
  long long v2 = *(_OWORD *)(v1 + 120);
  long long v3 = *(_OWORD *)(v1 + 152);
  *(_OWORD *)(a1 + 128) = *(_OWORD *)(v1 + 136);
  *(_OWORD *)(a1 + 144) = v3;
  *(_OWORD *)(a1 + 160) = *(_OWORD *)(v1 + 168);
  long long v4 = *(_OWORD *)(v1 + 56);
  long long v5 = *(_OWORD *)(v1 + 88);
  *(_OWORD *)(a1 + 64) = *(_OWORD *)(v1 + 72);
  *(_OWORD *)(a1 + 80) = v5;
  *(_OWORD *)(a1 + 96) = *(_OWORD *)(v1 + 104);
  *(_OWORD *)(a1 + 112) = v2;
  long long v6 = *(_OWORD *)(v1 + 24);
  *(_OWORD *)a1 = *(_OWORD *)(v1 + 8);
  *(_OWORD *)(a1 + 16) = v6;
  __n128 result = *(__n128 *)(v1 + 40);
  *(__n128 *)(a1 + 32) = result;
  *(_OWORD *)(a1 + 48) = v4;
  return result;
}

__n128 BNNS.FusedConvolutionParameters.weights.setter(uint64_t a1)
{
  long long v2 = *(_OWORD *)(a1 + 96);
  *(_OWORD *)(v1 + 120) = *(_OWORD *)(a1 + 112);
  long long v3 = *(_OWORD *)(a1 + 144);
  *(_OWORD *)(v1 + 136) = *(_OWORD *)(a1 + 128);
  *(_OWORD *)(v1 + 152) = v3;
  *(_OWORD *)(v1 + 168) = *(_OWORD *)(a1 + 160);
  long long v4 = *(_OWORD *)(a1 + 32);
  *(_OWORD *)(v1 + 56) = *(_OWORD *)(a1 + 48);
  long long v5 = *(_OWORD *)(a1 + 80);
  *(_OWORD *)(v1 + 72) = *(_OWORD *)(a1 + 64);
  *(_OWORD *)(v1 + 88) = v5;
  *(_OWORD *)(v1 + 104) = v2;
  __n128 result = *(__n128 *)a1;
  long long v7 = *(_OWORD *)(a1 + 16);
  *(_OWORD *)(v1 + 8) = *(_OWORD *)a1;
  *(_OWORD *)(v1 + 24) = v7;
  *(_OWORD *)(v1 + 40) = v4;
  return result;
}

uint64_t (*BNNS.FusedConvolutionParameters.weights.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedConvolutionParameters.bias.getter@<X0>(uint64_t a1@<X8>)
{
  outlined init with take of BNNSNDArrayDescriptor?(v1 + 184, (uint64_t)v4);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v4, a1);
}

uint64_t BNNS.FusedConvolutionParameters.bias.setter(uint64_t a1)
{
  return outlined init with take of BNNSNDArrayDescriptor?(a1, v1 + 184);
}

uint64_t (*BNNS.FusedConvolutionParameters.bias.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedConvolutionParameters.stride.getter()
{
  return *(void *)(v0 + 368);
}

uint64_t BNNS.FusedConvolutionParameters.stride.setter(uint64_t result, uint64_t a2)
{
  *(void *)(v2 + 368) = result;
  *(void *)(v2 + 376) = a2;
  return result;
}

uint64_t (*BNNS.FusedConvolutionParameters.stride.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedConvolutionParameters.dilationStride.getter()
{
  return *(void *)(v0 + 384);
}

uint64_t BNNS.FusedConvolutionParameters.dilationStride.setter(uint64_t result, uint64_t a2)
{
  *(void *)(v2 + 384) = result;
  *(void *)(v2 + 392) = a2;
  return result;
}

uint64_t (*BNNS.FusedConvolutionParameters.dilationStride.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedConvolutionParameters.groupSize.getter()
{
  return *(void *)(v0 + 400);
}

uint64_t BNNS.FusedConvolutionParameters.groupSize.setter(uint64_t result)
{
  *(void *)(v1 + 400) = result;
  return result;
}

uint64_t (*BNNS.FusedConvolutionParameters.groupSize.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

__n128 BNNS.FusedConvolutionParameters.padding.getter@<Q0>(uint64_t a1@<X8>)
{
  char v2 = *(unsigned char *)(v1 + 440);
  __n128 result = *(__n128 *)(v1 + 408);
  long long v4 = *(_OWORD *)(v1 + 424);
  *(__n128 *)a1 = result;
  *(_OWORD *)(a1 + 16) = v4;
  *(unsigned char *)(a1 + 32) = v2;
  return result;
}

__n128 BNNS.FusedConvolutionParameters.padding.setter(uint64_t a1)
{
  char v2 = *(unsigned char *)(a1 + 32);
  __n128 result = *(__n128 *)a1;
  long long v4 = *(_OWORD *)(a1 + 16);
  *(_OWORD *)(v1 + 408) = *(_OWORD *)a1;
  *(_OWORD *)(v1 + 424) = v4;
  *(unsigned char *)(v1 + 440) = v2;
  return result;
}

uint64_t (*BNNS.FusedConvolutionParameters.padding.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t protocol witness for FusableLayerParametersWrapper.filterType.getter in conformance BNNS.FusedConvolutionParameters()
{
  if (*v0) {
    return 6;
  }
  else {
    return 0;
  }
}

void *__swift_memcpy441_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x1B9uLL);
}

uint64_t getEnumTagSinglePayload for BNNS.FusedConvolutionParameters(unsigned __int8 *a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFF && a1[441]) {
    return (*(_DWORD *)a1 + 255);
  }
  unsigned int v3 = *a1;
  BOOL v4 = v3 >= 2;
  int v5 = v3 - 2;
  if (!v4) {
    int v5 = -1;
  }
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.FusedConvolutionParameters(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(unsigned char *)(result + 440) = 0;
    *(_OWORD *)(result + 424) = 0u;
    *(_OWORD *)(result + 408) = 0u;
    *(_OWORD *)(result + 392) = 0u;
    *(_OWORD *)(result + 376) = 0u;
    *(_OWORD *)(result + 360) = 0u;
    *(_OWORD *)(result + 344) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(void *)__n128 result = a2 - 255;
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 441) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 441) = 0;
    }
    if (a2) {
      *(unsigned char *)__n128 result = a2 + 1;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.FusedConvolutionParameters()
{
  return &type metadata for BNNS.FusedConvolutionParameters;
}

uint64_t sub_1D20FA0CC()
{
  return MEMORY[0x1F4186498](v0, 856, 7);
}

uint64_t vDSP.Radix.fftRadix.getter()
{
  return *v0;
}

BOOL static vDSP.Radix.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void vDSP.Radix.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int vDSP.Radix.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t vDSP.FFT.__allocating_init(log2n:radix:ofType:)(uint64_t a1, char *a2)
{
  return vDSP.FFT.init(log2n:radix:ofType:)(a1, a2);
}

uint64_t vDSP.FFT.init(log2n:radix:ofType:)(uint64_t a1, char *a2)
{
  uint64_t v3 = v2;
  char v5 = *a2;
  *(void *)(v3 + 16) = a1;
  *(unsigned char *)(v3 + 24) = v5;
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  char v10 = v5;
  uint64_t AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  uint64_t v8 = (*(uint64_t (**)(uint64_t, char *, uint64_t, uint64_t))(AssociatedConformanceWitness + 16))(a1, &v10, AssociatedTypeWitness, AssociatedConformanceWitness);
  if (v8)
  {
    *(void *)(v3 + 32) = v8;
  }
  else
  {
    type metadata accessor for vDSP.FFT();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v3;
}

uint64_t type metadata accessor for vDSP.FFT()
{
  return __swift_instantiateGenericMetadata();
}

uint64_t vDSP.FFT.transform<A>(input:output:direction:)(uint64_t a1, uint64_t a2, char *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  char v9 = *a3;
  uint64_t v10 = *(void *)(v8 + 32);
  uint64_t v11 = *(void *)(v8 + 16);
  v13[2] = a4;
  v13[3] = a5;
  v13[4] = v10;
  v13[5] = v11;
  v13[6] = a2;
  char v14 = v9;
  return _ss17withUnsafePointer2to_q0_x_q0_SPyxGq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(a1, (uint64_t)partial apply for closure #1 in static vDSP_FFTFunctions.fftTransform<A>(fftSetup:log2n:source:destination:direction:), (uint64_t)v13, a4, MEMORY[0x1E4FBC248], MEMORY[0x1E4FBC848] + 8, MEMORY[0x1E4FBC278], a8);
}

uint64_t static vDSP_FFTFunctions.fftTransform<A>(fftSetup:log2n:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, char *a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  char v8 = *a5;
  _OWORD v10[2] = a6;
  v10[3] = a7;
  v10[4] = a1;
  v10[5] = a2;
  v10[6] = a4;
  char v11 = v8;
  return _ss17withUnsafePointer2to_q0_x_q0_SPyxGq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(a3, (uint64_t)partial apply for closure #1 in static vDSP_FFTFunctions.fftTransform<A>(fftSetup:log2n:source:destination:direction:), (uint64_t)v10, a6, MEMORY[0x1E4FBC248], MEMORY[0x1E4FBC848] + 8, MEMORY[0x1E4FBC278], a8);
}

uint64_t vDSP.FFT.forward(input:output:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return vDSP.FFT.forward(input:output:)(a1, a2, a3, 0);
}

uint64_t vDSP.FFT.inverse(input:output:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return vDSP.FFT.forward(input:output:)(a1, a2, a3, 1);
}

uint64_t vDSP.FFT.forward(input:output:)(uint64_t a1, uint64_t a2, uint64_t a3, char a4)
{
  v10[0] = a1;
  v10[1] = a2;
  char v9 = a4;
  long long v6 = *(uint64_t (**)(void *, uint64_t, char *, uint64_t, _UNKNOWN **))(*(void *)v4 + 128);
  type metadata accessor for DSPSplitComplex(0);
  return v6(v10, a3, &v9, v7, &protocol witness table for DSPSplitComplex);
}

uint64_t vDSP.FFT.deinit()
{
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  uint64_t v2 = *(void *)(v0 + 32);
  uint64_t AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  (*(void (**)(uint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness + 40))(v2, AssociatedTypeWitness, AssociatedConformanceWitness);
  return v0;
}

uint64_t vDSP.FFT.__deallocating_deinit()
{
  vDSP.FFT.deinit();

  return swift_deallocClassInstance();
}

void vDSP.FFT2D.__allocating_init(width:height:ofType:)(uint64_t a1, uint64_t a2)
{
}

void vDSP.FFT2D.init(width:height:ofType:)(uint64_t a1, uint64_t a2)
{
  *(void *)(v2 + 40) = a1;
  *(void *)(v2 + 48) = a2;
  if ((unsigned __int128)(a1 * (__int128)a2) >> 64 != (a1 * a2) >> 63)
  {
    __break(1u);
    goto LABEL_7;
  }
  float v3 = log2f((float)(a1 * a2));
  if ((~LODWORD(v3) & 0x7F800000) == 0)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (v3 <= -1.0)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (v3 < 1.8447e19)
  {
    char v4 = 0;
    vDSP.FFT.init(log2n:radix:ofType:)((unint64_t)v3, &v4);
    return;
  }
LABEL_9:
  __break(1u);
}

uint64_t vDSP.FFT2D.transform<A>(input:output:direction:)(uint64_t a1, uint64_t a2, char *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  char v9 = *a3;
  uint64_t v10 = *(void *)(v8 + 32);
  _OWORD v12[2] = a4;
  v12[3] = a5;
  v12[4] = v10;
  long long v13 = *(_OWORD *)(v8 + 40);
  uint64_t v14 = a2;
  char v15 = v9;
  return _ss17withUnsafePointer2to_q0_x_q0_SPyxGq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(a1, (uint64_t)partial apply for closure #1 in static vDSP_FFTFunctions.fftTransform2D<A>(fftSetup:width:height:source:destination:direction:), (uint64_t)v12, a4, MEMORY[0x1E4FBC248], MEMORY[0x1E4FBC848] + 8, MEMORY[0x1E4FBC278], a8);
}

uint64_t static vDSP_FFTFunctions.fftTransform2D<A>(fftSetup:width:height:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, char *a6, uint64_t a7, uint64_t a8)
{
  char v8 = *a6;
  void v10[2] = a7;
  v10[3] = a8;
  v10[4] = a1;
  v10[5] = a2;
  v10[6] = a3;
  void v10[7] = a5;
  char v11 = v8;
  return _ss17withUnsafePointer2to_q0_x_q0_SPyxGq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(a4, (uint64_t)partial apply for closure #1 in static vDSP_FFTFunctions.fftTransform2D<A>(fftSetup:width:height:source:destination:direction:), (uint64_t)v10, a7, MEMORY[0x1E4FBC248], MEMORY[0x1E4FBC848] + 8, MEMORY[0x1E4FBC278], a8);
}

void vDSP.FFT2D.__allocating_init(log2n:radix:ofType:)()
{
}

void vDSP.FFT2D.init(log2n:radix:ofType:)()
{
}

uint64_t vDSP.FFT2D.__deallocating_deinit()
{
  vDSP.FFT.deinit();

  return swift_deallocClassInstance();
}

FFTSetup static vDSP_SplitComplexFloat.makeFFTSetup(log2n:radix:)(vDSP_Length a1, unsigned __int8 *a2)
{
  return vDSP_create_fftsetup(a1, *a2);
}

uint64_t static vDSP_SplitComplexFloat.transform(fftSetup:log2n:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, unsigned char *a5)
{
  return static vDSP_SplitComplexFloat.transform(fftSetup:log2n:source:destination:direction:)(a1, a2, a3, a4, a5, MEMORY[0x1E4F16908]);
}

void static vDSP_SplitComplexFloat.transform2D(fftSetup:width:height:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, unsigned __int8 *a6)
{
}

FFTSetup protocol witness for static vDSP_FourierTransformFunctions.makeFFTSetup(log2n:radix:) in conformance vDSP_SplitComplexFloat(vDSP_Length a1, unsigned __int8 *a2)
{
  return vDSP_create_fftsetup(a1, *a2);
}

void protocol witness for static vDSP_FourierTransformFunctions.transform(fftSetup:log2n:source:destination:direction:) in conformance vDSP_SplitComplexFloat(OpaqueFFTSetup *a1, vDSP_Length __Log2N, DSPSplitComplex *__A, const DSPSplitComplex *a4, unsigned char *a5)
{
  if (*a5) {
    FFTDirection v5 = -1;
  }
  else {
    FFTDirection v5 = 1;
  }
  vDSP_fft_zrop(a1, __A, 1, a4, 1, __Log2N, v5);
}

void protocol witness for static vDSP_FourierTransformFunctions.transform2D(fftSetup:width:height:source:destination:direction:) in conformance vDSP_SplitComplexFloat(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, unsigned __int8 *a6)
{
}

FFTSetupD static vDSP_SplitComplexDouble.makeFFTSetup(log2n:radix:)(vDSP_Length a1, unsigned __int8 *a2)
{
  return vDSP_create_fftsetupD(a1, *a2);
}

uint64_t static vDSP_SplitComplexDouble.transform(fftSetup:log2n:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, unsigned char *a5)
{
  return static vDSP_SplitComplexFloat.transform(fftSetup:log2n:source:destination:direction:)(a1, a2, a3, a4, a5, MEMORY[0x1E4F16910]);
}

uint64_t static vDSP_SplitComplexFloat.transform(fftSetup:log2n:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, unsigned char *a5, uint64_t (*a6)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  if (*a5) {
    uint64_t v6 = 0xFFFFFFFFLL;
  }
  else {
    uint64_t v6 = 1;
  }
  return a6(a1, a3, 1, a4, 1, a2, v6);
}

void static vDSP_SplitComplexDouble.transform2D(fftSetup:width:height:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, unsigned __int8 *a6)
{
}

FFTSetupD protocol witness for static vDSP_FourierTransformFunctions.makeFFTSetup(log2n:radix:) in conformance vDSP_SplitComplexDouble(vDSP_Length a1, unsigned __int8 *a2)
{
  return vDSP_create_fftsetupD(a1, *a2);
}

void protocol witness for static vDSP_FourierTransformFunctions.transform(fftSetup:log2n:source:destination:direction:) in conformance vDSP_SplitComplexDouble(OpaqueFFTSetupD *a1, vDSP_Length __Log2N, DSPDoubleSplitComplex *__A, const DSPDoubleSplitComplex *a4, unsigned char *a5)
{
  if (*a5) {
    FFTDirection v5 = -1;
  }
  else {
    FFTDirection v5 = 1;
  }
  vDSP_fft_zropD(a1, __A, 1, a4, 1, __Log2N, v5);
}

void protocol witness for static vDSP_FourierTransformFunctions.transform2D(fftSetup:width:height:source:destination:direction:) in conformance vDSP_SplitComplexDouble(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, unsigned __int8 *a6)
{
}

uint64_t closure #1 in static vDSP_FFTFunctions.fftTransform<A>(fftSetup:log2n:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, char a5)
{
  char v9 = a5 & 1;
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  char v13 = v9;
  uint64_t AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  return (*(uint64_t (**)(uint64_t, uint64_t, uint64_t, uint64_t, char *, uint64_t, uint64_t))(AssociatedConformanceWitness
                                                                                                 + 24))(a2, a3, a1, a4, &v13, AssociatedTypeWitness, AssociatedConformanceWitness);
}

uint64_t closure #1 in static vDSP_FFTFunctions.fftTransform2D<A>(fftSetup:width:height:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, char a6)
{
  char v10 = a6 & 1;
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  char v15 = v10;
  uint64_t AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  return (*(uint64_t (**)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, char *, uint64_t, uint64_t))(AssociatedConformanceWitness + 32))(a2, a3, a4, a1, a5, &v15, AssociatedTypeWitness, AssociatedConformanceWitness);
}

float *DSPSplitComplex.init(fromInputArray:realParts:imaginaryParts:)(uint64_t a1, char **a2, char **a3)
{
  uint64_t v12 = *MEMORY[0x1E4F143B8];
  uint64_t v6 = *a2;
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
    uint64_t v6 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v6 + 2), 0, v6);
  }
  *a2 = v6;
  uint64_t v7 = *a3;
  swift_bridgeObjectRetain();
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
    uint64_t v7 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v7 + 2), 0, v7);
  }
  *a3 = v7;
  swift_bridgeObjectRelease();
  __Z.realp = (float *)(v6 + 32);
  __Z.imagp = (float *)(v7 + 32);
  unint64_t v8 = *(void *)(a1 + 16);
  if (v8 >> 61) {
    __break(1u);
  }
  char v9 = (const DSPComplex *)specialized _copyCollectionToContiguousArray<A>(_:)((const void *)(a1 + 32), v8 >> 1);
  vDSP_ctoz(v9 + 4, 2, &__Z, 1, *(void *)(a1 + 16) >> 1);
  swift_bridgeObjectRelease();
  swift_release();
  return __Z.realp;
}

double *DSPDoubleSplitComplex.init(fromInputArray:realParts:imaginaryParts:)(uint64_t a1, char **a2, char **a3)
{
  uint64_t v12 = *MEMORY[0x1E4F143B8];
  uint64_t v6 = *a2;
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
    uint64_t v6 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v6 + 2), 0, v6);
  }
  *a2 = v6;
  uint64_t v7 = *a3;
  swift_bridgeObjectRetain();
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
    uint64_t v7 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v7 + 2), 0, v7);
  }
  *a3 = v7;
  swift_bridgeObjectRelease();
  __Z.realp = (double *)(v6 + 32);
  __Z.imagp = (double *)(v7 + 32);
  unint64_t v8 = *(void *)(a1 + 16);
  if (v8 >> 60) {
    __break(1u);
  }
  char v9 = (const DSPDoubleComplex *)specialized _copyCollectionToContiguousArray<A>(_:)((const void *)(a1 + 32), v8 >> 1);
  vDSP_ctozD(v9 + 2, 2, &__Z, 1, *(void *)(a1 + 16) >> 1);
  swift_bridgeObjectRelease();
  swift_release();
  return __Z.realp;
}

uint64_t Array<A>.init(fromSplitComplex:scale:count:)(float *a1, float *a2, int64_t a3, float a4)
{
  uint64_t v14 = *MEMORY[0x1E4F143B8];
  if (a3 >= 0) {
    uint64_t v4 = a3;
  }
  else {
    uint64_t v4 = a3 + 1;
  }
  if (a3 < -1) {
    __break(1u);
  }
  vDSP_Length v9 = v4 >> 1;
  if (a3 < 2)
  {
    uint64_t v10 = MEMORY[0x1E4FBC860];
  }
  else
  {
    type metadata accessor for DSPComplex(0);
    uint64_t v10 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v10 + 16) = v9;
  }
  __Z.realp = a1;
  __Z.imagp = a2;
  vDSP_ztoc(&__Z, 1, (DSPComplex *)(v10 + 32), 2, v9);
  *(void *)(v10 + 16) = v9;
  uint64_t v11 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a3, v10, a3, a4);
  swift_bridgeObjectRelease();
  return v11;
}

uint64_t Array<A>.init(fromSplitComplex:scale:count:)(double *a1, double *a2, int64_t a3, double a4)
{
  uint64_t v14 = *MEMORY[0x1E4F143B8];
  if (a3 >= 0) {
    uint64_t v4 = a3;
  }
  else {
    uint64_t v4 = a3 + 1;
  }
  if (a3 < -1) {
    __break(1u);
  }
  vDSP_Length v9 = v4 >> 1;
  if (a3 < 2)
  {
    uint64_t v10 = MEMORY[0x1E4FBC860];
  }
  else
  {
    type metadata accessor for DSPDoubleComplex(0);
    uint64_t v10 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v10 + 16) = v9;
  }
  __Z.realp = a1;
  __Z.imagp = a2;
  vDSP_ztocD(&__Z, 1, (DSPDoubleComplex *)(v10 + 32), 2, v9);
  *(void *)(v10 + 16) = v9;
  uint64_t v11 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a3, v10, a3, a4);
  swift_bridgeObjectRelease();
  return v11;
}

void *specialized _copyCollectionToContiguousArray<A>(_:)(const void *a1, uint64_t a2)
{
  if (!a2) {
    return (void *)MEMORY[0x1E4FBC860];
  }
  if (a2 <= 0)
  {
    size_t v4 = 8 * a2;
    FFTDirection v5 = (void *)MEMORY[0x1E4FBC860];
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
    size_t v4 = 8 * a2;
    FFTDirection v5 = (void *)swift_allocObject();
    int64_t v6 = _swift_stdlib_malloc_size(v5);
    uint64_t v7 = v6 - 32;
    if (v6 < 32) {
      uint64_t v7 = v6 - 25;
    }
    void v5[2] = a2;
    v5[3] = 2 * (v7 >> 3);
  }
  memcpy(v5 + 4, a1, v4);
  return v5;
}

{
  return specialized _copyCollectionToContiguousArray<A>(_:)(a1, a2, &demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
}

{
  size_t v4;
  void *v5;
  int64_t v6;
  uint64_t v7;

  if (!a2) {
    return (void *)MEMORY[0x1E4FBC860];
  }
  if (a2 <= 0)
  {
    size_t v4 = 8 * a2;
    FFTDirection v5 = (void *)MEMORY[0x1E4FBC860];
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<DSPComplex>);
    size_t v4 = 8 * a2;
    FFTDirection v5 = (void *)swift_allocObject();
    int64_t v6 = _swift_stdlib_malloc_size(v5);
    uint64_t v7 = v6 - 32;
    if (v6 < 32) {
      uint64_t v7 = v6 - 25;
    }
    void v5[2] = a2;
    v5[3] = 2 * (v7 >> 3);
  }
  memcpy(v5 + 4, a1, v4);
  return v5;
}

{
  size_t v4;
  void *v5;
  int64_t v6;
  uint64_t v7;

  if (!a2) {
    return (void *)MEMORY[0x1E4FBC860];
  }
  if (a2 <= 0)
  {
    size_t v4 = 16 * a2;
    FFTDirection v5 = (void *)MEMORY[0x1E4FBC860];
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<DSPDoubleComplex>);
    size_t v4 = 16 * a2;
    FFTDirection v5 = (void *)swift_allocObject();
    int64_t v6 = _swift_stdlib_malloc_size(v5);
    uint64_t v7 = v6 - 32;
    if (v6 < 32) {
      uint64_t v7 = v6 - 17;
    }
    void v5[2] = a2;
    v5[3] = 2 * (v7 >> 4);
  }
  memcpy(v5 + 4, a1, v4);
  return v5;
}

{
  return specialized _copyCollectionToContiguousArray<A>(_:)(a1, a2, &demangling cache variable for type metadata for _ContiguousArrayStorage<UInt32>);
}

void *specialized _copyCollectionToContiguousArray<A>(_:)(const void *a1, uint64_t a2, uint64_t *a3)
{
  if (!a2) {
    return (void *)MEMORY[0x1E4FBC860];
  }
  if (a2 <= 0)
  {
    size_t v5 = 4 * a2;
    int64_t v6 = (void *)MEMORY[0x1E4FBC860];
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(a3);
    size_t v5 = 4 * a2;
    int64_t v6 = (void *)swift_allocObject();
    int64_t v7 = _swift_stdlib_malloc_size(v6);
    uint64_t v8 = v7 - 32;
    if (v7 < 32) {
      uint64_t v8 = v7 - 29;
    }
    void v6[2] = a2;
    v6[3] = 2 * (v8 >> 2);
  }
  memcpy(v6 + 4, a1, v5);
  return v6;
}

void specialized vDSP.FFT2D.init(log2n:radix:ofType:)()
{
}

void specialized static vDSP_SplitComplexFloat.transform2D(fftSetup:width:height:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, unsigned __int8 *a6, void (*a7)(uint64_t, uint64_t, uint64_t, void, uint64_t, uint64_t, void, unint64_t, unint64_t, int))
{
  int v12 = *a6;
  float v13 = log2f((float)a2);
  if ((~LODWORD(v13) & 0x7F800000) == 0)
  {
    __break(1u);
    goto LABEL_12;
  }
  if (v13 <= -1.0)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  if (v13 >= 1.8447e19)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  float v14 = log2f((float)a3);
  if ((~LODWORD(v14) & 0x7F800000) == 0)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  if (v14 <= -1.0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  if (v14 >= 1.8447e19)
  {
LABEL_16:
    __break(1u);
    return;
  }
  if (v12) {
    int v15 = -1;
  }
  else {
    int v15 = 1;
  }
  a7(a1, a4, 1, 0, a5, 1, 0, (unint64_t)v13, (unint64_t)v14, v15);
}

uint64_t specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(vDSP_Length a1, uint64_t a2, vDSP_Length a3, float a4)
{
  uint64_t v11 = *MEMORY[0x1E4F143B8];
  if ((a1 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (a1)
  {
    uint64_t v8 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v8 + 16) = a1;
  }
  else
  {
    uint64_t v8 = MEMORY[0x1E4FBC860];
  }
  if (*(void *)(a2 + 16) >> 60) {
    goto LABEL_10;
  }
  float __B = a4;
  if ((a3 & 0x8000000000000000) != 0)
  {
LABEL_11:
    __break(1u);
LABEL_12:
    __break(1u);
  }
  vDSP_vsmul((const float *)(a2 + 32), 1, &__B, (float *)(v8 + 32), 1, a3);
  if (a1 < a3) {
    goto LABEL_12;
  }
  *(void *)(v8 + 16) = a3;
  return v8;
}

uint64_t specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(vDSP_Length a1, uint64_t a2, vDSP_Length a3, double a4)
{
  v10[1] = *(double *)MEMORY[0x1E4F143B8];
  if ((a1 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (a1)
  {
    uint64_t v8 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v8 + 16) = a1;
  }
  else
  {
    uint64_t v8 = MEMORY[0x1E4FBC860];
  }
  if (*(void *)(a2 + 16) >> 59) {
    goto LABEL_10;
  }
  v10[0] = a4;
  if ((a3 & 0x8000000000000000) != 0)
  {
LABEL_11:
    __break(1u);
LABEL_12:
    __break(1u);
  }
  vDSP_vsmulD((const double *)(a2 + 32), 1, v10, (double *)(v8 + 32), 1, a3);
  if (a1 < a3) {
    goto LABEL_12;
  }
  *(void *)(v8 + 16) = a3;
  return v8;
}

unint64_t lazy protocol witness table accessor for type vDSP.Radix and conformance vDSP.Radix()
{
  unint64_t result = lazy protocol witness table cache variable for type vDSP.Radix and conformance vDSP.Radix;
  if (!lazy protocol witness table cache variable for type vDSP.Radix and conformance vDSP.Radix)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.Radix and conformance vDSP.Radix);
  }
  return result;
}

_UNKNOWN **associated type witness table accessor for vDSP_FourierTransformable.FFTFunctions : vDSP_FourierTransformFunctions in DSPSplitComplex()
{
  return &protocol witness table for vDSP_SplitComplexFloat;
}

_UNKNOWN **associated type witness table accessor for vDSP_FourierTransformable.FFTFunctions : vDSP_FourierTransformFunctions in DSPDoubleSplitComplex()
{
  return &protocol witness table for vDSP_SplitComplexDouble;
}

unsigned char *storeEnumTagSinglePayload for vDSP.Radix(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 2 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 2) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFE) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFD)
  {
    unsigned int v6 = ((a2 - 254) >> 8) + 1;
    *unint64_t result = a2 + 2;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D20FB6ACLL);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 2;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vDSP.Radix()
{
  return &type metadata for vDSP.Radix;
}

uint64_t type metadata completion function for vDSP.FFT()
{
  return swift_initClassMetadata2();
}

uint64_t method lookup function for vDSP.FFT(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for vDSP.FFT);
}

uint64_t dispatch thunk of vDSP.FFT.__allocating_init(log2n:radix:ofType:)()
{
  return (*(uint64_t (**)(void))(v0 + 120))();
}

uint64_t dispatch thunk of vDSP.FFT.transform<A>(input:output:direction:)()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 128))();
}

uint64_t dispatch thunk of vDSP.FFT.forward(input:output:)()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 136))();
}

uint64_t dispatch thunk of vDSP.FFT.inverse(input:output:)()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 144))();
}

uint64_t type metadata completion function for vDSP.FFT2D()
{
  return swift_initClassMetadata2();
}

uint64_t type metadata accessor for vDSP.FFT2D()
{
  return __swift_instantiateGenericMetadata();
}

uint64_t method lookup function for vDSP.FFT2D(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for vDSP.FFT2D);
}

uint64_t dispatch thunk of vDSP.FFT2D.__allocating_init(width:height:ofType:)()
{
  return (*(uint64_t (**)(void))(v0 + 184))();
}

uint64_t dispatch thunk of static vDSP_FourierTransformFunctions.makeFFTSetup(log2n:radix:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return (*(uint64_t (**)(void))(a4 + 16))();
}

uint64_t dispatch thunk of static vDSP_FourierTransformFunctions.transform(fftSetup:log2n:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  return (*(uint64_t (**)(void))(a7 + 24))();
}

uint64_t dispatch thunk of static vDSP_FourierTransformFunctions.transform2D(fftSetup:width:height:source:destination:direction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  return (*(uint64_t (**)(void))(a8 + 32))();
}

uint64_t dispatch thunk of static vDSP_FourierTransformFunctions.destroySetup(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 40))();
}

ValueMetadata *type metadata accessor for vDSP_SplitComplexFloat()
{
  return &type metadata for vDSP_SplitComplexFloat;
}

ValueMetadata *type metadata accessor for vDSP_SplitComplexDouble()
{
  return &type metadata for vDSP_SplitComplexDouble;
}

uint64_t partial apply for closure #1 in static vDSP_FFTFunctions.fftTransform2D<A>(fftSetup:width:height:source:destination:direction:)(uint64_t a1)
{
  return closure #1 in static vDSP_FFTFunctions.fftTransform2D<A>(fftSetup:width:height:source:destination:direction:)(a1, *(void *)(v1 + 32), *(void *)(v1 + 40), *(void *)(v1 + 48), *(void *)(v1 + 56), *(unsigned char *)(v1 + 64));
}

uint64_t partial apply for closure #1 in static vDSP_FFTFunctions.fftTransform<A>(fftSetup:log2n:source:destination:direction:)(uint64_t a1)
{
  return closure #1 in static vDSP_FFTFunctions.fftTransform<A>(fftSetup:log2n:source:destination:direction:)(a1, *(void *)(v1 + 32), *(void *)(v1 + 40), *(void *)(v1 + 48), *(unsigned char *)(v1 + 56));
}

uint64_t static BNNS.RandomGeneratorMethod.== infix(_:_:)()
{
  return 1;
}

void BNNS.RandomGeneratorMethod.hash(into:)()
{
}

Swift::Int BNNS.RandomGeneratorMethod.hashValue.getter()
{
  return Hasher._finalize()();
}

uint64_t BNNS.RandomGeneratorState.deinit()
{
  MEMORY[0x1D26009C0](*(void *)(v0 + 24), -1, -1);
  return v0;
}

uint64_t BNNS.RandomGeneratorState.__deallocating_deinit()
{
  MEMORY[0x1D26009C0](*(void *)(v0 + 24), -1, -1);

  return swift_deallocClassInstance();
}

uint64_t BNNS.RandomGenerator.__allocating_init(method:seed:filterParameters:)(uint64_t a1, uint64_t a2, char a3, int a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v22 = *MEMORY[0x1E4F143B8];
  uint64_t v13 = swift_allocObject();
  if ((a3 & 1) == 0)
  {
    if (a6 == 1)
    {
      uint64_t v16 = a2;
      int v15 = 0;
    }
    else
    {
      int v18 = a4;
      uint64_t v19 = a5;
      uint64_t v20 = a6;
      uint64_t v21 = a7;
      int v15 = &v18;
      uint64_t v16 = a2;
    }
    uint64_t v14 = MEMORY[0x1D25FFED0](0, v16, v15);
    if (!v14) {
      goto LABEL_10;
    }
    goto LABEL_12;
  }
  if (a6 != 1)
  {
    int v18 = a4;
    uint64_t v19 = a5;
    uint64_t v20 = a6;
    uint64_t v21 = a7;
    uint64_t v14 = MEMORY[0x1D25FFEC0](0, &v18);
    if (!v14) {
      goto LABEL_10;
    }
LABEL_12:
    *(void *)(v13 + 16) = v14;
    *(void *)(v13 + 24) = a2;
    *(unsigned char *)(v13 + 32) = a3 & 1;
    return v13;
  }
  uint64_t v14 = MEMORY[0x1D25FFEC0](0, 0);
  if (v14) {
    goto LABEL_12;
  }
LABEL_10:
  swift_deallocPartialClassInstance();
  return 0;
}

uint64_t BNNS.RandomGenerator.init(method:seed:filterParameters:)(uint64_t a1, uint64_t a2, char a3, int a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v17 = *MEMORY[0x1E4F143B8];
  if ((a3 & 1) == 0)
  {
    if (a6 == 1)
    {
      uint64_t v11 = 0;
    }
    else
    {
      int v13 = a4;
      uint64_t v14 = a5;
      uint64_t v15 = a6;
      uint64_t v16 = a7;
      uint64_t v11 = &v13;
    }
    uint64_t v10 = MEMORY[0x1D25FFED0](0, a2, v11);
    if (!v10) {
      goto LABEL_10;
    }
    goto LABEL_12;
  }
  if (a6 != 1)
  {
    int v13 = a4;
    uint64_t v14 = a5;
    uint64_t v15 = a6;
    uint64_t v16 = a7;
    uint64_t v10 = MEMORY[0x1D25FFEC0](0, &v13);
    if (!v10) {
      goto LABEL_10;
    }
LABEL_12:
    *(void *)(v7 + 16) = v10;
    *(void *)(v7 + 24) = a2;
    *(unsigned char *)(v7 + 32) = a3 & 1;
    return v7;
  }
  uint64_t v10 = MEMORY[0x1D25FFEC0](0, 0);
  if (v10) {
    goto LABEL_12;
  }
LABEL_10:
  type metadata accessor for BNNS.RandomGenerator();
  swift_deallocPartialClassInstance();
  return 0;
}

uint64_t type metadata accessor for BNNS.RandomGenerator()
{
  return self;
}

uint64_t BNNS.RandomGenerator.deinit()
{
  MEMORY[0x1D25FFF10](*(void *)(v0 + 16));
  return v0;
}

uint64_t BNNS.RandomGenerator.__deallocating_deinit()
{
  MEMORY[0x1D25FFF10](*(void *)(v0 + 16));

  return swift_deallocClassInstance();
}

uint64_t BNNS.RandomGenerator.state.getter()
{
  type metadata accessor for BNNS.RandomGeneratorState();
  uint64_t v1 = swift_allocObject();
  uint64_t v2 = *(void *)(v0 + 16);
  swift_retain();
  uint64_t v3 = MEMORY[0x1D26003B0](v2);
  *(void *)(v1 + 16) = v3;
  uint64_t v4 = swift_slowAlloc();
  swift_release();
  *(void *)(v1 + 24) = v4;
  MEMORY[0x1D2600390](*(void *)(v0 + 16), v3, v4);
  return v1;
}

uint64_t type metadata accessor for BNNS.RandomGeneratorState()
{
  return self;
}

uint64_t BNNS.RandomGenerator.state.setter(uint64_t a1)
{
  MEMORY[0x1D26003A0](*(void *)(v1 + 16), *(void *)(a1 + 16), *(void *)(a1 + 24));

  return swift_release();
}

uint64_t (*BNNS.RandomGenerator.state.modify(uint64_t *a1))(void *a1)
{
  a1[1] = v1;
  type metadata accessor for BNNS.RandomGeneratorState();
  uint64_t v3 = swift_allocObject();
  uint64_t v4 = *(void *)(v1 + 16);
  swift_retain();
  uint64_t v5 = MEMORY[0x1D26003B0](v4);
  *(void *)(v3 + 16) = v5;
  uint64_t v6 = swift_slowAlloc();
  swift_release();
  *(void *)(v3 + 24) = v6;
  MEMORY[0x1D2600390](*(void *)(v1 + 16), v5, v6);
  *a1 = v3;
  return BNNS.RandomGenerator.state.modify;
}

uint64_t BNNS.RandomGenerator.state.modify(void *a1)
{
  MEMORY[0x1D26003A0](*(void *)(a1[1] + 16), *(void *)(*a1 + 16), *(void *)(*a1 + 24));

  return swift_release();
}

uint64_t static BNNSNDArrayDescriptor.allocate<A>(randomUniformUsing:range:shape:batchSize:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X3>, uint64_t a4@<X4>, uint64_t a5@<X5>, uint64_t a6@<X8>)
{
  uint64_t v22 = a6;
  uint64_t v27 = *MEMORY[0x1E4F143B8];
  uint64_t v11 = *(void *)(a4 - 8);
  MEMORY[0x1F4188790](a1);
  int v13 = (char *)&v21 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  outlined init with take of BNNS.Shape(v14, (uint64_t)v25);
  int v15 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 8))(a4, a5);
  helper #1 <A>(_:) in static BNNSNDArrayDescriptor.allocateUninitialized(scalarType:shape:batchSize:)(a3, (uint64_t)v25, v15, &v24);
  uint64_t v16 = *(void **)(a1 + 16);
  uint64_t v17 = *(void (**)(char *, uint64_t, uint64_t))(v11 + 16);
  v17(v13, a2, a4);
  lazy protocol witness table accessor for type Float and conformance Float();
  BinaryFloatingPoint.init<A>(_:)();
  float v18 = *(float *)v26;
  uint64_t v19 = type metadata accessor for ClosedRange();
  v17(v13, a2 + *(int *)(v19 + 36), a4);
  BinaryFloatingPoint.init<A>(_:)();
  if (BNNSRandomFillUniformFloat(v16, &v24, v18, *(float *)&v23.flags))
  {
    if (v24.data) {
      MEMORY[0x1D26009C0](v24.data, -1, -1);
    }
    _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&v23);
  }
  else
  {
    BNNSNDArrayDescriptor v23 = v24;
    _sSo21BNNSNDArrayDescriptoraSgWOi_((uint64_t)&v23);
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v23, (uint64_t)v26);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v26, v22);
}

uint64_t static BNNSNDArrayDescriptor.allocate<A>(randomUniformUsing:range:shape:batchSize:)@<X0>(void *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X8>)
{
  v140 = a1;
  uint64_t v129 = a8;
  uint64_t v145 = *MEMORY[0x1E4F143B8];
  uint64_t v13 = *(void *)(a7 + 8);
  uint64_t v123 = *(void *)(*(void *)(v13 + 24) + 16);
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  uint64_t v14 = MEMORY[0x1F4188790](AssociatedTypeWitness);
  v124 = &v118[-v15];
  uint64_t v16 = *(void *)(a5 - 8);
  uint64_t v17 = MEMORY[0x1F4188790](v14);
  v120 = &v118[-((v18 + 15) & 0xFFFFFFFFFFFFFFF0)];
  uint64_t v19 = MEMORY[0x1F4188790](v17);
  v135 = &v118[-v20];
  uint64_t v21 = MEMORY[0x1F4188790](v19);
  v137 = &v118[-v22];
  uint64_t v23 = MEMORY[0x1F4188790](v21);
  v125 = &v118[-v24];
  uint64_t v25 = MEMORY[0x1F4188790](v23);
  v121 = &v118[-v26];
  uint64_t v27 = MEMORY[0x1F4188790](v25);
  v127 = &v118[-v28];
  uint64_t v29 = MEMORY[0x1F4188790](v27);
  v131 = &v118[-v30];
  uint64_t v31 = MEMORY[0x1F4188790](v29);
  v138 = &v118[-v32];
  uint64_t v33 = MEMORY[0x1F4188790](v31);
  v128 = &v118[-v34];
  uint64_t v35 = MEMORY[0x1F4188790](v33);
  v134 = &v118[-v36];
  uint64_t v37 = MEMORY[0x1F4188790](v35);
  v133 = &v118[-v38];
  uint64_t v39 = MEMORY[0x1F4188790](v37);
  v126 = &v118[-v40];
  uint64_t v41 = MEMORY[0x1F4188790](v39);
  uint64_t v43 = &v118[-v42];
  MEMORY[0x1F4188790](v41);
  uint64_t v45 = &v118[-v44];
  outlined init with take of BNNS.Shape(a3, (uint64_t)v144);
  int v46 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 8))(a5, a6);
  helper #1 <A>(_:) in static BNNSNDArrayDescriptor.allocateUninitialized(scalarType:shape:batchSize:)(a4, (uint64_t)v144, v46, &v143);
  v130 = (void *)v140[2];
  uint64_t v47 = *(void (**)(unsigned char *, uint64_t, uint64_t))(v16 + 16);
  uint64_t v132 = a2;
  v47(v45, a2, a5);
  LOBYTE(a3) = dispatch thunk of static BinaryInteger.isSigned.getter();
  v139 = (void (*)(unsigned char *, unsigned char *, uint64_t))v47;
  v47(v43, (uint64_t)v45, a5);
  uint64_t v136 = v16;
  if ((a3 & 1) == 0)
  {
    v140 = *(void **)(v16 + 8);
    ((void (*)(unsigned char *, uint64_t))v140)(v43, a5);
    uint64_t v53 = v138;
    uint64_t v51 = v134;
    goto LABEL_23;
  }
  uint64_t v48 = dispatch thunk of BinaryInteger.bitWidth.getter();
  v140 = *(void **)(v16 + 8);
  ((void (*)(unsigned char *, uint64_t))v140)(v43, a5);
  if (v48 <= 64)
  {
    uint64_t v53 = v138;
    uint64_t v51 = v134;
    goto LABEL_23;
  }
  uint64_t v49 = (void (*)(void, void))v126;
  v139(v126, v45, a5);
  v142[0] = 0x8000000000000000;
  char v50 = dispatch thunk of static BinaryInteger.isSigned.getter();
  uint64_t v51 = v134;
  if (v50)
  {
    uint64_t v52 = dispatch thunk of BinaryInteger.bitWidth.getter();
    uint64_t v53 = v138;
    if (v52 >= 64)
    {
      lazy protocol witness table accessor for type Int64 and conformance Int64();
      dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
      LODWORD(v121) = dispatch thunk of static Comparable.< infix(_:_:)();
      uint64_t v62 = v49;
      uint64_t v49 = (void (*)(void, void))v140;
      ((void (*)(unsigned char *, uint64_t))v140)(v133, a5);
      v49(v62, a5);
      if ((v121 & 1) == 0) {
        goto LABEL_23;
      }
      goto LABEL_21;
    }
    uint64_t v54 = dispatch thunk of BinaryInteger._lowWord.getter();
    ((void (*)(void, uint64_t))v140)(v49, a5);
    BOOL v55 = v54 < v142[0];
  }
  else
  {
    char v56 = dispatch thunk of static BinaryInteger.isSigned.getter();
    uint64_t v57 = dispatch thunk of BinaryInteger.bitWidth.getter();
    if (v56)
    {
      if (v57 > 64)
      {
        lazy protocol witness table accessor for type Int64 and conformance Int64();
        uint64_t v58 = v133;
        dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
        char v59 = dispatch thunk of static Comparable.< infix(_:_:)();
        uint64_t v60 = v58;
        long long v61 = (void (*)(void, uint64_t))v140;
        ((void (*)(unsigned char *, uint64_t))v140)(v60, a5);
        v61(v49, a5);
        uint64_t v53 = v138;
        if ((v59 & 1) == 0) {
          goto LABEL_23;
        }
        goto LABEL_21;
      }
      swift_getAssociatedConformanceWitness();
      dispatch thunk of _ExpressibleByBuiltinIntegerLiteral.init(_builtinIntegerLiteral:)();
      uint64_t v64 = v133;
      dispatch thunk of ExpressibleByIntegerLiteral.init(integerLiteral:)();
      int v119 = dispatch thunk of static Comparable.< infix(_:_:)();
      ((void (*)(unsigned char *, uint64_t))v140)(v64, a5);
      uint64_t v65 = (uint64_t)v121;
      (*(void (**)(unsigned char *, void (*)(void, void), uint64_t))(v136 + 32))(v121, v49, a5);
      if (v119)
      {
LABEL_76:
        ((void (*)(uint64_t, uint64_t))v140)(v65, a5);
        __break(1u);
LABEL_77:
        ((void (*)(uint64_t, uint64_t))v140)(v65, a5);
        __break(1u);
      }
      uint64_t v66 = v142[0];
      uint64_t v49 = (void (*)(void, void))dispatch thunk of BinaryInteger._lowWord.getter();
      ((void (*)(uint64_t, uint64_t))v140)(v65, a5);
      BOOL v55 = (uint64_t)v49 < v66;
    }
    else
    {
      if (v57 >= 64)
      {
LABEL_22:
        ((void (*)(void, uint64_t))v140)(v49, a5);
        uint64_t v53 = v138;
        goto LABEL_23;
      }
      uint64_t v63 = dispatch thunk of BinaryInteger._lowWord.getter();
      ((void (*)(void, uint64_t))v140)(v49, a5);
      BOOL v55 = v63 < v142[0];
    }
    uint64_t v53 = v138;
  }
  if (v55)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
LABEL_23:
  uint64_t v67 = dispatch thunk of BinaryInteger.bitWidth.getter();
  v139(v51, v45, a5);
  if (v67 < 65)
  {
    uint64_t v69 = dispatch thunk of BinaryInteger.bitWidth.getter();
    ((void (*)(unsigned char *, uint64_t))v140)(v51, a5);
    if (v69 != 64)
    {
      long long v68 = v137;
      goto LABEL_35;
    }
    char v70 = dispatch thunk of static BinaryInteger.isSigned.getter();
    long long v68 = v137;
    if (v70) {
      goto LABEL_35;
    }
  }
  else
  {
    ((void (*)(unsigned char *, uint64_t))v140)(v51, a5);
    long long v68 = v137;
  }
  long long v71 = v128;
  v139(v128, v45, a5);
  v142[0] = 0x7FFFFFFFFFFFFFFFLL;
  char v72 = dispatch thunk of static BinaryInteger.isSigned.getter();
  uint64_t v73 = dispatch thunk of BinaryInteger.bitWidth.getter();
  if ((v72 & 1) == 0)
  {
    if (v73 > 63)
    {
      *(void *)&v141.flags = 0x7FFFFFFFFFFFFFFFLL;
      unint64_t v95 = v133;
      (*(void (**)(unsigned char *, unsigned char *, uint64_t))(v136 + 32))(v133, v71, a5);
      lazy protocol witness table accessor for type Int64 and conformance Int64();
      unint64_t v96 = v127;
      dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
      char v97 = dispatch thunk of static Comparable.< infix(_:_:)();
      unint64_t v98 = v140;
      ((void (*)(unsigned char *, uint64_t))v140)(v96, a5);
      unint64_t v99 = v95;
      long long v68 = v137;
      v140 = v98;
      ((void (*)(unsigned char *, uint64_t))v98)(v99, a5);
      uint64_t v53 = v138;
      if ((v97 & 1) == 0) {
        goto LABEL_35;
      }
LABEL_52:
      __break(1u);
      goto LABEL_53;
    }
LABEL_32:
    uint64_t v78 = dispatch thunk of BinaryInteger._lowWord.getter();
    ((void (*)(unsigned char *, uint64_t))v140)(v71, a5);
    uint64_t v53 = v138;
    if (v142[0] >= v78) {
      goto LABEL_35;
    }
    goto LABEL_52;
  }
  if (v73 <= 64) {
    goto LABEL_32;
  }
  lazy protocol witness table accessor for type Int64 and conformance Int64();
  long long v74 = v133;
  dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
  char v75 = dispatch thunk of static Comparable.< infix(_:_:)();
  long long v76 = v74;
  long long v68 = v137;
  long long v77 = (void (*)(unsigned char *, uint64_t))v140;
  ((void (*)(unsigned char *, uint64_t))v140)(v76, a5);
  v77(v71, a5);
  uint64_t v53 = v138;
  if (v75) {
    goto LABEL_52;
  }
LABEL_35:
  uint64_t v79 = v68;
  v134 = (unsigned char *)dispatch thunk of BinaryInteger._lowWord.getter();
  ((void (*)(unsigned char *, uint64_t))v140)(v45, a5);
  v128 = *(unsigned char **)(*(void *)(v13 + 32) + 8);
  uint64_t v80 = type metadata accessor for ClosedRange();
  long long v81 = v139;
  v139(v53, (unsigned char *)(v132 + *(int *)(v80 + 36)), a5);
  char v82 = dispatch thunk of static BinaryInteger.isSigned.getter();
  unint64_t v83 = v131;
  v81(v131, v53, a5);
  if ((v82 & 1) == 0)
  {
    ((void (*)(unsigned char *, uint64_t))v140)(v83, a5);
    v85 = v79;
    goto LABEL_42;
  }
  uint64_t v84 = dispatch thunk of BinaryInteger.bitWidth.getter();
  ((void (*)(unsigned char *, uint64_t))v140)(v83, a5);
  v85 = v79;
  if (v84 <= 64) {
    goto LABEL_42;
  }
  uint64_t v86 = (uint64_t)v125;
  v139(v125, v53, a5);
  v142[0] = 0x8000000000000000;
  if (dispatch thunk of static BinaryInteger.isSigned.getter())
  {
    if (dispatch thunk of BinaryInteger.bitWidth.getter() >= 64)
    {
      lazy protocol witness table accessor for type Int64 and conformance Int64();
      v110 = v133;
      dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
      char v111 = dispatch thunk of static Comparable.< infix(_:_:)();
      v112 = v110;
      v85 = v137;
      v113 = (void (*)(uint64_t, uint64_t))v140;
      ((void (*)(unsigned char *, uint64_t))v140)(v112, a5);
      v113(v86, a5);
      if ((v111 & 1) == 0) {
        goto LABEL_42;
      }
    }
    else
    {
      uint64_t v87 = dispatch thunk of BinaryInteger._lowWord.getter();
      ((void (*)(uint64_t, uint64_t))v140)(v86, a5);
      if (v87 >= v142[0]) {
        goto LABEL_42;
      }
    }
    goto LABEL_73;
  }
  char v104 = dispatch thunk of static BinaryInteger.isSigned.getter();
  uint64_t v105 = dispatch thunk of BinaryInteger.bitWidth.getter();
  if ((v104 & 1) == 0)
  {
    if (v105 >= 64)
    {
LABEL_74:
      ((void (*)(uint64_t, uint64_t))v140)(v86, a5);
      uint64_t v53 = v138;
      goto LABEL_42;
    }
    uint64_t v114 = dispatch thunk of BinaryInteger._lowWord.getter();
    ((void (*)(uint64_t, uint64_t))v140)(v86, a5);
    uint64_t v53 = v138;
    if (v114 >= v142[0]) {
      goto LABEL_42;
    }
    goto LABEL_73;
  }
  if (v105 > 64)
  {
    lazy protocol witness table accessor for type Int64 and conformance Int64();
    long long v106 = v133;
    dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
    char v107 = dispatch thunk of static Comparable.< infix(_:_:)();
    unint64_t v108 = v106;
    unint64_t v109 = (void (*)(uint64_t, uint64_t))v140;
    ((void (*)(unsigned char *, uint64_t))v140)(v108, a5);
    v109(v86, a5);
    uint64_t v53 = v138;
    if ((v107 & 1) == 0) {
      goto LABEL_42;
    }
    goto LABEL_73;
  }
  swift_getAssociatedConformanceWitness();
  dispatch thunk of _ExpressibleByBuiltinIntegerLiteral.init(_builtinIntegerLiteral:)();
  uint64_t v115 = v133;
  dispatch thunk of ExpressibleByIntegerLiteral.init(integerLiteral:)();
  char v116 = dispatch thunk of static Comparable.< infix(_:_:)();
  ((void (*)(unsigned char *, uint64_t))v140)(v115, a5);
  uint64_t v65 = (uint64_t)v120;
  (*(void (**)(unsigned char *, uint64_t, uint64_t))(v136 + 32))(v120, v86, a5);
  if (v116) {
    goto LABEL_77;
  }
  uint64_t v117 = v142[0];
  uint64_t v86 = dispatch thunk of BinaryInteger._lowWord.getter();
  ((void (*)(uint64_t, uint64_t))v140)(v65, a5);
  BOOL v55 = v86 < v117;
  uint64_t v53 = v138;
  if (v55)
  {
LABEL_73:
    __break(1u);
    goto LABEL_74;
  }
LABEL_42:
  uint64_t v88 = dispatch thunk of BinaryInteger.bitWidth.getter();
  v139(v85, v53, a5);
  if (v88 >= 65)
  {
    ((void (*)(unsigned char *, uint64_t))v140)(v85, a5);
    goto LABEL_46;
  }
  uint64_t v89 = dispatch thunk of BinaryInteger.bitWidth.getter();
  ((void (*)(unsigned char *, uint64_t))v140)(v85, a5);
  if (v89 == 64 && (dispatch thunk of static BinaryInteger.isSigned.getter() & 1) == 0)
  {
LABEL_46:
    v139(v135, v53, a5);
    v142[0] = 0x7FFFFFFFFFFFFFFFLL;
    char v90 = dispatch thunk of static BinaryInteger.isSigned.getter();
    uint64_t v91 = dispatch thunk of BinaryInteger.bitWidth.getter();
    if (v90)
    {
      if (v91 > 64)
      {
        lazy protocol witness table accessor for type Int64 and conformance Int64();
        unint64_t v92 = v133;
        dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
        unint64_t v93 = v135;
LABEL_54:
        uint64_t v65 = dispatch thunk of static Comparable.< infix(_:_:)();
        unint64_t v100 = (void (*)(unsigned char *, uint64_t))v140;
        ((void (*)(unsigned char *, uint64_t))v140)(v92, a5);
        v100(v93, a5);
        uint64_t v53 = v138;
        if ((v65 & 1) == 0) {
          goto LABEL_55;
        }
        __break(1u);
        goto LABEL_76;
      }
    }
    else if (v91 > 63)
    {
LABEL_53:
      *(void *)&v141.flags = 0x7FFFFFFFFFFFFFFFLL;
      unint64_t v93 = v133;
      (*(void (**)(unsigned char *, unsigned char *, uint64_t))(v136 + 32))(v133, v135, a5);
      lazy protocol witness table accessor for type Int64 and conformance Int64();
      unint64_t v92 = v127;
      dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
      goto LABEL_54;
    }
    v94 = v135;
    dispatch thunk of BinaryInteger._lowWord.getter();
    ((void (*)(unsigned char *, uint64_t))v140)(v94, a5);
    uint64_t v53 = v138;
  }
LABEL_55:
  int64_t v101 = dispatch thunk of BinaryInteger._lowWord.getter();
  ((void (*)(unsigned char *, uint64_t))v140)(v53, a5);
  if (BNNSRandomFillUniformInt(v130, &v143, (int64_t)v134, v101))
  {
    uint64_t v102 = v129;
    if (v143.data) {
      MEMORY[0x1D26009C0](v143.data, -1, -1);
    }
    _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&v141);
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v141, (uint64_t)v142);
  }
  else
  {
    BNNSNDArrayDescriptor v141 = v143;
    _sSo21BNNSNDArrayDescriptoraSgWOi_((uint64_t)&v141);
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v141, (uint64_t)v142);
    uint64_t v102 = v129;
  }
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v142, v102);
}

unint64_t lazy protocol witness table accessor for type Int64 and conformance Int64()
{
  unint64_t result = lazy protocol witness table cache variable for type Int64 and conformance Int64;
  if (!lazy protocol witness table cache variable for type Int64 and conformance Int64)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Int64 and conformance Int64);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type BNNS.RandomGeneratorMethod and conformance BNNS.RandomGeneratorMethod()
{
  unint64_t result = lazy protocol witness table cache variable for type BNNS.RandomGeneratorMethod and conformance BNNS.RandomGeneratorMethod;
  if (!lazy protocol witness table cache variable for type BNNS.RandomGeneratorMethod and conformance BNNS.RandomGeneratorMethod)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.RandomGeneratorMethod and conformance BNNS.RandomGeneratorMethod);
  }
  return result;
}

uint64_t sub_1D20FD28C@<X0>(uint64_t *a1@<X8>)
{
  uint64_t result = BNNS.RandomGenerator.state.getter();
  *a1 = result;
  return result;
}

void sub_1D20FD2B8()
{
}

unsigned char *storeEnumTagSinglePayload for BNNS.RandomGeneratorMethod(unsigned char *result, int a2, int a3)
{
  if ((a3 + 1) >= 0x10000) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) < 0x100) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2)
  {
    switch(v5)
    {
      case 1:
        *uint64_t result = a2;
        return result;
      case 2:
        *(_WORD *)uint64_t result = a2;
        return result;
      case 3:
        goto LABEL_19;
      case 4:
        *(_DWORD *)uint64_t result = a2;
        return result;
      default:
        return result;
    }
  }
  switch(v5)
  {
    case 1:
      *uint64_t result = 0;
      break;
    case 2:
      *(_WORD *)uint64_t result = 0;
      break;
    case 3:
LABEL_19:
      __break(1u);
      JUMPOUT(0x1D20FD368);
    case 4:
      *(_DWORD *)uint64_t result = 0;
      break;
    default:
      return result;
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.RandomGeneratorMethod()
{
  return &type metadata for BNNS.RandomGeneratorMethod;
}

uint64_t method lookup function for BNNS.RandomGeneratorState(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.RandomGeneratorState);
}

uint64_t method lookup function for BNNS.RandomGenerator(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.RandomGenerator);
}

uint64_t dispatch thunk of BNNS.RandomGenerator.__allocating_init(method:seed:filterParameters:)(uint64_t a1, uint64_t a2, char a3)
{
  return (*(uint64_t (**)(uint64_t, uint64_t, void))(v3 + 120))(a1, a2, a3 & 1);
}

uint64_t dispatch thunk of BNNS.RandomGenerator.state.getter()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 128))();
}

uint64_t dispatch thunk of BNNS.RandomGenerator.state.setter()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 136))();
}

uint64_t dispatch thunk of BNNS.RandomGenerator.state.modify()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 144))();
}

void vImage.PixelBuffer<>.clip(to:destination:)(uint64_t *a1, uint64_t a2, uint64_t a3, float a4, float a5)
{
  uint64_t v21 = *MEMORY[0x1E4F143B8];
  uint64_t v10 = *a1;
  uint64_t v11 = *v5;
  swift_bridgeObjectRetain();
  uint64_t v12 = vImage.PixelBuffer<>.count.getter(a2, a3);
  uint64_t v20 = v10;
  uint64_t v13 = vImage.PixelBuffer<>.count.getter(a2, a3);
  swift_bridgeObjectRelease();
  if (v12 != v13)
  {
    __break(1u);
    goto LABEL_7;
  }
  vDSP_Length v14 = vImage.PixelBuffer<>.count.getter(a2, a3);
  float __C = a5;
  float __B = a4;
  uint64_t v20 = v11;
  uint64_t v15 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v15)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  uint64_t v16 = (const float *)v15;
  uint64_t v20 = v10;
  uint64_t v17 = (float *)vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v17) {
    goto LABEL_9;
  }
  if ((v14 & 0x8000000000000000) != 0)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  vDSP_vclip(v16, 1, &__B, &__C, v17, 1, v14);
}

void vImage.PixelBuffer<>.colorThreshold(_:destination:)(uint64_t *a1, uint64_t a2, uint64_t a3, float a4)
{
  __B[1] = *MEMORY[0x1E4F143B8];
  uint64_t v8 = *a1;
  uint64_t v9 = *v4;
  swift_bridgeObjectRetain();
  uint64_t v10 = vImage.PixelBuffer<>.count.getter(a2, a3);
  __B[0] = v8;
  uint64_t v11 = vImage.PixelBuffer<>.count.getter(a2, a3);
  swift_bridgeObjectRelease();
  if (v10 != v11)
  {
    __break(1u);
    goto LABEL_7;
  }
  vDSP_Length v12 = vImage.PixelBuffer<>.count.getter(a2, a3);
  __B[0] = v9;
  uint64_t v13 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v13)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  vDSP_Length v14 = (const float *)v13;
  __B[0] = v8;
  uint64_t v15 = (float *)vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v15) {
    goto LABEL_9;
  }
  *(float *)float __B = a4;
  float v17 = 0.0;
  float __C = 1.0;
  if ((v12 & 0x8000000000000000) != 0)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  uint64_t v16 = v15;
  vDSP_vthrsc(v14, 1, (const float *)__B, &__C, v15, 1, v12);
  vDSP_vclip(v16, 1, &v17, &__C, v16, 1, v12);
}

void vImage.PixelBuffer<>.linearInterpolate(bufferB:interpolationConstant:destination:)(uint64_t *a1, uint64_t *a2, uint64_t a3, uint64_t a4, float a5)
{
  uint64_t v9 = *a1;
  uint64_t v10 = *a2;
  uint64_t v11 = *v5;
  swift_bridgeObjectRetain();
  uint64_t v12 = vImage.PixelBuffer<>.count.getter(a3, a4);
  uint64_t v23 = v10;
  uint64_t v13 = vImage.PixelBuffer<>.count.getter(a3, a4);
  swift_bridgeObjectRelease();
  if (v12 != v13)
  {
    __break(1u);
    goto LABEL_9;
  }
  swift_bridgeObjectRetain();
  uint64_t v14 = vImage.PixelBuffer<>.count.getter(a3, a4);
  uint64_t v23 = v9;
  uint64_t v15 = vImage.PixelBuffer<>.count.getter(a3, a4);
  swift_bridgeObjectRelease();
  if (v14 != v15)
  {
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  vDSP_Length v16 = vImage.PixelBuffer<>.count.getter(a3, a4);
  uint64_t v23 = v11;
  uint64_t v17 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v17)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  uint64_t v18 = (const float *)v17;
  uint64_t v23 = v9;
  uint64_t v19 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v19)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  uint64_t v20 = (const float *)v19;
  uint64_t v23 = v10;
  uint64_t v21 = (float *)vImage.PixelBuffer<>.vImageBuffer.getter();
  if (v21)
  {
    float v22 = a5;
    if ((v16 & 0x8000000000000000) == 0)
    {
      vDSP_vintb(v18, 1, v20, 1, &v22, v21, 1, v16);
      return;
    }
    goto LABEL_10;
  }
LABEL_13:
  __break(1u);
}

uint64_t vImage.Error.init(vImageError:)@<X0>(uint64_t a1@<X0>, char *a2@<X8>)
{
  uint64_t result = vImage.Error.init(rawValue:)(a1, &v5);
  char v4 = v5;
  if (v5 == 20) {
    char v4 = 11;
  }
  *a2 = v4;
  return result;
}

uint64_t vImage.Error.init(rawValue:)@<X0>(uint64_t result@<X0>, char *a2@<X8>)
{
  char v2 = 2;
  switch(result)
  {
    case -21784:
      *a2 = 19;
      break;
    case -21783:
      *a2 = 18;
      break;
    case -21782:
      *a2 = 17;
      break;
    case -21781:
      *a2 = 16;
      break;
    case -21780:
      *a2 = 15;
      break;
    case -21779:
      *a2 = 14;
      break;
    case -21778:
      *a2 = 13;
      break;
    case -21777:
      *a2 = 12;
      break;
    case -21776:
      *a2 = 11;
      break;
    case -21775:
      *a2 = 10;
      break;
    case -21774:
      *a2 = 9;
      break;
    case -21773:
      *a2 = 8;
      break;
    case -21772:
      *a2 = 7;
      break;
    case -21771:
      *a2 = 6;
      break;
    case -21770:
      *a2 = 5;
      break;
    case -21769:
      *a2 = 4;
      break;
    case -21768:
      *a2 = 3;
      break;
    case -21767:
      goto LABEL_22;
    case -21766:
      char v2 = 1;
LABEL_22:
      *a2 = v2;
      break;
    default:
      if (result) {
        *a2 = 20;
      }
      else {
        *a2 = 0;
      }
      break;
  }
  return result;
}

uint64_t vImage.Error.rawValue.getter()
{
  return qword_1D21384E8[*v0];
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance vImage.Error(char *a1, char *a2)
{
  return qword_1D21384E8[*a1] == qword_1D21384E8[*a2];
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance vImage.Error()
{
  uint64_t v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(qword_1D21384E8[v1]);
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance vImage.Error()
{
  Hasher._combine(_:)(qword_1D21384E8[*v0]);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance vImage.Error()
{
  uint64_t v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(qword_1D21384E8[v1]);
  return Hasher._finalize()();
}

uint64_t protocol witness for RawRepresentable.init(rawValue:) in conformance vImage.Error@<X0>(uint64_t *a1@<X0>, char *a2@<X8>)
{
  return vImage.Error.init(rawValue:)(*a1, a2);
}

void protocol witness for RawRepresentable.rawValue.getter in conformance vImage.Error(void *a1@<X8>)
{
  *a1 = qword_1D21384E8[*v1];
}

uint64_t protocol witness for Error._code.getter in conformance vImage.Error(uint64_t a1, uint64_t a2)
{
  unint64_t v4 = lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
  unint64_t v5 = lazy protocol witness table accessor for type Int and conformance Int();

  return MEMORY[0x1F4185E20](a1, a2, v4, v5);
}

uint64_t getEnumTagSinglePayload for vImage.Error(unsigned __int8 *a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0xED) {
    goto LABEL_17;
  }
  if (a2 + 19 >= 0xFFFF00) {
    int v2 = 4;
  }
  else {
    int v2 = 2;
  }
  if ((a2 + 19) >> 8 < 0xFF) {
    int v3 = 1;
  }
  else {
    int v3 = v2;
  }
  if (v3 == 4)
  {
    int v4 = *(_DWORD *)(a1 + 1);
    if (v4) {
      return (*a1 | (v4 << 8)) - 19;
    }
  }
  else
  {
    if (v3 == 2)
    {
      int v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1)) {
        goto LABEL_17;
      }
      return (*a1 | (v4 << 8)) - 19;
    }
    int v4 = a1[1];
    if (a1[1]) {
      return (*a1 | (v4 << 8)) - 19;
    }
  }
LABEL_17:
  unsigned int v6 = *a1;
  BOOL v7 = v6 >= 0x14;
  int v8 = v6 - 20;
  if (!v7) {
    int v8 = -1;
  }
  return (v8 + 1);
}

unsigned char *storeEnumTagSinglePayload for vImage.Error(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 19 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 19) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xED) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xEC)
  {
    unsigned int v6 = ((a2 - 237) >> 8) + 1;
    *uint64_t result = a2 + 19;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D20FDDA8);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 19;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vImage.Error()
{
  return &type metadata for vImage.Error;
}

unint64_t lazy protocol witness table accessor for type Int and conformance Int()
{
  unint64_t result = lazy protocol witness table cache variable for type Int and conformance Int;
  if (!lazy protocol witness table cache variable for type Int and conformance Int)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Int and conformance Int);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type Int and conformance Int;
  if (!lazy protocol witness table cache variable for type Int and conformance Int)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Int and conformance Int);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type Int and conformance Int;
  if (!lazy protocol witness table cache variable for type Int and conformance Int)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Int and conformance Int);
  }
  return result;
}

__n128 static BNNS.quantize(batchSize:input:output:axis:scale:bias:filterParameters:)(size_t a1, _OWORD *a2, _OWORD *a3, uint64_t a4, char a5, uint64_t a6, uint64_t a7, uint32_t a8, size_t a9, int (__cdecl *a10)(void **, size_t, size_t), void (__cdecl *a11)(void *))
{
  static BNNS.quantize(batchSize:input:output:axis:scale:bias:filterParameters:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, 0);
  return result;
}

__n128 static BNNS.dequantize(batchSize:input:output:axis:scale:bias:filterParameters:)(size_t a1, _OWORD *a2, _OWORD *a3, uint64_t a4, char a5, uint64_t a6, uint64_t a7, uint32_t a8, size_t a9, int (__cdecl *a10)(void **, size_t, size_t), void (__cdecl *a11)(void *))
{
  static BNNS.quantize(batchSize:input:output:axis:scale:bias:filterParameters:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, 1);
  return result;
}

void static BNNS.quantize(batchSize:input:output:axis:scale:bias:filterParameters:)(size_t a1, _OWORD *a2, _OWORD *a3, uint64_t a4, char a5, uint64_t a6, uint64_t a7, uint32_t a8, size_t a9, int (__cdecl *a10)(void **, size_t, size_t), void (__cdecl *a11)(void *), char a12)
{
  specialized static BNNS.quantizeDequantize(_:batchSize:input:output:axis:scale:bias:filterParameters:)(a12, a1, a2, a3, a4, a5 & 1, a6, a7, a8, a9, a10, a11);
  if (v12) {
}
  }

uint64_t specialized static BNNS.quantizeDequantize(_:batchSize:input:output:axis:scale:bias:filterParameters:)(char a1, size_t a2, _OWORD *a3, _OWORD *a4, uint64_t a5, char a6, uint64_t a7, uint64_t a8, uint32_t a9, size_t a10, int (__cdecl *a11)(void **, size_t, size_t), void (__cdecl *a12)(void *))
{
  uint64_t v140 = *MEMORY[0x1E4F143B8];
  outlined init with take of BNNSNDArrayDescriptor?(a8, (uint64_t)v133);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v133, (uint64_t)v138);
  outlined init with take of BNNSNDArrayDescriptor?(a7, (uint64_t)v132);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v132, (uint64_t)v139);
  outlined init with take of BNNSNDArrayDescriptor?(a7, (uint64_t)v137);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v137) != 1)
  {
    *(_OWORD *)((char *)&v137[19] + 8) = v137[8];
    *(_OWORD *)((char *)&v137[20] + 8) = v137[9];
    *(_OWORD *)((char *)&v137[21] + 8) = v137[10];
    *(_OWORD *)((char *)&v137[15] + 8) = v137[4];
    *(_OWORD *)((char *)&v137[16] + 8) = v137[5];
    *(_OWORD *)((char *)&v137[17] + 8) = v137[6];
    *(_OWORD *)((char *)&v137[18] + 8) = v137[7];
    *(_OWORD *)((char *)&v137[11] + 8) = v137[0];
    *(_OWORD *)((char *)&v137[12] + 8) = v137[1];
    *(_OWORD *)((char *)&v137[13] + 8) = v137[2];
    *(_OWORD *)((char *)&v137[14] + 8) = v137[3];
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v130);
    outlined init with take of BNNS.Shape((uint64_t)v130, (uint64_t)&layer_params);
    uint64_t result = _s10Accelerate4BNNSO5ShapeOWOg((uint64_t)&layer_params);
    if (result)
    {
      __break(1u);
LABEL_23:
      __break(1u);
      return result;
    }
  }
  outlined init with take of BNNSNDArrayDescriptor?(a8, (uint64_t)v136);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v136) != 1)
  {
    *(_OWORD *)((char *)&v136[19] + 8) = v136[8];
    *(_OWORD *)((char *)&v136[20] + 8) = v136[9];
    *(_OWORD *)((char *)&v136[21] + 8) = v136[10];
    *(_OWORD *)((char *)&v136[15] + 8) = v136[4];
    *(_OWORD *)((char *)&v136[16] + 8) = v136[5];
    *(_OWORD *)((char *)&v136[17] + 8) = v136[6];
    *(_OWORD *)((char *)&v136[18] + 8) = v136[7];
    *(_OWORD *)((char *)&v136[11] + 8) = v136[0];
    *(_OWORD *)((char *)&v136[12] + 8) = v136[1];
    *(_OWORD *)((char *)&v136[13] + 8) = v136[2];
    *(_OWORD *)((char *)&v136[14] + 8) = v136[3];
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v130);
    outlined init with take of BNNS.Shape((uint64_t)v130, (uint64_t)&layer_params);
    uint64_t result = _s10Accelerate4BNNSO5ShapeOWOg((uint64_t)&layer_params);
    if (result) {
      goto LABEL_23;
    }
  }
  BNNSQuantizerFunction v20 = a1 & 1;
  uint64_t v21 = 1 << a5;
  if ((unint64_t)a5 >= 0x40) {
    uint64_t v21 = 0;
  }
  if (a6 & 1 | (a5 < 0) | ((unint64_t)(a5 - 65) < 0xFFFFFFFFFFFFFF7FLL)) {
    size_t v22 = 0;
  }
  else {
    size_t v22 = v21;
  }
  size_t v101 = v22;
  outlined init with take of BNNSNDArrayDescriptor?(a7, (uint64_t)v135);
  uint64_t v23 = 0;
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v135) == 1)
  {
    size_t v24 = 0;
    size_t v102 = 0;
    size_t v103 = 0;
    size_t v99 = 0;
    size_t v100 = 0;
    size_t v97 = 0;
    size_t v98 = 0;
    size_t v96 = 0;
    size_t v93 = 0;
    size_t v94 = 0;
    size_t v91 = 0;
    size_t v92 = 0;
    size_t v25 = 0;
    size_t v89 = 0;
    size_t v90 = 0;
    uint64_t v87 = 0;
    size_t v88 = 0;
    uint64_t v86 = 0;
    BNNSDataType v85 = 0;
    uint64_t v104 = 0;
    uint64_t v26 = 0;
    uint64_t v95 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v139, (uint64_t)v130);
    uint64_t v104 = *(void *)v130;
    size_t v24 = *(void *)&v130[8];
    size_t v102 = *(void *)&v130[24];
    size_t v103 = *(void *)&v130[16];
    size_t v99 = *(void *)&v130[40];
    size_t v100 = *(void *)&v130[32];
    size_t v97 = *(void *)&v130[56];
    size_t v98 = *(void *)&v130[48];
    size_t v96 = *(void *)&v130[64];
    size_t v93 = *(void *)&v130[80];
    size_t v94 = *(void *)&v130[72];
    size_t v25 = *(void *)&v130[96];
    size_t v91 = *(void *)&v130[104];
    size_t v92 = *(void *)&v130[88];
    size_t v89 = *(void *)&v130[120];
    size_t v90 = *(void *)&v130[112];
    uint64_t v87 = *(void **)&v130[136];
    size_t v88 = *(void *)&v130[128];
    uint64_t v95 = *(void *)&v130[144];
    uint64_t v86 = *(void **)&v130[152];
    uint64_t v26 = *(void *)&v130[164];
    int v84 = *(_DWORD *)&v130[172];
    BNNSDataType v85 = *(_DWORD *)&v130[160];
  }
  outlined init with take of BNNSNDArrayDescriptor?(a8, (uint64_t)v134);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v134) == 1)
  {
    BNNSDataType v28 = 0;
    uint64_t v29 = 0;
    uint64_t v30 = 0;
    long long v31 = 0uLL;
    long long v32 = 0uLL;
    long long v33 = 0uLL;
    long long v34 = 0uLL;
    long long v35 = 0uLL;
    long long v36 = 0uLL;
    long long v37 = 0uLL;
    long long v38 = 0uLL;
    uint64_t v39 = 0;
    uint64_t v40 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v138, (uint64_t)v130);
    uint64_t v39 = *(void *)v130;
    long long v31 = *(_OWORD *)&v130[8];
    long long v32 = *(_OWORD *)&v130[24];
    long long v33 = *(_OWORD *)&v130[40];
    long long v34 = *(_OWORD *)&v130[56];
    long long v35 = *(_OWORD *)&v130[72];
    long long v36 = *(_OWORD *)&v130[88];
    long long v37 = *(_OWORD *)&v130[104];
    long long v38 = *(_OWORD *)&v130[120];
    uint64_t v30 = *(void **)&v130[136];
    uint64_t v40 = *(void *)&v130[144];
    uint64_t v29 = *(void **)&v130[152];
    uint64_t v23 = *(void *)&v130[164];
    BNNSDataType v28 = *(_DWORD *)&v130[160];
    int v27 = *(_DWORD *)&v130[172];
  }
  long long v41 = a3[6];
  *(_OWORD *)&v130[116] = a3[7];
  long long v42 = a3[9];
  *(_OWORD *)&v130[132] = a3[8];
  *(_OWORD *)&v130[148] = v42;
  *(_OWORD *)&v130[164] = a3[10];
  long long v43 = a3[2];
  *(_OWORD *)&v130[52] = a3[3];
  long long v44 = a3[5];
  *(_OWORD *)&v130[68] = a3[4];
  *(_OWORD *)&v130[84] = v44;
  *(_OWORD *)&v130[100] = v41;
  long long v45 = a3[1];
  *(_OWORD *)&v130[4] = *a3;
  *(_OWORD *)&v130[20] = v45;
  *(_OWORD *)&v130[36] = v43;
  layer_params.axis_mask = v101;
  layer_params.function = v20;
  *((_DWORD *)&layer_params.i_desc.data_bias + 1) = *(_DWORD *)&v130[176];
  *(_OWORD *)((char *)&layer_params.i_desc.stride[6] + 4) = *(_OWORD *)&v130[128];
  *(_OWORD *)((char *)&layer_params.i_desc.data + 4) = *(_OWORD *)&v130[144];
  *(_OWORD *)((char *)&layer_params.i_desc.table_data + 4) = *(_OWORD *)&v130[160];
  *(_OWORD *)((char *)&layer_params.i_desc.size[6] + 4) = *(_OWORD *)&v130[64];
  *(_OWORD *)((char *)layer_params.i_desc.stride + 4) = *(_OWORD *)&v130[80];
  *(_OWORD *)((char *)&layer_params.i_desc.stride[2] + 4) = *(_OWORD *)&v130[96];
  *(_OWORD *)((char *)&layer_params.i_desc.stride[4] + 4) = *(_OWORD *)&v130[112];
  *(_OWORD *)(&layer_params.function + 1) = *(_OWORD *)v130;
  *(_OWORD *)((char *)layer_params.i_desc.size + 4) = *(_OWORD *)&v130[16];
  *(_OWORD *)((char *)&layer_params.i_desc.size[2] + 4) = *(_OWORD *)&v130[32];
  *(_OWORD *)((char *)&layer_params.i_desc.size[4] + 4) = *(_OWORD *)&v130[48];
  layer_params.scale.size[0] = v24;
  layer_params.scale.size[1] = v103;
  layer_params.scale.size[2] = v102;
  layer_params.scale.size[3] = v100;
  layer_params.scale.size[4] = v99;
  layer_params.scale.size[5] = v98;
  layer_params.scale.size[6] = v97;
  layer_params.scale.size[7] = v96;
  layer_params.scale.stride[0] = v94;
  layer_params.scale.stride[1] = v93;
  layer_params.scale.stride[2] = v92;
  layer_params.scale.stride[3] = v25;
  layer_params.scale.stride[4] = v91;
  layer_params.scale.stride[5] = v90;
  layer_params.scale.stride[6] = v89;
  layer_params.scale.stride[7] = v88;
  layer_params.scale.data = v87;
  layer_params.scale.table_data = v86;
  layer_params.scale.table_data_type = v85;
  *((_DWORD *)&layer_params.scale.data_bias + 1) = v84;
  layer_params.bias.data = v30;
  layer_params.bias.table_data = v29;
  layer_params.bias.table_data_type = v28;
  *(void *)&layer_params.scale.data_scale = v26;
  *(_OWORD *)layer_params.bias.size = v31;
  *(_OWORD *)&layer_params.bias.size[2] = v32;
  *(_OWORD *)&layer_params.bias.size[4] = v33;
  *(_OWORD *)&layer_params.bias.size[6] = v34;
  *(_OWORD *)layer_params.bias.stride = v35;
  *(_OWORD *)&layer_params.bias.stride[2] = v36;
  *(_OWORD *)&layer_params.bias.stride[4] = v37;
  *(_OWORD *)&layer_params.bias.stride[6] = v38;
  *(void *)&layer_params.bias.data_scale = v23;
  *((_DWORD *)&layer_params.bias.data_bias + 1) = v27;
  long long v46 = a4[9];
  *(_OWORD *)&layer_params.o_desc.stride[7] = a4[8];
  *(_OWORD *)&layer_params.o_desc.data_type = v46;
  *(_OWORD *)&layer_params.o_desc.table_data_type = a4[10];
  long long v47 = a4[5];
  *(_OWORD *)&layer_params.o_desc.size[7] = a4[4];
  *(_OWORD *)&layer_params.o_desc.stride[1] = v47;
  long long v48 = a4[6];
  *(_OWORD *)&layer_params.o_desc.stride[5] = a4[7];
  *(_OWORD *)&layer_params.o_desc.stride[3] = v48;
  long long v49 = a4[1];
  *(_OWORD *)&layer_params.o_desc.flags = *a4;
  *(_OWORD *)&layer_params.o_desc.size[1] = v49;
  long long v50 = a4[2];
  *(_OWORD *)&layer_params.o_desc.size[5] = a4[3];
  *(_OWORD *)&layer_params.o_desc.size[3] = v50;
  *(void *)&layer_params.scale.flags = v104;
  *(void *)&layer_params.scale.data_type = v95;
  *(void *)&layer_params.bias.flags = v39;
  *(void *)&layer_params.bias.data_type = v40;
  if (a11 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v127);
    outlined init with take of BNNS.Shape((uint64_t)v127, (uint64_t)v128);
    outlined init with take of BNNS.Shape((uint64_t)v128, (uint64_t)v129);
    BNNS.Shape.size.getter((uint64_t)&v119);
    unint64_t v51 = v119;
    unint64_t v52 = v120;
    unint64_t v53 = v121;
    unint64_t v54 = v122;
    unint64_t v55 = v123;
    unint64_t v56 = v124;
    unint64_t v57 = v125;
    unint64_t v105 = v126;
    outlined init with take of BNNS.Shape((uint64_t)v128, (uint64_t)v129);
    BNNS.Shape.stride.getter((uint64_t)&v119);
    size_t v106 = specialized static BNNS.calculateBatchStride(size:stride:)(v51, v52, v53, v54, v55, v56, v57, v105, v119, v120, v121, v122, v123, v124, v125, v126);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v119);
    outlined init with take of BNNS.Shape((uint64_t)&v119, (uint64_t)v129);
    outlined init with take of BNNS.Shape((uint64_t)v129, (uint64_t)v118);
    BNNS.Shape.size.getter((uint64_t)&v109);
    unint64_t v58 = v109;
    unint64_t v59 = v110;
    unint64_t v60 = v111;
    unint64_t v61 = v112;
    unint64_t v62 = v113;
    unint64_t v63 = v114;
    unint64_t v65 = v115;
    unint64_t v64 = v116;
    outlined init with take of BNNS.Shape((uint64_t)v129, (uint64_t)v118);
    BNNS.Shape.stride.getter((uint64_t)&v109);
    size_t v66 = specialized static BNNS.calculateBatchStride(size:stride:)(v58, v59, v60, v61, v62, v63, v65, v64, v109, v110, v111, v112, v113, v114, v115, v116);
    p_BNNSFilterParameters filter_params = 0;
  }
  else
  {
    filter_params.flags = a9;
    filter_params.n_threads = a10;
    filter_params.alloc_memory = a11;
    filter_params.free_memory = a12;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v127);
    outlined init with take of BNNS.Shape((uint64_t)v127, (uint64_t)v128);
    outlined init with take of BNNS.Shape((uint64_t)v128, (uint64_t)v129);
    BNNS.Shape.size.getter((uint64_t)&v119);
    unint64_t v68 = v119;
    unint64_t v69 = v120;
    unint64_t v70 = v121;
    unint64_t v71 = v122;
    unint64_t v72 = v123;
    unint64_t v73 = v124;
    unint64_t v74 = v125;
    unint64_t v107 = v126;
    outlined init with take of BNNS.Shape((uint64_t)v128, (uint64_t)v129);
    BNNS.Shape.stride.getter((uint64_t)&v119);
    size_t v106 = specialized static BNNS.calculateBatchStride(size:stride:)(v68, v69, v70, v71, v72, v73, v74, v107, v119, v120, v121, v122, v123, v124, v125, v126);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v119);
    outlined init with take of BNNS.Shape((uint64_t)&v119, (uint64_t)v129);
    outlined init with take of BNNS.Shape((uint64_t)v129, (uint64_t)v118);
    BNNS.Shape.size.getter((uint64_t)&v109);
    unint64_t v75 = v109;
    unint64_t v76 = v110;
    unint64_t v77 = v111;
    unint64_t v78 = v112;
    unint64_t v79 = v113;
    unint64_t v80 = v114;
    unint64_t v82 = v115;
    unint64_t v81 = v116;
    outlined init with take of BNNS.Shape((uint64_t)v129, (uint64_t)v118);
    BNNS.Shape.stride.getter((uint64_t)&v109);
    size_t v66 = specialized static BNNS.calculateBatchStride(size:stride:)(v75, v76, v77, v78, v79, v80, v82, v81, v109, v110, v111, v112, v113, v114, v115, v116);
    p_BNNSFilterParameters filter_params = &filter_params;
  }
  uint64_t result = BNNSDirectApplyQuantizer(&layer_params, p_filter_params, a2, v106, v66);
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *unint64_t v83 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.computeNorm(input:output:axes:)(_OWORD *a1, _OWORD *a2, uint64_t a3)
{
  uint64_t v18 = *MEMORY[0x1E4F143B8];
  long long v3 = a1[9];
  *(_OWORD *)&src.stride[7] = a1[8];
  *(_OWORD *)&src.data_type = v3;
  *(_OWORD *)&src.table_data_type = a1[10];
  long long v4 = a1[5];
  *(_OWORD *)&src.size[7] = a1[4];
  *(_OWORD *)&src.stride[1] = v4;
  long long v5 = a1[7];
  *(_OWORD *)&src.stride[3] = a1[6];
  *(_OWORD *)&src.stride[5] = v5;
  long long v6 = a1[1];
  *(_OWORD *)&src.flags = *a1;
  *(_OWORD *)&src.size[1] = v6;
  long long v7 = a1[3];
  *(_OWORD *)&src.size[3] = a1[2];
  *(_OWORD *)&src.size[5] = v7;
  long long v8 = a2[9];
  *(_OWORD *)&v16.stride[7] = a2[8];
  *(_OWORD *)&v16.data_type = v8;
  *(_OWORD *)&v16.table_data_type = a2[10];
  long long v9 = a2[5];
  *(_OWORD *)&v16.size[7] = a2[4];
  *(_OWORD *)&v16.stride[1] = v9;
  long long v10 = a2[7];
  *(_OWORD *)&v16.stride[3] = a2[6];
  *(_OWORD *)&v16.stride[5] = v10;
  long long v11 = a2[1];
  *(_OWORD *)&v16.flags = *a2;
  *(_OWORD *)&v16.size[1] = v11;
  long long v12 = a2[3];
  *(_OWORD *)&v16.size[3] = a2[2];
  *(_OWORD *)&v16.size[5] = v12;
  uint32_t v13 = specialized static BNNS.computeAxisFlags(_:)(a3);
  uint64_t result = BNNSComputeNorm(&v16, &src, BNNSL2Norm, v13);
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    unsigned char *v15 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.computeNormBackward(input:output:axes:outputGradient:generatingInputGradient:)(uint64_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, _OWORD *a5)
{
  v27[1] = *MEMORY[0x1E4F143B8];
  long long v7 = a5[9];
  *(_OWORD *)&in_delta.stride[7] = a5[8];
  *(_OWORD *)&in_delta.data_type = v7;
  *(_OWORD *)&in_delta.table_data_type = a5[10];
  long long v8 = a5[5];
  *(_OWORD *)&in_delta.size[7] = a5[4];
  *(_OWORD *)&in_delta.stride[1] = v8;
  long long v9 = a5[7];
  *(_OWORD *)&in_delta.stride[3] = a5[6];
  *(_OWORD *)&in_delta.stride[5] = v9;
  long long v10 = a5[1];
  *(_OWORD *)&in_delta.flags = *a5;
  *(_OWORD *)&in_delta.size[1] = v10;
  long long v11 = a5[3];
  *(_OWORD *)&in_delta.size[3] = a5[2];
  *(_OWORD *)&in_delta.size[5] = v11;
  long long v12 = a4[9];
  *(_OWORD *)&v22.stride[7] = a4[8];
  *(_OWORD *)&v22.data_type = v12;
  *(_OWORD *)&v22.table_data_type = a4[10];
  long long v13 = a4[5];
  *(_OWORD *)&v22.size[7] = a4[4];
  *(_OWORD *)&v22.stride[1] = v13;
  long long v14 = a4[7];
  *(_OWORD *)&v22.stride[3] = a4[6];
  *(_OWORD *)&v22.stride[5] = v14;
  long long v15 = a4[1];
  *(_OWORD *)&v22.flags = *a4;
  *(_OWORD *)&v22.size[1] = v15;
  long long v16 = a4[3];
  *(_OWORD *)&v22.size[3] = a4[2];
  *(_OWORD *)&v22.size[5] = v16;
  outlined init with take of UnsafeMutableRawPointer?(a1 + 136, (uint64_t)v25);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v25, (uint64_t)&v26);
  uint64_t v17 = v26;
  if (!v26
    || (outlined init with take of UnsafeMutableRawPointer?(a2 + 136, (uint64_t)v24),
        outlined init with take of UnsafeMutableRawPointer?((uint64_t)v24, (uint64_t)v27),
        (uint64_t v18 = (const void *)v27[0]) == 0)
    || (uint32_t v19 = specialized static BNNS.computeAxisFlags(_:)(a3),
        uint64_t result = BNNSComputeNormBackward(v17, &in_delta, v18, &v22, BNNSL2Norm, v19),
        result))
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *uint64_t v21 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t BLAS.ThreadingModel.init(rawValue:)@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = result;
  return result;
}

void static BLAS.ThreadingModel.multiThreaded.getter(_DWORD *a1@<X8>)
{
  *a1 = 0;
}

void static BLAS.ThreadingModel.singleThreaded.getter(_DWORD *a1@<X8>)
{
  *a1 = 1;
}

uint64_t BLAS.ThreadingModel.rawValue.getter()
{
  return *v0;
}

uint64_t BLAS.ThreadingModel.rawValue.setter(uint64_t result)
{
  *uint64_t v1 = result;
  return result;
}

uint64_t (*BLAS.ThreadingModel.rawValue.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t static BLAS.threadingModel.getter@<X0>(_DWORD *a1@<X8>)
{
  uint64_t result = BLASGetThreading();
  *a1 = result;
  return result;
}

uint64_t static BLAS.threadingModel.setter(unsigned int *a1)
{
  return MEMORY[0x1F40D0EE0](*a1);
}

uint64_t (*static BLAS.threadingModel.modify(_DWORD *a1))(unsigned int *a1)
{
  *a1 = BLASGetThreading();
  return static BLAS.threadingModel.modify;
}

uint64_t static BLAS.threadingModel.modify(unsigned int *a1)
{
  return MEMORY[0x1F40D0EE0](*a1);
}

ValueMetadata *type metadata accessor for BLAS()
{
  return &type metadata for BLAS;
}

ValueMetadata *type metadata accessor for BLAS.ThreadingModel()
{
  return &type metadata for BLAS.ThreadingModel;
}

uint64_t vImageConverterRef.sourceBuffers(colorSpace:)(void *a1)
{
  return vImageConverterRef.sourceBuffers(colorSpace:)(a1, MEMORY[0x1E4F16FE0], MEMORY[0x1E4F16FD8]);
}

uint64_t vImageConverterRef.sourceBufferCount.getter()
{
  return vImageConverterRef.sourceBufferCount.getter(MEMORY[0x1E4F16FD8]);
}

uint64_t vImageConverterRef.destinationBuffers(colorSpace:)(void *a1)
{
  return vImageConverterRef.sourceBuffers(colorSpace:)(a1, MEMORY[0x1E4F16FC8], MEMORY[0x1E4F16FD0]);
}

uint64_t vImageConverterRef.sourceBuffers(colorSpace:)(void *a1, uint64_t (*a2)(uint64_t), uint64_t (*a3)(uint64_t))
{
  uint64_t v5 = v3;
  long long v7 = (const void *)a2(v3);
  uint64_t result = a3(v5);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    long long v9 = specialized _copyCollectionToContiguousArray<A>(_:)(v7, result);
    long long v10 = a1;
    uint64_t v11 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSays6UInt32VG_10Accelerate6vImageO10BufferTypeOSgs5NeverOTg507_sSo18vf15ConverterRefa10e41E13sourceBuffers10colorSpaceSayAC01vA0O10gh26OSgGSo07CGColorhC0a_tFAJs6D6VXEfU_So07CGColorP3RefaTf1cn_nTf4ng_nTm((uint64_t)v9, v10);
    swift_release();

    return v11;
  }
  return result;
}

uint64_t vImageConverterRef.destinationBufferCount.getter()
{
  return vImageConverterRef.sourceBufferCount.getter(MEMORY[0x1E4F16FD0]);
}

uint64_t vImageConverterRef.sourceBufferCount.getter(uint64_t (*a1)(uint64_t))
{
  uint64_t result = a1(v1);
  if (result < 0) {
    __break(1u);
  }
  return result;
}

uint64_t vImageConverterRef.mustOperateOutOfPlace(source:destination:flags:)(void *a1, vImagePixelCount a2, vImagePixelCount a3, size_t a4, void *a5, vImagePixelCount a6, vImagePixelCount a7, size_t a8, vImage_Flags *a9)
{
  uint64_t v20 = *MEMORY[0x1E4F143B8];
  vImage_Flags v10 = *a9;
  srcs.char data = a1;
  srcs.height = a2;
  srcs.width = a3;
  srcs.rowBytes = a4;
  dests.char data = a5;
  dests.height = a6;
  dests.width = a7;
  dests.rowBytes = a8;
  vImage_Error MustOperateOutOfPlace = vImageConverter_MustOperateOutOfPlace(v9, &srcs, &dests, v10);
  if (MustOperateOutOfPlace == -21780)
  {
    char v13 = 1;
  }
  else
  {
    uint64_t v12 = MustOperateOutOfPlace;
    if (MustOperateOutOfPlace)
    {
      lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
      swift_allocError();
      long long v15 = v14;
      vImage.Error.init(rawValue:)(v12, (char *)&srcs);
      char data = (char)srcs.data;
      if (LOBYTE(srcs.data) == 20) {
        char data = 11;
      }
      char *v15 = data;
      swift_willThrow();
    }
    else
    {
      char v13 = 0;
    }
  }
  return v13 & 1;
}

vImageConverterRef static vImageConverterRef.make(sourceFormat:destinationFormat:flags:)(uint64_t a1, uint64_t a2, vImage_Flags *a3)
{
  uint64_t v15 = *MEMORY[0x1E4F143B8];
  vImage_Flags v3 = *a3;
  vImage_Error error = 0;
  long long v4 = *(_OWORD *)(a2 + 16);
  *(_OWORD *)&destFormat.bitsPerComponent = *(_OWORD *)a2;
  *(_OWORD *)&destFormat.bitmapInfo = v4;
  *(void *)&destFormat.renderingIntent = *(void *)(a2 + 32);
  long long v5 = *(_OWORD *)(a1 + 16);
  *(_OWORD *)&srcFormat.bitsPerComponent = *(_OWORD *)a1;
  *(_OWORD *)&srcFormat.bitmapInfo = v5;
  *(void *)&srcFormat.renderingIntent = *(void *)(a1 + 32);
  vImageConverterRef v6 = vImageConverter_CreateWithCGImageFormat(&srcFormat, &destFormat, 0, v3, &error);
  vImageConverterRef v7 = v6;
  vImage_Error v8 = error;
  if (error)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    if ((unint64_t)(v8 + 21784) >= 0x13) {
      char v10 = 11;
    }
    else {
      char v10 = -5 - v8;
    }
  }
  else
  {
    if (v6) {
      return v7;
    }
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    char v10 = 11;
  }
  char *v9 = v10;
  swift_willThrow();
  return v7;
}

vImageConverterRef static vImageConverterRef.make(sourceFormat:destinationFormat:flags:)(uint64_t a1, vImageCVImageFormat *a2, vImage_Flags *a3)
{
  uint64_t v13 = *MEMORY[0x1E4F143B8];
  vImage_Flags v3 = *a3;
  vImage_Error error = 0;
  long long v4 = *(_OWORD *)(a1 + 16);
  *(_OWORD *)&srcFormat.bitsPerComponent = *(_OWORD *)a1;
  *(_OWORD *)&srcFormat.bitmapInfo = v4;
  *(void *)&srcFormat.renderingIntent = *(void *)(a1 + 32);
  vImageConverterRef v5 = vImageConverter_CreateForCGToCVImageFormat(&srcFormat, a2, 0, v3, &error);
  vImageConverterRef v6 = v5;
  vImage_Error v7 = error;
  if (error)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    if ((unint64_t)(v7 + 21784) >= 0x13) {
      char v9 = 11;
    }
    else {
      char v9 = -5 - v7;
    }
  }
  else
  {
    if (v5) {
      return v6;
    }
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    char v9 = 11;
  }
  char *v8 = v9;
  swift_willThrow();
  return v6;
}

vImageConverterRef static vImageConverterRef.make(sourceFormat:destinationFormat:flags:)(vImageCVImageFormat *a1, uint64_t a2, vImage_Flags *a3)
{
  uint64_t v13 = *MEMORY[0x1E4F143B8];
  vImage_Flags v3 = *a3;
  vImage_Error error = -21776;
  long long v4 = *(_OWORD *)(a2 + 16);
  *(_OWORD *)&destFormat.bitsPerComponent = *(_OWORD *)a2;
  *(_OWORD *)&destFormat.bitmapInfo = v4;
  *(void *)&destFormat.renderingIntent = *(void *)(a2 + 32);
  vImageConverterRef v5 = vImageConverter_CreateForCVToCGImageFormat(a1, &destFormat, 0, v3, &error);
  vImageConverterRef v6 = v5;
  vImage_Error v7 = error;
  if (error)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    if ((unint64_t)(v7 + 21784) >= 0x13) {
      char v9 = 11;
    }
    else {
      char v9 = -5 - v7;
    }
  }
  else
  {
    if (v5) {
      return v6;
    }
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    char v9 = 11;
  }
  char *v8 = v9;
  swift_willThrow();
  return v6;
}

vImage_Error vImageConverterRef.convert(source:destination:flags:)(void *a1, vImagePixelCount a2, vImagePixelCount a3, size_t a4, vImage_Buffer *dests, vImage_Flags *a6)
{
  uint64_t v15 = *MEMORY[0x1E4F143B8];
  vImage_Flags v8 = *a6;
  srcs.char data = a1;
  srcs.height = a2;
  srcs.width = a3;
  srcs.rowBytes = a4;
  vImage_Error result = vImageConvert_AnyToAny(v6, &srcs, dests, 0, v8);
  if (result)
  {
    uint64_t v10 = result;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    uint64_t v12 = v11;
    vImage.Error.init(rawValue:)(v10, (char *)&srcs);
    char data = (char)srcs.data;
    if (LOBYTE(srcs.data) == 20) {
      char data = 11;
    }
    *uint64_t v12 = data;
    return swift_willThrow();
  }
  return result;
}

uint64_t _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSays6UInt32VG_10Accelerate6vImageO10BufferTypeOSgs5NeverOTg507_sSo18vf15ConverterRefa10e41E13sourceBuffers10colorSpaceSayAC01vA0O10gh26OSgGSo07CGColorhC0a_tFAJs6D6VXEfU_So07CGColorP3RefaTf1cn_nTf4ng_nTm(uint64_t a1, CGColorSpace *a2)
{
  int64_t v2 = *(void *)(a1 + 16);
  uint64_t v3 = MEMORY[0x1E4FBC860];
  if (v2)
  {
    uint64_t v14 = MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v2, 0);
    uint64_t v3 = v14;
    vImageConverterRef v6 = (unsigned int *)(a1 + 32);
    do
    {
      unsigned int v7 = *v6++;
      CGColorSpaceModel Model = CGColorSpaceGetModel(a2);
      vImage.BufferType.init(bufferTypeCode:model:)(v7, Model, &v13);
      char v9 = v13;
      uint64_t v14 = v3;
      unint64_t v11 = *(void *)(v3 + 16);
      unint64_t v10 = *(void *)(v3 + 24);
      if (v11 >= v10 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v10 > 1), v11 + 1, 1);
        uint64_t v3 = v14;
      }
      *(void *)(v3 + 16) = v11 + 1;
      *(unsigned char *)(v3 + v11 + 32) = v9;
      --v2;
    }
    while (v2);
  }
  return v3;
}

uint64_t vDSP.Biquad.init(coefficients:channelCount:sectionCount:ofType:)@<X0>(uint64_t result@<X0>, unint64_t a2@<X1>, unint64_t a3@<X2>, uint64_t *a4@<X8>)
{
  if (is_mul_ok(a2, 5uLL))
  {
    if (is_mul_ok(5 * a2, a3))
    {
      uint64_t v7 = result;
      if (*(void *)(result + 16) == 5 * a2 * a3)
      {
        type metadata accessor for vDSP.BiquadRef();
        swift_allocObject();
        vImage_Error result = vDSP.BiquadRef.init(coefficients:channelCount:sectionCount:ofType:)(v7, a2, a3);
        if (result)
        {
LABEL_8:
          *a4 = result;
          return result;
        }
      }
      else
      {
        swift_bridgeObjectRelease();
      }
      vImage_Error result = 0;
      goto LABEL_8;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t type metadata accessor for vDSP.BiquadRef()
{
  return __swift_instantiateGenericMetadata();
}

uint64_t vDSP.BiquadRef.__allocating_init(coefficients:channelCount:sectionCount:ofType:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return vDSP.BiquadRef.init(coefficients:channelCount:sectionCount:ofType:)(a1, a2, a3);
}

uint64_t vDSP.Biquad.apply<A>(input:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return Array.init(unsafeUninitializedCapacity:initializingWith:)();
}

uint64_t closure #1 in vDSP.Biquad.apply<A>(input:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v13 = type metadata accessor for vDSP.Biquad();
  uint64_t v14 = type metadata accessor for UnsafeMutableBufferPointer();
  uint64_t WitnessTable = swift_getWitnessTable();
  vDSP.Biquad.apply<A, B>(input:output:)(a4, a1, v13, a6, v14, a8, WitnessTable);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
  *a2 = result;
  return result;
}

uint64_t partial apply for closure #1 in vDSP.Biquad.apply<A>(input:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in vDSP.Biquad.apply<A>(input:)(a1, a2, v2[6], v2[7], v2[2], v2[3], v2[4], v2[5]);
}

uint64_t vDSP.Biquad.apply<A, B>(input:output:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  vImage_Flags v8 = v7;
  type metadata accessor for vDSP.BiquadRef();
  char v15 = isKnownUniquelyReferenced<A>(_:)();
  uint64_t v16 = *v7;
  if (v15) {
    return vDSP.BiquadRef.apply<A, B>(input:output:)(a1, a2, a4, a5, a6, a7);
  }
  uint64_t v22 = a5;
  uint64_t v23 = a6;
  uint64_t v17 = *(void *)(v16 + 24);
  uint64_t v18 = *(void *)(v16 + 32);
  swift_allocObject();
  uint64_t v19 = swift_bridgeObjectRetain();
  uint64_t result = vDSP.BiquadRef.init(coefficients:channelCount:sectionCount:ofType:)(v19, v17, v18);
  if (result)
  {
    uint64_t v21 = result;
    swift_release();
    uint64_t *v8 = v21;
    a6 = v23;
    a5 = v22;
    return vDSP.BiquadRef.apply<A, B>(input:output:)(a1, a2, a4, a5, a6, a7);
  }
  __break(1u);
  return result;
}

uint64_t vDSP.BiquadRef.init(coefficients:channelCount:sectionCount:ofType:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  long long v4 = v3;
  uint64_t v8 = *(void *)(*v4 + 80);
  uint64_t v9 = *(void *)(v8 - 8);
  MEMORY[0x1F4188790](a1);
  unint64_t v11 = (char *)&v20 - v10;
  uint64_t v22 = *(void *)(v12 + 88);
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  MEMORY[0x1F4188790](AssociatedTypeWitness);
  v4[2] = a1;
  v4[3] = a2;
  uint64_t v21 = a2;
  v4[4] = a3;
  swift_getAssociatedConformanceWitness();
  uint64_t v23 = a1;
  swift_bridgeObjectRetain();
  dispatch thunk of _ExpressibleByBuiltinFloatLiteral.init(_builtinFloatLiteral:)();
  uint64_t result = dispatch thunk of ExpressibleByFloatLiteral.init(floatLiteral:)();
  if (a3 < 0)
  {
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v15 = 2 * a3;
  if (2 * a3 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (__OFADD__(v15, 2))
  {
LABEL_10:
    __break(1u);
    return result;
  }
  uint64_t v16 = specialized Array.init(repeating:count:)((uint64_t)v11, v15 + 2, v8);
  (*(void (**)(char *, uint64_t))(v9 + 8))(v11, v8);
  v4[5] = v16;
  uint64_t v17 = swift_getAssociatedTypeWitness();
  uint64_t AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  uint64_t v19 = (*(uint64_t (**)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness + 16))(v21, v23, a3, v17, AssociatedConformanceWitness);
  swift_bridgeObjectRelease();
  if (v19)
  {
    v4[6] = v19;
  }
  else
  {
    swift_bridgeObjectRelease();
    swift_bridgeObjectRelease();
    type metadata accessor for vDSP.BiquadRef();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return (uint64_t)v4;
}

uint64_t vDSP.BiquadRef.apply<A, B>(input:output:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v7 = v6;
  uint64_t v14 = *v6;
  uint64_t v15 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a6 + 8) + 16))(a4);
  if (result >= v15) {
    uint64_t v17 = v15;
  }
  else {
    uint64_t v17 = result;
  }
  if (v17 < 0)
  {
    __break(1u);
  }
  else
  {
    uint64_t v18 = v7[3];
    uint64_t v19 = v7[6];
    if (v18 == 1)
    {
      uint64_t v21 = v7[4];
      swift_beginAccess();
      static vDSP.BiquadFunctions.applyBiquadSingle<A, B, C>(source:destination:delays:setup:sectionCount:count:)(a1, a2, v7 + 5, v19, v21, v17, a3, a4, *(void *)(v14 + 80), a5, a6);
      return swift_endAccess();
    }
    else
    {
      uint64_t v20 = *(void *)(v14 + 88);
      uint64_t v23 = a3;
      uint64_t v24 = a4;
      uint64_t v25 = a5;
      uint64_t v26 = a6;
      uint64_t v27 = v20;
      uint64_t v28 = v20;
      uint64_t v29 = a2;
      uint64_t v30 = v17;
      uint64_t v31 = v18;
      uint64_t v32 = v19;
      return (*(uint64_t (**)(uint64_t (*)(), unsigned char *, uint64_t, uint64_t, uint64_t))(a5 + 24))(partial apply for closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:), v22, MEMORY[0x1E4FBC848] + 8, a3, a5);
    }
  }
  return result;
}

uint64_t static vDSP.BiquadFunctions.applyBiquadSingle<A, B, C>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1, uint64_t a2, uint64_t *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  type metadata accessor for Array();
  Array.reserveCapacity(_:)(0);
  uint64_t v14 = *a3;
  swift_bridgeObjectRetain();
  if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && (v14 < 0 || (v14 & 0x4000000000000000) != 0))
  {
    if (MEMORY[0x1D25FF9C0](v14, a9))
    {
      type metadata accessor for _ArrayBuffer();
      swift_getWitnessTable();
      uint64_t v19 = Array.init<A>(_:)();
      destructiveProjectEnumData for BNNS.ActivationFunction(v19);
      swift_unknownObjectRetain();
      unint64_t v15 = _ContiguousArrayBuffer.firstElementAddress.getter();
      swift_release();
      goto LABEL_12;
    }
    swift_bridgeObjectRelease();
    unint64_t v15 = 0;
  }
  else
  {
    swift_bridgeObjectRelease();
    if (_swift_isClassOrObjCExistentialType()) {
      unint64_t v15 = (v14 & 0xFFFFFFFFFFFFFF8)
    }
          + ((*(unsigned __int8 *)(*(void *)(a9 - 8) + 80) + 32) & ~(unint64_t)*(unsigned __int8 *)(*(void *)(a9 - 8) + 80));
    else {
      unint64_t v15 = v14
    }
          + ((*(unsigned __int8 *)(*(void *)(a9 - 8) + 80) + 32) & ~(unint64_t)*(unsigned __int8 *)(*(void *)(a9 - 8) + 80));
  }
  if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && (v14 < 0 || (v14 & 0x4000000000000000) != 0))
  {
    specialized _ArrayBuffer._nonNative.getter(v14);
    swift_unknownObjectRetain();
    if (v15) {
      goto LABEL_12;
    }
    goto LABEL_11;
  }
  _swift_isClassOrObjCExistentialType();
  swift_bridgeObjectRetain();
  if (!v15) {
LABEL_11:
  }
    unint64_t v15 = ~*(_DWORD *)(*(void *)(a9 - 8) + 80) | 0xFFFFFFFFFFFFFF00;
LABEL_12:
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  uint64_t AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  (*(void (**)(uint64_t, uint64_t, unint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness + 24))(a1, a2, v15, a4, a5, a6, a7, a8, a10, a11, AssociatedTypeWitness, AssociatedConformanceWitness);
  return swift_unknownObjectRelease();
}

uint64_t static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, long long a9, uint64_t a10)
{
  _OWORD v11[2] = a6;
  _OWORD v11[3] = a7;
  v11[4] = a8;
  long long v12 = a9;
  uint64_t v13 = a10;
  uint64_t v14 = a2;
  uint64_t v15 = a5;
  uint64_t v16 = a4;
  uint64_t v17 = a3;
  return (*(uint64_t (**)(uint64_t (*)(), void *, uint64_t, uint64_t, uint64_t))(a8 + 24))(partial apply for closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:), v11, MEMORY[0x1E4FBC848] + 8, a6, a8);
}

uint64_t vDSP.BiquadRef.deinit(uint64_t a1)
{
  static vDSP.BiquadFunctions.destroySetup<A>(ofType:channelCount:biquadSetup:)(a1, *(void *)(v1 + 24), *(void *)(v1 + 48));
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  return v1;
}

uint64_t static vDSP.BiquadFunctions.destroySetup<A>(ofType:channelCount:biquadSetup:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  uint64_t AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  return (*(uint64_t (**)(uint64_t, uint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness + 40))(a2, a3, AssociatedTypeWitness, AssociatedConformanceWitness);
}

uint64_t vDSP.BiquadRef.__deallocating_deinit(uint64_t a1)
{
  vDSP.BiquadRef.deinit(a1);

  return swift_deallocClassInstance();
}

uint64_t closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)()
{
  swift_getAssociatedTypeWitness();
  uint64_t result = UnsafeBufferPointer.baseAddress.getter();
  if (result)
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(void))(v1 + 16))(partial apply for closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:));
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)(uint64_t a1, unint64_t a2, unint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  uint64_t v45 = a4;
  swift_getAssociatedTypeWitness();
  uint64_t result = UnsafeBufferPointer.baseAddress.getter();
  if (result)
  {
    if (a3)
    {
      uint64_t v42 = a11;
      long long v43 = v39;
      uint64_t v40 = a10;
      uint64_t v47 = 0;
      unint64_t v48 = a3;
      unint64_t v46 = a2 / a3;
      uint64_t v18 = MEMORY[0x1F4188790](result);
      uint64_t v44 = a5;
      uint64_t v19 = a6;
      v38[2] = a6;
      v38[3] = a7;
      v38[4] = a8;
      v38[5] = a9;
      uint64_t v41 = a9;
      v38[6] = v20;
      v38[7] = v21;
      v38[8] = v45;
      v38[9] = v22;
      uint64_t v45 = v18;
      uint64_t v23 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Range<UInt>);
      swift_getAssociatedTypeWitness();
      v39[0] = a7;
      uint64_t v24 = type metadata accessor for UnsafePointer();
      v39[1] = v24;
      unint64_t v25 = lazy protocol witness table accessor for type Range<UInt> and conformance <> Range<A>();
      uint64_t v49 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:), (uint64_t)v38, v23, v24, MEMORY[0x1E4FBC248], v25, MEMORY[0x1E4FBC278], v26);
      uint64_t v47 = 0;
      unint64_t v48 = a3;
      MEMORY[0x1F4188790](v49);
      uint64_t v27 = v39[0];
      v38[-8] = v19;
      v38[-7] = v27;
      uint64_t v29 = v40;
      uint64_t v28 = v41;
      v38[-6] = a8;
      v38[-5] = v28;
      uint64_t v30 = v42;
      v38[-4] = v29;
      v38[-3] = v30;
      unint64_t v31 = v46;
      v38[-2] = v45;
      v38[-1] = v31;
      uint64_t v32 = type metadata accessor for UnsafeMutablePointer();
      uint64_t v47 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #2 in closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:), (uint64_t)&v38[-10], v23, v32, MEMORY[0x1E4FBC248], v25, MEMORY[0x1E4FBC278], v33);
      uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
      type metadata accessor for Array();
      Array.reserveCapacity(_:)(0);
      uint64_t v35 = v49;
      type metadata accessor for Array();
      swift_bridgeObjectRetain();
      Array.reserveCapacity(_:)(0);
      uint64_t v36 = v47;
      uint64_t AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
      (*(void (**)(uint64_t, uint64_t, uint64_t, unint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness
                                                                                            + 32))(v44, v35 + 32, v36 + 32, v46, AssociatedTypeWitness, AssociatedConformanceWitness);
      swift_bridgeObjectRelease();
      return swift_bridgeObjectRelease_n();
    }
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t closure #2 in closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)@<X0>(unint64_t *a1@<X0>, unint64_t a2@<X2>, void *a3@<X8>)
{
  unint64_t v5 = *a1;
  swift_getAssociatedTypeWitness();
  type metadata accessor for UnsafeMutablePointer();
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  type metadata accessor for UnsafeMutablePointer();
  uint64_t result = swift_dynamicCast();
  if (is_mul_ok(v5, a2))
  {
    if (((v5 * a2) & 0x8000000000000000) == 0)
    {
      *a3 = v8 + *(void *)(*(void *)(AssociatedTypeWitness - 8) + 72) * v5 * a2;
      return result;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t static vDSP.VectorizableFloat.makeBiquadSetup(channelCount:coefficients:sectionCount:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.VectorizableFloat.makeBiquadSetup(channelCount:coefficients:sectionCount:)(a1, a2, a3, MEMORY[0x1E4F16840], MEMORY[0x1E4F16860]);
}

uint64_t static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return specialized static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(a1, a2, a3, a4, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:));
}

void static vDSP.VectorizableFloat.applyMulti(setup:pInputs:pOutputs:count:)(vDSP_biquadm_SetupStruct *a1, const float **a2, float **__Y, vDSP_Length __N)
{
}

uint64_t static vDSP.VectorizableFloat.destroySetup(channelCount:biquadSetup:)(uint64_t a1, uint64_t a2)
{
  return static vDSP.VectorizableFloat.destroySetup(channelCount:biquadSetup:)(a1, a2, MEMORY[0x1E4F16850], MEMORY[0x1E4F16870]);
}

uint64_t protocol witness for static vDSP_BiquadFunctions.makeBiquadSetup(channelCount:coefficients:sectionCount:) in conformance vDSP.VectorizableFloat(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return protocol witness for static vDSP_BiquadFunctions.makeBiquadSetup(channelCount:coefficients:sectionCount:) in conformance vDSP.VectorizableFloat(a1, a2, a3, a4, a5, MEMORY[0x1E4F16840], MEMORY[0x1E4F16860]);
}

uint64_t protocol witness for static vDSP_BiquadFunctions.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:) in conformance vDSP.VectorizableFloat(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return specialized static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(a1, a2, a3, a4, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:));
}

void protocol witness for static vDSP_BiquadFunctions.applyMulti(setup:pInputs:pOutputs:count:) in conformance vDSP.VectorizableFloat(vDSP_biquadm_SetupStruct *a1, const float **a2, float **__Y, vDSP_Length __N)
{
}

uint64_t protocol witness for static vDSP_BiquadFunctions.destroySetup(channelCount:biquadSetup:) in conformance vDSP.VectorizableFloat(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return protocol witness for static vDSP_BiquadFunctions.destroySetup(channelCount:biquadSetup:) in conformance vDSP.VectorizableFloat(a1, a2, a3, a4, MEMORY[0x1E4F16850], MEMORY[0x1E4F16870]);
}

uint64_t static vDSP.VectorizableDouble.makeBiquadSetup(channelCount:coefficients:sectionCount:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.VectorizableFloat.makeBiquadSetup(channelCount:coefficients:sectionCount:)(a1, a2, a3, MEMORY[0x1E4F16848], MEMORY[0x1E4F16868]);
}

uint64_t static vDSP.VectorizableFloat.makeBiquadSetup(channelCount:coefficients:sectionCount:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(uint64_t, uint64_t), uint64_t (*a5)(uint64_t, uint64_t, uint64_t))
{
  uint64_t v6 = a2 + 32;
  if (a1 == 1) {
    return a4(v6, a3);
  }
  else {
    return a5(v6, a3, a1);
  }
}

uint64_t static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return specialized static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(a1, a2, a3, a4, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:));
}

void static vDSP.VectorizableDouble.applyMulti(setup:pInputs:pOutputs:count:)(vDSP_biquadm_SetupStructD *a1, const double **a2, double **__Y, vDSP_Length __N)
{
}

uint64_t static vDSP.VectorizableDouble.destroySetup(channelCount:biquadSetup:)(uint64_t a1, uint64_t a2)
{
  return static vDSP.VectorizableFloat.destroySetup(channelCount:biquadSetup:)(a1, a2, MEMORY[0x1E4F16858], MEMORY[0x1E4F16878]);
}

uint64_t static vDSP.VectorizableFloat.destroySetup(channelCount:biquadSetup:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t), uint64_t (*a4)(uint64_t))
{
  if (a1 == 1) {
    return a3(a2);
  }
  else {
    return a4(a2);
  }
}

uint64_t protocol witness for static vDSP_BiquadFunctions.makeBiquadSetup(channelCount:coefficients:sectionCount:) in conformance vDSP.VectorizableDouble(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return protocol witness for static vDSP_BiquadFunctions.makeBiquadSetup(channelCount:coefficients:sectionCount:) in conformance vDSP.VectorizableFloat(a1, a2, a3, a4, a5, MEMORY[0x1E4F16848], MEMORY[0x1E4F16868]);
}

uint64_t protocol witness for static vDSP_BiquadFunctions.makeBiquadSetup(channelCount:coefficients:sectionCount:) in conformance vDSP.VectorizableFloat(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t), uint64_t (*a7)(uint64_t, uint64_t, uint64_t))
{
  uint64_t v8 = a2 + 32;
  if (a1 == 1) {
    return a6(v8, a3);
  }
  else {
    return a7(v8, a3, a1);
  }
}

uint64_t protocol witness for static vDSP_BiquadFunctions.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:) in conformance vDSP.VectorizableDouble(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return specialized static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(a1, a2, a3, a4, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:));
}

void protocol witness for static vDSP_BiquadFunctions.applyMulti(setup:pInputs:pOutputs:count:) in conformance vDSP.VectorizableDouble(vDSP_biquadm_SetupStructD *a1, const double **a2, double **__Y, vDSP_Length __N)
{
}

uint64_t protocol witness for static vDSP_BiquadFunctions.destroySetup(channelCount:biquadSetup:) in conformance vDSP.VectorizableDouble(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return protocol witness for static vDSP_BiquadFunctions.destroySetup(channelCount:biquadSetup:) in conformance vDSP.VectorizableFloat(a1, a2, a3, a4, MEMORY[0x1E4F16858], MEMORY[0x1E4F16878]);
}

uint64_t protocol witness for static vDSP_BiquadFunctions.destroySetup(channelCount:biquadSetup:) in conformance vDSP.VectorizableFloat(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(uint64_t), uint64_t (*a6)(uint64_t))
{
  if (a1 == 1) {
    return a5(a2);
  }
  else {
    return a6(a2);
  }
}

uint64_t specialized static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  _OWORD v11[2] = a6;
  _OWORD v11[3] = a7;
  v11[4] = a8;
  void v11[5] = a9;
  v11[6] = a1;
  v11[7] = a4;
  v11[8] = a3;
  v11[9] = a5;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t, uint64_t))(a9 + 16))(a10, v11, MEMORY[0x1E4FBC848] + 8, a7, a9);
}

_UNKNOWN **associated type witness table accessor for vDSP_FloatingPointBiquadFilterable.BiquadFunctions : vDSP_BiquadFunctions in Float()
{
  return &protocol witness table for vDSP.VectorizableFloat;
}

_UNKNOWN **associated type witness table accessor for vDSP_FloatingPointBiquadFilterable.BiquadFunctions : vDSP_BiquadFunctions in Double()
{
  return &protocol witness table for vDSP.VectorizableDouble;
}

uint64_t type metadata accessor for vDSP.Biquad()
{
  return __swift_instantiateGenericMetadata();
}

uint64_t type metadata completion function for vDSP.BiquadRef()
{
  return swift_initClassMetadata2();
}

uint64_t dispatch thunk of static vDSP_BiquadFunctions.makeBiquadSetup(channelCount:coefficients:sectionCount:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return (*(uint64_t (**)(void))(a5 + 16))();
}

uint64_t dispatch thunk of static vDSP_BiquadFunctions.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  return (*(uint64_t (**)(void))(a12 + 24))();
}

uint64_t dispatch thunk of static vDSP_BiquadFunctions.applyMulti(setup:pInputs:pOutputs:count:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return (*(uint64_t (**)(void))(a6 + 32))();
}

uint64_t dispatch thunk of static vDSP_BiquadFunctions.destroySetup(channelCount:biquadSetup:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return (*(uint64_t (**)(void))(a4 + 40))();
}

uint64_t partial apply for closure #1 in static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(a1, a2, MEMORY[0x1E4F16838]);
}

uint64_t partial apply for closure #1 in static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.VectorizableFloat.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(a1, a2, MEMORY[0x1E4F16830]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.VectorizableDouble.applySingle<A, B>(source:destination:delays:setup:sectionCount:count:)(uint64_t result, uint64_t a2, uint64_t (*a3)(void, void, uint64_t, uint64_t, uint64_t, uint64_t, void))
{
  if (result)
  {
    uint64_t v4 = **(void **)(v3 + 32);
    if (v4) {
      return a3(*(void *)(v3 + 16), *(void *)(v3 + 24), result, 1, v4, 1, *(void *)(v3 + 40));
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)()
{
  return closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)();
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)(uint64_t a1)
{
  return closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)(a1, *(void *)(v1 + 64), *(void *)(v1 + 72), *(void *)(v1 + 80), *(void *)(v1 + 88), *(void *)(v1 + 16), *(void *)(v1 + 24), *(void *)(v1 + 32), *(void *)(v1 + 40), *(void *)(v1 + 48), *(void *)(v1 + 56));
}

unint64_t *partial apply for closure #1 in closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)@<X0>(unint64_t *result@<X0>, void *a2@<X8>)
{
  unint64_t v4 = *(void *)(v2 + 72);
  unint64_t v5 = *result;
  if (is_mul_ok(*result, v4))
  {
    unint64_t v6 = v5 * v4;
    if (((v5 * v4) & 0x8000000000000000) == 0)
    {
      uint64_t v7 = *(void *)(v2 + 64);
      uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
      uint64_t v9 = *(void *)(AssociatedTypeWitness - 8);
      uint64_t result = (unint64_t *)(AssociatedTypeWitness - 8);
      *a2 = v7 + *(void *)(v9 + 72) * v6;
      return result;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

unint64_t lazy protocol witness table accessor for type Range<UInt> and conformance <> Range<A>()
{
  unint64_t result = lazy protocol witness table cache variable for type Range<UInt> and conformance <> Range<A>;
  if (!lazy protocol witness table cache variable for type Range<UInt> and conformance <> Range<A>)
  {
    __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for Range<UInt>);
    lazy protocol witness table accessor for type Int and conformance Int();
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Range<UInt> and conformance <> Range<A>);
  }
  return result;
}

uint64_t partial apply for closure #2 in closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)@<X0>(unint64_t *a1@<X0>, void *a2@<X8>)
{
  return closure #2 in closure #1 in closure #1 in static vDSP.BiquadFunctions.applyBiquadMulti<A, B>(source:destination:setup:channelCount:count:)(a1, *(void *)(v2 + 72), a2);
}

uint64_t BNNS.DropoutLayer.__allocating_init(input:output:rate:seed:control:filterParameters:)(_OWORD *a1, long long *a2, int a3, char a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9)
{
  uint64_t v43 = *MEMORY[0x1E4F143B8];
  long long v16 = a2[8];
  long long v17 = a2[9];
  long long v18 = a2[6];
  __src[18] = a2[7];
  __src[19] = v16;
  long long v19 = a2[10];
  __src[20] = v17;
  __src[21] = v19;
  long long v20 = a2[4];
  long long v21 = a2[5];
  long long v22 = a2[2];
  __src[14] = a2[3];
  __src[15] = v20;
  __src[16] = v21;
  __src[17] = v18;
  long long v23 = *a2;
  __src[12] = a2[1];
  __src[13] = v22;
  long long v24 = a1[9];
  __src[8] = a1[8];
  __src[9] = v24;
  __src[10] = a1[10];
  __src[11] = v23;
  long long v25 = a1[5];
  __src[4] = a1[4];
  __src[5] = v25;
  long long v26 = a1[7];
  __src[6] = a1[6];
  __src[7] = v26;
  long long v27 = a1[1];
  __src[0] = *a1;
  __src[1] = v27;
  long long v28 = a1[3];
  __src[2] = a1[2];
  __src[3] = v28;
  memcpy(__dst, __src, sizeof(__dst));
  float v39 = a9;
  int v40 = a3;
  char v41 = a4;
  if (a7 == 1)
  {
    uint64_t v29 = 0;
  }
  else
  {
    int v34 = a5;
    uint64_t v35 = a6;
    uint64_t v36 = a7;
    uint64_t v37 = a8;
    uint64_t v29 = &v34;
  }
  uint64_t v30 = MEMORY[0x1D25FFFE0](__dst, v29);
  type metadata accessor for BNNS.DropoutLayer();
  uint64_t v31 = swift_allocObject();
  uint64_t v32 = v31;
  if (v30)
  {
    *(void *)(v31 + 16) = v30;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v32;
}

uint64_t type metadata accessor for BNNS.DropoutLayer()
{
  return self;
}

uint64_t BNNS.DropoutLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.DropoutLayer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

uint64_t BNNS.FullyConnectedLayer.__allocating_init(input:output:weights:bias:activation:filterParameters:)(_OWORD *a1, _OWORD *a2, long long *a3, uint64_t a4, uint64_t *a5, uint32_t a6, size_t a7, int (__cdecl *a8)(void **, size_t, size_t), void (__cdecl *a9)(void *))
{
  uint64_t v74 = *MEMORY[0x1E4F143B8];
  outlined init with take of BNNSNDArrayDescriptor?(a4, (uint64_t)v71);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v71, (uint64_t)v73);
  uint64_t v14 = *a5;
  char v15 = *((unsigned char *)a5 + 8);
  outlined init with take of BNNSNDArrayDescriptor?(a4, (uint64_t)v72);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v72) == 1)
  {
    size_t v64 = 0;
    size_t v65 = 0;
    size_t v62 = 0;
    size_t v63 = 0;
    size_t v60 = 0;
    size_t v61 = 0;
    size_t v57 = 0;
    size_t v58 = 0;
    size_t v55 = 0;
    size_t v56 = 0;
    size_t v53 = 0;
    size_t v54 = 0;
    size_t v51 = 0;
    size_t v52 = 0;
    size_t v50 = 0;
    size_t v16 = 0;
    long long v17 = 0;
    long long v18 = 0;
    BNNSDataType v19 = 0;
    uint64_t v59 = 0;
    uint64_t v20 = 0;
    uint64_t v49 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v73, (uint64_t)__src);
    uint64_t v59 = *(void *)&__src[0];
    size_t v64 = *(void *)&__src[1];
    size_t v65 = *((void *)&__src[0] + 1);
    size_t v62 = *(void *)&__src[2];
    size_t v63 = *((void *)&__src[1] + 1);
    size_t v60 = *(void *)&__src[3];
    size_t v61 = *((void *)&__src[2] + 1);
    size_t v57 = *(void *)&__src[4];
    size_t v58 = *((void *)&__src[3] + 1);
    size_t v55 = *(void *)&__src[5];
    size_t v56 = *((void *)&__src[4] + 1);
    size_t v53 = *(void *)&__src[6];
    size_t v54 = *((void *)&__src[5] + 1);
    size_t v51 = *(void *)&__src[7];
    size_t v52 = *((void *)&__src[6] + 1);
    size_t v16 = *(void *)&__src[8];
    size_t v50 = *((void *)&__src[7] + 1);
    long long v17 = (void *)*((void *)&__src[8] + 1);
    long long v18 = (void *)*((void *)&__src[9] + 1);
    uint64_t v49 = *(void *)&__src[9];
    BNNSDataType v19 = __src[10];
    uint64_t v20 = *(void *)((char *)&__src[10] + 4);
    int v46 = HIDWORD(__src[10]);
  }
  *(void *)&__src[0] = v14;
  BYTE8(__src[0]) = v15;
  BNNS.ActivationFunction.bnnsActivation.getter((uint64_t)&v70);
  long long v21 = a1[9];
  __src[8] = a1[8];
  __src[9] = v21;
  long long v22 = a1[5];
  __src[4] = a1[4];
  __src[5] = v22;
  long long v23 = a1[7];
  __src[6] = a1[6];
  __src[7] = v23;
  long long v24 = a1[1];
  __src[0] = *a1;
  __src[1] = v24;
  long long v25 = a1[3];
  __src[2] = a1[2];
  __src[3] = v25;
  long long v26 = a3[8];
  long long v27 = a3[9];
  long long v28 = a3[6];
  __src[18] = a3[7];
  __src[19] = v26;
  long long v29 = a3[10];
  __src[20] = v27;
  __src[21] = v29;
  long long v30 = a3[4];
  long long v31 = a3[5];
  long long v32 = a3[2];
  __src[14] = a3[3];
  __src[15] = v30;
  long long v33 = a1[10];
  __src[16] = v31;
  __src[17] = v28;
  long long v34 = *a3;
  long long v35 = a3[1];
  __src[10] = v33;
  __src[11] = v34;
  __src[12] = v35;
  __src[13] = v32;
  long long v36 = a2[9];
  __src[30] = a2[8];
  __src[31] = v36;
  __src[32] = a2[10];
  long long v37 = a2[5];
  __src[26] = a2[4];
  __src[27] = v37;
  long long v38 = a2[7];
  __src[28] = a2[6];
  __src[29] = v38;
  long long v39 = a2[1];
  _OWORD __src[22] = *a2;
  __src[23] = v39;
  long long v40 = a2[3];
  __src[24] = a2[2];
  __src[25] = v40;
  memcpy(&__dst, __src, 0x210uLL);
  *(void *)&__dst.bias.flags = v59;
  __dst.bias.size[0] = v65;
  __dst.bias.size[1] = v64;
  __dst.bias.size[2] = v63;
  __dst.bias.size[3] = v62;
  __dst.bias.size[4] = v61;
  __dst.bias.size[5] = v60;
  __dst.bias.size[6] = v58;
  __dst.bias.size[7] = v57;
  __dst.bias.stride[0] = v56;
  __dst.bias.stride[1] = v55;
  __dst.bias.stride[2] = v54;
  __dst.bias.stride[3] = v53;
  __dst.bias.stride[4] = v52;
  __dst.bias.stride[5] = v51;
  __dst.bias.stride[6] = v50;
  __dst.bias.stride[7] = v16;
  __dst.bias.char data = v17;
  *(void *)&__dst.bias.data_type = v49;
  __dst.bias.table_char data = v18;
  __dst.bias.table_data_type = v19;
  *(void *)&__dst.bias.data_scale = v20;
  *((_DWORD *)&__dst.bias.data_bias + 1) = v46;
  __dst.activation = v70;
  if (a8 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    p_BNNSFilterParameters filter_params = 0;
  }
  else
  {
    filter_params.flags = a6;
    filter_params.n_threads = a7;
    filter_params.alloc_memory = a8;
    filter_params.free_memory = a9;
    p_BNNSFilterParameters filter_params = &filter_params;
  }
  uint64_t v42 = BNNSFilterCreateLayerFullyConnected(&__dst, p_filter_params);
  type metadata accessor for BNNS.FullyConnectedLayer();
  uint64_t v43 = swift_allocObject();
  uint64_t v44 = v43;
  if (v42)
  {
    *(void *)(v43 + 16) = v42;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v44;
}

uint64_t type metadata accessor for BNNS.FullyConnectedLayer()
{
  return self;
}

uint64_t BNNS.FullyConnectedLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.FullyConnectedLayer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

uint64_t _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF(void (*a1)(char *, char *), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v31 = a5;
  uint64_t v32 = a8;
  uint64_t v10 = v9;
  long long v40 = a1;
  uint64_t v41 = a2;
  uint64_t v30 = *(void *)(a5 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v42 = (char *)&v28 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  long long v38 = *(void (***)(char *, uint64_t))(AssociatedTypeWitness - 8);
  uint64_t v14 = MEMORY[0x1F4188790](AssociatedTypeWitness);
  size_t v16 = (char *)&v28 - v15;
  uint64_t v17 = a4;
  MEMORY[0x1F4188790](v14);
  long long v35 = (char *)&v28 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v19 = swift_getAssociatedTypeWitness();
  uint64_t v33 = *(void *)(v19 - 8);
  uint64_t v34 = v19;
  MEMORY[0x1F4188790](v19);
  long long v21 = (char *)&v28 - v20;
  uint64_t v36 = v8;
  uint64_t v22 = dispatch thunk of Collection.count.getter();
  if (!v22) {
    return static Array._allocateUninitialized(_:)();
  }
  Swift::Int v23 = v22;
  long long v29 = v16;
  uint64_t v45 = MEMORY[0x1D25FFC30](v17);
  uint64_t v37 = type metadata accessor for ContiguousArray();
  ContiguousArray.reserveCapacity(_:)(v23);
  uint64_t v43 = v21;
  uint64_t result = dispatch thunk of Collection.startIndex.getter();
  if (v23 < 0)
  {
    __break(1u);
  }
  else
  {
    long long v25 = (void (**)(char *))(v38 + 2);
    ++v38;
    long long v39 = v25;
    long long v26 = v29;
    while (1)
    {
      long long v27 = (void (*)(char *, void))dispatch thunk of Collection.subscript.read();
      (*v39)(v26);
      v27(v44, 0);
      v40(v26, v42);
      if (v10) {
        break;
      }
      uint64_t v10 = 0;
      (*v38)(v26, AssociatedTypeWitness);
      ContiguousArray.append(_:)();
      dispatch thunk of Collection.formIndex(after:)();
      if (!--v23)
      {
        (*(void (**)(char *, uint64_t))(v33 + 8))(v43, v34);
        return v45;
      }
    }
    (*v38)(v26, AssociatedTypeWitness);
    (*(void (**)(char *, uint64_t))(v33 + 8))(v43, v34);
    swift_release();
    return (*(uint64_t (**)(uint64_t, char *, uint64_t))(v30 + 32))(v32, v42, v31);
  }
  return result;
}

vImage_Error vImageConverterRef.convert<A, B>(from:to:)(uint64_t a1, uint64_t a2)
{
  uint64_t v13 = *MEMORY[0x1E4F143B8];
  vImage_Error v11 = 0;
  type metadata accessor for vImage.PixelBuffer();
  v12.char data = (void *)vImage.PixelBuffer<>.vImageBuffer.getter();
  v12.height = v4;
  v12.width = v5;
  v12.rowBytes = v6;
  vImage_Error result = closure #1 in vImageConverterRef.convert<A, B>(from:to:)(&v12, a2, &v11, v2);
  vImage_Error v8 = v11;
  if (v11)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    if ((unint64_t)(v8 + 21784) >= 0x13) {
      char v10 = 11;
    }
    else {
      char v10 = -5 - v8;
    }
    char *v9 = v10;
    return swift_willThrow();
  }
  return result;
}

vImage_Error closure #1 in vImageConverterRef.convert<A, B>(from:to:)(const vImage_Buffer *a1, uint64_t a2, vImage_Error *a3, vImageConverter *a4)
{
  uint64_t v12 = *MEMORY[0x1E4F143B8];
  type metadata accessor for vImage.PixelBuffer();
  dests.char data = (void *)vImage.PixelBuffer<>.vImageBuffer.getter();
  dests.height = v7;
  dests.width = v8;
  dests.rowBytes = v9;
  vImage_Error result = vImageConvert_AnyToAny(a4, a1, &dests, 0, 0x100u);
  *a3 = result;
  return result;
}

uint64_t vImageConverterRef.convert<A, B>(from:to:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v36 = a6;
  uint64_t v37 = a1;
  uint64_t v33 = a3;
  uint64_t v34 = a4;
  uint64_t v35 = a5;
  type metadata accessor for vImage.PixelBuffer();
  uint64_t v10 = type metadata accessor for Array();
  type metadata accessor for vImage_Buffer(0);
  uint64_t v12 = v11;
  uint64_t WitnessTable = swift_getWitnessTable();
  uint64_t v15 = (const vImage_Buffer *)_sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in vImageConverterRef.convert<A, B>(from:to:), (uint64_t)v32, v10, v12, MEMORY[0x1E4FBC248], WitnessTable, MEMORY[0x1E4FBC278], v14);
  uint64_t v37 = a2;
  uint64_t v28 = a3;
  uint64_t v29 = a4;
  uint64_t v30 = a5;
  uint64_t v31 = a6;
  type metadata accessor for vImage.PixelBuffer();
  uint64_t v16 = type metadata accessor for Array();
  uint64_t v17 = swift_getWitnessTable();
  uint64_t v19 = (const vImage_Buffer *)_sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #2 in vImageConverterRef.convert<A, B>(from:to:), (uint64_t)v27, v16, v12, MEMORY[0x1E4FBC248], v17, MEMORY[0x1E4FBC278], v18);
  vImage_Error v20 = vImageConvert_AnyToAny(converter, v15 + 1, v19 + 1, 0, 0x100u);
  swift_bridgeObjectRelease();
  uint64_t result = swift_bridgeObjectRelease();
  if (v20)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    Swift::Int v23 = v22;
    vImage.Error.init(rawValue:)(v20, (char *)&v37);
    char v24 = v37;
    if (v37 == 20) {
      char v24 = 11;
    }
    *Swift::Int v23 = v24;
    return swift_willThrow();
  }
  return result;
}

uint64_t partial apply for closure #1 in vImageConverterRef.convert<A, B>(from:to:)@<X0>(uint64_t *a1@<X8>)
{
  type metadata accessor for vImage.PixelBuffer();
  uint64_t result = vImage.PixelBuffer<>.vImageBuffer.getter();
  *a1 = result;
  a1[1] = v3;
  a1[2] = v4;
  a1[3] = v5;
  return result;
}

uint64_t partial apply for closure #2 in vImageConverterRef.convert<A, B>(from:to:)@<X0>(uint64_t *a1@<X8>)
{
  type metadata accessor for vImage.PixelBuffer();
  uint64_t result = vImage.PixelBuffer<>.vImageBuffer.getter();
  *a1 = result;
  a1[1] = v3;
  a1[2] = v4;
  a1[3] = v5;
  return result;
}

vImageConverterRef static vImageConverterRef.make(sourceFormat:destinationFormat:colorConversionInfo:)(uint64_t a1, uint64_t a2, CGColorConversionInfoRef colorConversionInfoRef)
{
  uint64_t v14 = *MEMORY[0x1E4F143B8];
  vImage_Error error = 0;
  long long v3 = *(_OWORD *)(a2 + 16);
  *(_OWORD *)&dFormat.bitsPerComponent = *(_OWORD *)a2;
  *(_OWORD *)&dFormat.bitmapInfo = v3;
  *(void *)&dFormat.renderingIntent = *(void *)(a2 + 32);
  long long v4 = *(_OWORD *)(a1 + 16);
  *(_OWORD *)&sFormat.bitsPerComponent = *(_OWORD *)a1;
  *(_OWORD *)&sFormat.bitmapInfo = v4;
  *(void *)&sFormat.renderingIntent = *(void *)(a1 + 32);
  vImageConverterRef v5 = vImageConverter_CreateWithCGColorConversionInfo(colorConversionInfoRef, &sFormat, &dFormat, 0, 0x100u, &error);
  vImageConverterRef v6 = v5;
  vImage_Error v7 = error;
  if (error)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    if ((unint64_t)(v7 + 21784) >= 0x13) {
      char v9 = 11;
    }
    else {
      char v9 = -5 - v7;
    }
  }
  else
  {
    if (v5) {
      return v6;
    }
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    char v9 = 11;
  }
  char *v8 = v9;
  swift_willThrow();
  return v6;
}

int64_t vImageConverterRef.makeCVToCGPixelBuffers(referencing:)(uint64_t a1)
{
  return vImageConverterRef.makeCVToCGPixelBuffers(referencing:)(a1, MEMORY[0x1E4F16FD8], MEMORY[0x1E4F16F80]);
}

int64_t vImageConverterRef.makeCGToCVPixelBuffers(referencing:)(uint64_t a1)
{
  return vImageConverterRef.makeCVToCGPixelBuffers(referencing:)(a1, MEMORY[0x1E4F16FD0], MEMORY[0x1E4F16F88]);
}

int64_t vImageConverterRef.makeCVToCGPixelBuffers(referencing:)(uint64_t a1, uint64_t (*a2)(uint64_t), uint64_t (*a3)(char *, uint64_t, uint64_t, uint64_t))
{
  int64_t result = a2(v3);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    int64_t v7 = result;
    vImagePixelCount v8 = (char *)MEMORY[0x1E4FBC860];
    if (result)
    {
      uint64_t v28 = MEMORY[0x1E4FBC860];
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, result, 0);
      vImagePixelCount v8 = (char *)v28;
      unint64_t v9 = *(void *)(v28 + 16);
      uint64_t v10 = 32 * v9 + 32;
      long long v11 = 0uLL;
      do
      {
        uint64_t v28 = (uint64_t)v8;
        unint64_t v12 = *((void *)v8 + 3);
        unint64_t v13 = v9 + 1;
        if (v9 >= v12 >> 1)
        {
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v12 > 1), v9 + 1, 1);
          long long v11 = 0uLL;
          vImagePixelCount v8 = (char *)v28;
        }
        *((void *)v8 + 2) = v13;
        uint64_t v14 = &v8[v10];
        *(_OWORD *)uint64_t v14 = v11;
        *((_OWORD *)v14 + 1) = v11;
        v10 += 32;
        unint64_t v9 = v13;
        --v7;
      }
      while (v7);
    }
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
      vImagePixelCount v8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v8 + 2), 0, v8);
    }
    uint64_t v15 = a3(v8 + 32, v3, a1, 768);
    if (v15)
    {
      uint64_t v16 = v15;
      swift_bridgeObjectRelease();
      lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
      swift_allocError();
      uint64_t v18 = v17;
      vImage.Error.init(rawValue:)(v16, (char *)&v28);
      char v19 = v28;
      if (v28 == 20) {
        char v19 = 11;
      }
      *uint64_t v18 = v19;
      swift_willThrow();
    }
    else
    {
      int64_t v20 = *((void *)v8 + 2);
      if (v20)
      {
        uint64_t v28 = MEMORY[0x1E4FBC860];
        swift_bridgeObjectRetain();
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v20, 0);
        uint64_t v16 = v28;
        long long v21 = (uint64_t *)(v8 + 56);
        do
        {
          long long v27 = *(_OWORD *)(v21 - 3);
          uint64_t v22 = *(v21 - 1);
          uint64_t v23 = *v21;
          __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
          uint64_t v24 = swift_allocObject();
          *(_OWORD *)(v24 + 16) = xmmword_1D2135280;
          *(_OWORD *)(v24 + 32) = v27;
          *(void *)(v24 + 48) = v22;
          *(void *)(v24 + 56) = v23;
          *(void *)(v24 + 64) = 0;
          uint64_t v28 = v16;
          unint64_t v26 = *(void *)(v16 + 16);
          unint64_t v25 = *(void *)(v16 + 24);
          if (v26 >= v25 >> 1)
          {
            specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v25 > 1), v26 + 1, 1);
            uint64_t v16 = v28;
          }
          v21 += 4;
          *(void *)(v16 + 16) = v26 + 1;
          *(void *)(v16 + 8 * v26 + 32) = v24;
          --v20;
        }
        while (v20);
        swift_bridgeObjectRelease_n();
      }
      else
      {
        swift_bridgeObjectRelease();
        return MEMORY[0x1E4FBC860];
      }
    }
    return v16;
  }
  return result;
}

double static BNNSActivation.identity.getter@<D0>(uint64_t a1@<X8>)
{
  *(_DWORD *)a1 = 0;
  *(int32x2_t *)(a1 + 4) = vdup_n_s32(0x7FC00000u);
  *(_DWORD *)(a1 + 12) = 1;
  double result = 0.0;
  *(_OWORD *)(a1 + 16) = 0u;
  *(_OWORD *)(a1 + 32) = 0u;
  return result;
}

uint64_t static BNNSActivationFunction.rectifiedLinear.getter()
{
  return 1;
}

void __swiftcall BNNSActivation.init(function:alpha:beta:)(BNNSActivation *__return_ptr retstr, BNNSActivationFunction function, Swift::Float alpha, Swift::Float beta)
{
  if (function - 9 < 2)
  {
    __break(1u);
  }
  else
  {
    retstr->function = function;
    retstr->alpha = alpha;
    retstr->beta = beta;
    retstr->iscale = 1;
    *(_OWORD *)&retstr->ioffset = 0u;
    *(_OWORD *)&retstr->ioffset_per_channel = 0u;
  }
}

uint64_t static BNNSActivationFunction.leakyRectifiedLinear.getter()
{
  return 2;
}

uint64_t static BNNSActivationFunction.sigmoid.getter()
{
  return 3;
}

uint64_t static BNNSActivationFunction.tanh.getter()
{
  return 4;
}

uint64_t static BNNSActivationFunction.scaledTanh.getter()
{
  return 5;
}

uint64_t static BNNSActivationFunction.abs.getter()
{
  return 6;
}

uint64_t static BNNSActivationFunction.linear.getter()
{
  return 7;
}

uint64_t static BNNSActivationFunction.clamp.getter()
{
  return 8;
}

uint64_t static BNNSActivationFunction.softmax.getter()
{
  return 11;
}

uint64_t static BNNSPoolingFunction.max.getter()
{
  return 0;
}

uint64_t static BNNSDataType.float16.getter()
{
  return 65552;
}

uint64_t static BNNSDataType.float.getter()
{
  return 65568;
}

uint64_t static BNNSDataType.int8.getter()
{
  return 131080;
}

uint64_t static BNNSDataType.int16.getter()
{
  return 131088;
}

uint64_t static BNNSDataType.int32.getter()
{
  return 131104;
}

uint64_t static BNNSDataType.indexed8.getter()
{
  return 524296;
}

uint64_t static BNNSDataType.uint8.getter()
{
  return 262152;
}

uint64_t static BNNSDataType.uint16.getter()
{
  return 262160;
}

uint64_t static BNNSDataType.uint32.getter()
{
  return 262176;
}

uint64_t BNNSDataTypeFloat16.getter()
{
  return BNNSDataTypeFloat16;
}

uint64_t BNNSDataTypeFloat16.setter(BNNSDataType a1)
{
  uint64_t result = swift_beginAccess();
  BNNSDataTypeFloat16 = a1;
  return result;
}

uint64_t (*BNNSDataTypeFloat16.modify())()
{
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSDataTypeFloat32.getter()
{
  return BNNSDataTypeFloat32;
}

uint64_t BNNSDataTypeFloat32.setter(BNNSDataType a1)
{
  uint64_t result = swift_beginAccess();
  BNNSDataTypeFloat32 = a1;
  return result;
}

uint64_t (*BNNSDataTypeFloat32.modify())()
{
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSDataTypeInt8.getter()
{
  return BNNSDataTypeInt8;
}

uint64_t BNNSDataTypeInt8.setter(BNNSDataType a1)
{
  uint64_t result = swift_beginAccess();
  BNNSDataTypeInt8 = a1;
  return result;
}

uint64_t (*BNNSDataTypeInt8.modify())()
{
  return BNNSDataTypeInt8.modify;
}

uint64_t BNNSDataTypeInt16.getter()
{
  return BNNSDataTypeInt16;
}

uint64_t BNNSDataTypeInt16.setter(BNNSDataType a1)
{
  uint64_t result = swift_beginAccess();
  BNNSDataTypeInt16 = a1;
  return result;
}

uint64_t (*BNNSDataTypeInt16.modify())()
{
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSDataTypeInt32.getter()
{
  return BNNSDataTypeInt32;
}

uint64_t BNNSDataTypeInt32.setter(BNNSDataType a1)
{
  uint64_t result = swift_beginAccess();
  BNNSDataTypeInt32 = a1;
  return result;
}

uint64_t (*BNNSDataTypeInt32.modify())()
{
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSDataTypeIndexed8.getter()
{
  return BNNSDataTypeIndexed8;
}

uint64_t BNNSDataTypeIndexed8.setter(BNNSDataType a1)
{
  uint64_t result = swift_beginAccess();
  BNNSDataTypeIndexed8 = a1;
  return result;
}

uint64_t (*BNNSDataTypeIndexed8.modify())()
{
  return BNNSDataTypeFloat32.modify;
}

uint64_t static BNNSPoolingFunction.average.getter()
{
  return 1;
}

uint64_t BNNSPoolingFunctionMax.getter()
{
  return BNNSPoolingFunctionMax;
}

uint64_t BNNSPoolingFunctionMax.setter(BNNSPoolingFunction a1)
{
  uint64_t result = swift_beginAccess();
  BNNSPoolingFunctionMax = a1;
  return result;
}

uint64_t (*BNNSPoolingFunctionMax.modify())()
{
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSPoolingFunctionAverage.getter()
{
  return BNNSPoolingFunctionAverage;
}

uint64_t BNNSPoolingFunctionAverage.setter(BNNSPoolingFunction a1)
{
  uint64_t result = swift_beginAccess();
  BNNSPoolingFunctionAverage = a1;
  return result;
}

uint64_t (*BNNSPoolingFunctionAverage.modify())()
{
  return BNNSDataTypeFloat32.modify;
}

uint64_t static BNNSActivationFunction.identity.getter()
{
  return 0;
}

uint64_t static BNNSActivationFunction.integerLinearSaturate.getter()
{
  return 9;
}

uint64_t static BNNSActivationFunction.integerLinearSaturatePerChannel.getter()
{
  return 10;
}

uint64_t BNNSActivationFunctionIdentity.getter()
{
  return BNNSActivationFunctionIdentity;
}

uint64_t BNNSActivationFunctionIdentity.setter(BNNSActivationFunction a1)
{
  uint64_t result = swift_beginAccess();
  BNNSActivationFunctionIdentity = a1;
  return result;
}

uint64_t (*BNNSActivationFunctionIdentity.modify())()
{
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSActivationFunctionRectifiedLinear.getter()
{
  return BNNSActivationFunctionRectifiedLinear;
}

uint64_t BNNSActivationFunctionRectifiedLinear.setter(BNNSActivationFunction a1)
{
  uint64_t result = swift_beginAccess();
  BNNSActivationFunctionRectifiedLinear = a1;
  return result;
}

uint64_t (*BNNSActivationFunctionRectifiedLinear.modify())()
{
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSActivationFunctionLeakyRectifiedLinear.getter()
{
  return BNNSActivationFunctionLeakyRectifiedLinear;
}

uint64_t BNNSActivationFunctionLeakyRectifiedLinear.setter(BNNSActivationFunction a1)
{
  uint64_t result = swift_beginAccess();
  BNNSActivationFunctionLeakyRectifiedLinear = a1;
  return result;
}

uint64_t (*BNNSActivationFunctionLeakyRectifiedLinear.modify())()
{
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSActivationFunctionSigmoid.getter()
{
  return BNNSActivationFunctionSigmoid;
}

uint64_t BNNSActivationFunctionSigmoid.setter(BNNSActivationFunction a1)
{
  uint64_t result = swift_beginAccess();
  BNNSActivationFunctionSigmoid = a1;
  return result;
}

uint64_t (*BNNSActivationFunctionSigmoid.modify())()
{
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSActivationFunctionTanh.getter()
{
  return BNNSActivationFunctionTanh;
}

uint64_t BNNSActivationFunctionTanh.setter(BNNSActivationFunction a1)
{
  uint64_t result = swift_beginAccess();
  BNNSActivationFunctionTanh = a1;
  return result;
}

uint64_t (*BNNSActivationFunctionTanh.modify())()
{
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSActivationFunctionScaledTanh.getter()
{
  return BNNSActivationFunctionScaledTanh;
}

uint64_t BNNSActivationFunctionScaledTanh.setter(BNNSActivationFunction a1)
{
  uint64_t result = swift_beginAccess();
  BNNSActivationFunctionScaledTanh = a1;
  return result;
}

uint64_t (*BNNSActivationFunctionScaledTanh.modify())()
{
  return BNNSDataTypeFloat32.modify;
}

uint64_t BNNSActivationFunctionAbs.getter()
{
  return BNNSActivationFunctionAbs;
}

uint64_t BNNSActivationFunctionAbs.setter(BNNSActivationFunction a1)
{
  uint64_t result = swift_beginAccess();
  BNNSActivationFunctionAbs = a1;
  return result;
}

uint64_t (*BNNSActivationFunctionAbs.modify())()
{
  return BNNSDataTypeFloat32.modify;
}

uint64_t static BNNSFlags.useClientPointer.getter()
{
  return 1;
}

uint64_t BNNSFlagsUseClientPtr.getter()
{
  return BNNSFlagsUseClientPtr;
}

uint64_t BNNSFlagsUseClientPtr.setter(BNNSFlags a1)
{
  uint64_t result = swift_beginAccess();
  BNNSFlagsUseClientPtr = a1;
  return result;
}

uint64_t (*BNNSFlagsUseClientPtr.modify())()
{
  return BNNSDataTypeFloat32.modify;
}

void __swiftcall BNNSImageStackDescriptor.init(width:height:channels:row_stride:image_stride:data_type:)(BNNSImageStackDescriptor *__return_ptr retstr, Swift::Int width, Swift::Int height, Swift::Int channels, Swift::Int row_stride, Swift::Int image_stride, BNNSDataType data_type)
{
  if (data_type == BNNSDataTypeIndexed8)
  {
    __break(1u);
  }
  else
  {
    retstr->width = width;
    retstr->height = height;
    retstr->channels = channels;
    retstr->row_stride = row_stride;
    retstr->image_stride = image_stride;
    retstr->data_type = data_type;
    *(void *)&retstr->data_scale = 1065353216;
  }
}

BNNSVectorDescriptor __swiftcall BNNSVectorDescriptor.init(size:data_type:)(Swift::Int size, BNNSDataType data_type)
{
  if (data_type == BNNSDataTypeIndexed8)
  {
    __break(1u);
  }
  else
  {
    float v2 = 1.0;
    float v3 = 0.0;
  }
  result.size = size;
  result.data_bias = v3;
  result.data_scale = v2;
  result.data_type = data_type;
  return result;
}

uint64_t BNNSLayerData.init(data:data_type:data_scale:data_bias:)@<X0>(uint64_t result@<X0>, int a2@<W1>, uint64_t a3@<X8>, float a4@<S0>, float a5@<S1>)
{
  if (a2 == 524296)
  {
    __break(1u);
  }
  else
  {
    *(void *)a3 = result;
    *(_DWORD *)(a3 + 8) = a2;
    *(float *)(a3 + 12) = a4;
    *(float *)(a3 + 16) = a5;
    *(void *)(a3 + 24) = 0;
  }
  return result;
}

void static BNNSLayerData.zero.getter(uint64_t a1@<X8>)
{
  *(void *)(a1 + 24) = 0;
  *(void *)a1 = 0;
  *(void *)(a1 + 8) = 0;
  *(_DWORD *)(a1 + 16) = 0;
}

double static BNNSLayerData.indexed8(data:data_table:)@<D0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  *(void *)a3 = a1;
  *(_DWORD *)(a3 + 8) = 524296;
  *(void *)&double result = 1065353216;
  *(void *)(a3 + 12) = 1065353216;
  *(void *)(a3 + 24) = a2;
  return result;
}

int32x2_t static BNNSActivation.integerLinearSaturate(scale:offset:shift:)@<D0>(int a1@<W0>, int a2@<W1>, int a3@<W2>, uint64_t a4@<X8>)
{
  *(_DWORD *)a4 = 9;
  int32x2_t result = vdup_n_s32(0x7FC00000u);
  *(int32x2_t *)(a4 + 4) = result;
  *(_DWORD *)(a4 + 12) = a1;
  *(_DWORD *)(a4 + 16) = a2;
  *(_DWORD *)(a4 + 20) = a3;
  *(void *)(a4 + 32) = 0;
  *(void *)(a4 + 40) = 0;
  *(void *)(a4 + 24) = 0;
  return result;
}

double static BNNSActivation.integerLinearSaturatePerChannel(scale:offset:shift:)@<D0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X8>)
{
  *(_DWORD *)a4 = 10;
  *(int32x2_t *)(a4 + 4) = vdup_n_s32(0x7FC00000u);
  *(void *)&double result = 1;
  *(void *)(a4 + 12) = 1;
  *(_DWORD *)(a4 + 20) = 0;
  *(void *)(a4 + 24) = a1;
  *(void *)(a4 + 32) = a2;
  *(void *)(a4 + 40) = a3;
  return result;
}

void __swiftcall BNNSConvolutionLayerParameters.init(x_stride:y_stride:x_padding:y_padding:k_width:k_height:in_channels:out_channels:weights:)(BNNSConvolutionLayerParameters *__return_ptr retstr, Swift::Int x_stride, Swift::Int y_stride, Swift::Int x_padding, Swift::Int y_padding, Swift::Int k_width, Swift::Int k_height, Swift::Int in_channels, Swift::Int out_channels, BNNSLayerData *weights)
{
  char data = weights->data;
  BNNSDataType data_type = weights->data_type;
  data_table = weights->data_table;
  retstr->x_stride = x_stride;
  retstr->y_stride = y_stride;
  retstr->x_padding = x_padding;
  retstr->y_padding = y_padding;
  retstr->k_width = k_width;
  retstr->k_height = k_height;
  retstr->in_channels = in_channels;
  retstr->out_channels = out_channels;
  retstr->weights.char data = data;
  retstr->weights.BNNSDataType data_type = data_type;
  *(void *)&retstr->weights.data_scale = *(void *)&weights->data_scale;
  retstr->bias.data_table = 0;
  retstr->activation.function = BNNSActivationFunctionIdentity;
  retstr->bias.char data = 0;
  *(void *)&retstr->bias.BNNSDataType data_type = 0;
  retstr->weights.data_table = data_table;
  retstr->bias.data_bias = 0.0;
  *(int32x2_t *)&retstr->activation.alpha = vdup_n_s32(0x7FC00000u);
  retstr->activation.iscale = 1;
  *(_OWORD *)&retstr->activation.ioffset = 0u;
  *(_OWORD *)&retstr->activation.ioffset_per_channel = 0u;
}

void __swiftcall BNNSPoolingLayerParameters.init(x_stride:y_stride:x_padding:y_padding:k_width:k_height:in_channels:out_channels:pooling_function:)(BNNSPoolingLayerParameters *__return_ptr retstr, Swift::Int x_stride, Swift::Int y_stride, Swift::Int x_padding, Swift::Int y_padding, Swift::Int k_width, Swift::Int k_height, Swift::Int in_channels, Swift::Int out_channels, BNNSPoolingFunction pooling_function)
{
  retstr->x_stride = x_stride;
  retstr->y_stride = y_stride;
  retstr->x_padding = x_padding;
  retstr->y_padding = y_padding;
  retstr->k_width = k_width;
  retstr->k_height = k_height;
  retstr->in_channels = in_channels;
  retstr->out_channels = out_channels;
  retstr->pooling_function = pooling_function;
  retstr->bias.data_table = 0;
  retstr->activation.function = BNNSActivationFunctionIdentity;
  retstr->bias.char data = 0;
  *(void *)&retstr->bias.BNNSDataType data_type = 0;
  retstr->bias.data_bias = 0.0;
  *(int32x2_t *)&retstr->activation.alpha = vdup_n_s32(0x7FC00000u);
  retstr->activation.iscale = 1;
  *(_OWORD *)&retstr->activation.ioffset = 0u;
  *(_OWORD *)&retstr->activation.ioffset_per_channel = 0u;
}

void __swiftcall BNNSFullyConnectedLayerParameters.init(in_size:out_size:weights:)(BNNSFullyConnectedLayerParameters *__return_ptr retstr, Swift::Int in_size, Swift::Int out_size, BNNSLayerData *weights)
{
  char data = weights->data;
  BNNSDataType data_type = weights->data_type;
  data_table = weights->data_table;
  retstr->in_size = in_size;
  retstr->out_size = out_size;
  retstr->weights.char data = data;
  retstr->weights.BNNSDataType data_type = data_type;
  *(void *)&retstr->weights.data_scale = *(void *)&weights->data_scale;
  retstr->bias.data_table = 0;
  retstr->activation.function = BNNSActivationFunctionIdentity;
  retstr->bias.char data = 0;
  *(void *)&retstr->bias.BNNSDataType data_type = 0;
  retstr->weights.data_table = data_table;
  retstr->bias.data_bias = 0.0;
  *(int32x2_t *)&retstr->activation.alpha = vdup_n_s32(0x7FC00000u);
  retstr->activation.iscale = 1;
  *(_OWORD *)&retstr->activation.ioffset = 0u;
  *(_OWORD *)&retstr->activation.ioffset_per_channel = 0u;
}

uint64_t AccelerateBuffer<>.withUnsafeBufferPointer<A>(_:)@<X0>(uint64_t a1@<X3>, uint64_t a2@<X8>)
{
  uint64_t v5 = type metadata accessor for Optional();
  MEMORY[0x1F4188790](v5 - 8);
  int64_t v7 = (char *)&v10 - v6;
  uint64_t result = dispatch thunk of Sequence.withContiguousStorageIfAvailable<A>(_:)();
  if (!v2)
  {
    uint64_t v9 = *(void *)(a1 - 8);
    uint64_t result = (*(uint64_t (**)(char *, uint64_t, uint64_t))(v9 + 48))(v7, 1, a1);
    if (result == 1) {
      __break(1u);
    }
    else {
      return (*(uint64_t (**)(uint64_t, char *, uint64_t))(v9 + 32))(a2, v7, a1);
    }
  }
  return result;
}

uint64_t AccelerateMutableBuffer<>.withUnsafeMutableBufferPointer<A>(_:)@<X0>(uint64_t a1@<X3>, uint64_t a2@<X8>)
{
  uint64_t v5 = type metadata accessor for Optional();
  MEMORY[0x1F4188790](v5 - 8);
  int64_t v7 = (char *)&v10 - v6;
  uint64_t result = dispatch thunk of MutableCollection.withContiguousMutableStorageIfAvailable<A>(_:)();
  if (!v2)
  {
    uint64_t v9 = *(void *)(a1 - 8);
    uint64_t result = (*(uint64_t (**)(char *, uint64_t, uint64_t))(v9 + 48))(v7, 1, a1);
    if (result == 1) {
      __break(1u);
    }
    else {
      return (*(uint64_t (**)(uint64_t, char *, uint64_t))(v9 + 32))(a2, v7, a1);
    }
  }
  return result;
}

void *protocol witness for AccelerateMutableBuffer.withUnsafeMutableBufferPointer<A>(_:) in conformance [A](void (*a1)(void *), uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v16 = a3;
  uint64_t v6 = (uint64_t)v4;
  Array._makeMutableAndUnique()();
  uint64_t v8 = *v4;
  uint64_t v9 = *(void *)(a4 + 16);
  if (_swift_isClassOrObjCExistentialType()) {
    uint64_t v10 = v8 & 0xFFFFFFFFFFFFFF8;
  }
  else {
    uint64_t v10 = v8;
  }
  uint64_t v11 = *(void *)(v10 + 16);
  if (_swift_isClassOrObjCExistentialType()) {
    uint64_t v12 = v8 & 0xFFFFFFFFFFFFFF8;
  }
  else {
    uint64_t v12 = v8;
  }
  v15[0] = v12
         + ((*(unsigned __int8 *)(*(void *)(v9 - 8) + 80) + 32) & ~(unint64_t)*(unsigned __int8 *)(*(void *)(v9 - 8) + 80));
  uint64_t v13 = v15[0];
  v15[1] = v11;
  a1(v15);
  return $defer #1 <A><A1>() in Array.withUnsafeMutableBufferPointer<A>(_:)(v15, v13, v11, v6, v9, v16, MEMORY[0x1E4FBB320]);
}

void protocol witness for AccelerateBuffer.count.getter in conformance [A]()
{
}

uint64_t protocol witness for AccelerateBuffer.withUnsafeBufferPointer<A>(_:) in conformance [A]()
{
  return Array.withUnsafeBufferPointer<A>(_:)();
}

void *protocol witness for AccelerateMutableBuffer.withUnsafeMutableBufferPointer<A>(_:) in conformance ContiguousArray<A>(void (*a1)(void *), uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v12 = a3;
  ContiguousArray._makeMutableAndUnique()();
  uint64_t v7 = *(void *)(*(void *)v4 + 16);
  uint64_t v8 = *(void *)(a4 + 16);
  v11[0] = *(void *)v4
         + ((*(unsigned __int8 *)(*(void *)(v8 - 8) + 80) + 32) & ~(unint64_t)*(unsigned __int8 *)(*(void *)(v8 - 8) + 80));
  uint64_t v9 = v11[0];
  v11[1] = v7;
  a1(v11);
  return $defer #1 <A><A1>() in Array.withUnsafeMutableBufferPointer<A>(_:)(v11, v9, v7, v4, v8, v12, MEMORY[0x1E4FBBB30]);
}

uint64_t protocol witness for AccelerateBuffer.count.getter in conformance ContiguousArray<A>(uint64_t a1)
{
  return MEMORY[0x1F4184F38](*v1, *(void *)(a1 + 16));
}

uint64_t protocol witness for AccelerateBuffer.withUnsafeBufferPointer<A>(_:) in conformance ContiguousArray<A>()
{
  return ContiguousArray.withUnsafeBufferPointer<A>(_:)();
}

uint64_t protocol witness for AccelerateMutableBuffer.withUnsafeMutableBufferPointer<A>(_:) in conformance ArraySlice<A>(void (*a1)(void *), uint64_t a2, uint64_t a3, uint64_t a4)
{
  v10[3] = a3;
  uint64_t v6 = *(void *)(a4 + 16);
  Swift::Int v7 = ArraySlice._getCount()();
  ArraySlice._makeMutableAndUnique()();
  v10[0] = *(void *)(v4 + 8) + *(void *)(*(void *)(v6 - 8) + 72) * *(void *)(v4 + 16);
  uint64_t v8 = v10[0];
  v10[1] = v7;
  a1(v10);
  return $defer #1 <A><A1>() in ArraySlice.withUnsafeMutableBufferPointer<A>(_:)(v10, v8, v7, v4, v6);
}

uint64_t protocol witness for AccelerateBuffer.count.getter in conformance ArraySlice<A>(uint64_t a1)
{
  return MEMORY[0x1F4184A00](*v1, v1[1], v1[2], v1[3], *(void *)(a1 + 16));
}

uint64_t protocol witness for AccelerateBuffer.withUnsafeBufferPointer<A>(_:) in conformance ArraySlice<A>()
{
  return ArraySlice.withUnsafeBufferPointer<A>(_:)();
}

uint64_t protocol witness for AccelerateBuffer.withUnsafeBufferPointer<A>(_:) in conformance UnsafeBufferPointer<A>@<X0>(uint64_t a1@<X2>, uint64_t a2@<X8>)
{
  return AccelerateBuffer<>.withUnsafeBufferPointer<A>(_:)(a1, a2);
}

uint64_t protocol witness for AccelerateMutableBuffer.withUnsafeMutableBufferPointer<A>(_:) in conformance UnsafeMutableBufferPointer<A>@<X0>(uint64_t a1@<X2>, uint64_t a2@<X8>)
{
  return AccelerateMutableBuffer<>.withUnsafeMutableBufferPointer<A>(_:)(a1, a2);
}

uint64_t protocol witness for AccelerateBuffer.withUnsafeBufferPointer<A>(_:) in conformance UnsafeMutableBufferPointer<A>@<X0>(uint64_t a1@<X2>, uint64_t a2@<X8>)
{
  return AccelerateBuffer<>.withUnsafeBufferPointer<A>(_:)(a1, a2);
}

uint64_t protocol witness for AccelerateBuffer.count.getter in conformance <> Slice<A>(uint64_t a1)
{
  uint64_t WitnessTable = swift_getWitnessTable();

  return MEMORY[0x1F4184390](a1, WitnessTable);
}

uint64_t protocol witness for AccelerateBuffer.withUnsafeBufferPointer<A>(_:) in conformance <> Slice<A>@<X0>(uint64_t a1@<X2>, uint64_t a2@<X8>)
{
  return AccelerateBuffer<>.withUnsafeBufferPointer<A>(_:)(a1, a2);
}

uint64_t protocol witness for AccelerateMutableBuffer.withUnsafeMutableBufferPointer<A>(_:) in conformance <> Slice<A>@<X0>(uint64_t a1@<X2>, uint64_t a2@<X8>)
{
  return AccelerateMutableBuffer<>.withUnsafeMutableBufferPointer<A>(_:)(a1, a2);
}

uint64_t AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:));
}

{
  return AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:));
}

{
  return AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:));
}

{
  return AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:));
}

uint64_t closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(uint64_t result, void (*a2)(uint64_t *))
{
  uint64_t v2 = *(void *)result;
  if (*(void *)result)
  {
    uint64_t v4 = *(void *)(result + 8);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    uint64_t result = swift_allocObject();
    *(_OWORD *)(result + 16) = xmmword_1D2135280;
    if ((v4 & 0x8000000000000000) == 0)
    {
      *(void *)(result + 32) = v2;
      *(void *)(result + 40) = 1;
      *(void *)(result + 48) = v4;
      *(void *)(result + 56) = v4;
      *(void *)(result + 64) = 0;
      uint64_t v5 = result;
      a2(&v5);
      return swift_bridgeObjectRelease();
    }
    __break(1u);
  }
  __break(1u);
  return result;
}

{
  uint64_t v2;
  uint64_t v3;
  uint64_t v5;

  uint64_t v2 = *(void *)result;
  if (*(void *)result)
  {
    float v3 = *(void *)(result + 8);
    if ((unint64_t)(v3 - 0x2000000000000000) >> 62 == 3)
    {
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
      uint64_t result = swift_allocObject();
      *(_OWORD *)(result + 16) = xmmword_1D2135280;
      if ((v3 & 0x8000000000000000) == 0)
      {
        *(void *)(result + 32) = v2;
        *(void *)(result + 40) = 1;
        *(void *)(result + 48) = v3;
        *(void *)(result + 56) = 4 * v3;
        *(void *)(result + 64) = 0;
        uint64_t v5 = result;
        a2(&v5);
        return swift_bridgeObjectRelease();
      }
    }
    else
    {
      __break(1u);
    }
    __break(1u);
  }
  __break(1u);
  return result;
}

{
  uint64_t v2;
  uint64_t v3;
  uint64_t v5;

  uint64_t v2 = *(void *)result;
  if (*(void *)result)
  {
    float v3 = *(void *)(result + 8);
    if (v3 + 0x4000000000000000 < 0)
    {
      __break(1u);
    }
    else
    {
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
      uint64_t result = swift_allocObject();
      *(_OWORD *)(result + 16) = xmmword_1D2135280;
      if ((v3 & 0x8000000000000000) == 0)
      {
        *(void *)(result + 32) = v2;
        *(void *)(result + 40) = 1;
        *(void *)(result + 48) = v3;
        *(void *)(result + 56) = 2 * v3;
        *(void *)(result + 64) = 0;
        uint64_t v5 = result;
        a2(&v5);
        return swift_bridgeObjectRelease();
      }
    }
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(uint64_t a1)
{
  return partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(a1, (uint64_t (*)(uint64_t, void, void, void, void, void))closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:));
}

{
  return partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(a1);
}

{
  uint64_t v1;

  return closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(a1, *(void (**)(uint64_t *))(v1 + 40));
}

{
  return partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(a1, (uint64_t (*)(uint64_t, void, void, void, void, void))closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:));
}

uint64_t partial apply for closure #1 in AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(uint64_t a1, uint64_t (*a2)(uint64_t, void, void, void, void, void))
{
  return a2(a1, v2[5], v2[6], v2[2], v2[3], v2[4]);
}

uint64_t AccelerateMutableBuffer<>.withUnsafePixelBuffer<A>(body:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  v7[2] = a3;
  v7[3] = a4;
  v7[4] = a5;
  v7[5] = a1;
  v7[6] = a2;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(a5 + 16))(a6, v7, a4, a3);
}

Accelerate::AccelerateMatrixOrder_optional __swiftcall AccelerateMatrixOrder.init(rawValue:)(Swift::Int rawValue)
{
  if (rawValue == 102) {
    v1.value = Accelerate_AccelerateMatrixOrder_columnMajor;
  }
  else {
    v1.value = Accelerate_AccelerateMatrixOrder_unknownDefault;
  }
  if (rawValue == 101) {
    return 0;
  }
  else {
    return v1;
  }
}

uint64_t AccelerateMatrixOrder.rawValue.getter(char a1)
{
  return (a1 & 1u) + 101;
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance AccelerateMatrixOrder()
{
  int v1 = *v0;
  Hasher.init(_seed:)();
  if (v1) {
    Swift::UInt v2 = 102;
  }
  else {
    Swift::UInt v2 = 101;
  }
  Hasher._combine(_:)(v2);
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance AccelerateMatrixOrder()
{
  if (*v0) {
    Swift::UInt v1 = 102;
  }
  else {
    Swift::UInt v1 = 101;
  }
  Hasher._combine(_:)(v1);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance AccelerateMatrixOrder()
{
  int v1 = *v0;
  Hasher.init(_seed:)();
  if (v1) {
    Swift::UInt v2 = 102;
  }
  else {
    Swift::UInt v2 = 101;
  }
  Hasher._combine(_:)(v2);
  return Hasher._finalize()();
}

void *protocol witness for RawRepresentable.init(rawValue:) in conformance AccelerateMatrixOrder@<X0>(void *result@<X0>, char *a2@<X8>)
{
  if (*result == 102) {
    char v2 = 1;
  }
  else {
    char v2 = 2;
  }
  if (*result == 101) {
    char v3 = 0;
  }
  else {
    char v3 = v2;
  }
  *a2 = v3;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance AccelerateMatrixOrder(uint64_t *a1@<X8>)
{
  uint64_t v2 = 101;
  if (*v1) {
    uint64_t v2 = 102;
  }
  *a1 = v2;
}

uint64_t vImage.PixelBuffer<>.rowCount.getter()
{
  return vImage.PixelBuffer.height.getter();
}

uint64_t vImage.PixelBuffer<>.columnCount.getter()
{
  return vImage.PixelBuffer.width.getter();
}

uint64_t vImage.PixelBuffer<>.accelerateMatrixOrder.getter()
{
  return 0;
}

uint64_t vImage.PixelBuffer<>.leadingDimension.getter(uint64_t a1, uint64_t a2)
{
  return vImage.PixelBuffer<>.rowStride.getter(a1, a2);
}

uint64_t instantiation function for generic protocol witness table for [A](uint64_t a1)
{
  uint64_t result = swift_getWitnessTable();
  *(void *)(a1 + 8) = result;
  return result;
}

uint64_t instantiation function for generic protocol witness table for ContiguousArray<A>(uint64_t a1)
{
  uint64_t result = swift_getWitnessTable();
  *(void *)(a1 + 8) = result;
  return result;
}

uint64_t instantiation function for generic protocol witness table for ArraySlice<A>(uint64_t a1)
{
  uint64_t result = swift_getWitnessTable();
  *(void *)(a1 + 8) = result;
  return result;
}

uint64_t instantiation function for generic protocol witness table for UnsafeMutableBufferPointer<A>(uint64_t a1)
{
  uint64_t result = swift_getWitnessTable();
  *(void *)(a1 + 8) = result;
  return result;
}

uint64_t instantiation function for generic protocol witness table for <> Slice<A>(uint64_t a1)
{
  uint64_t result = swift_getWitnessTable();
  *(void *)(a1 + 8) = result;
  return result;
}

unint64_t lazy protocol witness table accessor for type AccelerateMatrixOrder and conformance AccelerateMatrixOrder()
{
  unint64_t result = lazy protocol witness table cache variable for type AccelerateMatrixOrder and conformance AccelerateMatrixOrder;
  if (!lazy protocol witness table cache variable for type AccelerateMatrixOrder and conformance AccelerateMatrixOrder)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type AccelerateMatrixOrder and conformance AccelerateMatrixOrder);
  }
  return result;
}

uint64_t protocol witness for AccelerateMatrixBuffer.leadingDimension.getter in conformance <> vImage.PixelBuffer<A>(uint64_t a1, uint64_t a2)
{
  return vImage.PixelBuffer<>.leadingDimension.getter(a1, *(void *)(a2 - 8));
}

uint64_t instantiation function for generic protocol witness table for <> vImage.PixelBuffer<A>(uint64_t a1)
{
  uint64_t result = swift_getWitnessTable();
  *(void *)(a1 + 8) = result;
  return result;
}

{
  uint64_t result;

  uint64_t result = swift_getWitnessTable();
  *(void *)(a1 + 8) = result;
  return result;
}

uint64_t dispatch thunk of AccelerateBuffer.count.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

uint64_t dispatch thunk of AccelerateBuffer.withUnsafeBufferPointer<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return (*(uint64_t (**)(void))(a5 + 24))();
}

uint64_t dispatch thunk of AccelerateMutableBuffer.withUnsafeMutableBufferPointer<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return (*(uint64_t (**)(void))(a5 + 16))();
}

unsigned char *storeEnumTagSinglePayload for AccelerateMatrixOrder(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 1 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFF) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFE)
  {
    unsigned int v6 = ((a2 - 255) >> 8) + 1;
    *uint64_t result = a2 + 1;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D21045ACLL);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 1;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for AccelerateMatrixOrder()
{
  return &type metadata for AccelerateMatrixOrder;
}

uint64_t dispatch thunk of AccelerateMatrixBuffer.rowCount.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

uint64_t dispatch thunk of AccelerateMatrixBuffer.columnCount.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 24))();
}

uint64_t dispatch thunk of AccelerateMatrixBuffer.accelerateMatrixOrder.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 32))() & 1;
}

uint64_t dispatch thunk of AccelerateMatrixBuffer.leadingDimension.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 40))();
}

uint64_t dispatch thunk of AccelerateMatrixBuffer.withUnsafeBufferPointer<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return (*(uint64_t (**)(void))(a5 + 48))();
}

uint64_t dispatch thunk of AccelerateMutableMatrixBuffer.withUnsafeMutableBufferPointer<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return (*(uint64_t (**)(void))(a5 + 16))();
}

void *$defer #1 <A><A1>() in Array.withUnsafeMutableBufferPointer<A>(_:)(void *result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(void, uint64_t))
{
  if (*result)
  {
    if (*result == a2)
    {
      if (result[1] == a3) {
        return (void *)a7(0, a5);
      }
    }
    else
    {
      __break(1u);
    }
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t $defer #1 <A><A1>() in ArraySlice.withUnsafeMutableBufferPointer<A>(_:)(void *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  if (*a1)
  {
    if (*a1 == a2)
    {
      if (a1[1] == a3) {
        return MEMORY[0x1F4184A40](0, a5);
      }
    }
    else
    {
      __break(1u);
    }
    __break(1u);
  }
  __break(1u);
  return MEMORY[0x1F4184A40](a1, a2);
}

uint64_t BNNSNDArrayDescriptor.shape.getter@<X0>(uint64_t a1@<X8>)
{
  int v3 = *(_DWORD *)(v1 + 4);
  if (v3 < 0x40000)
  {
    if (v3 >= 196608)
    {
      if (v3 <= 196609)
      {
        if (v3 == 196608)
        {
          uint64_t v20 = *(void *)(v1 + 24);
          uint64_t v21 = *(void *)(v1 + 88);
          long long v53 = *(_OWORD *)(v1 + 8);
          *(void *)size_t v54 = v20;
          *(_OWORD *)&v54[8] = *(_OWORD *)(v1 + 72);
          *(void *)&v54[24] = v21;
          _s10Accelerate4BNNSO5ShapeOWOi4_((uint64_t)&v53);
        }
        else
        {
          uint64_t v42 = *(void *)(v1 + 24);
          uint64_t v43 = *(void *)(v1 + 88);
          long long v53 = *(_OWORD *)(v1 + 8);
          *(void *)size_t v54 = v42;
          *(_OWORD *)&v54[8] = *(_OWORD *)(v1 + 72);
          *(void *)&v54[24] = v43;
          _s10Accelerate4BNNSO5ShapeOWOi7_((uint64_t)&v53);
        }
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      }
      switch(v3)
      {
        case 196610:
          uint64_t v25 = *(void *)(v1 + 24);
          uint64_t v26 = *(void *)(v1 + 88);
          long long v53 = *(_OWORD *)(v1 + 8);
          *(void *)size_t v54 = v25;
          *(_OWORD *)&v54[8] = *(_OWORD *)(v1 + 72);
          *(void *)&v54[24] = v26;
          _s10Accelerate4BNNSO5ShapeOWOi8_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
        case 229376:
          uint64_t v11 = *(void *)(v1 + 24);
          uint64_t v12 = *(void *)(v1 + 88);
          long long v53 = *(_OWORD *)(v1 + 8);
          *(void *)size_t v54 = v11;
          *(_OWORD *)&v54[8] = *(_OWORD *)(v1 + 72);
          *(void *)&v54[24] = v12;
          _s10Accelerate4BNNSO5ShapeOWOi6_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
        case 229377:
          uint64_t v37 = *(void *)(v1 + 24);
          uint64_t v38 = *(void *)(v1 + 88);
          long long v53 = *(_OWORD *)(v1 + 8);
          *(void *)size_t v54 = v37;
          *(_OWORD *)&v54[8] = *(_OWORD *)(v1 + 72);
          *(void *)&v54[24] = v38;
          _s10Accelerate4BNNSO5ShapeOWOi5_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      }
    }
    else if (v3 <= 0x20000)
    {
      if (v3 == 0x10000)
      {
        uint64_t v17 = *(void *)(v1 + 72);
        *(void *)&long long v53 = *(void *)(v1 + 8);
        *((void *)&v53 + 1) = v17;
        _s10Accelerate4BNNSO5ShapeOWOi_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      }
      if (v3 == 0x20000)
      {
        long long v39 = *(_OWORD *)(v1 + 72);
        long long v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)size_t v54 = v39;
        _s10Accelerate4BNNSO5ShapeOWOi1_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      }
    }
    else
    {
      switch(v3)
      {
        case 131073:
          long long v22 = *(_OWORD *)(v1 + 72);
          long long v53 = *(_OWORD *)(v1 + 8);
          *(_OWORD *)size_t v54 = v22;
          _s10Accelerate4BNNSO5ShapeOWOi0_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
        case 163840:
          long long v7 = *(_OWORD *)(v1 + 72);
          long long v53 = *(_OWORD *)(v1 + 8);
          *(_OWORD *)size_t v54 = v7;
          _s10Accelerate4BNNSO5ShapeOWOi3_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
        case 163841:
          long long v33 = *(_OWORD *)(v1 + 72);
          long long v53 = *(_OWORD *)(v1 + 8);
          *(_OWORD *)size_t v54 = v33;
          _s10Accelerate4BNNSO5ShapeOWOi2_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      }
    }
  }
  else if (v3 < 425984)
  {
    if (v3 <= 294912)
    {
      if (v3 == 0x40000)
      {
        long long v18 = *(_OWORD *)(v1 + 24);
        long long v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)size_t v54 = v18;
        long long v19 = *(_OWORD *)(v1 + 88);
        *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 72);
        *(_OWORD *)&v54[32] = v19;
        _s10Accelerate4BNNSO5ShapeOWOi9_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      }
      if (v3 == 294912)
      {
        long long v40 = *(_OWORD *)(v1 + 24);
        long long v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)size_t v54 = v40;
        long long v41 = *(_OWORD *)(v1 + 88);
        *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 72);
        *(_OWORD *)&v54[32] = v41;
        _s10Accelerate4BNNSO5ShapeOWOi11_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      }
    }
    else
    {
      switch(v3)
      {
        case 294913:
          long long v23 = *(_OWORD *)(v1 + 24);
          long long v53 = *(_OWORD *)(v1 + 8);
          *(_OWORD *)size_t v54 = v23;
          long long v24 = *(_OWORD *)(v1 + 88);
          *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 72);
          *(_OWORD *)&v54[32] = v24;
          _s10Accelerate4BNNSO5ShapeOWOi10_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
        case 360448:
          uint64_t v8 = *(void *)(v1 + 40);
          uint64_t v9 = *(void *)(v1 + 104);
          long long v10 = *(_OWORD *)(v1 + 24);
          long long v53 = *(_OWORD *)(v1 + 8);
          *(_OWORD *)size_t v54 = v10;
          *(void *)&v54[16] = v8;
          *(_OWORD *)&v54[24] = *(_OWORD *)(v1 + 72);
          *(_OWORD *)&v54[40] = *(_OWORD *)(v1 + 88);
          *(void *)&v54[56] = v9;
          _s10Accelerate4BNNSO5ShapeOWOi13_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
        case 360449:
          uint64_t v34 = *(void *)(v1 + 40);
          uint64_t v35 = *(void *)(v1 + 104);
          long long v36 = *(_OWORD *)(v1 + 24);
          long long v53 = *(_OWORD *)(v1 + 8);
          *(_OWORD *)size_t v54 = v36;
          *(void *)&v54[16] = v34;
          *(_OWORD *)&v54[24] = *(_OWORD *)(v1 + 72);
          *(_OWORD *)&v54[40] = *(_OWORD *)(v1 + 88);
          *(void *)&v54[56] = v35;
          _s10Accelerate4BNNSO5ShapeOWOi12_((uint64_t)&v53);
          return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      }
    }
  }
  else if (v3 > 491520)
  {
    switch(v3)
    {
      case 491521:
        uint64_t v30 = *(void *)(v1 + 56);
        uint64_t v31 = *(void *)(v1 + 120);
        long long v32 = *(_OWORD *)(v1 + 24);
        long long v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)size_t v54 = v32;
        *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 40);
        *(void *)&v54[32] = v30;
        *(_OWORD *)&v54[40] = *(_OWORD *)(v1 + 72);
        *(_OWORD *)&v54[56] = *(_OWORD *)(v1 + 88);
        *(_OWORD *)&v54[72] = *(_OWORD *)(v1 + 104);
        *(void *)&v54[88] = v31;
        _s10Accelerate4BNNSO5ShapeOWOi16_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      case 557056:
        long long v13 = *(_OWORD *)(v1 + 24);
        long long v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)size_t v54 = v13;
        long long v14 = *(_OWORD *)(v1 + 56);
        *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 40);
        *(_OWORD *)&v54[32] = v14;
        long long v15 = *(_OWORD *)(v1 + 88);
        *(_OWORD *)&v54[48] = *(_OWORD *)(v1 + 72);
        *(_OWORD *)&v54[64] = v15;
        long long v16 = *(_OWORD *)(v1 + 120);
        *(_OWORD *)&v54[80] = *(_OWORD *)(v1 + 104);
        long long v55 = v16;
        _s10Accelerate4BNNSO5ShapeOWOi19_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      case 557057:
        long long v47 = *(_OWORD *)(v1 + 24);
        long long v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)size_t v54 = v47;
        long long v48 = *(_OWORD *)(v1 + 56);
        *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 40);
        *(_OWORD *)&v54[32] = v48;
        long long v49 = *(_OWORD *)(v1 + 88);
        *(_OWORD *)&v54[48] = *(_OWORD *)(v1 + 72);
        *(_OWORD *)&v54[64] = v49;
        long long v50 = *(_OWORD *)(v1 + 120);
        *(_OWORD *)&v54[80] = *(_OWORD *)(v1 + 104);
        long long v55 = v50;
        _s10Accelerate4BNNSO5ShapeOWOi18_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
    }
  }
  else
  {
    switch(v3)
    {
      case 425984:
        long long v27 = *(_OWORD *)(v1 + 24);
        long long v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)size_t v54 = v27;
        long long v28 = *(_OWORD *)(v1 + 72);
        *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 40);
        *(_OWORD *)&v54[32] = v28;
        long long v29 = *(_OWORD *)(v1 + 104);
        *(_OWORD *)&v54[48] = *(_OWORD *)(v1 + 88);
        *(_OWORD *)&v54[64] = v29;
        _s10Accelerate4BNNSO5ShapeOWOi15_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      case 425985:
        long long v4 = *(_OWORD *)(v1 + 24);
        long long v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)size_t v54 = v4;
        long long v5 = *(_OWORD *)(v1 + 72);
        *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 40);
        *(_OWORD *)&v54[32] = v5;
        long long v6 = *(_OWORD *)(v1 + 104);
        *(_OWORD *)&v54[48] = *(_OWORD *)(v1 + 88);
        *(_OWORD *)&v54[64] = v6;
        _s10Accelerate4BNNSO5ShapeOWOi14_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
      case 491520:
        uint64_t v44 = *(void *)(v1 + 56);
        uint64_t v45 = *(void *)(v1 + 120);
        long long v46 = *(_OWORD *)(v1 + 24);
        long long v53 = *(_OWORD *)(v1 + 8);
        *(_OWORD *)size_t v54 = v46;
        *(_OWORD *)&v54[16] = *(_OWORD *)(v1 + 40);
        *(void *)&v54[32] = v44;
        *(_OWORD *)&v54[40] = *(_OWORD *)(v1 + 72);
        *(_OWORD *)&v54[56] = *(_OWORD *)(v1 + 88);
        *(_OWORD *)&v54[72] = *(_OWORD *)(v1 + 104);
        *(void *)&v54[88] = v45;
        _s10Accelerate4BNNSO5ShapeOWOi17_((uint64_t)&v53);
        return outlined init with take of BNNS.Shape((uint64_t)&v53, a1);
    }
  }
  _StringGuts.grow(_:)(18);
  v52._object = (void *)0x80000001D213CA70;
  v52._countAndFlagsBits = 0xD000000000000010;
  String.append(_:)(v52);
  type metadata accessor for BNNSDataLayout(0);
  _print_unlocked<A, B>(_:_:)();
  uint64_t result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

__n128 static BNNSNDArrayDescriptor.allocateUninitialized(scalarType:shape:batchSize:)@<Q0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X8>)
{
  outlined init with take of BNNS.Shape(a3, (uint64_t)v16);
  int v9 = (*(uint64_t (**)(uint64_t, uint64_t))(a2 + 8))(a1, a2);
  helper #1 <A>(_:) in static BNNSNDArrayDescriptor.allocateUninitialized(scalarType:shape:batchSize:)(a4, (uint64_t)v16, v9, v15);
  long long v10 = v15[9];
  *(_OWORD *)(a5 + 128) = v15[8];
  *(_OWORD *)(a5 + 144) = v10;
  *(_OWORD *)(a5 + 160) = v15[10];
  long long v11 = v15[5];
  *(_OWORD *)(a5 + 64) = v15[4];
  *(_OWORD *)(a5 + 80) = v11;
  long long v12 = v15[7];
  *(_OWORD *)(a5 + 96) = v15[6];
  *(_OWORD *)(a5 + 112) = v12;
  long long v13 = v15[1];
  *(_OWORD *)a5 = v15[0];
  *(_OWORD *)(a5 + 16) = v13;
  __n128 result = (__n128)v15[3];
  *(_OWORD *)(a5 + 32) = v15[2];
  *(__n128 *)(a5 + 48) = result;
  return result;
}

Swift::Void __swiftcall BNNSNDArrayDescriptor.deallocate()()
{
  outlined init with take of BNNSNDArrayDescriptor?(v0 + 136, (uint64_t)&v1, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v1, (uint64_t)&v2, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  if (v2) {
    MEMORY[0x1D26009C0](v2, -1, -1);
  }
}

uint64_t static Float.bnnsDataType.getter()
{
  return 65568;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance Float()
{
  return 65568;
}

uint64_t static Float16.bnnsDataType.getter()
{
  return 65552;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance Float16()
{
  return 65552;
}

uint64_t static Int8.bnnsDataType.getter()
{
  return 131080;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance Int8()
{
  return 131080;
}

uint64_t static Int16.bnnsDataType.getter()
{
  return 131088;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance Int16()
{
  return 131088;
}

uint64_t static Int32.bnnsDataType.getter()
{
  return 131104;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance Int32()
{
  return 131104;
}

uint64_t static UInt8.bnnsDataType.getter()
{
  return 262152;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance UInt8()
{
  return 262152;
}

uint64_t static UInt16.bnnsDataType.getter()
{
  return 262160;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance UInt16()
{
  return 262160;
}

uint64_t static UInt32.bnnsDataType.getter()
{
  return 262176;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance UInt32()
{
  return 262176;
}

uint64_t static Bool.bnnsDataType.getter()
{
  return 1048584;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance Bool()
{
  return 1048584;
}

uint64_t static Int64.bnnsDataType.getter()
{
  return 131136;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance Int64()
{
  return 131136;
}

uint64_t static UInt64.bnnsDataType.getter()
{
  return 262208;
}

uint64_t protocol witness for static BNNSScalar.bnnsDataType.getter in conformance UInt64()
{
  return 262208;
}

__n128 BNNSNDArrayDescriptor.init(dataType:shape:)@<Q0>(int a1@<W0>, uint64_t a2@<X1>, __n128 *a3@<X8>)
{
  outlined init with take of BNNS.Shape(a2, (uint64_t)v12);
  outlined init with take of BNNS.Shape((uint64_t)v12, (uint64_t)v10);
  specialized static BNNS.makeArrayDescriptor(shape:data:dataType:)((uint64_t)v10, 0, a1, (uint64_t)v11);
  __n128 v5 = v11[9];
  a3[8] = v11[8];
  a3[9] = v5;
  a3[10] = v11[10];
  __n128 v6 = v11[5];
  a3[4] = v11[4];
  a3[5] = v6;
  __n128 v7 = v11[7];
  a3[6] = v11[6];
  a3[7] = v7;
  __n128 v8 = v11[1];
  *a3 = v11[0];
  a3[1] = v8;
  __n128 result = v11[3];
  a3[2] = v11[2];
  a3[3] = result;
  return result;
}

uint64_t BNNSNDArrayDescriptor.init(data:scalarType:shape:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X8>)
{
  outlined init with take of BNNS.Shape(a5, (uint64_t)v14);
  int v11 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 8))(a3, a4);
  helper #1 <A>(_:) in BNNSNDArrayDescriptor.init(data:scalarType:shape:)(a1, a2, (uint64_t)v14, v11, a3, (uint64_t)v13);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v13, a6, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
}

void helper #1 <A>(_:) in BNNSNDArrayDescriptor.init(data:scalarType:shape:)(uint64_t a1@<X1>, uint64_t a2@<X2>, uint64_t a3@<X3>, int a4@<W4>, uint64_t a5@<X5>, uint64_t a6@<X8>)
{
  if (a1) {
    uint64_t v6 = a2 - a1;
  }
  else {
    uint64_t v6 = 0;
  }
  uint64_t v7 = *(void *)(*(void *)(a5 - 8) + 72);
  if (!v7)
  {
    __break(1u);
LABEL_12:
    __break(1u);
    return;
  }
  if (v6 == 0x8000000000000000 && v7 == -1) {
    goto LABEL_12;
  }
  uint64_t v17 = v6 / v7;
  outlined init with take of BNNS.Shape(a3, (uint64_t)v23);
  BNNS.Shape.size.getter((uint64_t)&v18);
  long long v9 = v18;
  long long v10 = v19;
  long long v11 = v20;
  unint64_t v12 = v21;
  unint64_t v13 = v22;
  outlined init with take of BNNS.Shape(a3, (uint64_t)v23);
  BNNS.Shape.stride.getter((uint64_t)&v18);
  if (v17 == specialized static BNNS.calculateBatchStride(size:stride:)(v9, *((unint64_t *)&v9 + 1), v10, *((unint64_t *)&v10 + 1), v11, *((unint64_t *)&v11 + 1), v12, v13, v18, *((unint64_t *)&v18 + 1), v19, *((unint64_t *)&v19 + 1), v20, *((unint64_t *)&v20 + 1), v21, v22))
  {
    outlined init with take of BNNS.Shape(a3, (uint64_t)v23);
    specialized static BNNS.makeArrayDescriptor(shape:data:dataType:)((uint64_t)v23, a1, a4, (uint64_t)&v18);
    _sSo21BNNSNDArrayDescriptoraSgWOi_((uint64_t)&v18);
  }
  else
  {
    _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&v18);
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v18, (uint64_t)v23, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v23, a6, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
}

uint64_t BNNSNDArrayDescriptor.init<A>(data:shape:)@<X0>(uint64_t a1@<X1>, uint64_t a2@<X2>, uint64_t a3@<X3>, uint64_t a4@<X4>, uint64_t a5@<X8>)
{
  outlined init with take of BNNS.Shape(a2, (uint64_t)v31);
  uint64_t v9 = UnsafeBufferPointer.baseAddress.getter();
  if (!v9) {
    goto LABEL_4;
  }
  uint64_t v10 = v9;
  outlined init with take of BNNS.Shape((uint64_t)v31, (uint64_t)v30);
  BNNS.Shape.size.getter((uint64_t)&v24);
  long long v11 = v24;
  long long v12 = v25;
  uint64_t v22 = a4;
  uint64_t v23 = a5;
  unint64_t v14 = v27;
  unint64_t v13 = v26;
  uint64_t v20 = v10;
  uint64_t v21 = a3;
  unint64_t v16 = v28;
  unint64_t v15 = v29;
  outlined init with take of BNNS.Shape((uint64_t)v31, (uint64_t)v30);
  BNNS.Shape.stride.getter((uint64_t)&v24);
  unint64_t v17 = v13;
  a5 = v23;
  if (specialized static BNNS.calculateBatchStride(size:stride:)(v11, *((unint64_t *)&v11 + 1), v12, *((unint64_t *)&v12 + 1), v17, v14, v16, v15, v24, *((unint64_t *)&v24 + 1), v25, *((unint64_t *)&v25 + 1), v26, v27, v28, v29) == a1)
  {
    outlined init with take of BNNS.Shape((uint64_t)v31, (uint64_t)v30);
    int v18 = (*(uint64_t (**)(uint64_t, uint64_t))(v22 + 8))(v21, v22);
    specialized static BNNS.makeArrayDescriptor(shape:data:dataType:)((uint64_t)v30, v20, v18, (uint64_t)&v24);
    _sSo21BNNSNDArrayDescriptoraSgWOi_((uint64_t)&v24);
  }
  else
  {
LABEL_4:
    _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&v24);
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v24, (uint64_t)v30, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v30, a5, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
}

void helper #1 <A>(_:) in static BNNSNDArrayDescriptor.allocateUninitialized(scalarType:shape:batchSize:)(uint64_t a1@<X1>, uint64_t a2@<X2>, int a3@<W3>, _OWORD *a4@<X8>)
{
  outlined init with take of BNNS.Shape(a2, (uint64_t)v25);
  BNNS.Shape.size.getter((uint64_t)&v20);
  long long v7 = v20;
  long long v8 = v21;
  long long v9 = v22;
  unint64_t v18 = v24;
  unint64_t v19 = v23;
  outlined init with take of BNNS.Shape(a2, (uint64_t)v25);
  BNNS.Shape.stride.getter((uint64_t)&v20);
  int64_t v10 = specialized static BNNS.calculateBatchStride(size:stride:)(v7, *((unint64_t *)&v7 + 1), v8, *((unint64_t *)&v8 + 1), v9, *((unint64_t *)&v9 + 1), v19, v18, v20, *((unint64_t *)&v20 + 1), v21, *((unint64_t *)&v21 + 1), v22, *((unint64_t *)&v22 + 1), v23, v24);
  if ((unsigned __int128)(a1 * (__int128)v10) >> 64 == (a1 * v10) >> 63)
  {
    uint64_t v11 = static UnsafeMutablePointer.allocate(capacity:)();
    outlined init with take of BNNS.Shape(a2, (uint64_t)&v20);
    specialized static BNNS.makeArrayDescriptor(shape:data:dataType:)((uint64_t)&v20, v11, a3, (uint64_t)v25);
    long long v12 = v25[9];
    a4[8] = v25[8];
    a4[9] = v12;
    a4[10] = v25[10];
    long long v13 = v25[5];
    a4[4] = v25[4];
    a4[5] = v13;
    long long v14 = v25[7];
    a4[6] = v25[6];
    a4[7] = v14;
    long long v15 = v25[1];
    *a4 = v25[0];
    a4[1] = v15;
    long long v16 = v25[3];
    a4[2] = v25[2];
    a4[3] = v16;
  }
  else
  {
    __break(1u);
  }
}

int64_t static BNNSNDArrayDescriptor.allocate<A>(initializingFrom:shape:batchSize:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, _OWORD *a7@<X8>)
{
  uint64_t v54 = a3;
  uint64_t v48 = a1;
  uint64_t v11 = *(void *)(a5 + 8);
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  uint64_t v46 = *(void *)(AssociatedTypeWitness - 8);
  uint64_t v47 = AssociatedTypeWitness;
  MEMORY[0x1F4188790](AssociatedTypeWitness);
  uint64_t v45 = (char *)&v44 - v13;
  outlined init with take of BNNS.Shape(a2, (uint64_t)v72);
  uint64_t v49 = v11;
  uint64_t v50 = a4;
  uint64_t v14 = swift_getAssociatedTypeWitness();
  long long v15 = *(uint64_t (**)(uint64_t, uint64_t))(a6 + 8);
  uint64_t v53 = v14;
  int v51 = v15(v14, a6);
  outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)v71);
  BNNS.Shape.size.getter((uint64_t)&v63);
  unint64_t v17 = v63;
  unint64_t v16 = v64;
  unint64_t v18 = v65;
  unint64_t v19 = v66;
  unint64_t v20 = v67;
  unint64_t v21 = v68;
  unint64_t v22 = v69;
  unint64_t v52 = v70;
  outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)v71);
  BNNS.Shape.stride.getter((uint64_t)&v63);
  int64_t v23 = specialized static BNNS.calculateBatchStride(size:stride:)(v17, v16, v18, v19, v20, v21, v22, v52, v63, v64, v65, v66, v67, v68, v69, v70);
  int64_t result = v54 * v23;
  if ((unsigned __int128)(v54 * (__int128)v23) >> 64 != (v54 * v23) >> 63)
  {
    __break(1u);
    goto LABEL_6;
  }
  uint64_t v25 = static UnsafeMutablePointer.allocate(capacity:)();
  outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)&v63);
  uint64_t v44 = v25;
  specialized static BNNS.makeArrayDescriptor(shape:data:dataType:)((uint64_t)&v63, v25, v51, (uint64_t)v71);
  outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)&v63);
  BNNS.Shape.size.getter((uint64_t)&v55);
  unint64_t v26 = v55;
  unint64_t v27 = v56;
  unint64_t v28 = v57;
  unint64_t v29 = v58;
  unint64_t v30 = v59;
  unint64_t v31 = v60;
  unint64_t v32 = v61;
  unint64_t v52 = v62;
  outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)&v63);
  BNNS.Shape.stride.getter((uint64_t)&v55);
  int64_t result = specialized static BNNS.calculateBatchStride(size:stride:)(v26, v27, v28, v29, v30, v31, v32, v52, v55, v56, v57, v58, v59, v60, v61, v62);
  if ((unsigned __int128)(v54 * (__int128)result) >> 64 != (v54 * result) >> 63)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v33 = v53;
  uint64_t v34 = UnsafeMutableBufferPointer.init(start:count:)();
  uint64_t v36 = v35;
  uint64_t v37 = v45;
  uint64_t v38 = UnsafeMutableBufferPointer.initialize<A>(from:)();
  (*(void (**)(char *, uint64_t))(v46 + 8))(v37, v47);
  int64_t result = MEMORY[0x1D25FFAE0](v34, v36, v33);
  if (v38 == result)
  {
    long long v39 = v71[9];
    a7[8] = v71[8];
    a7[9] = v39;
    a7[10] = v71[10];
    long long v40 = v71[5];
    a7[4] = v71[4];
    a7[5] = v40;
    long long v41 = v71[7];
    a7[6] = v71[6];
    a7[7] = v41;
    long long v42 = v71[1];
    *a7 = v71[0];
    a7[1] = v42;
    long long v43 = v71[3];
    a7[2] = v71[2];
    a7[3] = v43;
    return result;
  }
LABEL_7:
  __break(1u);
  return result;
}

int64_t static BNNSNDArrayDescriptor.allocate<A>(randomIn:shape:batchSize:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, _OWORD *a7@<X8>)
{
  uint64_t v29 = a6;
  *(void *)&long long v30 = a4;
  *((void *)&v30 + 1) = a5;
  uint64_t v32 = a3;
  uint64_t v28 = a1;
  outlined init with take of BNNS.Shape(a2, (uint64_t)v40);
  outlined init with take of BNNS.Shape((uint64_t)v40, (uint64_t)v39);
  BNNS.Shape.size.getter((uint64_t)&v33);
  long long v8 = v33;
  long long v9 = v34;
  long long v10 = v35;
  unint64_t v11 = v36;
  unint64_t v31 = v37;
  outlined init with take of BNNS.Shape((uint64_t)v40, (uint64_t)v39);
  BNNS.Shape.stride.getter((uint64_t)&v33);
  uint64_t v12 = v32;
  int64_t result = specialized static BNNS.calculateBatchStride(size:stride:)(v8, *((unint64_t *)&v8 + 1), v9, *((unint64_t *)&v9 + 1), v10, *((unint64_t *)&v10 + 1), v11, v31, v33, *((unint64_t *)&v33 + 1), v34, *((unint64_t *)&v34 + 1), v35, *((unint64_t *)&v35 + 1), v36, v37);
  if ((unsigned __int128)(v12 * (__int128)result) >> 64 == (v12 * result) >> 63)
  {
    if (((v32 * result) & 0x8000000000000000) == 0)
    {
      *(void *)&v39[0] = 0;
      *((void *)&v39[0] + 1) = v32 * result;
      MEMORY[0x1F4188790](result);
      long long v14 = v30;
      v25[1] = v30;
      uint64_t v26 = v29;
      uint64_t v27 = v28;
      uint64_t v15 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Range<Int>);
      unint64_t v16 = lazy protocol witness table accessor for type Range<Int> and conformance <> Range<A>();
      uint64_t v38 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in static BNNSNDArrayDescriptor.allocate<A>(randomIn:shape:batchSize:), (uint64_t)v25, v15, v14, MEMORY[0x1E4FBC248], v16, MEMORY[0x1E4FBC278], v17);
      outlined init with take of BNNS.Shape((uint64_t)v40, (uint64_t)&v33);
      uint64_t v18 = type metadata accessor for Array();
      uint64_t WitnessTable = swift_getWitnessTable();
      static BNNSNDArrayDescriptor.allocate<A>(initializingFrom:shape:batchSize:)((uint64_t)&v38, (uint64_t)&v33, v12, v18, WitnessTable, *((uint64_t *)&v14 + 1), v39);
      int64_t result = swift_bridgeObjectRelease();
      long long v20 = v39[9];
      a7[8] = v39[8];
      a7[9] = v20;
      a7[10] = v39[10];
      long long v21 = v39[5];
      a7[4] = v39[4];
      a7[5] = v21;
      long long v22 = v39[7];
      a7[6] = v39[6];
      a7[7] = v22;
      long long v23 = v39[1];
      *a7 = v39[0];
      a7[1] = v23;
      long long v24 = v39[3];
      a7[2] = v39[2];
      a7[3] = v24;
      return result;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

int64_t static BNNSNDArrayDescriptor.allocate<A>(randomIn:shape:batchSize:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, _OWORD *a8@<X8>)
{
  *(void *)&long long v29 = a6;
  *((void *)&v29 + 1) = a7;
  *(void *)&long long v30 = a4;
  *((void *)&v30 + 1) = a5;
  uint64_t v32 = a3;
  uint64_t v28 = a1;
  outlined init with take of BNNS.Shape(a2, (uint64_t)v40);
  outlined init with take of BNNS.Shape((uint64_t)v40, (uint64_t)v39);
  BNNS.Shape.size.getter((uint64_t)&v33);
  long long v9 = v33;
  long long v10 = v34;
  long long v11 = v35;
  unint64_t v12 = v36;
  unint64_t v31 = v37;
  outlined init with take of BNNS.Shape((uint64_t)v40, (uint64_t)v39);
  BNNS.Shape.stride.getter((uint64_t)&v33);
  uint64_t v13 = v32;
  int64_t result = specialized static BNNS.calculateBatchStride(size:stride:)(v9, *((unint64_t *)&v9 + 1), v10, *((unint64_t *)&v10 + 1), v11, *((unint64_t *)&v11 + 1), v12, v31, v33, *((unint64_t *)&v33 + 1), v34, *((unint64_t *)&v34 + 1), v35, *((unint64_t *)&v35 + 1), v36, v37);
  if ((unsigned __int128)(v13 * (__int128)result) >> 64 == (v13 * result) >> 63)
  {
    if (((v32 * result) & 0x8000000000000000) == 0)
    {
      *(void *)&v39[0] = 0;
      *((void *)&v39[0] + 1) = v32 * result;
      MEMORY[0x1F4188790](result);
      long long v15 = v30;
      v26[1] = v30;
      v26[2] = v29;
      uint64_t v27 = v28;
      uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Range<Int>);
      unint64_t v17 = lazy protocol witness table accessor for type Range<Int> and conformance <> Range<A>();
      uint64_t v38 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in static BNNSNDArrayDescriptor.allocate<A>(randomIn:shape:batchSize:), (uint64_t)v26, v16, v15, MEMORY[0x1E4FBC248], v17, MEMORY[0x1E4FBC278], v18);
      outlined init with take of BNNS.Shape((uint64_t)v40, (uint64_t)&v33);
      uint64_t v19 = type metadata accessor for Array();
      uint64_t WitnessTable = swift_getWitnessTable();
      static BNNSNDArrayDescriptor.allocate<A>(initializingFrom:shape:batchSize:)((uint64_t)&v38, (uint64_t)&v33, v13, v19, WitnessTable, *((uint64_t *)&v15 + 1), v39);
      int64_t result = swift_bridgeObjectRelease();
      long long v21 = v39[9];
      a8[8] = v39[8];
      a8[9] = v21;
      a8[10] = v39[10];
      long long v22 = v39[5];
      a8[4] = v39[4];
      a8[5] = v22;
      long long v23 = v39[7];
      a8[6] = v39[6];
      a8[7] = v23;
      long long v24 = v39[1];
      *a8 = v39[0];
      a8[1] = v24;
      long long v25 = v39[3];
      a8[2] = v39[2];
      a8[3] = v25;
      return result;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

int64_t static BNNSNDArrayDescriptor.allocate<A, B>(randomIn:using:shape:batchSize:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, _OWORD *a9@<X8>)
{
  uint64_t v33 = a8;
  uint64_t v34 = a5;
  uint64_t v35 = a7;
  uint64_t v31 = a6;
  uint64_t v32 = a2;
  uint64_t v37 = a4;
  uint64_t v30 = a1;
  outlined init with take of BNNS.Shape(a3, (uint64_t)v45);
  outlined init with take of BNNS.Shape((uint64_t)v45, (uint64_t)v44);
  BNNS.Shape.size.getter((uint64_t)&v38);
  long long v10 = v38;
  long long v11 = v39;
  long long v12 = v40;
  unint64_t v13 = v41;
  unint64_t v36 = v42;
  outlined init with take of BNNS.Shape((uint64_t)v45, (uint64_t)v44);
  BNNS.Shape.stride.getter((uint64_t)&v38);
  uint64_t v14 = v37;
  int64_t result = specialized static BNNS.calculateBatchStride(size:stride:)(v10, *((unint64_t *)&v10 + 1), v11, *((unint64_t *)&v11 + 1), v12, *((unint64_t *)&v12 + 1), v13, v36, v38, *((unint64_t *)&v38 + 1), v39, *((unint64_t *)&v39 + 1), v40, *((unint64_t *)&v40 + 1), v41, v42);
  if ((unsigned __int128)(v14 * (__int128)result) >> 64 == (v14 * result) >> 63)
  {
    if (((v37 * result) & 0x8000000000000000) == 0)
    {
      *(void *)&v44[0] = 0;
      *((void *)&v44[0] + 1) = v37 * result;
      MEMORY[0x1F4188790](result);
      uint64_t v17 = v34;
      uint64_t v16 = v35;
      v29[2] = v34;
      v29[3] = v31;
      v29[4] = v35;
      v29[5] = v33;
      void v29[6] = v18;
      v29[7] = v30;
      v29[8] = v32;
      uint64_t v19 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Range<Int>);
      unint64_t v20 = lazy protocol witness table accessor for type Range<Int> and conformance <> Range<A>();
      uint64_t v43 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in static BNNSNDArrayDescriptor.allocate<A, B>(randomIn:using:shape:batchSize:), (uint64_t)v29, v19, v17, MEMORY[0x1E4FBC248], v20, MEMORY[0x1E4FBC278], v21);
      outlined init with take of BNNS.Shape((uint64_t)v45, (uint64_t)&v38);
      uint64_t v22 = type metadata accessor for Array();
      uint64_t WitnessTable = swift_getWitnessTable();
      static BNNSNDArrayDescriptor.allocate<A>(initializingFrom:shape:batchSize:)((uint64_t)&v43, (uint64_t)&v38, v14, v22, WitnessTable, v16, v44);
      int64_t result = swift_bridgeObjectRelease();
      long long v24 = v44[9];
      a9[8] = v44[8];
      a9[9] = v24;
      a9[10] = v44[10];
      long long v25 = v44[5];
      a9[4] = v44[4];
      a9[5] = v25;
      long long v26 = v44[7];
      a9[6] = v44[6];
      a9[7] = v26;
      long long v27 = v44[1];
      *a9 = v44[0];
      a9[1] = v27;
      long long v28 = v44[3];
      a9[2] = v44[2];
      a9[3] = v28;
      return result;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

{
  long long v10;
  long long v11;
  long long v12;
  unint64_t v13;
  uint64_t v14;
  int64_t result;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  unint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t WitnessTable;
  long long v25;
  long long v26;
  long long v27;
  long long v28;
  long long v29;
  void v30[11];
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  unint64_t v37;
  uint64_t v38;
  long long v39;
  long long v40;
  long long v41;
  unint64_t v42;
  unint64_t v43;
  uint64_t v44;
  _OWORD v45[11];
  unsigned char v46[144];

  uint64_t v34 = a8;
  uint64_t v35 = a5;
  unint64_t v36 = a7;
  uint64_t v31 = a6;
  uint64_t v32 = a1;
  long long v38 = a4;
  uint64_t v33 = a2;
  outlined init with take of BNNS.Shape(a3, (uint64_t)v46);
  outlined init with take of BNNS.Shape((uint64_t)v46, (uint64_t)v45);
  BNNS.Shape.size.getter((uint64_t)&v39);
  long long v10 = v39;
  long long v11 = v40;
  long long v12 = v41;
  unint64_t v13 = v42;
  uint64_t v37 = v43;
  outlined init with take of BNNS.Shape((uint64_t)v46, (uint64_t)v45);
  BNNS.Shape.stride.getter((uint64_t)&v39);
  uint64_t v14 = v38;
  int64_t result = specialized static BNNS.calculateBatchStride(size:stride:)(v10, *((unint64_t *)&v10 + 1), v11, *((unint64_t *)&v11 + 1), v12, *((unint64_t *)&v12 + 1), v13, v37, v39, *((unint64_t *)&v39 + 1), v40, *((unint64_t *)&v40 + 1), v41, *((unint64_t *)&v41 + 1), v42, v43);
  if ((unsigned __int128)(v14 * (__int128)result) >> 64 == (v14 * result) >> 63)
  {
    if (((v38 * result) & 0x8000000000000000) == 0)
    {
      *(void *)&v45[0] = 0;
      *((void *)&v45[0] + 1) = v38 * result;
      MEMORY[0x1F4188790](result);
      uint64_t v17 = v35;
      uint64_t v16 = v36;
      v30[2] = v35;
      v30[3] = v31;
      v30[4] = v36;
      v30[5] = v34;
      _DWORD v30[6] = v19;
      v30[7] = v18;
      v30[8] = v32;
      v30[9] = v33;
      unint64_t v20 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Range<Int>);
      uint64_t v21 = lazy protocol witness table accessor for type Range<Int> and conformance <> Range<A>();
      uint64_t v44 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in static BNNSNDArrayDescriptor.allocate<A, B>(randomIn:using:shape:batchSize:), (uint64_t)v30, v20, v17, MEMORY[0x1E4FBC248], v21, MEMORY[0x1E4FBC278], v22);
      outlined init with take of BNNS.Shape((uint64_t)v46, (uint64_t)&v39);
      long long v23 = type metadata accessor for Array();
      uint64_t WitnessTable = swift_getWitnessTable();
      static BNNSNDArrayDescriptor.allocate<A>(initializingFrom:shape:batchSize:)((uint64_t)&v44, (uint64_t)&v39, v14, v23, WitnessTable, v16, v45);
      int64_t result = swift_bridgeObjectRelease();
      long long v25 = v45[9];
      a9[8] = v45[8];
      a9[9] = v25;
      a9[10] = v45[10];
      long long v26 = v45[5];
      a9[4] = v45[4];
      a9[5] = v26;
      long long v27 = v45[7];
      a9[6] = v45[6];
      a9[7] = v27;
      long long v28 = v45[1];
      *a9 = v45[0];
      a9[1] = v28;
      long long v29 = v45[3];
      a9[2] = v45[2];
      a9[3] = v29;
      return result;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

int64_t static BNNSNDArrayDescriptor.allocate<A>(repeating:shape:batchSize:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, _OWORD *a6@<X8>)
{
  outlined init with take of BNNS.Shape(a2, (uint64_t)v31);
  outlined init with take of BNNS.Shape((uint64_t)v31, (uint64_t)v30);
  BNNS.Shape.size.getter((uint64_t)&v24);
  long long v7 = v24;
  long long v8 = v25;
  long long v9 = v26;
  unint64_t v11 = v27;
  unint64_t v10 = v28;
  outlined init with take of BNNS.Shape((uint64_t)v31, (uint64_t)v30);
  BNNS.Shape.stride.getter((uint64_t)&v24);
  int64_t result = specialized static BNNS.calculateBatchStride(size:stride:)(v7, *((unint64_t *)&v7 + 1), v8, *((unint64_t *)&v8 + 1), v9, *((unint64_t *)&v9 + 1), v11, v10, v24, *((unint64_t *)&v24 + 1), v25, *((unint64_t *)&v25 + 1), v26, *((unint64_t *)&v26 + 1), v27, v28);
  if ((unsigned __int128)(a3 * (__int128)result) >> 64 == (a3 * result) >> 63)
  {
    uint64_t v29 = specialized Array.init(repeating:count:)(a1, a3 * result, a4);
    outlined init with take of BNNS.Shape((uint64_t)v31, (uint64_t)&v24);
    uint64_t v13 = type metadata accessor for Array();
    uint64_t WitnessTable = swift_getWitnessTable();
    static BNNSNDArrayDescriptor.allocate<A>(initializingFrom:shape:batchSize:)((uint64_t)&v29, (uint64_t)&v24, a3, v13, WitnessTable, a5, v30);
    int64_t result = swift_bridgeObjectRelease();
    long long v15 = v30[9];
    a6[8] = v30[8];
    a6[9] = v15;
    a6[10] = v30[10];
    long long v16 = v30[5];
    a6[4] = v30[4];
    a6[5] = v16;
    long long v17 = v30[7];
    a6[6] = v30[6];
    a6[7] = v17;
    long long v18 = v30[1];
    *a6 = v30[0];
    a6[1] = v18;
    long long v19 = v30[3];
    a6[2] = v30[2];
    a6[3] = v19;
  }
  else
  {
    __break(1u);
  }
  return result;
}

int64_t BNNSNDArrayDescriptor.makeArray<A>(of:batchSize:)(uint64_t a1, uint64_t a2)
{
  outlined init with take of BNNSNDArrayDescriptor?(v2 + 136, (uint64_t)v19, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v19, (uint64_t)&v20, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  if (!v20) {
    return 0;
  }
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v17);
  outlined init with take of BNNS.Shape((uint64_t)v17, (uint64_t)v18);
  outlined init with take of BNNS.Shape((uint64_t)v18, (uint64_t)v16);
  BNNS.Shape.size.getter((uint64_t)&v11);
  long long v4 = v11;
  long long v5 = v12;
  long long v6 = v13;
  unint64_t v7 = v14;
  unint64_t v10 = v15;
  outlined init with take of BNNS.Shape((uint64_t)v18, (uint64_t)v16);
  BNNS.Shape.stride.getter((uint64_t)&v11);
  int64_t result = specialized static BNNS.calculateBatchStride(size:stride:)(v4, *((unint64_t *)&v4 + 1), v5, *((unint64_t *)&v5 + 1), v6, *((unint64_t *)&v6 + 1), v7, v10, v11, *((unint64_t *)&v11 + 1), v12, *((unint64_t *)&v12 + 1), v13, *((unint64_t *)&v13 + 1), v14, v15);
  if ((unsigned __int128)(a2 * (__int128)result) >> 64 == (a2 * result) >> 63)
  {
    v17[0] = UnsafeBufferPointer.init(start:count:)();
    v17[1] = v9;
    type metadata accessor for UnsafeBufferPointer();
    swift_getWitnessTable();
    return Array.init<A>(_:)();
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t BNNSNDArrayDescriptor.init(data:scalarType:shape:batchSize:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X8>)
{
  uint64_t result = outlined init with take of BNNS.Shape(a5, (uint64_t)v16);
  if (a6 < 1)
  {
    __break(1u);
  }
  else
  {
    int v14 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 8))(a3, a4);
    helper #1 <A>(_:) in BNNSNDArrayDescriptor.init(data:scalarType:shape:batchSize:)(a1, a2, (uint64_t)v16, a6, v14, a3, (uint64_t)v15);
    return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v15, a7, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  }
  return result;
}

void helper #1 <A>(_:) in BNNSNDArrayDescriptor.init(data:scalarType:shape:batchSize:)(uint64_t a1@<X1>, uint64_t a2@<X2>, uint64_t a3@<X3>, uint64_t a4@<X4>, int a5@<W5>, uint64_t a6@<X6>, uint64_t a7@<X8>)
{
  if (a1) {
    uint64_t v7 = a2 - a1;
  }
  else {
    uint64_t v7 = 0;
  }
  uint64_t v8 = *(void *)(*(void *)(a6 - 8) + 72);
  if (!v8)
  {
    __break(1u);
    goto LABEL_13;
  }
  if (v7 == 0x8000000000000000 && v8 == -1) {
    goto LABEL_14;
  }
  uint64_t v19 = *(void *)(*(void *)(a6 - 8) + 72);
  uint64_t v20 = v7;
  outlined init with take of BNNS.Shape(a3, (uint64_t)v27);
  BNNS.Shape.size.getter((uint64_t)&v22);
  long long v11 = v22;
  long long v12 = v23;
  long long v13 = v24;
  unint64_t v15 = v25;
  unint64_t v14 = v26;
  outlined init with take of BNNS.Shape(a3, (uint64_t)v27);
  BNNS.Shape.stride.getter((uint64_t)&v22);
  int64_t v16 = specialized static BNNS.calculateBatchStride(size:stride:)(v11, *((unint64_t *)&v11 + 1), v12, *((unint64_t *)&v12 + 1), v13, *((unint64_t *)&v13 + 1), v15, v14, v22, *((unint64_t *)&v22 + 1), v23, *((unint64_t *)&v23 + 1), v24, *((unint64_t *)&v24 + 1), v25, v26);
  if ((unsigned __int128)(v16 * (__int128)a4) >> 64 != (v16 * a4) >> 63)
  {
LABEL_13:
    __break(1u);
LABEL_14:
    __break(1u);
    return;
  }
  if (v20 / v19 == v16 * a4)
  {
    outlined init with take of BNNS.Shape(a3, (uint64_t)v27);
    specialized static BNNS.makeArrayDescriptor(shape:data:dataType:)((uint64_t)v27, a1, a5, (uint64_t)&v22);
    _sSo21BNNSNDArrayDescriptoraSgWOi_((uint64_t)&v22);
  }
  else
  {
    _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&v22);
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v22, (uint64_t)v27, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v27, a7, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
}

uint64_t BNNSNDArrayDescriptor.init<A>(data:shape:batchSize:)@<X0>(uint64_t a1@<X1>, uint64_t a2@<X2>, uint64_t a3@<X3>, uint64_t a4@<X4>, uint64_t a5@<X5>, uint64_t a6@<X8>)
{
  uint64_t result = outlined init with take of BNNS.Shape(a2, (uint64_t)v27);
  if (a3 < 1)
  {
    __break(1u);
  }
  else
  {
    uint64_t v12 = UnsafeBufferPointer.baseAddress.getter();
    if (!v12) {
      goto LABEL_6;
    }
    uint64_t v18 = v12;
    uint64_t v19 = a4;
    outlined init with take of BNNS.Shape((uint64_t)v27, (uint64_t)v26);
    BNNS.Shape.size.getter((uint64_t)&v21);
    long long v13 = v21;
    long long v14 = v22;
    long long v15 = v23;
    unint64_t v16 = v24;
    unint64_t v20 = v25;
    outlined init with take of BNNS.Shape((uint64_t)v27, (uint64_t)v26);
    BNNS.Shape.stride.getter((uint64_t)&v21);
    uint64_t result = specialized static BNNS.calculateBatchStride(size:stride:)(v13, *((unint64_t *)&v13 + 1), v14, *((unint64_t *)&v14 + 1), v15, *((unint64_t *)&v15 + 1), v16, v20, v21, *((unint64_t *)&v21 + 1), v22, *((unint64_t *)&v22 + 1), v23, *((unint64_t *)&v23 + 1), v24, v25);
    if ((unsigned __int128)(result * (__int128)a3) >> 64 == (result * a3) >> 63)
    {
      if (result * a3 == a1)
      {
        outlined init with take of BNNS.Shape((uint64_t)v27, (uint64_t)v26);
        int v17 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 8))(v19, a5);
        specialized static BNNS.makeArrayDescriptor(shape:data:dataType:)((uint64_t)v26, v18, v17, (uint64_t)&v21);
        _sSo21BNNSNDArrayDescriptoraSgWOi_((uint64_t)&v21);
LABEL_7:
        outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v21, (uint64_t)v26, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
        return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v26, a6, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      }
LABEL_6:
      _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&v21);
      goto LABEL_7;
    }
  }
  __break(1u);
  return result;
}

uint64_t specialized Array.init(repeating:count:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t result = static Array._allocateUninitialized(_:)();
  uint64_t v15 = result;
  if (a2 < 0)
  {
    __break(1u);
  }
  else
  {
    if (a2)
    {
      uint64_t v8 = v7;
      uint64_t v9 = *(void *)(a3 - 8);
      unint64_t v10 = *(void (**)(uint64_t, uint64_t, uint64_t))(v9 + 16);
      v10(v7, a1, a3);
      uint64_t v11 = a2 - 1;
      if (v11)
      {
        uint64_t v12 = *(void *)(v9 + 72);
        uint64_t v13 = v8 + v12;
        do
        {
          v10(v13, a1, a3);
          v13 += v12;
          --v11;
        }
        while (v11);
      }
    }
    uint64_t v14 = type metadata accessor for Array();
    destructiveProjectEnumData for BNNS.ActivationFunction(v14);
    return v15;
  }
  return result;
}

uint64_t partial apply for closure #1 in static BNNSNDArrayDescriptor.allocate<A>(randomIn:shape:batchSize:)()
{
  return static FixedWidthInteger.random(in:)();
}

{
  return static BinaryFloatingPoint<>.random(in:)();
}

unint64_t lazy protocol witness table accessor for type Range<Int> and conformance <> Range<A>()
{
  unint64_t result = lazy protocol witness table cache variable for type Range<Int> and conformance <> Range<A>;
  if (!lazy protocol witness table cache variable for type Range<Int> and conformance <> Range<A>)
  {
    __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for Range<Int>);
    lazy protocol witness table accessor for type Int and conformance Int();
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Range<Int> and conformance <> Range<A>);
  }
  return result;
}

uint64_t partial apply for closure #1 in static BNNSNDArrayDescriptor.allocate<A, B>(randomIn:using:shape:batchSize:)()
{
  return static FixedWidthInteger.random<A>(in:using:)();
}

{
  return static BinaryFloatingPoint<>.random<A>(in:using:)();
}

uint64_t dispatch thunk of static BNNSScalar.bnnsDataType.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 8))();
}

uint64_t BNNS.RelationalOperator.value.setter(uint64_t result)
{
  *uint64_t v1 = result;
  return result;
}

uint64_t (*BNNS.RelationalOperator.value.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t static BNNS.RelationalOperator.equal.getter()
{
  return 0;
}

uint64_t static BNNS.RelationalOperator.less.getter()
{
  return 1;
}

uint64_t static BNNS.RelationalOperator.lessEqual.getter()
{
  return 2;
}

uint64_t static BNNS.RelationalOperator.greater.getter()
{
  return 3;
}

uint64_t static BNNS.RelationalOperator.greaterEqual.getter()
{
  return 4;
}

uint64_t static BNNS.RelationalOperator.notEqual.getter()
{
  return 5;
}

uint64_t static BNNS.RelationalOperator.and.getter()
{
  return 6;
}

uint64_t static BNNS.RelationalOperator.or.getter()
{
  return 7;
}

uint64_t static BNNS.RelationalOperator.not.getter()
{
  return 8;
}

uint64_t static BNNS.RelationalOperator.nand.getter()
{
  return 9;
}

uint64_t static BNNS.RelationalOperator.nor.getter()
{
  return 10;
}

uint64_t static BNNS.RelationalOperator.xor.getter()
{
  return 11;
}

uint64_t static BNNS.compare(_:_:using:output:)(_OWORD *a1, _OWORD *a2, BNNSRelationalOperator a3, _OWORD *a4)
{
  uint64_t v24 = *MEMORY[0x1E4F143B8];
  long long v4 = a1[9];
  *(_OWORD *)&in0.stride[7] = a1[8];
  *(_OWORD *)&in0.BNNSDataType data_type = v4;
  *(_OWORD *)&in0.table_BNNSDataType data_type = a1[10];
  long long v5 = a1[5];
  *(_OWORD *)&in0.size[7] = a1[4];
  *(_OWORD *)&in0.stride[1] = v5;
  long long v6 = a1[7];
  *(_OWORD *)&in0.stride[3] = a1[6];
  *(_OWORD *)&in0.stride[5] = v6;
  long long v7 = a1[1];
  *(_OWORD *)&in0.flags = *a1;
  *(_OWORD *)&in0.size[1] = v7;
  long long v8 = a1[3];
  *(_OWORD *)&in0.size[3] = a1[2];
  *(_OWORD *)&in0.size[5] = v8;
  long long v9 = a2[9];
  *(_OWORD *)&in1.stride[7] = a2[8];
  *(_OWORD *)&in1.BNNSDataType data_type = v9;
  *(_OWORD *)&in1.table_BNNSDataType data_type = a2[10];
  long long v10 = a2[5];
  *(_OWORD *)&in1.size[7] = a2[4];
  *(_OWORD *)&in1.stride[1] = v10;
  long long v11 = a2[7];
  *(_OWORD *)&in1.stride[3] = a2[6];
  *(_OWORD *)&in1.stride[5] = v11;
  long long v12 = a2[1];
  *(_OWORD *)&in1.flags = *a2;
  *(_OWORD *)&in1.size[1] = v12;
  long long v13 = a2[3];
  *(_OWORD *)&in1.size[3] = a2[2];
  *(_OWORD *)&in1.size[5] = v13;
  long long v14 = a4[9];
  *(_OWORD *)&v21.stride[7] = a4[8];
  *(_OWORD *)&v21.BNNSDataType data_type = v14;
  *(_OWORD *)&v21.table_BNNSDataType data_type = a4[10];
  long long v15 = a4[5];
  *(_OWORD *)&v21.size[7] = a4[4];
  *(_OWORD *)&v21.stride[1] = v15;
  long long v16 = a4[7];
  *(_OWORD *)&v21.stride[3] = a4[6];
  *(_OWORD *)&v21.stride[5] = v16;
  long long v17 = a4[1];
  *(_OWORD *)&v21.flags = *a4;
  *(_OWORD *)&v21.size[1] = v17;
  long long v18 = a4[3];
  *(_OWORD *)&v21.size[3] = a4[2];
  *(_OWORD *)&v21.size[5] = v18;
  uint64_t result = BNNSCompareTensor(&in0, &in1, a3, &v21);
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *unint64_t v20 = 0;
    return swift_willThrow();
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.RelationalOperator()
{
  return &type metadata for BNNS.RelationalOperator;
}

uint64_t vImage.PixelBuffer<>.applyLookup(_:destination:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2 = (uint64_t (*)(void *, void *, uint64_t, void))MEMORY[0x1E4F17158];

  return vImage.PixelBuffer<>.applyLookup(_:destination:)(a1, a2, v2);
}

{
  uint64_t (*v2)(void *, void *, uint64_t, void);
  uint64_t vars8;

  uint64_t v2 = (uint64_t (*)(void *, void *, uint64_t, void))MEMORY[0x1E4F17020];

  return vImage.PixelBuffer<>.applyLookup(_:destination:)(a1, a2, v2);
}

{
  uint64_t (*v2)(void *, void *, uint64_t, void);
  uint64_t vars8;

  uint64_t v2 = (uint64_t (*)(void *, void *, uint64_t, void))MEMORY[0x1E4F17030];

  return vImage.PixelBuffer<>.applyLookup(_:destination:)(a1, a2, v2);
}

{
  uint64_t (*v2)(void *, void *, uint64_t, void);
  uint64_t vars8;

  uint64_t v2 = (uint64_t (*)(void *, void *, uint64_t, void))MEMORY[0x1E4F17028];

  return vImage.PixelBuffer<>.applyLookup(_:destination:)(a1, a2, v2);
}

vImage_Error vImage.PixelBuffer<>.applyLookup(_:destination:)(uint64_t a1, uint64_t a2)
{
  uint64_t v16 = *MEMORY[0x1E4F143B8];
  if (*(void *)(a1 + 16) != 256)
  {
    __break(1u);
    goto LABEL_16;
  }
  int v3 = *(void **)v2;
  if (!*(void *)(*(void *)v2 + 16))
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  vImagePixelCount v4 = v3[6];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  vImagePixelCount v5 = v3[5];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v4)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v5)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  long long v6 = *(void **)a2;
  if (!*(void *)(*(void *)a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v7 = v6[6];
  if (v7 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v8 = v6[5];
  if (v8 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v8)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v4 != v7)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v5 != v8) {
    goto LABEL_27;
  }
  long long v9 = (void *)v3[4];
  size_t v10 = v3[7];
  src.char data = v9;
  src.height = v5;
  src.width = v4;
  src.rowBytes = v10;
  long long v11 = (void *)v6[4];
  size_t v12 = v6[7];
  dest.char data = v11;
  dest.height = v5;
  dest.width = v4;
  dest.rowBytes = v12;
  return vImageLookupTable_Planar8toPlanar24(&src, &dest, (const uint32_t *)(a1 + 32), 0);
}

{
  uint64_t v2;
  void *v3;
  vImagePixelCount v4;
  vImagePixelCount v5;
  void *v6;
  uint64_t v7;
  uint64_t v8;
  void *v9;
  size_t v10;
  void *v11;
  size_t v12;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v16;

  uint64_t v16 = *MEMORY[0x1E4F143B8];
  if (*(void *)(a1 + 16) != 4096)
  {
    __break(1u);
    goto LABEL_16;
  }
  int v3 = *(void **)v2;
  if (!*(void *)(*(void *)v2 + 16))
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  vImagePixelCount v4 = v3[6];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  vImagePixelCount v5 = v3[5];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v4)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v5)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  long long v6 = *(void **)a2;
  if (!*(void *)(*(void *)a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v7 = v6[6];
  if (v7 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v8 = v6[5];
  if (v8 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v8)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v4 != v7)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v5 != v8) {
    goto LABEL_27;
  }
  long long v9 = (void *)v3[4];
  size_t v10 = v3[7];
  src.char data = v9;
  src.height = v5;
  src.width = v4;
  src.rowBytes = v10;
  long long v11 = (void *)v6[4];
  size_t v12 = v6[7];
  dest.char data = v11;
  dest.height = v5;
  dest.width = v4;
  dest.rowBytes = v12;
  return vImageLookupTable_PlanarFtoPlanar8(&src, &dest, (const Pixel_8 *)(a1 + 32), 0);
}

{
  uint64_t v2;
  vImagePixelCount v3;
  void *v4;
  vImagePixelCount v5;
  vImagePixelCount v6;
  void *v7;
  uint64_t v8;
  uint64_t v9;
  void *v10;
  size_t v11;
  void *v12;
  size_t v13;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v17;

  long long v17 = *MEMORY[0x1E4F143B8];
  int v3 = *(void *)(a1 + 16);
  if (!v3)
  {
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v4 = *(void **)v2;
  if (!*(void *)(*(void *)v2 + 16))
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  vImagePixelCount v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  long long v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v5)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v6)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = *(void **)a2;
  if (!*(void *)(*(void *)a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v8 = v7[6];
  if (v8 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  long long v9 = v7[5];
  if (v9 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v8)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v9)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v5 != v8)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v6 != v9) {
    goto LABEL_27;
  }
  size_t v10 = (void *)v4[4];
  long long v11 = v4[7];
  src.char data = v10;
  src.height = v6;
  src.width = v5;
  src.rowBytes = v11;
  size_t v12 = (void *)v7[4];
  long long v13 = v7[7];
  dest.char data = v12;
  dest.height = v6;
  dest.width = v5;
  dest.rowBytes = v13;
  return vImageInterpolatedLookupTable_PlanarF(&src, &dest, (const Pixel_F *)(a1 + 32), v3, 1.0, 0.0, 0);
}

{
  uint64_t v2;
  void *v3;
  vImagePixelCount v4;
  vImagePixelCount v5;
  void *v6;
  uint64_t v7;
  uint64_t v8;
  void *v9;
  size_t v10;
  void *v11;
  size_t v12;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v16;

  uint64_t v16 = *MEMORY[0x1E4F143B8];
  if (*(void *)(a1 + 16) != 0x10000)
  {
    __break(1u);
    goto LABEL_16;
  }
  int v3 = *(void **)v2;
  if (!*(void *)(*(void *)v2 + 16))
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  vImagePixelCount v4 = v3[6];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  vImagePixelCount v5 = v3[5];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v4)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v5)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  long long v6 = *(void **)a2;
  if (!*(void *)(*(void *)a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v7 = v6[6];
  if (v7 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v8 = v6[5];
  if (v8 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v8)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v4 != v7)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v5 != v8) {
    goto LABEL_27;
  }
  long long v9 = (void *)v3[4];
  size_t v10 = v3[7];
  src.char data = v9;
  src.height = v5;
  src.width = v4;
  src.rowBytes = v10;
  long long v11 = (void *)v6[4];
  size_t v12 = v6[7];
  dest.char data = v11;
  dest.height = v5;
  dest.width = v4;
  dest.rowBytes = v12;
  return vImageLookupTable_Planar16(&src, &dest, (const Pixel_16U *)(a1 + 32), 0);
}

uint64_t vImage.PixelBuffer<>.applyLookup(_:destination:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(void *, void *, uint64_t, void))
{
  v16[4] = *MEMORY[0x1E4F143B8];
  if (*(void *)(a1 + 16) != 256)
  {
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v4 = *(void **)v3;
  if (!*(void *)(*(void *)v3 + 16))
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v5 = v4[6];
  if (v5 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v6 = v4[5];
  if (v6 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v5)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v6)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = *(void **)a2;
  if (!*(void *)(*(void *)a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v8 = v7[6];
  if (v8 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v9 = v7[5];
  if (v9 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v8)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v9)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v5 != v8)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v6 != v9) {
    goto LABEL_27;
  }
  uint64_t v10 = v4[4];
  uint64_t v11 = v4[7];
  v16[0] = v10;
  v16[1] = v6;
  _OWORD v16[2] = v5;
  v16[3] = v11;
  uint64_t v12 = v7[4];
  uint64_t v13 = v7[7];
  v15[0] = v12;
  v15[1] = v6;
  void v15[2] = v5;
  v15[3] = v13;
  return a3(v16, v15, a1 + 32, 0);
}

vImage_Error vImage.PixelBuffer<>.applyLookup(alphaTable:redTable:greenTable:blueTable:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v23 = *MEMORY[0x1E4F143B8];
  if (a1 && *(void *)(a1 + 16) != 256) {
    goto LABEL_44;
  }
  if (a2 && *(void *)(a2 + 16) != 256) {
    goto LABEL_45;
  }
  if (a3 && *(void *)(a3 + 16) != 256) {
    goto LABEL_46;
  }
  if (a4 && *(void *)(a4 + 16) != 256) {
LABEL_47:
  }
    __break(1u);
  uint64_t v6 = *(void **)v5;
  if (!*(void *)(*(void *)v5 + 16))
  {
    __break(1u);
    goto LABEL_33;
  }
  vImagePixelCount v7 = v6[6];
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  vImagePixelCount v8 = v6[5];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (!v7)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (!v8)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v9 = *(void **)a5;
  if (!*(void *)(*(void *)a5 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  uint64_t v10 = v9[6];
  if (v10 < 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  uint64_t v11 = v9[5];
  if (v11 < 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v10)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v11)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (v7 != v10)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (v8 != v11)
  {
LABEL_43:
    __break(1u);
LABEL_44:
    __break(1u);
LABEL_45:
    __break(1u);
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  uint64_t v12 = (void *)v6[4];
  size_t v13 = v6[7];
  src.char data = v12;
  src.height = v8;
  src.width = v7;
  src.rowBytes = v13;
  long long v14 = (void *)v9[4];
  size_t v15 = v9[7];
  dest.char data = v14;
  dest.height = v8;
  uint64_t v16 = (const Pixel_8 *)(a1 + 32);
  if (!a1) {
    uint64_t v16 = 0;
  }
  long long v17 = (const Pixel_8 *)(a2 + 32);
  if (!a2) {
    long long v17 = 0;
  }
  if (a3) {
    long long v18 = (const Pixel_8 *)(a3 + 32);
  }
  else {
    long long v18 = 0;
  }
  if (a4) {
    uint64_t v19 = (const Pixel_8 *)(a4 + 32);
  }
  else {
    uint64_t v19 = 0;
  }
  dest.width = v7;
  dest.rowBytes = v15;
  return vImageTableLookUp_ARGB8888(&src, &dest, v16, v17, v18, v19, 0);
}

BOOL static BNNS.DescriptorType.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void BNNS.DescriptorType.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

uint64_t BNNS.ArithmeticUnaryFunction.bnnsArithmeticFunction.getter()
{
  return dword_1D2138EBC[*v0];
}

void *static BNNS.ArithmeticUnaryFunction.allCases.getter()
{
  return &outlined read-only object #0 of static BNNS.ArithmeticUnaryFunction.allCases.getter;
}

void protocol witness for static CaseIterable.allCases.getter in conformance BNNS.ArithmeticUnaryFunction(void *a1@<X8>)
{
  *a1 = &outlined read-only object #0 of static BNNS.ArithmeticUnaryFunction.allCases.getter;
}

void *static BNNS.ArithmeticBinaryFunction.allCases.getter()
{
  return &outlined read-only object #0 of static BNNS.ArithmeticBinaryFunction.allCases.getter;
}

void protocol witness for static CaseIterable.allCases.getter in conformance BNNS.ArithmeticBinaryFunction(void *a1@<X8>)
{
  *a1 = &outlined read-only object #0 of static BNNS.ArithmeticBinaryFunction.allCases.getter;
}

uint64_t BNNS.UnaryArithmeticLayer.__allocating_init(input:inputDescriptorType:output:outputDescriptorType:function:activation:filterParameters:)(_OWORD *a1, unsigned __int8 *a2, _OWORD *a3, unsigned __int8 *a4, uint64_t a5, uint64_t *a6, int a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v64 = *MEMORY[0x1E4F143B8];
  long long v12 = a1[9];
  v42[8] = a1[8];
  v42[9] = v12;
  v42[10] = a1[10];
  long long v13 = a1[5];
  v42[4] = a1[4];
  void v42[5] = v13;
  long long v14 = a1[7];
  v42[6] = a1[6];
  v42[7] = v14;
  long long v15 = a1[1];
  v42[0] = *a1;
  v42[1] = v15;
  long long v16 = a1[3];
  v42[2] = a1[2];
  v42[3] = v16;
  long long v17 = a3[5];
  *(_OWORD *)&v57[68] = a3[4];
  long long v18 = a3[2];
  *(_OWORD *)&v57[52] = a3[3];
  long long v19 = a3[6];
  *(_OWORD *)&v57[116] = a3[7];
  long long v20 = a3[9];
  *(_OWORD *)&v57[132] = a3[8];
  *(_OWORD *)&v57[148] = v20;
  *(_OWORD *)&v57[164] = a3[10];
  *(_OWORD *)&v57[84] = v17;
  *(_OWORD *)&v57[100] = v19;
  long long v21 = a3[1];
  *(_OWORD *)&v57[4] = *a3;
  *(_OWORD *)&v57[20] = v21;
  *(_OWORD *)&v57[36] = v18;
  long long v53 = *(_OWORD *)&v57[144];
  long long v54 = *(_OWORD *)&v57[160];
  long long v49 = *(_OWORD *)&v57[80];
  long long v50 = *(_OWORD *)&v57[96];
  long long v51 = *(_OWORD *)&v57[112];
  long long v52 = *(_OWORD *)&v57[128];
  long long v48 = *(_OWORD *)&v57[64];
  long long v44 = *(_OWORD *)v57;
  long long v45 = *(_OWORD *)&v57[16];
  long long v46 = *(_OWORD *)&v57[32];
  int v22 = *a4;
  uint64_t v23 = *a6;
  int v43 = *a2;
  int v55 = *(_DWORD *)&v57[176];
  long long v47 = *(_OWORD *)&v57[48];
  int v56 = v22;
  int v24 = BNNS.ArithmeticUnaryFunction.bnnsArithmeticFunction.getter();
  v34[1] = HIDWORD(v23);
  BNNS.ActivationFunction.bnnsActivation.getter((uint64_t)&v58);
  uint64_t v37 = v59;
  v34[0] = v24;
  uint64_t v35 = v42;
  int v36 = v58;
  uint64_t v38 = v60;
  int v39 = v61;
  long long v40 = v62;
  uint64_t v41 = v63;
  if (a9 == 1)
  {
    unint64_t v25 = 0;
  }
  else
  {
    int v30 = a7;
    uint64_t v31 = a8;
    uint64_t v32 = a9;
    uint64_t v33 = a10;
    unint64_t v25 = &v30;
  }
  uint64_t v26 = MEMORY[0x1D25FFFB0](v34, v25);
  type metadata accessor for BNNS.UnaryArithmeticLayer();
  uint64_t v27 = swift_allocObject();
  uint64_t v28 = v27;
  if (v26)
  {
    *(void *)(v27 + 16) = v26;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v28;
}

uint64_t BNNS.UnaryArithmeticLayer.apply(batchSize:input:output:)(size_t a1, uint64_t a2, uint64_t a3)
{
  return specialized static BNNS.arithmeticLayerApply(_:batchSize:input:output:)(v3, a1, a2, a3);
}

uint64_t BNNS.UnaryArithmeticLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:)(size_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, long long *a5)
{
  return specialized static BNNS.arithmeticLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:)(v5, a1, a2, a3, a4, a5);
}

uint64_t type metadata accessor for BNNS.UnaryArithmeticLayer()
{
  return self;
}

uint64_t BNNS.UnaryArithmeticLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.BinaryArithmeticLayer.__allocating_init(inputA:inputADescriptorType:inputB:inputBDescriptorType:output:outputDescriptorType:function:activation:filterParameters:)(_OWORD *a1, unsigned __int8 *a2, _OWORD *a3, unsigned __int8 *a4, _OWORD *a5, unsigned __int8 *a6, char *a7, uint64_t *a8, int a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  uint64_t v86 = *MEMORY[0x1E4F143B8];
  long long v12 = a1[9];
  v50[8] = a1[8];
  v50[9] = v12;
  v50[10] = a1[10];
  long long v13 = a1[5];
  v50[4] = a1[4];
  v50[5] = v13;
  long long v14 = a1[7];
  v50[6] = a1[6];
  v50[7] = v14;
  long long v15 = a1[1];
  v50[0] = *a1;
  v50[1] = v15;
  long long v16 = a1[3];
  v50[2] = a1[2];
  v50[3] = v16;
  long long v17 = a3[6];
  *(_OWORD *)&v79[116] = a3[7];
  long long v18 = a3[9];
  *(_OWORD *)&v79[132] = a3[8];
  *(_OWORD *)&v79[148] = v18;
  *(_OWORD *)&v79[164] = a3[10];
  long long v19 = a3[2];
  *(_OWORD *)&v79[52] = a3[3];
  long long v20 = a3[5];
  *(_OWORD *)&v79[68] = a3[4];
  *(_OWORD *)&v79[84] = v20;
  *(_OWORD *)&v79[100] = v17;
  long long v21 = a3[1];
  *(_OWORD *)&v79[4] = *a3;
  *(_OWORD *)&v79[20] = v21;
  *(_OWORD *)&v79[36] = v19;
  long long v22 = a5[6];
  *(_OWORD *)&v78[116] = a5[7];
  long long v23 = a5[9];
  *(_OWORD *)&v78[132] = a5[8];
  *(_OWORD *)&v78[148] = v23;
  *(_OWORD *)&v78[164] = a5[10];
  long long v24 = a5[2];
  *(_OWORD *)&v78[52] = a5[3];
  long long v25 = a5[5];
  *(_OWORD *)&v78[68] = a5[4];
  *(_OWORD *)&v78[84] = v25;
  *(_OWORD *)&v78[100] = v22;
  long long v26 = a5[1];
  *(_OWORD *)&v78[4] = *a5;
  *(_OWORD *)&v78[20] = v26;
  *(_OWORD *)&v78[36] = v24;
  long long v61 = *(_OWORD *)&v79[144];
  long long v62 = *(_OWORD *)&v79[160];
  long long v57 = *(_OWORD *)&v79[80];
  long long v58 = *(_OWORD *)&v79[96];
  int v27 = *a2;
  int v28 = *a4;
  int v29 = *a6;
  uint64_t v30 = *a7;
  uint64_t v31 = *a8;
  long long v59 = *(_OWORD *)&v79[112];
  long long v60 = *(_OWORD *)&v79[128];
  int v51 = v27;
  int v63 = *(_DWORD *)&v79[176];
  long long v56 = *(_OWORD *)&v79[64];
  long long v52 = *(_OWORD *)v79;
  long long v53 = *(_OWORD *)&v79[16];
  long long v54 = *(_OWORD *)&v79[32];
  long long v55 = *(_OWORD *)&v79[48];
  int v64 = v28;
  long long v73 = *(_OWORD *)&v78[128];
  long long v74 = *(_OWORD *)&v78[144];
  long long v75 = *(_OWORD *)&v78[160];
  long long v69 = *(_OWORD *)&v78[64];
  long long v70 = *(_OWORD *)&v78[80];
  long long v71 = *(_OWORD *)&v78[96];
  long long v72 = *(_OWORD *)&v78[112];
  long long v65 = *(_OWORD *)v78;
  long long v66 = *(_OWORD *)&v78[16];
  long long v67 = *(_OWORD *)&v78[32];
  long long v68 = *(_OWORD *)&v78[48];
  int v76 = *(_DWORD *)&v78[176];
  int v77 = v29;
  int v32 = dword_1D2138F28[v30];
  v42[1] = HIDWORD(v31);
  BNNS.ActivationFunction.bnnsActivation.getter((uint64_t)&v80);
  uint64_t v45 = v81;
  v42[0] = v32;
  int v43 = v50;
  int v44 = v80;
  uint64_t v46 = v82;
  int v47 = v83;
  long long v48 = v84;
  uint64_t v49 = v85;
  if (a11 == 1)
  {
    uint64_t v33 = 0;
  }
  else
  {
    int v38 = a9;
    uint64_t v39 = a10;
    uint64_t v40 = a11;
    uint64_t v41 = a12;
    uint64_t v33 = &v38;
  }
  uint64_t v34 = MEMORY[0x1D25FFFB0](v42, v33);
  type metadata accessor for BNNS.BinaryArithmeticLayer();
  uint64_t v35 = swift_allocObject();
  uint64_t v36 = v35;
  if (v34)
  {
    *(void *)(v35 + 16) = v34;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v36;
}

uint64_t BNNS.BinaryArithmeticLayer.apply(batchSize:inputA:inputB:output:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return specialized static BNNS.arithmeticLayerApply(_:batchSize:inputA:inputB:output:)(v4, a1, a2, a3, a4);
}

uint64_t BNNS.BinaryArithmeticLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _OWORD *a5, _OWORD *a6, long long *a7)
{
  return specialized static BNNS.arithmeticLayerApplyBackward(_:batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(v7, a1, a2, a3, a4, a5, a6, a7);
}

uint64_t closure #1 in closure #1 in static BNNS.arithmeticLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:)@<X0>(BNNSNDArrayDescriptor *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, size_t a4@<X3>, char **a5@<X4>, uint64_t a6@<X6>, uint64_t a7@<X7>, int *a8@<X8>)
{
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>>);
  uint64_t v10 = swift_allocObject();
  *(_OWORD *)(v10 + 16) = xmmword_1D2135280;
  *(void *)(v10 + 32) = a2;
  BNNSNDArrayDescriptor in_delta = (BNNSNDArrayDescriptor **)(v10 + 32);
  filter = *(void **)(a3 + 16);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  uint64_t v11 = swift_allocObject();
  *(_OWORD *)(v11 + 16) = xmmword_1D2135280;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v72);
  outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)v75);
  outlined init with take of BNNS.Shape((uint64_t)v75, (uint64_t)v71);
  BNNS.Shape.size.getter((uint64_t)&v63);
  unint64_t v43 = v63;
  unint64_t v12 = v64;
  unint64_t v13 = v65;
  unint64_t v14 = v66;
  unint64_t v15 = v67;
  unint64_t v16 = v68;
  unint64_t v17 = v69;
  unint64_t v18 = v70;
  outlined init with take of BNNS.Shape((uint64_t)v75, (uint64_t)v71);
  BNNS.Shape.stride.getter((uint64_t)&v63);
  *(void *)(v11 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v43, v12, v13, v14, v15, v16, v17, v18, v63, v64, v65, v66, v67, v68, v69, v70);
  in_stride = (size_t *)(v11 + 32);
  outlined init with take of UnsafeMutableRawPointer?(a7 + 136, (uint64_t)v73);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v73, (uint64_t)&v74);
  uint64_t v39 = v74;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v63);
  outlined init with take of BNNS.Shape((uint64_t)&v63, (uint64_t)v71);
  outlined init with take of BNNS.Shape((uint64_t)v71, (uint64_t)v72);
  BNNS.Shape.size.getter((uint64_t)&v55);
  unint64_t v19 = v55;
  unint64_t v20 = v56;
  unint64_t v21 = v57;
  unint64_t v22 = v58;
  unint64_t v23 = v59;
  unint64_t v24 = v60;
  unint64_t v26 = v61;
  unint64_t v25 = v62;
  outlined init with take of BNNS.Shape((uint64_t)v71, (uint64_t)v72);
  BNNS.Shape.stride.getter((uint64_t)&v55);
  size_t out_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v19, v20, v21, v22, v23, v24, v26, v25, v55, v56, v57, v58, v59, v60, v61, v62);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v55);
  outlined init with take of BNNS.Shape((uint64_t)&v55, (uint64_t)v72);
  outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)v54);
  BNNS.Shape.size.getter((uint64_t)&v49);
  long long v27 = v49;
  long long v28 = v50;
  long long v29 = v51;
  unint64_t v31 = v52;
  unint64_t v30 = v53;
  outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)v54);
  BNNS.Shape.stride.getter((uint64_t)&v49);
  size_t out_delta_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v27, *((unint64_t *)&v27 + 1), v28, *((unint64_t *)&v28 + 1), v29, *((unint64_t *)&v29 + 1), v31, v30, v49, *((unint64_t *)&v49 + 1), v50, *((unint64_t *)&v50 + 1), v51, *((unint64_t *)&v51 + 1), v52, v53);
  uint64_t v33 = *a5;
  char isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
  *a5 = v33;
  if ((isUniquelyReferenced_nonNull_native & 1) == 0) {
    uint64_t v33 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v33 + 2), 0, v33);
  }
  *a5 = v33;
  swift_bridgeObjectRetain();
  int v35 = BNNSArithmeticFilterApplyBackwardBatch(filter, a4, 1uLL, (const void **)v33 + 4, in_stride, in_delta, (const size_t *)(a6 + 32), v39, out_stride, a1, out_delta_stride);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  uint64_t result = swift_bridgeObjectRelease();
  *a8 = v35;
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static BNNS.arithmeticLayerApplyBackward(_:batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)@<X0>(BNNSNDArrayDescriptor *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, size_t a5@<X4>, char **a6@<X5>, int *a7@<X8>, uint64_t a8, uint64_t a9)
{
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>>);
  uint64_t v12 = swift_allocObject();
  *(_OWORD *)(v12 + 16) = xmmword_1D2135290;
  *(void *)(v12 + 32) = a2;
  unint64_t v55 = (BNNSNDArrayDescriptor **)(v12 + 32);
  *(void *)(v12 + 40) = a3;
  long long v54 = *(void **)(a4 + 16);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  uint64_t v87 = swift_allocObject();
  *(_OWORD *)(v87 + 16) = xmmword_1D2135290;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v82);
  outlined init with take of BNNS.Shape((uint64_t)v82, (uint64_t)v85);
  outlined init with take of BNNS.Shape((uint64_t)v85, (uint64_t)v86);
  BNNS.Shape.size.getter((uint64_t)&v74);
  unint64_t v13 = v74;
  unint64_t v14 = v75;
  unint64_t v15 = v76;
  unint64_t v16 = v77;
  unint64_t v17 = v78;
  unint64_t v18 = v79;
  unint64_t v19 = v80;
  unint64_t v20 = v81;
  outlined init with take of BNNS.Shape((uint64_t)v85, (uint64_t)v86);
  BNNS.Shape.stride.getter((uint64_t)&v74);
  unint64_t v21 = specialized static BNNS.calculateBatchStride(size:stride:)(v13, v14, v15, v16, v17, v18, v19, v20, v74, v75, v76, v77, v78, v79, v80, v81);
  uint64_t v22 = v87;
  *(void *)(v87 + 32) = v21;
  in_stride = (const size_t *)(v22 + 32);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v74);
  outlined init with take of BNNS.Shape((uint64_t)&v74, (uint64_t)v86);
  outlined init with take of BNNS.Shape((uint64_t)v86, (uint64_t)v73);
  BNNS.Shape.size.getter((uint64_t)&v65);
  unint64_t v23 = v65;
  unint64_t v24 = v66;
  unint64_t v25 = v67;
  unint64_t v26 = v68;
  unint64_t v27 = v69;
  unint64_t v28 = v70;
  unint64_t v29 = v71;
  unint64_t v30 = v72;
  outlined init with take of BNNS.Shape((uint64_t)v86, (uint64_t)v73);
  BNNS.Shape.stride.getter((uint64_t)&v65);
  unint64_t v31 = specialized static BNNS.calculateBatchStride(size:stride:)(v23, v24, v25, v26, v27, v28, v29, v30, v65, v66, v67, v68, v69, v70, v71, v72);
  *(void *)(v87 + 40) = v31;
  outlined init with take of UnsafeMutableRawPointer?(a9 + 136, (uint64_t)v83);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v83, (uint64_t)&v84);
  long long v51 = v84;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v73);
  outlined init with take of BNNS.Shape((uint64_t)v73, (uint64_t)&v74);
  outlined init with take of BNNS.Shape((uint64_t)&v74, (uint64_t)v82);
  BNNS.Shape.size.getter((uint64_t)&v65);
  unint64_t v32 = v65;
  unint64_t v33 = v66;
  unint64_t v34 = v67;
  unint64_t v35 = v68;
  unint64_t v36 = v69;
  unint64_t v37 = v70;
  unint64_t v39 = v71;
  unint64_t v38 = v72;
  outlined init with take of BNNS.Shape((uint64_t)&v74, (uint64_t)v82);
  BNNS.Shape.stride.getter((uint64_t)&v65);
  size_t out_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v32, v33, v34, v35, v36, v37, v39, v38, v65, v66, v67, v68, v69, v70, v71, v72);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v65);
  outlined init with take of BNNS.Shape((uint64_t)&v65, (uint64_t)v82);
  outlined init with take of BNNS.Shape((uint64_t)v82, (uint64_t)v64);
  BNNS.Shape.size.getter((uint64_t)&v59);
  long long v40 = v59;
  long long v41 = v60;
  long long v42 = v61;
  unint64_t v44 = v62;
  unint64_t v43 = v63;
  outlined init with take of BNNS.Shape((uint64_t)v82, (uint64_t)v64);
  BNNS.Shape.stride.getter((uint64_t)&v59);
  size_t out_delta_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v40, *((unint64_t *)&v40 + 1), v41, *((unint64_t *)&v41 + 1), v42, *((unint64_t *)&v42 + 1), v44, v43, v59, *((unint64_t *)&v59 + 1), v60, *((unint64_t *)&v60 + 1), v61, *((unint64_t *)&v61 + 1), v62, v63);
  uint64_t v46 = *a6;
  char isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
  *a6 = v46;
  if ((isUniquelyReferenced_nonNull_native & 1) == 0) {
    uint64_t v46 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v46 + 2), 0, v46);
  }
  *a6 = v46;
  swift_bridgeObjectRetain();
  int v48 = BNNSArithmeticFilterApplyBackwardBatch(v54, a5, 2uLL, (const void **)v46 + 4, in_stride, v55, (const size_t *)(a8 + 32), v51, out_stride, a1, out_delta_stride);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  uint64_t result = swift_bridgeObjectRelease();
  *a7 = v48;
  return result;
}

uint64_t specialized static BNNS.arithmeticLayerApply(_:batchSize:input:output:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4)
{
  v38[1] = *MEMORY[0x1E4F143B8];
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v36);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v36, (uint64_t)&v37);
  uint64_t v7 = v37;
  if (v37
    && (outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v35),
        outlined init with take of UnsafeMutableRawPointer?((uint64_t)v35, (uint64_t)v38),
        v38[0]))
  {
    in = v7;
    uint64_t v22 = (void *)v38[0];
    unint64_t v23 = *(void **)(a1 + 16);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
    uint64_t v24 = swift_allocObject();
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v32);
    outlined init with take of BNNS.Shape((uint64_t)v32, (uint64_t)v33);
    outlined init with take of BNNS.Shape((uint64_t)v33, (uint64_t)v31);
    BNNS.Shape.size.getter((uint64_t)&v25);
    long long v8 = v25;
    long long v9 = v26;
    long long v10 = v27;
    unint64_t v12 = v28;
    unint64_t v11 = v29;
    outlined init with take of BNNS.Shape((uint64_t)v33, (uint64_t)v31);
    BNNS.Shape.stride.getter((uint64_t)&v25);
    *(void *)(v24 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v8, *((unint64_t *)&v8 + 1), v9, *((unint64_t *)&v9 + 1), v10, *((unint64_t *)&v10 + 1), v12, v11, v25, *((unint64_t *)&v25 + 1), v26, *((unint64_t *)&v26 + 1), v27, *((unint64_t *)&v27 + 1), v28, v29);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v32);
    outlined init with take of BNNS.Shape((uint64_t)v32, (uint64_t)v34);
    outlined init with take of BNNS.Shape((uint64_t)v34, (uint64_t)v31);
    BNNS.Shape.size.getter((uint64_t)&v25);
    long long v13 = v25;
    long long v14 = v26;
    long long v15 = v27;
    unint64_t v16 = v28;
    unint64_t v21 = v29;
    outlined init with take of BNNS.Shape((uint64_t)v34, (uint64_t)v31);
    BNNS.Shape.stride.getter((uint64_t)&v25);
    size_t v17 = specialized static BNNS.calculateBatchStride(size:stride:)(v13, *((unint64_t *)&v13 + 1), v14, *((unint64_t *)&v14 + 1), v15, *((unint64_t *)&v15 + 1), v16, v21, v25, *((unint64_t *)&v25 + 1), v26, *((unint64_t *)&v26 + 1), v27, *((unint64_t *)&v27 + 1), v28, v29);
    LODWORD(v11) = BNNSArithmeticFilterApplyBatch(v23, a2, 1uLL, (const void **)&in, (const size_t *)(v24 + 32), v22, v17);
    swift_setDeallocating();
    uint64_t result = swift_deallocClassInstance();
    if (!v11) {
      return result;
    }
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *unint64_t v19 = 0;
  }
  else
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *unint64_t v20 = 2;
  }
  return swift_willThrow();
}

uint64_t specialized static BNNS.arithmeticLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4, _OWORD *a5, long long *a6)
{
  v46[1] = *MEMORY[0x1E4F143B8];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer?>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135280;
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v45);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v45, (uint64_t)v46);
  *(void *)(inited + 32) = v46[0];
  long long v41 = (char *)inited;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  uint64_t v31 = swift_initStackObject();
  *(_OWORD *)(v31 + 16) = xmmword_1D2135280;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v43);
  outlined init with take of BNNS.Shape((uint64_t)v43, (uint64_t)v44);
  outlined init with take of BNNS.Shape((uint64_t)v44, (uint64_t)&v42);
  BNNS.Shape.size.getter((uint64_t)&v35);
  unint64_t v10 = v36;
  long long v11 = v37;
  long long v12 = v38;
  unint64_t v29 = v39;
  unint64_t v30 = v35;
  unint64_t v28 = v40;
  outlined init with take of BNNS.Shape((uint64_t)v44, (uint64_t)&v42);
  BNNS.Shape.stride.getter((uint64_t)&v35);
  *(void *)(v31 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v30, v10, v11, *((unint64_t *)&v11 + 1), v12, *((unint64_t *)&v12 + 1), v29, v28, v35, v36, v37, *((unint64_t *)&v37 + 1), v38, *((unint64_t *)&v38 + 1), v39, v40);
  long long v13 = a6[8];
  long long v14 = a6[9];
  long long v15 = a6[6];
  v43[7] = a6[7];
  v43[8] = v13;
  long long v16 = a6[10];
  v43[9] = v14;
  v43[10] = v16;
  long long v17 = a6[4];
  long long v18 = a6[5];
  long long v19 = a6[2];
  void v43[3] = a6[3];
  v43[4] = v17;
  v43[5] = v18;
  v43[6] = v15;
  long long v20 = *a6;
  v43[1] = a6[1];
  v43[2] = v19;
  long long v21 = a5[9];
  *(_OWORD *)&v42.stride[7] = a5[8];
  *(_OWORD *)&v42.BNNSDataType data_type = v21;
  *(_OWORD *)&v42.table_BNNSDataType data_type = a5[10];
  v43[0] = v20;
  long long v22 = a5[5];
  *(_OWORD *)&v42.size[7] = a5[4];
  *(_OWORD *)&v42.stride[1] = v22;
  long long v23 = a5[7];
  *(_OWORD *)&v42.stride[3] = a5[6];
  *(_OWORD *)&v42.stride[5] = v23;
  long long v24 = a5[1];
  *(_OWORD *)&v42.flags = *a5;
  *(_OWORD *)&v42.size[1] = v24;
  long long v25 = a5[3];
  *(_OWORD *)&v42.size[3] = a5[2];
  *(_OWORD *)&v42.size[5] = v25;
  closure #1 in closure #1 in static BNNS.arithmeticLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:)(&v42, (uint64_t)v43, a1, a2, &v41, v31, a4, (int *)&v35);
  swift_setDeallocating();
  if (v35)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    unsigned char *v26 = 0;
    swift_willThrow();
  }
  return swift_bridgeObjectRelease();
}

uint64_t type metadata accessor for BNNS.BinaryArithmeticLayer()
{
  return self;
}

uint64_t specialized static BNNS.arithmeticLayerApply(_:batchSize:inputA:inputB:output:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v55);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v55, (uint64_t)&v56);
  uint64_t v9 = v56;
  if (v56
    && (outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v54),
        outlined init with take of UnsafeMutableRawPointer?((uint64_t)v54, (uint64_t)&v57),
        (uint64_t v10 = v57) != 0)
    && (outlined init with take of UnsafeMutableRawPointer?(a5 + 136, (uint64_t)v53),
        outlined init with take of UnsafeMutableRawPointer?((uint64_t)v53, (uint64_t)&v58),
        v58))
  {
    out = v58;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
    uint64_t v11 = swift_allocObject();
    *(_OWORD *)(v11 + 16) = xmmword_1D2135290;
    *(void *)(v11 + 32) = v9;
    unint64_t v34 = (const void **)(v11 + 32);
    *(void *)(v11 + 40) = v10;
    unint64_t v33 = *(void **)(a1 + 16);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
    uint64_t v36 = swift_allocObject();
    *(_OWORD *)(v36 + 16) = xmmword_1D2135290;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v49);
    outlined init with take of BNNS.Shape((uint64_t)v49, (uint64_t)v50);
    outlined init with take of BNNS.Shape((uint64_t)v50, (uint64_t)v48);
    BNNS.Shape.size.getter((uint64_t)&v43);
    long long v12 = v43;
    long long v13 = v44;
    long long v14 = v45;
    unint64_t v32 = v46;
    unint64_t v29 = v47;
    outlined init with take of BNNS.Shape((uint64_t)v50, (uint64_t)v48);
    BNNS.Shape.stride.getter((uint64_t)&v43);
    *(void *)(v36 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v12, *((unint64_t *)&v12 + 1), v13, *((unint64_t *)&v13 + 1), v14, *((unint64_t *)&v14 + 1), v32, v29, v43, *((unint64_t *)&v43 + 1), v44, *((unint64_t *)&v44 + 1), v45, *((unint64_t *)&v45 + 1), v46, v47);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v48);
    outlined init with take of BNNS.Shape((uint64_t)v48, (uint64_t)v51);
    outlined init with take of BNNS.Shape((uint64_t)v51, (uint64_t)&v43);
    BNNS.Shape.size.getter((uint64_t)&v37);
    unint64_t v15 = v38;
    long long v16 = v39;
    long long v17 = v40;
    unint64_t v18 = v41;
    unint64_t v28 = v42;
    unint64_t v30 = v37;
    outlined init with take of BNNS.Shape((uint64_t)v51, (uint64_t)&v43);
    BNNS.Shape.stride.getter((uint64_t)&v37);
    *(void *)(v36 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v30, v15, v16, *((unint64_t *)&v16 + 1), v17, *((unint64_t *)&v17 + 1), v18, v28, v37, v38, v39, *((unint64_t *)&v39 + 1), v40, *((unint64_t *)&v40 + 1), v41, v42);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v49);
    outlined init with take of BNNS.Shape((uint64_t)v49, (uint64_t)v52);
    outlined init with take of BNNS.Shape((uint64_t)v52, (uint64_t)v48);
    BNNS.Shape.size.getter((uint64_t)&v43);
    long long v19 = v43;
    long long v20 = v44;
    long long v21 = v45;
    unint64_t v22 = v46;
    unint64_t v31 = v47;
    outlined init with take of BNNS.Shape((uint64_t)v52, (uint64_t)v48);
    BNNS.Shape.stride.getter((uint64_t)&v43);
    size_t v23 = specialized static BNNS.calculateBatchStride(size:stride:)(v19, *((unint64_t *)&v19 + 1), v20, *((unint64_t *)&v20 + 1), v21, *((unint64_t *)&v21 + 1), v22, v31, v43, *((unint64_t *)&v43 + 1), v44, *((unint64_t *)&v44 + 1), v45, *((unint64_t *)&v45 + 1), v46, v47);
    int v24 = BNNSArithmeticFilterApplyBatch(v33, a2, 2uLL, v34, (const size_t *)(v36 + 32), out, v23);
    swift_bridgeObjectRelease();
    uint64_t result = swift_bridgeObjectRelease();
    if (!v24) {
      return result;
    }
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    unsigned char *v26 = 0;
  }
  else
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    unsigned char *v27 = 2;
  }
  return swift_willThrow();
}

uint64_t specialized static BNNS.arithmeticLayerApplyBackward(_:batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4, uint64_t a5, _OWORD *a6, _OWORD *a7, long long *a8)
{
  v66[1] = *MEMORY[0x1E4F143B8];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer?>);
  uint64_t v13 = swift_allocObject();
  *(_OWORD *)(v13 + 16) = xmmword_1D2135290;
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v64);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v64, (uint64_t)&v65);
  *(void *)(v13 + 32) = v65;
  outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v63);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v63, (uint64_t)v66);
  *(void *)(v13 + 40) = v66[0];
  uint64_t v57 = (char *)v13;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  uint64_t v50 = swift_allocObject();
  *(_OWORD *)(v50 + 16) = xmmword_1D2135290;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v60);
  outlined init with take of BNNS.Shape((uint64_t)v60, (uint64_t)v61);
  outlined init with take of BNNS.Shape((uint64_t)v61, (uint64_t)v59);
  BNNS.Shape.size.getter((uint64_t)&v58);
  long long v14 = *(_OWORD *)&v58.flags;
  unint64_t v15 = v58.size[2];
  long long v16 = *(_OWORD *)&v58.size[3];
  unint64_t v45 = v58.size[1];
  unint64_t v41 = v58.size[6];
  unint64_t v43 = v58.size[5];
  outlined init with take of BNNS.Shape((uint64_t)v61, (uint64_t)v59);
  BNNS.Shape.stride.getter((uint64_t)&v58);
  *(void *)(v50 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v14, *((unint64_t *)&v14 + 1), v45, v15, v16, *((unint64_t *)&v16 + 1), v43, v41, *(unint64_t *)&v58.flags, v58.size[0], v58.size[1], v58.size[2], v58.size[3], v58.size[4], v58.size[5], v58.size[6]);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v59);
  outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v62);
  outlined init with take of BNNS.Shape((uint64_t)v62, (uint64_t)&v58);
  BNNS.Shape.size.getter((uint64_t)&v51);
  unint64_t v17 = v51;
  long long v18 = v53;
  long long v19 = v54;
  unint64_t v46 = v52;
  unint64_t v42 = v56;
  unint64_t v44 = v55;
  outlined init with take of BNNS.Shape((uint64_t)v62, (uint64_t)&v58);
  BNNS.Shape.stride.getter((uint64_t)&v51);
  unint64_t v20 = specialized static BNNS.calculateBatchStride(size:stride:)(v17, v46, v18, *((unint64_t *)&v18 + 1), v19, *((unint64_t *)&v19 + 1), v44, v42, v51, v52, v53, *((unint64_t *)&v53 + 1), v54, *((unint64_t *)&v54 + 1), v55, v56);
  long long v21 = a7[9];
  v60[8] = a7[8];
  v60[9] = v21;
  v60[10] = a7[10];
  long long v22 = a7[5];
  v60[4] = a7[4];
  v60[5] = v22;
  long long v23 = a7[7];
  v60[6] = a7[6];
  v60[7] = v23;
  long long v24 = a7[1];
  v60[0] = *a7;
  v60[1] = v24;
  long long v25 = a7[3];
  v60[2] = a7[2];
  v60[3] = v25;
  long long v26 = a8[8];
  long long v27 = a8[9];
  long long v28 = a8[6];
  v59[7] = a8[7];
  v59[8] = v26;
  long long v29 = a8[10];
  v59[9] = v27;
  v59[10] = v29;
  long long v30 = a8[4];
  long long v31 = a8[5];
  long long v32 = a8[2];
  v59[3] = a8[3];
  v59[4] = v30;
  *(void *)(v50 + 40) = v20;
  v59[5] = v31;
  v59[6] = v28;
  long long v33 = *a8;
  v59[1] = a8[1];
  v59[2] = v32;
  long long v34 = a6[9];
  *(_OWORD *)&v58.stride[7] = a6[8];
  *(_OWORD *)&v58.BNNSDataType data_type = v34;
  *(_OWORD *)&v58.table_BNNSDataType data_type = a6[10];
  v59[0] = v33;
  long long v35 = a6[5];
  *(_OWORD *)&v58.size[7] = a6[4];
  *(_OWORD *)&v58.stride[1] = v35;
  long long v36 = a6[7];
  *(_OWORD *)&v58.stride[3] = a6[6];
  *(_OWORD *)&v58.stride[5] = v36;
  long long v37 = a6[1];
  *(_OWORD *)&v58.flags = *a6;
  *(_OWORD *)&v58.size[1] = v37;
  long long v38 = a6[3];
  *(_OWORD *)&v58.size[3] = a6[2];
  *(_OWORD *)&v58.size[5] = v38;
  closure #1 in closure #1 in closure #1 in static BNNS.arithmeticLayerApplyBackward(_:batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(&v58, (uint64_t)v60, (uint64_t)v59, a1, a2, &v57, (int *)&v51, v50, a5);
  swift_setDeallocating();
  swift_deallocClassInstance();
  if (v51)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v39 = 0;
    swift_willThrow();
  }
  return swift_bridgeObjectRelease();
}

unint64_t lazy protocol witness table accessor for type BNNS.DescriptorType and conformance BNNS.DescriptorType()
{
  unint64_t result = lazy protocol witness table cache variable for type BNNS.DescriptorType and conformance BNNS.DescriptorType;
  if (!lazy protocol witness table cache variable for type BNNS.DescriptorType and conformance BNNS.DescriptorType)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.DescriptorType and conformance BNNS.DescriptorType);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type BNNS.ArithmeticUnaryFunction and conformance BNNS.ArithmeticUnaryFunction()
{
  unint64_t result = lazy protocol witness table cache variable for type BNNS.ArithmeticUnaryFunction and conformance BNNS.ArithmeticUnaryFunction;
  if (!lazy protocol witness table cache variable for type BNNS.ArithmeticUnaryFunction and conformance BNNS.ArithmeticUnaryFunction)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.ArithmeticUnaryFunction and conformance BNNS.ArithmeticUnaryFunction);
  }
  return result;
}

uint64_t associated type witness table accessor for CaseIterable.AllCases : Collection in BNNS.ArithmeticUnaryFunction()
{
  return lazy protocol witness table accessor for type [BNNS.ArithmeticUnaryFunction] and conformance [A](&lazy protocol witness table cache variable for type [BNNS.ArithmeticUnaryFunction] and conformance [A], &demangling cache variable for type metadata for [BNNS.ArithmeticUnaryFunction]);
}

unint64_t lazy protocol witness table accessor for type BNNS.ArithmeticBinaryFunction and conformance BNNS.ArithmeticBinaryFunction()
{
  unint64_t result = lazy protocol witness table cache variable for type BNNS.ArithmeticBinaryFunction and conformance BNNS.ArithmeticBinaryFunction;
  if (!lazy protocol witness table cache variable for type BNNS.ArithmeticBinaryFunction and conformance BNNS.ArithmeticBinaryFunction)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.ArithmeticBinaryFunction and conformance BNNS.ArithmeticBinaryFunction);
  }
  return result;
}

uint64_t associated type witness table accessor for CaseIterable.AllCases : Collection in BNNS.ArithmeticBinaryFunction()
{
  return lazy protocol witness table accessor for type [BNNS.ArithmeticUnaryFunction] and conformance [A](&lazy protocol witness table cache variable for type [BNNS.ArithmeticBinaryFunction] and conformance [A], &demangling cache variable for type metadata for [BNNS.ArithmeticBinaryFunction]);
}

uint64_t lazy protocol witness table accessor for type [BNNS.ArithmeticUnaryFunction] and conformance [A](unint64_t *a1, uint64_t *a2)
{
  uint64_t result = *a1;
  if (!result)
  {
    __swift_instantiateConcreteTypeFromMangledNameAbstract(a2);
    uint64_t result = swift_getWitnessTable();
    atomic_store(result, a1);
  }
  return result;
}

unsigned char *storeEnumTagSinglePayload for BNNS.DescriptorType(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 2 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 2) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFE) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFD)
  {
    unsigned int v6 = ((a2 - 254) >> 8) + 1;
    *uint64_t result = a2 + 2;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D2109B10);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 2;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.DescriptorType()
{
  return &type metadata for BNNS.DescriptorType;
}

uint64_t getEnumTagSinglePayload for BNNS.ArithmeticUnaryFunction(unsigned __int8 *a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0xE6) {
    goto LABEL_17;
  }
  if (a2 + 26 >= 0xFFFF00) {
    int v2 = 4;
  }
  else {
    int v2 = 2;
  }
  if ((a2 + 26) >> 8 < 0xFF) {
    int v3 = 1;
  }
  else {
    int v3 = v2;
  }
  if (v3 == 4)
  {
    int v4 = *(_DWORD *)(a1 + 1);
    if (v4) {
      return (*a1 | (v4 << 8)) - 26;
    }
  }
  else
  {
    if (v3 == 2)
    {
      int v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1)) {
        goto LABEL_17;
      }
      return (*a1 | (v4 << 8)) - 26;
    }
    int v4 = a1[1];
    if (a1[1]) {
      return (*a1 | (v4 << 8)) - 26;
    }
  }
LABEL_17:
  unsigned int v6 = *a1;
  BOOL v7 = v6 >= 0x1B;
  int v8 = v6 - 27;
  if (!v7) {
    int v8 = -1;
  }
  return (v8 + 1);
}

unsigned char *storeEnumTagSinglePayload for BNNS.ArithmeticUnaryFunction(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 26 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 26) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xE6) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xE5)
  {
    unsigned int v6 = ((a2 - 230) >> 8) + 1;
    *uint64_t result = a2 + 26;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D2109CA4);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 26;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.ArithmeticUnaryFunction()
{
  return &type metadata for BNNS.ArithmeticUnaryFunction;
}

uint64_t getEnumTagSinglePayload for BNNS.ArithmeticBinaryFunction(unsigned __int8 *a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0xF5) {
    goto LABEL_17;
  }
  if (a2 + 11 >= 0xFFFF00) {
    int v2 = 4;
  }
  else {
    int v2 = 2;
  }
  if ((a2 + 11) >> 8 < 0xFF) {
    int v3 = 1;
  }
  else {
    int v3 = v2;
  }
  if (v3 == 4)
  {
    int v4 = *(_DWORD *)(a1 + 1);
    if (v4) {
      return (*a1 | (v4 << 8)) - 11;
    }
  }
  else
  {
    if (v3 == 2)
    {
      int v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1)) {
        goto LABEL_17;
      }
      return (*a1 | (v4 << 8)) - 11;
    }
    int v4 = a1[1];
    if (a1[1]) {
      return (*a1 | (v4 << 8)) - 11;
    }
  }
LABEL_17:
  unsigned int v6 = *a1;
  BOOL v7 = v6 >= 0xC;
  int v8 = v6 - 12;
  if (!v7) {
    int v8 = -1;
  }
  return (v8 + 1);
}

unsigned char *storeEnumTagSinglePayload for BNNS.ArithmeticBinaryFunction(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 11 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 11) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xF5) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xF4)
  {
    unsigned int v6 = ((a2 - 245) >> 8) + 1;
    *uint64_t result = a2 + 11;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D2109E38);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 11;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.ArithmeticBinaryFunction()
{
  return &type metadata for BNNS.ArithmeticBinaryFunction;
}

uint64_t method lookup function for BNNS.UnaryArithmeticLayer(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.UnaryArithmeticLayer);
}

uint64_t dispatch thunk of BNNS.UnaryArithmeticLayer.apply(batchSize:input:output:)(uint64_t a1, uint64_t *a2, uint64_t *a3)
{
  uint64_t v4 = a2[17];
  int v5 = *((_DWORD *)a2 + 36);
  uint64_t v6 = a2[19];
  int v7 = *((_DWORD *)a2 + 40);
  uint64_t v8 = a3[17];
  int v9 = *((_DWORD *)a3 + 36);
  uint64_t v10 = a3[19];
  int v11 = *((_DWORD *)a3 + 40);
  long long v12 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *))(*(void *)v3 + 96);
  uint64_t v28 = *a2;
  long long v29 = *(_OWORD *)(a2 + 1);
  long long v30 = *(_OWORD *)(a2 + 3);
  long long v31 = *(_OWORD *)(a2 + 5);
  long long v32 = *(_OWORD *)(a2 + 7);
  long long v33 = *(_OWORD *)(a2 + 9);
  long long v34 = *(_OWORD *)(a2 + 11);
  long long v35 = *(_OWORD *)(a2 + 13);
  long long v36 = *(_OWORD *)(a2 + 15);
  uint64_t v37 = v4;
  int v38 = v5;
  uint64_t v39 = v6;
  int v40 = v7;
  uint64_t v41 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v14 = *a3;
  long long v15 = *(_OWORD *)(a3 + 1);
  long long v16 = *(_OWORD *)(a3 + 3);
  long long v17 = *(_OWORD *)(a3 + 5);
  long long v18 = *(_OWORD *)(a3 + 7);
  long long v19 = *(_OWORD *)(a3 + 9);
  long long v20 = *(_OWORD *)(a3 + 11);
  long long v21 = *(_OWORD *)(a3 + 13);
  long long v22 = *(_OWORD *)(a3 + 15);
  uint64_t v23 = v8;
  int v24 = v9;
  uint64_t v25 = v10;
  int v26 = v11;
  uint64_t v27 = *(uint64_t *)((char *)a3 + 164);
  return v12(a1, &v28, &v14);
}

uint64_t dispatch thunk of BNNS.UnaryArithmeticLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5)
{
  uint64_t v6 = a2[17];
  int v7 = *((_DWORD *)a2 + 36);
  uint64_t v8 = a2[19];
  int v9 = *((_DWORD *)a2 + 40);
  uint64_t v10 = a3[17];
  int v11 = *((_DWORD *)a3 + 36);
  uint64_t v12 = a3[19];
  int v13 = *((_DWORD *)a3 + 40);
  uint64_t v14 = a4[17];
  int v15 = *((_DWORD *)a4 + 36);
  uint64_t v16 = a4[19];
  int v17 = *((_DWORD *)a4 + 40);
  uint64_t v18 = a5[17];
  int v19 = *((_DWORD *)a5 + 36);
  uint64_t v20 = a5[19];
  int v21 = *((_DWORD *)a5 + 40);
  size_t v98 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(void *)v5 + 104);
  uint64_t v84 = *a2;
  long long v85 = *(_OWORD *)(a2 + 1);
  long long v86 = *(_OWORD *)(a2 + 3);
  long long v87 = *(_OWORD *)(a2 + 5);
  long long v88 = *(_OWORD *)(a2 + 7);
  long long v89 = *(_OWORD *)(a2 + 9);
  long long v90 = *(_OWORD *)(a2 + 11);
  long long v91 = *(_OWORD *)(a2 + 13);
  long long v92 = *(_OWORD *)(a2 + 15);
  uint64_t v93 = v6;
  int v94 = v7;
  uint64_t v95 = v8;
  int v96 = v9;
  uint64_t v97 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v70 = *a3;
  long long v71 = *(_OWORD *)(a3 + 1);
  long long v72 = *(_OWORD *)(a3 + 3);
  long long v73 = *(_OWORD *)(a3 + 5);
  long long v74 = *(_OWORD *)(a3 + 7);
  long long v75 = *(_OWORD *)(a3 + 9);
  long long v76 = *(_OWORD *)(a3 + 11);
  long long v22 = *(_OWORD *)(a3 + 15);
  long long v77 = *(_OWORD *)(a3 + 13);
  long long v23 = *(_OWORD *)(a4 + 1);
  long long v24 = *(_OWORD *)(a4 + 3);
  long long v25 = *(_OWORD *)(a4 + 5);
  long long v26 = *(_OWORD *)(a4 + 7);
  long long v27 = *(_OWORD *)(a4 + 9);
  long long v28 = *(_OWORD *)(a4 + 11);
  long long v29 = *(_OWORD *)(a4 + 13);
  uint64_t v30 = *a4;
  long long v31 = *(_OWORD *)(a4 + 15);
  long long v32 = *(_OWORD *)(a5 + 1);
  long long v33 = *(_OWORD *)(a5 + 3);
  long long v34 = *(_OWORD *)(a5 + 5);
  long long v35 = *(_OWORD *)(a5 + 7);
  long long v36 = *(_OWORD *)(a5 + 9);
  long long v37 = *(_OWORD *)(a5 + 11);
  long long v38 = *(_OWORD *)(a5 + 13);
  uint64_t v39 = *a5;
  long long v40 = *(_OWORD *)(a5 + 15);
  long long v78 = v22;
  uint64_t v79 = v10;
  int v80 = v11;
  uint64_t v81 = v12;
  int v82 = v13;
  uint64_t v83 = *(uint64_t *)((char *)a3 + 164);
  *(void *)&long long v22 = *(uint64_t *)((char *)a5 + 164);
  uint64_t v56 = v30;
  long long v57 = v23;
  *(void *)&long long v23 = *(uint64_t *)((char *)a4 + 164);
  long long v58 = v24;
  long long v59 = v25;
  long long v60 = v26;
  long long v61 = v27;
  long long v62 = v28;
  long long v63 = v29;
  long long v64 = v31;
  uint64_t v65 = v14;
  int v66 = v15;
  uint64_t v67 = v16;
  int v68 = v17;
  uint64_t v69 = v23;
  uint64_t v42 = v39;
  long long v43 = v32;
  long long v44 = v33;
  long long v45 = v34;
  long long v46 = v35;
  long long v47 = v36;
  long long v48 = v37;
  long long v49 = v38;
  long long v50 = v40;
  uint64_t v51 = v18;
  int v52 = v19;
  uint64_t v53 = v20;
  int v54 = v21;
  uint64_t v55 = v22;
  return v98(a1, &v84, &v70, &v56, &v42);
}

uint64_t method lookup function for BNNS.BinaryArithmeticLayer(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.BinaryArithmeticLayer);
}

uint64_t dispatch thunk of BNNS.BinaryArithmeticLayer.apply(batchSize:inputA:inputB:output:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4)
{
  uint64_t v5 = a2[17];
  int v6 = *((_DWORD *)a2 + 36);
  uint64_t v7 = a2[19];
  int v8 = *((_DWORD *)a2 + 40);
  uint64_t v9 = a3[17];
  int v10 = *((_DWORD *)a3 + 36);
  uint64_t v11 = a3[19];
  int v12 = *((_DWORD *)a3 + 40);
  uint64_t v13 = a4[17];
  int v14 = *((_DWORD *)a4 + 36);
  uint64_t v15 = a4[19];
  int v16 = *((_DWORD *)a4 + 40);
  int v17 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *))(*(void *)v4 + 96);
  uint64_t v52 = *a2;
  long long v53 = *(_OWORD *)(a2 + 1);
  long long v54 = *(_OWORD *)(a2 + 3);
  long long v55 = *(_OWORD *)(a2 + 5);
  long long v56 = *(_OWORD *)(a2 + 7);
  long long v57 = *(_OWORD *)(a2 + 9);
  long long v58 = *(_OWORD *)(a2 + 11);
  long long v59 = *(_OWORD *)(a2 + 13);
  long long v60 = *(_OWORD *)(a2 + 15);
  uint64_t v61 = v5;
  int v62 = v6;
  uint64_t v63 = v7;
  int v64 = v8;
  uint64_t v65 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v38 = *a3;
  long long v39 = *(_OWORD *)(a3 + 1);
  long long v40 = *(_OWORD *)(a3 + 3);
  long long v41 = *(_OWORD *)(a3 + 5);
  long long v42 = *(_OWORD *)(a3 + 7);
  long long v43 = *(_OWORD *)(a3 + 9);
  long long v44 = *(_OWORD *)(a3 + 11);
  long long v45 = *(_OWORD *)(a3 + 13);
  long long v46 = *(_OWORD *)(a3 + 15);
  uint64_t v47 = v9;
  int v48 = v10;
  uint64_t v49 = v11;
  int v50 = v12;
  uint64_t v51 = *(uint64_t *)((char *)a3 + 164);
  uint64_t v24 = *a4;
  long long v25 = *(_OWORD *)(a4 + 1);
  long long v26 = *(_OWORD *)(a4 + 3);
  long long v18 = *(_OWORD *)(a4 + 7);
  long long v19 = *(_OWORD *)(a4 + 9);
  long long v20 = *(_OWORD *)(a4 + 11);
  long long v21 = *(_OWORD *)(a4 + 13);
  long long v22 = *(_OWORD *)(a4 + 15);
  long long v27 = *(_OWORD *)(a4 + 5);
  long long v28 = v18;
  long long v29 = v19;
  long long v30 = v20;
  long long v31 = v21;
  long long v32 = v22;
  uint64_t v33 = v13;
  int v34 = v14;
  uint64_t v35 = v15;
  int v36 = v16;
  uint64_t v37 = *(uint64_t *)((char *)a4 + 164);
  return v17(a1, &v52, &v38, &v24);
}

uint64_t dispatch thunk of BNNS.BinaryArithmeticLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t a4, uint64_t a5, uint64_t *a6, uint64_t *a7)
{
  uint64_t v76 = a2[17];
  int v75 = *((_DWORD *)a2 + 36);
  uint64_t v74 = a2[19];
  int v73 = *((_DWORD *)a2 + 40);
  uint64_t v72 = a3[17];
  int v71 = *((_DWORD *)a3 + 36);
  uint64_t v8 = a3[19];
  int v9 = *((_DWORD *)a3 + 40);
  uint64_t v10 = *(void *)(a4 + 136);
  int v11 = *(_DWORD *)(a4 + 144);
  uint64_t v12 = *(void *)(a4 + 152);
  int v13 = *(_DWORD *)(a4 + 160);
  uint64_t v14 = *(void *)(a5 + 136);
  int v15 = *(_DWORD *)(a5 + 144);
  uint64_t v16 = *(void *)(a5 + 152);
  int v17 = *(_DWORD *)(a5 + 160);
  uint64_t v18 = a6[17];
  int v19 = *((_DWORD *)a6 + 36);
  uint64_t v20 = a6[19];
  int v21 = *((_DWORD *)a6 + 40);
  uint64_t v22 = a7[17];
  int v23 = *((_DWORD *)a7 + 36);
  uint64_t v24 = a7[19];
  int v25 = *((_DWORD *)a7 + 40);
  long long v26 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(void *)v7 + 104);
  uint64_t v27 = *a2;
  long long v28 = *(_OWORD *)(a2 + 1);
  long long v29 = *(_OWORD *)(a2 + 3);
  long long v30 = *(_OWORD *)(a2 + 5);
  long long v31 = *(_OWORD *)(a2 + 7);
  long long v32 = *(_OWORD *)(a2 + 9);
  long long v33 = *(_OWORD *)(a2 + 13);
  long long v34 = *(_OWORD *)(a2 + 15);
  uint64_t v35 = *(uint64_t *)((char *)a2 + 164);
  long long v36 = *(_OWORD *)(a3 + 1);
  long long v37 = *(_OWORD *)(a3 + 3);
  long long v38 = *(_OWORD *)(a3 + 5);
  long long v39 = *(_OWORD *)(a3 + 7);
  long long v40 = *(_OWORD *)(a3 + 9);
  long long v41 = *(_OWORD *)(a3 + 11);
  long long v42 = *(_OWORD *)(a3 + 13);
  uint64_t v43 = *a3;
  long long v44 = *(_OWORD *)(a3 + 15);
  uint64_t v45 = *(uint64_t *)((char *)a3 + 164);
  long long v154 = *(_OWORD *)(a2 + 11);
  long long v155 = v33;
  long long v156 = v34;
  uint64_t v161 = v35;
  long long v149 = v28;
  long long v150 = v29;
  long long v151 = v30;
  long long v152 = v31;
  long long v153 = v32;
  long long v140 = v41;
  long long v141 = v42;
  long long v142 = v44;
  uint64_t v147 = v45;
  long long v46 = *(_OWORD *)(a4 + 8);
  long long v47 = *(_OWORD *)(a4 + 24);
  long long v48 = *(_OWORD *)(a4 + 40);
  long long v49 = *(_OWORD *)(a4 + 56);
  long long v50 = *(_OWORD *)(a4 + 72);
  long long v51 = *(_OWORD *)(a4 + 88);
  long long v52 = *(_OWORD *)(a4 + 104);
  *(void *)&long long v34 = *(void *)a4;
  long long v53 = *(_OWORD *)(a4 + 120);
  *(void *)&long long v41 = *(void *)(a4 + 164);
  long long v135 = v36;
  long long v136 = v37;
  long long v137 = v38;
  long long v138 = v39;
  long long v139 = v40;
  long long v126 = v51;
  long long v127 = v52;
  long long v128 = v53;
  uint64_t v133 = v41;
  long long v54 = *(_OWORD *)(a5 + 88);
  long long v121 = v46;
  long long v55 = *(_OWORD *)(a5 + 104);
  long long v122 = v47;
  long long v56 = *(_OWORD *)(a5 + 120);
  long long v123 = v48;
  *(void *)&long long v48 = *(void *)(a5 + 164);
  long long v124 = v49;
  long long v125 = v50;
  long long v112 = v54;
  long long v113 = v55;
  long long v114 = v56;
  uint64_t v119 = v48;
  long long v57 = *(_OWORD *)(a5 + 24);
  long long v58 = *(_OWORD *)(a5 + 40);
  long long v59 = *(_OWORD *)(a5 + 56);
  long long v60 = *(_OWORD *)(a5 + 72);
  *(void *)&long long v54 = *(void *)a5;
  long long v107 = *(_OWORD *)(a5 + 8);
  long long v108 = v57;
  long long v109 = v58;
  long long v110 = v59;
  long long v111 = v60;
  long long v98 = *(_OWORD *)(a6 + 11);
  long long v99 = *(_OWORD *)(a6 + 13);
  long long v100 = *(_OWORD *)(a6 + 15);
  uint64_t v105 = *(uint64_t *)((char *)a6 + 164);
  uint64_t v148 = v27;
  long long v61 = *(_OWORD *)(a6 + 1);
  long long v62 = *(_OWORD *)(a6 + 3);
  uint64_t v134 = v43;
  long long v63 = *(_OWORD *)(a6 + 5);
  uint64_t v120 = v34;
  long long v64 = *(_OWORD *)(a6 + 7);
  uint64_t v106 = v54;
  uint64_t v92 = *a6;
  long long v65 = *(_OWORD *)(a6 + 9);
  long long v93 = v61;
  long long v94 = v62;
  long long v95 = v63;
  long long v96 = v64;
  long long v97 = v65;
  uint64_t v78 = *a7;
  long long v66 = *(_OWORD *)(a7 + 3);
  long long v79 = *(_OWORD *)(a7 + 1);
  long long v80 = v66;
  long long v67 = *(_OWORD *)(a7 + 7);
  long long v81 = *(_OWORD *)(a7 + 5);
  long long v82 = v67;
  long long v68 = *(_OWORD *)(a7 + 11);
  long long v83 = *(_OWORD *)(a7 + 9);
  long long v84 = v68;
  long long v69 = *(_OWORD *)(a7 + 15);
  long long v85 = *(_OWORD *)(a7 + 13);
  long long v86 = v69;
  uint64_t v91 = *(uint64_t *)((char *)a7 + 164);
  uint64_t v157 = v76;
  int v158 = v75;
  uint64_t v159 = v74;
  int v160 = v73;
  uint64_t v143 = v72;
  int v144 = v71;
  uint64_t v145 = v8;
  int v146 = v9;
  uint64_t v129 = v10;
  int v130 = v11;
  uint64_t v131 = v12;
  int v132 = v13;
  uint64_t v115 = v14;
  int v116 = v15;
  uint64_t v117 = v16;
  int v118 = v17;
  uint64_t v101 = v18;
  int v102 = v19;
  uint64_t v103 = v20;
  int v104 = v21;
  uint64_t v87 = v22;
  int v88 = v23;
  uint64_t v89 = v24;
  int v90 = v25;
  return v26(a1, &v148, &v134, &v120, &v106, &v92, &v78);
}

uint64_t static vDSP.linearInterpolate<A, B>(_:_:using:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(_:_:using:));
}

{
  uint64_t v5;

  uint64_t v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(_:_:using:));
}

uint64_t closure #1 in static vDSP.linearInterpolate<A, B>(_:_:using:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9)
{
  uint64_t v17 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v18 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(a3, a4, a1, a5, a6, v17, a7, a8, a9, v18);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  *a2 = result;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(_:_:using:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.linearInterpolate<A, B>(_:_:using:)(a1, a2, *(void *)(v2 + 48), *(void *)(v2 + 56), *(void *)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), *(float *)(v2 + 64));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.linearInterpolate<A, B>(_:_:using:)(a1, a2, *(void *)(v2 + 48), *(void *)(v2 + 56), *(void *)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), *(double *)(v2 + 64));
}

uint64_t static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, uint64_t a10)
{
  uint64_t v46 = a5;
  uint64_t v47 = a8;
  uint64_t v51 = a6;
  uint64_t v49 = a2;
  uint64_t v45 = *(void *)(a5 - 8);
  uint64_t v14 = MEMORY[0x1F4188790](a1);
  uint64_t v16 = (char *)&v41 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = *(void *)(v17 - 8);
  MEMORY[0x1F4188790](v14);
  uint64_t v20 = (char *)&v41 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  int v21 = *(void (**)(char *))(v18 + 16);
  uint64_t v43 = v22;
  v21(v20);
  int v23 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  uint64_t v44 = a7;
  uint64_t v24 = v23(a4, a7);
  uint64_t v41 = a10;
  uint64_t v25 = *(void *)(a10 + 8);
  long long v26 = *(uint64_t (**)(uint64_t, uint64_t))(v25 + 16);
  uint64_t v48 = a3;
  long long v50 = v26;
  uint64_t v27 = v26(v51, v25);
  long long v28 = *(uint64_t (**)(char *, uint64_t))(v18 + 8);
  uint64_t v42 = a4;
  uint64_t result = v28(v20, a4);
  if (v24 != v27)
  {
    __break(1u);
    goto LABEL_6;
  }
  long long v30 = v16;
  uint64_t v32 = v45;
  uint64_t v31 = v46;
  (*(void (**)(char *, uint64_t, uint64_t))(v45 + 16))(v30, v49, v46);
  uint64_t v33 = v47;
  uint64_t v34 = (*(uint64_t (**)(uint64_t, uint64_t))(v47 + 16))(v31, v47);
  uint64_t v35 = v48;
  uint64_t v36 = v51;
  uint64_t v37 = v50(v51, v25);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v32 + 8))(v30, v31);
  if (v34 != v37)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  uint64_t result = v50(v36, v25);
  if ((result & 0x8000000000000000) == 0)
  {
    uint64_t v38 = MEMORY[0x1F4188790](result);
    uint64_t v39 = v41;
    *(&v41 - 10) = v42;
    *(&v41 - 9) = v31;
    uint64_t v40 = v44;
    *(&v41 - 8) = v36;
    *(&v41 - 7) = v40;
    *(&v41 - 6) = v33;
    *(&v41 - 5) = v39;
    *(&v41 - 4) = v49;
    *(&v41 - 3) = v35;
    *((float *)&v41 - 4) = a9;
    *(&v41 - 1) = v38;
    return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t)))(v40 + 24))(partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:));
  }
LABEL_7:
  __break(1u);
  return result;
}

void closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(float **a1, const float *__A, int a3, const float *__B, int a5, vDSP_Length __N, float a7)
{
  if (!__A)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!__B)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  float v7 = a7;
  if (*a1)
  {
    vDSP_vintb(__A, 1, __B, 1, &v7, *a1, 1, __N);
    return;
  }
LABEL_7:
  __break(1u);
}

uint64_t closure #1 in static vDSP.linearInterpolate<A, B>(_:_:using:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9)
{
  uint64_t v17 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v18 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(a3, a4, a1, a5, a6, v17, a7, a8, a9, v18);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  *a2 = result;
  return result;
}

uint64_t static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, uint64_t a10)
{
  uint64_t v46 = a5;
  uint64_t v47 = a8;
  uint64_t v51 = a6;
  uint64_t v49 = a2;
  uint64_t v45 = *(void *)(a5 - 8);
  uint64_t v14 = MEMORY[0x1F4188790](a1);
  uint64_t v16 = (char *)&v41 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = *(void *)(v17 - 8);
  MEMORY[0x1F4188790](v14);
  uint64_t v20 = (char *)&v41 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  int v21 = *(void (**)(char *))(v18 + 16);
  uint64_t v43 = v22;
  v21(v20);
  int v23 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  uint64_t v44 = a7;
  uint64_t v24 = v23(a4, a7);
  uint64_t v41 = a10;
  uint64_t v25 = *(void *)(a10 + 8);
  long long v26 = *(uint64_t (**)(uint64_t, uint64_t))(v25 + 16);
  uint64_t v48 = a3;
  long long v50 = v26;
  uint64_t v27 = v26(v51, v25);
  long long v28 = *(uint64_t (**)(char *, uint64_t))(v18 + 8);
  uint64_t v42 = a4;
  uint64_t result = v28(v20, a4);
  if (v24 != v27)
  {
    __break(1u);
    goto LABEL_6;
  }
  long long v30 = v16;
  uint64_t v32 = v45;
  uint64_t v31 = v46;
  (*(void (**)(char *, uint64_t, uint64_t))(v45 + 16))(v30, v49, v46);
  uint64_t v33 = v47;
  uint64_t v34 = (*(uint64_t (**)(uint64_t, uint64_t))(v47 + 16))(v31, v47);
  uint64_t v35 = v48;
  uint64_t v36 = v51;
  uint64_t v37 = v50(v51, v25);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v32 + 8))(v30, v31);
  if (v34 != v37)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  uint64_t result = v50(v36, v25);
  if ((result & 0x8000000000000000) == 0)
  {
    uint64_t v38 = MEMORY[0x1F4188790](result);
    uint64_t v39 = v41;
    *(&v41 - 10) = v42;
    *(&v41 - 9) = v31;
    uint64_t v40 = v44;
    *(&v41 - 8) = v36;
    *(&v41 - 7) = v40;
    *(&v41 - 6) = v33;
    *(&v41 - 5) = v39;
    *(&v41 - 4) = v49;
    *(&v41 - 3) = v35;
    *((double *)&v41 - 2) = a9;
    *(&v41 - 1) = v38;
    return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t)))(v40 + 24))(partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:));
  }
LABEL_7:
  __break(1u);
  return result;
}

void closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(double **a1, const double *__A, int a3, const double *__B, int a5, vDSP_Length __N, double a7)
{
  if (!__A)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!__B)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  double v7 = a7;
  if (*a1)
  {
    vDSP_vintbD(__A, 1, __B, 1, &v7, *a1, 1, __N);
    return;
  }
LABEL_7:
  __break(1u);
}

uint64_t static vDSP.linearInterpolate<A, B>(elementsOf:using:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.linearInterpolate<A, B>(elementsOf:using:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(elementsOf:using:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.linearInterpolate<A, B>(elementsOf:using:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(elementsOf:using:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v13;
  uint64_t v14;
  char *v15;
  void (*v16)(char *);
  uint64_t v17;
  uint64_t (*v18)(uint64_t, uint64_t);
  uint64_t v19;
  uint64_t v20;
  uint64_t (*v21)(uint64_t, uint64_t);
  uint64_t v22;
  uint64_t result;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;

  uint64_t v34 = a7;
  uint64_t v35 = a1;
  uint64_t v36 = a4;
  int v13 = *(void *)(a5 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v15 = (char *)&v30 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v16 = *(void (**)(char *))(v13 + 16);
  uint64_t v32 = v17;
  v16(v15);
  uint64_t v18 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  uint64_t v33 = a8;
  uint64_t v19 = v18(a5, a8);
  uint64_t v31 = a9;
  uint64_t v20 = *(void *)(a9 + 8);
  int v21 = *(uint64_t (**)(uint64_t, uint64_t))(v20 + 16);
  uint64_t v22 = v21(a6, v20);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v13 + 8))(v15, a5);
  if (v19 != v22)
  {
    __break(1u);
    goto LABEL_6;
  }
  uint64_t result = v21(a6, v20);
  if (result < 0)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v24 = result;
  uint64_t v25 = v34;
  long long v26 = v36;
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(v34 + 16))(v36, v34);
  if ((result & 0x8000000000000000) == 0)
  {
    uint64_t v27 = MEMORY[0x1F4188790](result);
    *(&v30 - 10) = v26;
    *(&v30 - 9) = a5;
    *(&v30 - 8) = a6;
    *(&v30 - 7) = v25;
    long long v28 = v31;
    *(&v30 - 6) = v33;
    *(&v30 - 5) = v28;
    *(&v30 - 4) = v32;
    *(&v30 - 3) = a3;
    *(&v30 - 2) = v24;
    *(&v30 - 1) = v27;
    return (*(uint64_t (**)(uint64_t))(v25 + 24))(v29);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vDSP.linearInterpolate<A, B>(elementsOf:using:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, void *))
{
  uint64_t v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  v18[2] = a3;
  v18[3] = a4;
  v18[4] = a5;
  v18[5] = a6;
  v18[6] = a1;
  v18[7] = a2;
  return a8(v16, a7, v18);
}

uint64_t closure #1 in static vDSP.linearInterpolate<A, B>(elementsOf:using:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, unint64_t *a10, void (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v17 = __swift_instantiateConcreteTypeFromMangledName(a9);
  uint64_t v18 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a10, a9);
  a11(a3, a4, a1, a5, a6, v17, a7, a8, v18);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
  *a2 = result;
  return result;
}

void *closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(void *result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t, void, uint64_t))
{
  if (!a2)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!a4)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*result) {
    return (void *)a8(a2, a4, 1, *result, 1);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(v2 + 40);
  uint64_t v4 = *(void *)(v2 + 72);
  int v5 = *(_DWORD *)(v2 + 80);
  uint64_t v6 = *(void *)(v2 + 88);
  _OWORD v8[2] = *(void *)(v2 + 16);
  long long v9 = *(_OWORD *)(v2 + 24);
  uint64_t v10 = v3;
  long long v11 = *(_OWORD *)(v2 + 48);
  uint64_t v12 = v4;
  uint64_t v13 = a1;
  uint64_t v14 = a2;
  int v15 = v5;
  uint64_t v16 = v6;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t), void *, uint64_t, void))(v11 + 24))(partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:), v8, MEMORY[0x1E4FBC848] + 8, v9);
}

{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  void v8[3];
  long long v9;
  uint64_t v10;
  long long v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;

  uint64_t v3 = *(void *)(v2 + 40);
  uint64_t v4 = *(void *)(v2 + 72);
  int v5 = *(void *)(v2 + 80);
  uint64_t v6 = *(void *)(v2 + 88);
  _OWORD v8[2] = *(void *)(v2 + 16);
  long long v9 = *(_OWORD *)(v2 + 24);
  uint64_t v10 = v3;
  long long v11 = *(_OWORD *)(v2 + 48);
  uint64_t v12 = v4;
  uint64_t v13 = a1;
  uint64_t v14 = a2;
  int v15 = v5;
  uint64_t v16 = v6;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t), void *, uint64_t, void))(v11 + 24))(partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:), v8, MEMORY[0x1E4FBC848] + 8, v9);
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(elementsOf:using:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(elementsOf:using:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:));
}

{
  return partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(elementsOf:using:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:));
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:));
}

{
  return partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:));
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(elementsOf:using:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  return closure #1 in static vDSP.linearInterpolate<A, B>(elementsOf:using:)(a1, a2, v5[6], v5[7], v5[2], v5[3], v5[4], v5[5], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 40);
  uint64_t v5 = *(void *)(v3 + 72);
  v7[2] = *(void *)(v3 + 16);
  long long v8 = *(_OWORD *)(v3 + 24);
  uint64_t v9 = v4;
  long long v10 = *(_OWORD *)(v3 + 48);
  uint64_t v11 = v5;
  uint64_t v12 = a1;
  uint64_t v13 = a2;
  long long v14 = *(_OWORD *)(v3 + 80);
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, void))(v10 + 24))(a3, v7, MEMORY[0x1E4FBC848] + 8, v8);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:));
}

void *partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(void *a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, MEMORY[0x1E4F16CD0]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, MEMORY[0x1E4F16CC8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 32);
  uint64_t v5 = *(void *)(v3 + 56);
  v7[1] = *(_OWORD *)(v3 + 72);
  uint64_t v8 = a1;
  uint64_t v9 = a2;
  long long v10 = *(_OWORD *)(v3 + 88);
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, uint64_t))(v5 + 16))(a3, v7, MEMORY[0x1E4FBC848] + 8, v4);
}

void *partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(void *a1, uint64_t (*a2)(uint64_t, uint64_t, uint64_t, void, uint64_t))
{
  return closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(elementsOf:using:result:)(a1, v2[2], v2[3], v2[4], v2[5], v2[6], v2[7], a2);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(v2 + 32);
  uint64_t v4 = *(void *)(v2 + 56);
  uint64_t v5 = *(void *)(v2 + 88);
  v7[1] = *(_OWORD *)(v2 + 72);
  uint64_t v8 = a1;
  uint64_t v9 = a2;
  uint64_t v10 = v5;
  return (*(uint64_t (**)(void (*)(double **), _OWORD *, uint64_t, uint64_t))(v4 + 16))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:), v7, MEMORY[0x1E4FBC848] + 8, v3);
}

{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  int v5;
  _OWORD v7[2];
  uint64_t v8;
  uint64_t v9;
  int v10;

  uint64_t v3 = *(void *)(v2 + 32);
  uint64_t v4 = *(void *)(v2 + 56);
  uint64_t v5 = *(_DWORD *)(v2 + 88);
  v7[1] = *(_OWORD *)(v2 + 72);
  uint64_t v8 = a1;
  uint64_t v9 = a2;
  uint64_t v10 = v5;
  return (*(uint64_t (**)(void (*)(float **), _OWORD *, uint64_t, uint64_t))(v4 + 16))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:), v7, MEMORY[0x1E4FBC848] + 8, v3);
}

void partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(double **a1)
{
  closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(a1, *(const double **)(v1 + 16), *(void *)(v1 + 24), *(const double **)(v1 + 32), *(void *)(v1 + 40), *(void *)(v1 + 56), *(double *)(v1 + 48));
}

void partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(float **a1)
{
  closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(_:_:using:result:)(a1, *(const float **)(v1 + 16), *(void *)(v1 + 24), *(const float **)(v1 + 32), *(void *)(v1 + 40), *(void *)(v1 + 56), *(float *)(v1 + 48));
}

BOOL static BNNS.InterpolationMethod.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void BNNS.InterpolationMethod.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int BNNS.InterpolationMethod.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t BNNS.ResizeLayer.__allocating_init(interpolationMethod:input:output:alignsCorners:filterParameters:)(unsigned char *a1, _OWORD *a2, long long *a3, char a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v55 = *MEMORY[0x1E4F143B8];
  long long v8 = a3[8];
  long long v9 = a3[9];
  long long v10 = a3[6];
  long long v50 = a3[7];
  long long v51 = v8;
  long long v11 = a3[10];
  long long v52 = v9;
  long long v53 = v11;
  long long v12 = a3[4];
  long long v48 = a3[5];
  long long v49 = v10;
  long long v13 = a3[2];
  long long v46 = a3[3];
  long long v47 = v12;
  long long v14 = a3[1];
  long long v43 = *a3;
  long long v44 = v14;
  long long v45 = v13;
  long long v15 = a2[5];
  *(_OWORD *)&v29[68] = a2[4];
  long long v16 = a2[2];
  *(_OWORD *)&v29[52] = a2[3];
  long long v17 = a2[6];
  *(_OWORD *)&v29[116] = a2[7];
  long long v18 = a2[9];
  *(_OWORD *)&v29[132] = a2[8];
  *(_OWORD *)&v29[148] = v18;
  *(_OWORD *)&v29[164] = a2[10];
  *(_OWORD *)&v29[84] = v15;
  *(_OWORD *)&v29[100] = v17;
  long long v19 = a2[1];
  *(_OWORD *)&v29[4] = *a2;
  *(_OWORD *)&v29[20] = v19;
  *(_OWORD *)&v29[36] = v16;
  long long v39 = *(_OWORD *)&v29[128];
  long long v40 = *(_OWORD *)&v29[144];
  long long v41 = *(_OWORD *)&v29[160];
  long long v35 = *(_OWORD *)&v29[64];
  long long v36 = *(_OWORD *)&v29[80];
  long long v37 = *(_OWORD *)&v29[96];
  long long v38 = *(_OWORD *)&v29[112];
  long long v31 = *(_OWORD *)v29;
  long long v32 = *(_OWORD *)&v29[16];
  long long v33 = *(_OWORD *)&v29[32];
  BOOL v30 = (*a1 & 1) == 0;
  int v42 = *(_DWORD *)&v29[176];
  long long v34 = *(_OWORD *)&v29[48];
  char v54 = a4;
  if (a7 == 1)
  {
    uint64_t v20 = 0;
  }
  else
  {
    int v25 = a5;
    uint64_t v26 = a6;
    uint64_t v27 = a7;
    uint64_t v28 = a8;
    uint64_t v20 = &v25;
  }
  uint64_t v21 = MEMORY[0x1D2600080](&v30, v20);
  type metadata accessor for BNNS.ResizeLayer();
  uint64_t v22 = swift_allocObject();
  uint64_t v23 = v22;
  if (v21)
  {
    *(void *)(v22 + 16) = v21;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v23;
}

uint64_t type metadata accessor for BNNS.ResizeLayer()
{
  return self;
}

uint64_t BNNS.ResizeLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.ResizeLayer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

unint64_t lazy protocol witness table accessor for type BNNS.InterpolationMethod and conformance BNNS.InterpolationMethod()
{
  unint64_t result = lazy protocol witness table cache variable for type BNNS.InterpolationMethod and conformance BNNS.InterpolationMethod;
  if (!lazy protocol witness table cache variable for type BNNS.InterpolationMethod and conformance BNNS.InterpolationMethod)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.InterpolationMethod and conformance BNNS.InterpolationMethod);
  }
  return result;
}

unsigned char *storeEnumTagSinglePayload for BNNS.InterpolationMethod(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 1 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFF) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFE)
  {
    unsigned int v6 = ((a2 - 255) >> 8) + 1;
    *unint64_t result = a2 + 1;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D210C020);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 1;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.InterpolationMethod()
{
  return &type metadata for BNNS.InterpolationMethod;
}

uint64_t static vForce.ceil<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.ceil<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.ceil<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vForce.ceil<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.ceil<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.ceil<A, B>(_:result:));
}

uint64_t static vForce.ceil<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.ceil<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.ceil<A, B>(_:result:));
}

uint64_t closure #1 in static vForce.ceil<A>(_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t *a6, unint64_t *a7, void (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(a6);
  uint64_t v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a7, a6);
  a8(a3, a1, a4, v16, a5, v17);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vForce.floor<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.floor<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.floor<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.floor<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.floor<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.floor<A, B>(_:result:));
}

uint64_t static vForce.copysign<A, B>(magnitudes:signs:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.copysign<A, B>(magnitudes:signs:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.copysign<A, B>(magnitudes:signs:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.copysign<A, B, C>(magnitudes:signs:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  char *v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  void (*v23)(char *);
  uint64_t v24;
  void (*v25)(char *, uint64_t, uint64_t);
  uint64_t (*v26)(uint64_t, uint64_t);
  uint64_t v27;
  uint64_t (*v28)(uint64_t);
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  BOOL v33;
  uint64_t v34;
  void (*v35)(char *, uint64_t);
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v44;
  uint64_t v45;
  uint64_t (*v46)(uint64_t, uint64_t);
  uint64_t v47;
  uint64_t v48;
  char *v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;
  char *v56;
  int v57;
  uint64_t v58;

  long long v52 = a3;
  long long v53 = a6;
  long long v58 = *MEMORY[0x1E4F143B8];
  long long v14 = *(void *)(a5 - 8);
  long long v51 = a9;
  long long v15 = MEMORY[0x1F4188790](a1);
  long long v56 = (char *)&v44 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v17 = MEMORY[0x1F4188790](v15);
  long long v19 = (char *)&v44 - v18;
  MEMORY[0x1F4188790](v17);
  uint64_t v21 = (char *)&v44 - ((v20 + 15) & 0xFFFFFFFFFFFFFFF0);
  long long v50 = v22;
  uint64_t v23 = *(void (**)(char *))(v22 + 16);
  long long v47 = v24;
  v23(v21);
  char v54 = v14;
  int v25 = *(void (**)(char *, uint64_t, uint64_t))(v14 + 16);
  long long v44 = a2;
  v25(v19, a2, a5);
  uint64_t v26 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  long long v48 = a7;
  long long v49 = v21;
  uint64_t v55 = a4;
  long long v46 = v26;
  uint64_t v27 = v26(a4, a7);
  uint64_t v28 = *(uint64_t (**)(uint64_t))(a8 + 16);
  long long v29 = v28(a5);
  v25(v56, (uint64_t)v19, a5);
  long long v45 = a8;
  if (v27 == v29)
  {
    BOOL v30 = ((uint64_t (*)(uint64_t, uint64_t))v28)(a5, a8);
    long long v31 = v51;
    long long v32 = v53;
    long long v33 = v30 != (*(uint64_t (**)(uint64_t))(*(void *)(v51 + 8) + 16))(v53);
    long long v34 = v31;
  }
  else
  {
    long long v33 = 1;
    long long v32 = v53;
    long long v34 = v51;
  }
  long long v35 = *(void (**)(char *, uint64_t))(v54 + 8);
  v35(v56, a5);
  v35(v19, a5);
  long long v36 = v55;
  (*(void (**)(char *, uint64_t))(v50 + 8))(v49, v55);
  if (v33)
  {
    __break(1u);
    goto LABEL_9;
  }
  long long v37 = v47;
  long long v38 = v48;
  long long v39 = v46(v36, v48);
  if (v39 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
  }
  if (v39 > 0x7FFFFFFF) {
    goto LABEL_10;
  }
  long long v57 = v39;
  MEMORY[0x1F4188790](v39);
  *(&v44 - 10) = v36;
  *(&v44 - 9) = v40;
  *(&v44 - 8) = v32;
  *(&v44 - 7) = v38;
  *(&v44 - 6) = v45;
  *(&v44 - 5) = v34;
  long long v41 = v44;
  *(&v44 - 4) = v37;
  *(&v44 - 3) = v41;
  *(&v44 - 2) = (uint64_t)&v57;
  return (*(uint64_t (**)(uint64_t))(v34 + 16))(v42);
}

uint64_t closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, unint64_t *a10, void (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v17 = __swift_instantiateConcreteTypeFromMangledName(a9);
  uint64_t v18 = *(void *)(a8 + 8);
  uint64_t v19 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a10, a9);
  a11(a3, a4, a1, a5, a6, v17, a7, v18, v19);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  *a2 = result;
  return result;
}

uint64_t static vForce.truncatingRemainder<A, B>(dividends:divisors:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.truncatingRemainder<A, B>(dividends:divisors:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.truncatingRemainder<A, B>(dividends:divisors:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  void (*v24)(char *);
  uint64_t v25;
  void (*v26)(char *, uint64_t, uint64_t);
  uint64_t (*v27)(uint64_t, uint64_t);
  uint64_t v28;
  uint64_t v29;
  uint64_t (*v30)(uint64_t);
  uint64_t v31;
  uint64_t v32;
  char *v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  BOOL v38;
  void (*v39)(char *, uint64_t);
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  uint64_t v50;
  char *v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;
  uint64_t v56;
  uint64_t v57;
  char *v58;
  int v59;
  uint64_t v60;

  uint64_t v55 = a3;
  long long v56 = a6;
  long long v60 = *MEMORY[0x1E4F143B8];
  long long v14 = *(void *)(a5 - 8);
  char v54 = a9;
  long long v15 = MEMORY[0x1F4188790](a1);
  uint64_t v17 = (char *)&v46 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = MEMORY[0x1F4188790](v15);
  uint64_t v20 = (char *)&v46 - v19;
  MEMORY[0x1F4188790](v18);
  uint64_t v22 = (char *)&v46 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  long long v52 = v23;
  uint64_t v24 = *(void (**)(char *))(v23 + 16);
  long long v47 = v25;
  v24(v22);
  long long v53 = v14;
  uint64_t v26 = *(void (**)(char *, uint64_t, uint64_t))(v14 + 16);
  long long v48 = a2;
  v26(v20, a2, a5);
  uint64_t v27 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  long long v51 = v22;
  long long v57 = a4;
  long long v49 = a7;
  uint64_t v28 = v27(a4, a7);
  long long v29 = a8;
  BOOL v30 = *(uint64_t (**)(uint64_t))(a8 + 16);
  long long v31 = v29;
  long long v32 = v30(a5);
  long long v58 = v17;
  long long v33 = v17;
  long long v34 = a5;
  v26(v33, (uint64_t)v20, a5);
  long long v50 = v31;
  if (v28 == v32)
  {
    long long v35 = ((uint64_t (*)(uint64_t, uint64_t))v30)(a5, v31);
    long long v36 = v54;
    long long v37 = v56;
    long long v38 = v35 != (*(uint64_t (**)(uint64_t))(*(void *)(v54 + 8) + 16))(v56);
  }
  else
  {
    long long v38 = 1;
    long long v37 = v56;
    long long v36 = v54;
  }
  long long v39 = *(void (**)(char *, uint64_t))(v53 + 8);
  v39(v58, a5);
  v39(v20, a5);
  long long v40 = v57;
  (*(void (**)(char *, uint64_t))(v52 + 8))(v51, v57);
  if (v38)
  {
    __break(1u);
    goto LABEL_9;
  }
  long long v41 = (*(uint64_t (**)(uint64_t))(*(void *)(v36 + 8) + 16))(v37);
  if (v41 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
  }
  if (v41 > 0x7FFFFFFF) {
    goto LABEL_10;
  }
  long long v59 = v41;
  MEMORY[0x1F4188790](v41);
  *(&v46 - 10) = v40;
  *(&v46 - 9) = v34;
  int v42 = v49;
  *(&v46 - 8) = v37;
  *(&v46 - 7) = v42;
  *(&v46 - 6) = v50;
  *(&v46 - 5) = v36;
  long long v43 = v48;
  *(&v46 - 4) = v47;
  *(&v46 - 3) = v43;
  *(&v46 - 2) = (uint64_t)&v59;
  return (*(uint64_t (**)(uint64_t))(v36 + 16))(v44);
}

uint64_t static vForce.remainder<A, B>(dividends:divisors:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.remainder<A, B>(dividends:divisors:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.remainder<A, B>(dividends:divisors:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.remainder<A, B, C>(dividends:divisors:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

uint64_t static vForce.copysign<A, B>(magnitudes:signs:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t))
{
  uint64_t v38 = a7;
  long long v39 = a8;
  uint64_t v40 = a6;
  uint64_t v12 = *(void *)(a4 - 8);
  uint64_t v13 = MEMORY[0x1F4188790](a1);
  long long v15 = (char *)&v34 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v17 = *(void *)(v16 - 8);
  MEMORY[0x1F4188790](v13);
  uint64_t v19 = (char *)&v34 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v20 = *(void (**)(char *))(v17 + 16);
  uint64_t v35 = v21;
  v20(v19);
  uint64_t v22 = *(void (**)(char *, uint64_t, uint64_t))(v12 + 16);
  uint64_t v37 = a2;
  v22(v15, a2, a4);
  uint64_t v23 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  uint64_t v36 = a5;
  uint64_t v24 = v23(a3, a5);
  uint64_t v25 = (*(uint64_t (**)(uint64_t))(*(void *)(v40 + 8) + 16))(a4);
  (*(void (**)(char *, uint64_t))(v12 + 8))(v15, a4);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v17 + 8))(v19, a3);
  if (v24 == v25)
  {
    uint64_t v27 = v35;
    uint64_t v28 = v36;
    uint64_t v29 = v23(a3, v36);
    uint64_t v30 = MEMORY[0x1F4188790](v29);
    *(&v34 - 6) = a3;
    *(&v34 - 5) = a4;
    uint64_t v31 = v40;
    *(&v34 - 4) = v28;
    *(&v34 - 3) = v31;
    uint64_t v33 = v37;
    uint64_t v32 = v38;
    *(&v34 - 2) = v27;
    *(&v34 - 1) = v33;
    return v39(v30, v32);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(uint64_t a1, uint64_t a2, uint64_t *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(void))
{
  uint64_t result = *a3;
  if (!*a3)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!a4)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (a1) {
    return a7();
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vForce.trunc<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.trunc<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.trunc<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.trunc<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.trunc<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.trunc<A, B>(_:result:));
}

uint64_t static vForce.nearestInteger<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.nearestInteger<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.nearestInteger<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.nearestInteger<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.nearestInteger<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.nearestInteger<A, B>(_:result:));
}

uint64_t static vForce.rsqrt<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.rsqrt<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.rsqrt<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.rsqrt<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.rsqrt<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.rsqrt<A, B>(_:result:));
}

uint64_t static vForce.sqrt<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.sqrt<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.sqrt<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.sqrt<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.sqrt<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.sqrt<A, B>(_:result:));
}

uint64_t static vForce.reciprocal<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.reciprocal<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.reciprocal<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.reciprocal<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.reciprocal<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.reciprocal<A, B>(_:result:));
}

uint64_t static vForce.exp<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.exp<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.exp<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.exp<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.exp<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.exp<A, B>(_:result:));
}

uint64_t static vForce.expm1<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.expm1<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.expm1<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.expm1<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.expm1<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.expm1<A, B>(_:result:));
}

uint64_t static vForce.exp2<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.exp2<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.exp2<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.exp2<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.exp2<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.exp2<A, B>(_:result:));
}

uint64_t static vForce.log2<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.log2<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.log2<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.log2<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.log2<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.log2<A, B>(_:result:));
}

uint64_t static vForce.log10<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.log10<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.log10<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.log10<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.log10<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.log10<A, B>(_:result:));
}

uint64_t static vForce.logb<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.logb<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.logb<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.logb<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.logb<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.logb<A, B>(_:result:));
}

uint64_t static vForce.pow<A, B>(bases:exponents:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.pow<A, B>(bases:exponents:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.pow<A, B>(bases:exponents:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.pow<A, B>(bases:exponents:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.pow<A, B>(bases:exponents:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.pow<A, B, C>(bases:exponents:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

uint64_t static vForce.pow<A, B>(bases:exponents:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t))
{
  uint64_t v38 = a7;
  long long v39 = a8;
  uint64_t v13 = *(void *)(a4 - 8);
  uint64_t v14 = MEMORY[0x1F4188790](a1);
  uint64_t v16 = (char *)&v34 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = *(void *)(v17 - 8);
  MEMORY[0x1F4188790](v14);
  uint64_t v20 = (char *)&v34 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v21 = *(void (**)(char *))(v18 + 16);
  uint64_t v34 = v22;
  v21(v20);
  uint64_t v23 = *(void (**)(char *, uint64_t, uint64_t))(v13 + 16);
  uint64_t v36 = a2;
  v23(v16, a2, a4);
  uint64_t v24 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  uint64_t v35 = a5;
  uint64_t v40 = v24(a3, a5);
  uint64_t v37 = a6;
  uint64_t v25 = *(void *)(a6 + 8);
  uint64_t v26 = *(uint64_t (**)(uint64_t, uint64_t))(v25 + 16);
  uint64_t v27 = v26(a4, v25);
  (*(void (**)(char *, uint64_t))(v13 + 8))(v16, a4);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v18 + 8))(v20, a3);
  if (v40 == v27)
  {
    uint64_t v29 = v36;
    uint64_t v30 = v26(a4, v25);
    uint64_t v31 = MEMORY[0x1F4188790](v30);
    *(&v34 - 6) = a3;
    *(&v34 - 5) = a4;
    uint64_t v33 = v37;
    uint64_t v32 = v38;
    *(&v34 - 4) = v35;
    *(&v34 - 3) = v33;
    *(&v34 - 2) = v34;
    *(&v34 - 1) = v29;
    return v39(v31, v32);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in static vForce.pow<A, B>(bases:exponents:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, unint64_t *a10, void (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(a9);
  uint64_t v17 = *(void *)(a8 + 8);
  uint64_t v18 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a10, a9);
  a11(a3, a4, a1, a5, a6, v16, a7, v17, v18);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(v17 + 16))(a6, v17);
  *a2 = result;
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(uint64_t a1, uint64_t a2, uint64_t *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(void))
{
  uint64_t result = *a3;
  if (!*a3)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!a1)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (a4) {
    return a7();
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vForce.sin<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.sin<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.sin<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.sin<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.sin<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.sin<A, B>(_:result:));
}

uint64_t static vForce.sinPi<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.sinPi<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.sinPi<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.sinPi<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.sinPi<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.sinPi<A, B>(_:result:));
}

uint64_t static vForce.cos<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.cos<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.cos<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.cos<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.cos<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.cos<A, B>(_:result:));
}

uint64_t static vForce.cosPi<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.cosPi<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.cosPi<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.cosPi<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.cosPi<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.cosPi<A, B>(_:result:));
}

uint64_t static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v13;
  char *v14;
  uint64_t v15;
  void (*v16)(char *);
  uint64_t v17;
  uint64_t (*v18)(uint64_t, uint64_t);
  uint64_t v19;
  uint64_t v20;
  uint64_t (*v21)(void);
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v35;
  uint64_t v36;
  uint64_t (*v37)(uint64_t, uint64_t);
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  int v44;
  uint64_t v45;

  uint64_t v40 = a3;
  long long v41 = a6;
  long long v43 = a5;
  long long v45 = *MEMORY[0x1E4F143B8];
  MEMORY[0x1F4188790](a1);
  uint64_t v14 = (char *)&v35 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  int v42 = v15;
  uint64_t v16 = *(void (**)(char *))(v15 + 16);
  uint64_t v38 = v17;
  v16(v14);
  uint64_t v18 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  long long v39 = a7;
  uint64_t v37 = v18;
  uint64_t v19 = v18(a4, a7);
  uint64_t v36 = a8;
  uint64_t v20 = *(void *)(a8 + 8);
  uint64_t v21 = *(uint64_t (**)(void))(v20 + 16);
  uint64_t v22 = a2;
  uint64_t v23 = v43;
  if (v19 != v21())
  {
LABEL_9:
    (*(void (**)(char *, uint64_t))(v42 + 8))(v14, a4);
    __break(1u);
  }
  uint64_t v35 = v22;
  uint64_t v24 = ((uint64_t (*)(uint64_t, uint64_t))v21)(v23, v20);
  uint64_t v25 = v40;
  uint64_t v26 = v41;
  uint64_t v27 = (*(uint64_t (**)(uint64_t))(*(void *)(a9 + 8) + 16))(v41);
  (*(void (**)(char *, uint64_t))(v42 + 8))(v14, a4);
  if (v24 != v27)
  {
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v28 = v38;
  uint64_t v29 = v39;
  uint64_t v30 = v37(a4, v39);
  if (v30 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (v30 > 0x7FFFFFFF)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  long long v44 = v30;
  MEMORY[0x1F4188790](v30);
  uint64_t v31 = v43;
  *(&v35 - 10) = a4;
  *(&v35 - 9) = v31;
  *(&v35 - 8) = v26;
  *(&v35 - 7) = v29;
  uint64_t v32 = v36;
  *(&v35 - 6) = v36;
  *(&v35 - 5) = a9;
  *(&v35 - 4) = v25;
  *(&v35 - 3) = v28;
  *(&v35 - 2) = (uint64_t)&v44;
  return (*(uint64_t (**)(uint64_t))(v32 + 16))(v33);
}

uint64_t closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1, uint64_t a2, uint64_t *a3, void *a4, uint64_t a5, uint64_t (*a6)(void))
{
  uint64_t result = *a3;
  if (!*a3)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!*a4)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (a1) {
    return a6();
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vForce.tan<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.tan<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.tan<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.tan<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.tan<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.tan<A, B>(_:result:));
}

uint64_t static vForce.tanPi<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.tanPi<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.tanPi<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.tanPi<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.tanPi<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.tanPi<A, B>(_:result:));
}

uint64_t static vForce.asin<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.asin<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.asin<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.asin<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.asin<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.asin<A, B>(_:result:));
}

uint64_t static vForce.acos<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.acos<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.acos<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.acos<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.acos<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.acos<A, B>(_:result:));
}

uint64_t static vForce.atan<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.atan<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.atan<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.atan<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.atan<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.atan<A, B>(_:result:));
}

uint64_t static vForce.sinh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.sinh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.sinh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.sinh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.sinh<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.sinh<A, B>(_:result:));
}

uint64_t static vForce.cosh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.cosh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.cosh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.cosh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.cosh<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.cosh<A, B>(_:result:));
}

uint64_t static vForce.tanh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.tanh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.tanh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.tanh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.tanh<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.tanh<A, B>(_:result:));
}

uint64_t static vForce.asinh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.asinh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.asinh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.asinh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.asinh<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.asinh<A, B>(_:result:));
}

uint64_t static vForce.acosh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.acosh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.acosh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.acosh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.acosh<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.acosh<A, B>(_:result:));
}

uint64_t static vForce.atanh<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.atanh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.ceil<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.atanh<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.atanh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.atanh<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.ceil<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.atanh<A, B>(_:result:));
}

uint64_t static vForce.ceil<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(uint64_t, uint64_t, void *))
{
  uint64_t v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  _OWORD v12[2] = a2;
  v12[3] = a3;
  v12[4] = a1;
  return a5(v10, a4, v12);
}

uint64_t static vForce.ceil<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v25 = a7;
  uint64_t v27 = a4;
  uint64_t v29 = *MEMORY[0x1E4F143B8];
  uint64_t v11 = *(void *)(a3 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v13 = (char *)v24 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v14 = *(void (**)(char *))(v11 + 16);
  uint64_t v26 = v15;
  v14(v13);
  uint64_t v16 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  uint64_t v17 = v16(a3, a5);
  v24[0] = a6;
  v24[1] = a2;
  uint64_t v18 = (*(uint64_t (**)(uint64_t))(*(void *)(a6 + 8) + 16))(v27);
  (*(void (**)(char *, uint64_t))(v11 + 8))(v13, a3);
  if (v17 != v18)
  {
    __break(1u);
    goto LABEL_6;
  }
  uint64_t v19 = v26;
  uint64_t v20 = v16(a3, a5);
  if (v20 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  if (v20 > 0x7FFFFFFF) {
    goto LABEL_7;
  }
  int v28 = v20;
  MEMORY[0x1F4188790](v20);
  uint64_t v21 = v27;
  v24[-6] = a3;
  v24[-5] = v21;
  uint64_t v22 = v24[0];
  v24[-4] = a5;
  v24[-3] = v22;
  v24[-2] = v19;
  v24[-1] = &v28;
  return (*(uint64_t (**)(uint64_t))(v22 + 16))(v25);
}

uint64_t partial apply for closure #1 in static vForce.ceil<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.ceil<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.ceil<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.floor<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.floor<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.floor<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.floor<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.floor<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.floor<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vForce.copysign<A, B, C>(magnitudes:signs:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vForce.copysign<A, B>(magnitudes:signs:));
}

{
  return partial apply for closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vForce.copysign<A, B, C>(magnitudes:signs:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vForce.copysign<A, B>(magnitudes:signs:));
}

uint64_t partial apply for closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:));
}

uint64_t partial apply for closure #1 in static vForce.truncatingRemainder<A, B>(dividends:divisors:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vForce.copysign<A, B>(magnitudes:signs:));
}

{
  return partial apply for closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vForce.copysign<A, B>(magnitudes:signs:));
}

uint64_t partial apply for closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:));
}

uint64_t partial apply for closure #1 in static vForce.remainder<A, B>(dividends:divisors:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vForce.remainder<A, B, C>(dividends:divisors:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vForce.copysign<A, B>(magnitudes:signs:));
}

{
  return partial apply for closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vForce.remainder<A, B, C>(dividends:divisors:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vForce.copysign<A, B>(magnitudes:signs:));
}

uint64_t partial apply for closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:));
}

uint64_t partial apply for closure #1 in static vForce.trunc<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.trunc<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.trunc<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.trunc<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.trunc<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.trunc<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.nearestInteger<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.nearestInteger<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.nearestInteger<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.nearestInteger<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.nearestInteger<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.nearestInteger<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.rsqrt<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.rsqrt<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.rsqrt<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.rsqrt<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.rsqrt<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.rsqrt<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sqrt<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.sqrt<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.sqrt<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sqrt<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sqrt<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sqrt<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.reciprocal<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.reciprocal<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.reciprocal<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.reciprocal<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.reciprocal<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.reciprocal<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.exp<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.exp<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.exp<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.exp<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.exp<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.exp<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.expm1<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.expm1<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.expm1<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.expm1<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.expm1<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.expm1<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.exp2<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.exp2<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.exp2<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.exp2<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.exp2<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.exp2<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.log2<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.log2<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.log2<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.log2<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.log2<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.log2<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.log10<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.log10<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.log10<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.log10<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.log10<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.log10<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.logb<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.logb<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.logb<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.logb<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.logb<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.logb<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.pow<A, B>(bases:exponents:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vForce.pow<A, B, C>(bases:exponents:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vForce.pow<A, B>(bases:exponents:));
}

{
  return partial apply for closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vForce.pow<A, B, C>(bases:exponents:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vForce.pow<A, B>(bases:exponents:));
}

uint64_t partial apply for closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:));
}

uint64_t partial apply for closure #1 in static vForce.copysign<A, B>(magnitudes:signs:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))
{
  return a6(a1, a2, v6[6], v6[7], v6[2], v6[3], v6[4], v6[5], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vForce.sin<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.sin<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.sin<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sin<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sin<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sin<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sinPi<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.sinPi<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.sinPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sinPi<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sinPi<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sinPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cos<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.cos<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.cos<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cos<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.cos<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.cos<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cosPi<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.cosPi<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.cosPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cosPi<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.cosPi<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.cosPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1)
{
  return partial apply for closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:));
}

{
  return partial apply for closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:));
}

uint64_t partial apply for closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1, uint64_t a2)
{
  uint64_t v4 = *(void *)(v2 + 48);
  uint64_t v3 = *(void *)(v2 + 56);
  uint64_t v5 = *(void *)(v2 + 72);
  uint64_t v6 = *(void *)(v2 + 80);
  long long v7 = *(_OWORD *)(v2 + 32);
  v9[1] = *(_OWORD *)(v2 + 16);
  _OWORD v9[2] = v7;
  uint64_t v10 = v4;
  uint64_t v11 = v3;
  uint64_t v12 = v5;
  uint64_t v13 = a1;
  uint64_t v14 = v6;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, void))(v3 + 16))(a2, v9, MEMORY[0x1E4FBC848] + 8, v7);
}

uint64_t partial apply for closure #1 in static vForce.tan<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.tan<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.tan<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.tan<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.tan<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.tan<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.tanPi<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.tanPi<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.tanPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.tanPi<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.tanPi<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.tanPi<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.asin<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.asin<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.asin<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.asin<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.asin<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.asin<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.acos<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.acos<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.acos<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.acos<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.acos<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.acos<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.atan<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.atan<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.atan<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.atan<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.atan<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.atan<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sinh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.sinh<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.sinh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.sinh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sinh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.sinh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cosh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.cosh<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.cosh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.cosh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.cosh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.cosh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.tanh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.tanh<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.tanh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.tanh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.tanh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.tanh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.asinh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.asinh<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.asinh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.asinh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.asinh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.asinh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.acosh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.acosh<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.acosh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.acosh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.acosh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.acosh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.atanh<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.atanh<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.ceil<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.atanh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.atanh<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17208]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17210]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.acosh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F171B8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F171C0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.asinh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F171D8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F171E0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.tanh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F173E0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F173E8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.cosh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17248]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17250]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sinh<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F173A0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F173A8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.atan<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F171E8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17200]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.acos<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F171A8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F171B0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.asin<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F171C8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F171D0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.tanPi<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F173F0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F173F8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.tan<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F173D0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F173D8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, a2, *(uint64_t **)(v2 + 16), *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F17388]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(a1, a2, *(uint64_t **)(v2 + 16), *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F17390]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sincos<A, B, C>(_:sinResult:cosResult:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = v2[2];
  uint64_t v4 = v2[5];
  uint64_t v5 = v2[10];
  _OWORD v7[2] = v2[9];
  void v7[3] = a1;
  v7[4] = v5;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v4 + 24))(a2, v7, MEMORY[0x1E4FBC848] + 8, v3);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.cosPi<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17258]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17260]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.cos<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17238]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17240]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sinPi<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F173B0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F173B8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sin<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17380]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17398]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F17330]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F17338]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.logb<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17308]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17310]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.log10<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F172D8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F172E0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.log2<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F172F8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17300]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.exp2<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17270]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17278]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.expm1<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17288]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17290]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.exp<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17268]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17280]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.reciprocal<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17350]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17358]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.sqrt<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F173C0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F173C8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.rsqrt<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17370]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17378]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.nearestInteger<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17320]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17328]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.trunc<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F172C0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F172C8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.remainder<A, B, C>(dividends:divisors:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F17360]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F17368]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.truncatingRemainder<A, B, C>(dividends:divisors:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F172B0]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F172B8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F17228]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.copysign<A, B, C>(magnitudes:signs:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F17230]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = v3[3];
  uint64_t v5 = v3[6];
  _OWORD v7[2] = v3[9];
  void v7[3] = a1;
  v7[4] = a2;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v5 + 24))(a3, v7, MEMORY[0x1E4FBC848] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.floor<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F172A0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F172A8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.ceil<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17218]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17220]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.atanh<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(void))
{
  uint64_t result = **(void **)(v3 + 16);
  if (result)
  {
    if (a1) {
      return a3();
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t static BNNS.SparsityType.== infix(_:_:)()
{
  return 1;
}

void BNNS.SparsityType.hash(into:)()
{
}

Swift::Int BNNS.SparsityType.hashValue.getter()
{
  return Hasher._finalize()();
}

void BNNS.SparseParameters.init(type:ratio:targetSystem:)(int a1@<W1>, int a2@<W2>, int a3@<W3>, _DWORD *a4@<X8>)
{
  *a4 = a1;
  a4[1] = a2;
  a4[2] = a3;
}

uint64_t BNNS.SparseParameters.ratio.getter()
{
  return *(void *)v0;
}

uint64_t BNNS.SparseParameters.targetSystem.getter()
{
  return *(unsigned int *)(v0 + 8);
}

uint64_t static BNNS.FullyConnectedLayer.sparsify(batchSize:inputLayout:inputDenseShape:inputValues:output:sparseParameters:workspace:filterParameters:)(size_t a1, void *__src, long long *a3, _OWORD *a4, BNNSNDArrayDescriptor *a5, uint64_t *a6, void *a7, uint64_t a8, char a9, uint64_t a10, size_t a11, int (__cdecl *a12)(void **, size_t, size_t), void (__cdecl *a13)(void *))
{
  outlined init with take of BNNS.SparseLayout(__src, __dst);
  uint64_t v16 = *a6;
  int v17 = *((_DWORD *)a6 + 2);
  char v18 = *((unsigned char *)a6 + 12);
  if (a9 & 1 | (a7 == 0)) {
    size_t v19 = 0;
  }
  else {
    size_t v19 = a8 - (void)a7;
  }
  outlined init with take of BNNS.SparseLayout(__dst, v51);
  int v20 = _s10Accelerate4BNNSO12SparseLayoutOWOg((uint64_t)v51);
  uint64_t v21 = (long long *)_s10Accelerate4BNNSO12SparseLayoutOWOj0_((uint64_t)v51);
  if (v20 == 1)
  {
    long long v22 = v21[9];
    unsigned char v53[8] = v21[8];
    v53[9] = v22;
    long long v23 = v21[11];
    v53[10] = v21[10];
    long long v24 = v21[5];
    v53[4] = v21[4];
    v53[5] = v24;
    long long v25 = v21[7];
    v53[6] = v21[6];
    v53[7] = v25;
    long long v26 = v21[1];
    v53[0] = *v21;
    v53[1] = v26;
    long long v27 = v21[3];
    v53[2] = v21[2];
    v53[3] = v27;
    long long v28 = v21[18];
    long long v29 = v21[20];
    long long v30 = v21[21];
    unsigned char v54[8] = v21[19];
    v54[9] = v29;
    v54[10] = v30;
    long long v31 = v21[14];
    long long v32 = v21[16];
    long long v33 = v21[17];
    v54[4] = v21[15];
    v54[5] = v32;
    v54[6] = v33;
    v54[7] = v28;
    long long v34 = v21[12];
    long long v35 = v21[13];
    v54[0] = v23;
    v54[1] = v34;
    v54[2] = v35;
    v54[3] = v31;
    uint64_t v48 = v16;
    int v49 = v17;
    if (a9) {
      uint64_t v36 = 0;
    }
    else {
      uint64_t v36 = a7;
    }
    char v50 = v18;
    return specialized static BNNS.FullyConnectedLayer.convertCSRtoOpaque(batchSize:inputDenseShape:inputColumnIndices:inputRowStarts:inputValues:output:sparseParameters:workspace:workspaceSize:filterParameters:)(a1, a3, v53, v54, a4, a5, (uint64_t)&v48, v36, v19, a10, a11, a12, a13);
  }
  else
  {
    long long v38 = v21[9];
    v52[8] = v21[8];
    v52[9] = v38;
    v52[10] = v21[10];
    long long v39 = v21[5];
    v52[4] = v21[4];
    v52[5] = v39;
    long long v40 = v21[7];
    v52[6] = v21[6];
    v52[7] = v40;
    long long v41 = v21[1];
    v52[0] = *v21;
    v52[1] = v41;
    long long v42 = v21[3];
    v52[2] = v21[2];
    v52[3] = v42;
    uint64_t v48 = 0;
    int v49 = 0;
    if (a9) {
      long long v43 = 0;
    }
    else {
      long long v43 = a7;
    }
    char v50 = 1;
    return specialized static BNNS.FullyConnectedLayer.convertCOOtoOpaque(batchSize:inputDenseShape:inputIndices:inputValues:output:sparseParameters:workspace:workspaceSize:filterParameters:)(a1, a3, v52, a4, a5, (uint64_t)&v48, v43, v19, a10, a11, a12, a13);
  }
}

void *outlined init with take of BNNS.SparseLayout(void *__src, void *__dst)
{
  return memcpy(__dst, __src, 0x160uLL);
}

uint64_t _s10Accelerate4BNNSO12SparseLayoutOWOg(uint64_t a1)
{
  return *(void *)(a1 + 344) >> 63;
}

uint64_t _s10Accelerate4BNNSO12SparseLayoutOWOj0_(uint64_t result)
{
  *(void *)(result + 344) &= ~0x8000000000000000;
  return result;
}

uint64_t specialized static BNNS.FullyConnectedLayer.convertCOOtoOpaque(batchSize:inputDenseShape:inputIndices:inputValues:output:sparseParameters:workspace:workspaceSize:filterParameters:)(size_t batch_size, long long *a2, _OWORD *a3, _OWORD *a4, BNNSNDArrayDescriptor *out, uint64_t a6, void *a7, size_t a8, uint64_t a9, size_t a10, int (__cdecl *a11)(void **, size_t, size_t), void (__cdecl *a12)(void *))
{
  uint64_t v46 = *MEMORY[0x1E4F143B8];
  uint32_t v13 = *(_DWORD *)a6;
  uint32_t v12 = *(_DWORD *)(a6 + 4);
  BNNSTargetSystem v14 = *(_DWORD *)(a6 + 8);
  BOOL v15 = *(unsigned char *)(a6 + 12) == 0;
  if (*(unsigned char *)(a6 + 12)) {
    uint32_t v13 = 0;
  }
  sparse_params.flags = 0;
  if (!v15)
  {
    uint32_t v12 = 0;
    BNNSTargetSystem v14 = BNNSTargetSystemGeneric;
  }
  sparse_params.sparsity_ratio[0] = v13;
  sparse_params.sparsity_ratio[1] = v12;
  sparse_params.sparsity_type = BNNSSparsityTypeUnstructured;
  sparse_params.target_system = v14;
  long long v16 = a2[8];
  long long v17 = a2[9];
  long long v18 = a2[6];
  *(_OWORD *)&in_dense_shape.stride[5] = a2[7];
  *(_OWORD *)&in_dense_shape.stride[7] = v16;
  long long v19 = a2[10];
  *(_OWORD *)&in_dense_shape.BNNSDataType data_type = v17;
  *(_OWORD *)&in_dense_shape.table_BNNSDataType data_type = v19;
  long long v20 = a2[4];
  long long v21 = a2[5];
  long long v22 = a2[2];
  *(_OWORD *)&in_dense_shape.size[5] = a2[3];
  *(_OWORD *)&in_dense_shape.size[7] = v20;
  *(_OWORD *)&in_dense_shape.stride[1] = v21;
  *(_OWORD *)&in_dense_shape.stride[3] = v18;
  long long v23 = *a2;
  *(_OWORD *)&in_dense_shape.size[1] = a2[1];
  *(_OWORD *)&in_dense_shape.size[3] = v22;
  long long v24 = a3[9];
  *(_OWORD *)&in_indices.stride[7] = a3[8];
  *(_OWORD *)&in_indices.BNNSDataType data_type = v24;
  *(_OWORD *)&in_indices.table_BNNSDataType data_type = a3[10];
  *(_OWORD *)&in_dense_shape.flags = v23;
  long long v25 = a3[5];
  *(_OWORD *)&in_indices.size[7] = a3[4];
  *(_OWORD *)&in_indices.stride[1] = v25;
  long long v26 = a3[7];
  *(_OWORD *)&in_indices.stride[3] = a3[6];
  *(_OWORD *)&in_indices.stride[5] = v26;
  long long v27 = a3[1];
  *(_OWORD *)&in_indices.flags = *a3;
  *(_OWORD *)&in_indices.size[1] = v27;
  long long v28 = a3[3];
  *(_OWORD *)&in_indices.size[3] = a3[2];
  *(_OWORD *)&in_indices.size[5] = v28;
  long long v29 = a4[8];
  long long v30 = a4[9];
  long long v31 = a4[6];
  *(_OWORD *)&in_values.stride[5] = a4[7];
  *(_OWORD *)&in_values.stride[7] = v29;
  long long v32 = a4[10];
  *(_OWORD *)&in_values.BNNSDataType data_type = v30;
  *(_OWORD *)&in_values.table_BNNSDataType data_type = v32;
  if (a11 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    long long v33 = a4[4];
    *(_OWORD *)&in_values.stride[1] = a4[5];
    *(_OWORD *)&in_values.stride[3] = v31;
    long long v34 = a4[1];
    *(_OWORD *)&in_values.flags = *a4;
    *(_OWORD *)&in_values.size[1] = v34;
    long long v35 = a4[2];
    *(_OWORD *)&in_values.size[5] = a4[3];
    *(_OWORD *)&in_values.size[7] = v33;
    *(_OWORD *)&in_values.size[3] = v35;
    uint64_t result = BNNSNDArrayFullyConnectedSparsifySparseCOO(&in_dense_shape, &in_indices, &in_values, out, &sparse_params, batch_size, a7, a8, 0);
  }
  else
  {
    long long v37 = a4[5];
    *(_OWORD *)&in_values.size[7] = a4[4];
    filter_params.flags = a9;
    filter_params.n_threads = a10;
    filter_params.alloc_memory = a11;
    filter_params.free_memory = a12;
    *(_OWORD *)&in_values.stride[1] = v37;
    *(_OWORD *)&in_values.stride[3] = v31;
    long long v38 = a4[1];
    *(_OWORD *)&in_values.flags = *a4;
    *(_OWORD *)&in_values.size[1] = v38;
    long long v39 = a4[3];
    *(_OWORD *)&in_values.size[3] = a4[2];
    *(_OWORD *)&in_values.size[5] = v39;
    uint64_t result = BNNSNDArrayFullyConnectedSparsifySparseCOO(&in_dense_shape, &in_indices, &in_values, out, &sparse_params, batch_size, a7, a8, &filter_params);
  }
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v40 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t specialized static BNNS.FullyConnectedLayer.convertCSRtoOpaque(batchSize:inputDenseShape:inputColumnIndices:inputRowStarts:inputValues:output:sparseParameters:workspace:workspaceSize:filterParameters:)(size_t batch_size, _OWORD *a2, long long *a3, _OWORD *a4, _OWORD *a5, BNNSNDArrayDescriptor *out, uint64_t a7, void *a8, size_t workspace_size, uint64_t a10, size_t a11, int (__cdecl *a12)(void **, size_t, size_t), void (__cdecl *a13)(void *))
{
  uint64_t v52 = *MEMORY[0x1E4F143B8];
  uint32_t v14 = *(_DWORD *)a7;
  uint32_t v13 = *(_DWORD *)(a7 + 4);
  BNNSTargetSystem v15 = *(_DWORD *)(a7 + 8);
  if (*(unsigned char *)(a7 + 12))
  {
    uint32_t v14 = 0;
    uint32_t v13 = 0;
    BNNSTargetSystem v15 = BNNSTargetSystemGeneric;
  }
  sparse_params.flags = 0;
  sparse_params.sparsity_ratio[0] = v14;
  sparse_params.sparsity_ratio[1] = v13;
  sparse_params.sparsity_type = BNNSSparsityTypeUnstructured;
  sparse_params.target_system = v15;
  long long v16 = a2[9];
  *(_OWORD *)&in_dense_shape.stride[7] = a2[8];
  *(_OWORD *)&in_dense_shape.BNNSDataType data_type = v16;
  *(_OWORD *)&in_dense_shape.table_BNNSDataType data_type = a2[10];
  long long v17 = a2[5];
  *(_OWORD *)&in_dense_shape.size[7] = a2[4];
  *(_OWORD *)&in_dense_shape.stride[1] = v17;
  long long v18 = a2[7];
  *(_OWORD *)&in_dense_shape.stride[3] = a2[6];
  *(_OWORD *)&in_dense_shape.stride[5] = v18;
  long long v19 = a2[1];
  *(_OWORD *)&in_dense_shape.flags = *a2;
  *(_OWORD *)&in_dense_shape.size[1] = v19;
  long long v20 = a2[3];
  *(_OWORD *)&in_dense_shape.size[3] = a2[2];
  *(_OWORD *)&in_dense_shape.size[5] = v20;
  long long v21 = a3[8];
  long long v22 = a3[9];
  long long v23 = a3[6];
  *(_OWORD *)&in_column_indices.stride[5] = a3[7];
  *(_OWORD *)&in_column_indices.stride[7] = v21;
  long long v24 = a3[10];
  *(_OWORD *)&in_column_indices.BNNSDataType data_type = v22;
  *(_OWORD *)&in_column_indices.table_BNNSDataType data_type = v24;
  long long v25 = a3[4];
  long long v26 = a3[5];
  long long v27 = a3[2];
  *(_OWORD *)&in_column_indices.size[5] = a3[3];
  *(_OWORD *)&in_column_indices.size[7] = v25;
  *(_OWORD *)&in_column_indices.stride[1] = v26;
  *(_OWORD *)&in_column_indices.stride[3] = v23;
  long long v28 = *a3;
  *(_OWORD *)&in_column_indices.size[1] = a3[1];
  *(_OWORD *)&in_column_indices.size[3] = v27;
  long long v29 = a4[9];
  *(_OWORD *)&in_row_starts.stride[7] = a4[8];
  *(_OWORD *)&in_row_starts.BNNSDataType data_type = v29;
  *(_OWORD *)&in_row_starts.table_BNNSDataType data_type = a4[10];
  *(_OWORD *)&in_column_indices.flags = v28;
  long long v30 = a4[5];
  *(_OWORD *)&in_row_starts.size[7] = a4[4];
  *(_OWORD *)&in_row_starts.stride[1] = v30;
  long long v31 = a4[7];
  *(_OWORD *)&in_row_starts.stride[3] = a4[6];
  *(_OWORD *)&in_row_starts.stride[5] = v31;
  long long v32 = a4[1];
  *(_OWORD *)&in_row_starts.flags = *a4;
  *(_OWORD *)&in_row_starts.size[1] = v32;
  long long v33 = a4[3];
  *(_OWORD *)&in_row_starts.size[3] = a4[2];
  *(_OWORD *)&in_row_starts.size[5] = v33;
  long long v34 = a5[8];
  long long v35 = a5[9];
  long long v36 = a5[6];
  *(_OWORD *)&in_values.stride[5] = a5[7];
  *(_OWORD *)&in_values.stride[7] = v34;
  long long v37 = a5[10];
  *(_OWORD *)&in_values.BNNSDataType data_type = v35;
  *(_OWORD *)&in_values.table_BNNSDataType data_type = v37;
  if (a12 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    long long v38 = a5[4];
    *(_OWORD *)&in_values.stride[1] = a5[5];
    *(_OWORD *)&in_values.stride[3] = v36;
    long long v39 = a5[1];
    *(_OWORD *)&in_values.flags = *a5;
    *(_OWORD *)&in_values.size[1] = v39;
    long long v40 = a5[2];
    *(_OWORD *)&in_values.size[5] = a5[3];
    *(_OWORD *)&in_values.size[7] = v38;
    *(_OWORD *)&in_values.size[3] = v40;
    uint64_t result = BNNSNDArrayFullyConnectedSparsifySparseCSR(&in_dense_shape, &in_column_indices, &in_row_starts, &in_values, out, &sparse_params, batch_size, a8, workspace_size, 0);
  }
  else
  {
    long long v42 = a5[5];
    *(_OWORD *)&in_values.size[7] = a5[4];
    filter_params.flags = a10;
    filter_params.n_threads = a11;
    filter_params.alloc_memory = a12;
    filter_params.free_memory = a13;
    *(_OWORD *)&in_values.stride[1] = v42;
    *(_OWORD *)&in_values.stride[3] = v36;
    long long v43 = a5[1];
    *(_OWORD *)&in_values.flags = *a5;
    *(_OWORD *)&in_values.size[1] = v43;
    long long v44 = a5[3];
    *(_OWORD *)&in_values.size[3] = a5[2];
    *(_OWORD *)&in_values.size[5] = v44;
    uint64_t result = BNNSNDArrayFullyConnectedSparsifySparseCSR(&in_dense_shape, &in_column_indices, &in_row_starts, &in_values, out, &sparse_params, batch_size, a8, workspace_size, &filter_params);
  }
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v45 = 0;
    return swift_willThrow();
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type BNNS.SparsityType and conformance BNNS.SparsityType()
{
  unint64_t result = lazy protocol witness table cache variable for type BNNS.SparsityType and conformance BNNS.SparsityType;
  if (!lazy protocol witness table cache variable for type BNNS.SparsityType and conformance BNNS.SparsityType)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.SparsityType and conformance BNNS.SparsityType);
  }
  return result;
}

unsigned char *storeEnumTagSinglePayload for BNNS.SparsityType(unsigned char *result, int a2, int a3)
{
  if ((a3 + 1) >= 0x10000) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) < 0x100) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2)
  {
    switch(v5)
    {
      case 1:
        *unint64_t result = a2;
        return result;
      case 2:
        *(_WORD *)unint64_t result = a2;
        return result;
      case 3:
        goto LABEL_19;
      case 4:
        *(_DWORD *)unint64_t result = a2;
        return result;
      default:
        return result;
    }
  }
  switch(v5)
  {
    case 1:
      *unint64_t result = 0;
      break;
    case 2:
      *(_WORD *)unint64_t result = 0;
      break;
    case 3:
LABEL_19:
      __break(1u);
      JUMPOUT(0x1D211308CLL);
    case 4:
      *(_DWORD *)unint64_t result = 0;
      break;
    default:
      return result;
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.SparsityType()
{
  return &type metadata for BNNS.SparsityType;
}

uint64_t __swift_memcpy12_4(uint64_t result, uint64_t *a2)
{
  uint64_t v2 = *a2;
  *(_DWORD *)(result + 8) = *((_DWORD *)a2 + 2);
  *(void *)unint64_t result = v2;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.SparseParameters(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 12)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for BNNS.SparseParameters(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(_DWORD *)(result + 8) = 0;
    *(void *)unint64_t result = (a2 - 1);
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 12) = v3;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.SparseParameters()
{
  return &type metadata for BNNS.SparseParameters;
}

void *__swift_memcpy352_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x160uLL);
}

uint64_t getEnumTagSinglePayload for BNNS.SparseLayout(uint64_t a1, int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0 && *(unsigned char *)(a1 + 352)) {
    return *(_DWORD *)a1 + 0x80000000;
  }
  unsigned int v2 = *(_DWORD *)(a1 + 148);
  if (v2 > 0x80000000) {
    int v3 = ~v2;
  }
  else {
    int v3 = -1;
  }
  return (v3 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.SparseLayout(uint64_t result, int a2, int a3)
{
  if (a2 < 0)
  {
    *(void *)(result + 344) = 0;
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(void *)unint64_t result = a2 ^ 0x80000000;
    if (a3 < 0) {
      *(unsigned char *)(result + 352) = 1;
    }
  }
  else
  {
    if ((a3 & 0x80000000) == 0)
    {
      if (!a2) {
        return result;
      }
LABEL_8:
      *(_OWORD *)(result + 112) = 0u;
      *(_OWORD *)(result + 128) = 0u;
      *(_OWORD *)(result + 80) = 0u;
      *(_OWORD *)(result + 96) = 0u;
      *(_OWORD *)(result + 48) = 0u;
      *(_OWORD *)(result + 64) = 0u;
      *(_OWORD *)(result + 16) = 0u;
      *(_OWORD *)(result + 32) = 0u;
      *(_OWORD *)unint64_t result = 0u;
      *(void *)(result + 144) = (unint64_t)-a2 << 32;
      *(_OWORD *)(result + 168) = 0u;
      *(_OWORD *)(result + 184) = 0u;
      *(_OWORD *)(result + 200) = 0u;
      *(_OWORD *)(result + 216) = 0u;
      *(_OWORD *)(result + 232) = 0u;
      *(_OWORD *)(result + 248) = 0u;
      *(void *)(result + 344) = 0;
      *(_OWORD *)(result + 152) = 0u;
      result += 152;
      *(_OWORD *)(result + 112) = 0u;
      *(_OWORD *)(result + 128) = 0u;
      *(_OWORD *)(result + 144) = 0u;
      *(_OWORD *)(result + 160) = 0u;
      *(_OWORD *)(result + 176) = 0u;
      return result;
    }
    *(unsigned char *)(result + 352) = 0;
    if (a2) {
      goto LABEL_8;
    }
  }
  return result;
}

uint64_t destructiveInjectEnumTag for BNNS.SparseLayout(uint64_t result, int a2)
{
  uint64_t v2 = *(unsigned int *)(result + 168);
  uint64_t v3 = *(unsigned int *)(result + 320);
  *(void *)(result + 144) = *(unsigned int *)(result + 144);
  *(void *)(result + 168) = v2;
  *(void *)(result + 320) = v3;
  *(_DWORD *)(result + 348) = a2 << 31;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.SparseLayout()
{
  return &type metadata for BNNS.SparseLayout;
}

double BNNS.FusedUnaryArithmeticParameters.layerParameters(input:output:)@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, uint64_t *a3@<X8>)
{
  *(_OWORD *)&v18[100] = a2[6];
  *(_OWORD *)&v18[116] = a2[7];
  *(_OWORD *)&v18[132] = a2[8];
  *(_OWORD *)&v18[148] = a2[9];
  *(_OWORD *)&v18[164] = a2[10];
  *(_OWORD *)&v18[52] = a2[3];
  *(_OWORD *)&v18[68] = a2[4];
  *(_OWORD *)&v18[84] = a2[5];
  *(_OWORD *)&v18[4] = *a2;
  *(_OWORD *)&v18[20] = a2[1];
  int v6 = *(unsigned __int8 *)(v3 + 8);
  int v7 = *(unsigned __int8 *)(v3 + 9);
  *(_OWORD *)&v18[36] = a2[2];
  uint64_t v8 = swift_slowAlloc();
  long long v9 = a1[9];
  *(_OWORD *)(v8 + 128) = a1[8];
  *(_OWORD *)(v8 + 144) = v9;
  *(_OWORD *)(v8 + 160) = a1[10];
  long long v10 = a1[5];
  *(_OWORD *)(v8 + 64) = a1[4];
  *(_OWORD *)(v8 + 80) = v10;
  long long v11 = a1[7];
  *(_OWORD *)(v8 + 96) = a1[6];
  *(_OWORD *)(v8 + 112) = v11;
  long long v12 = a1[1];
  *(_OWORD *)uint64_t v8 = *a1;
  *(_OWORD *)(v8 + 16) = v12;
  long long v13 = a1[3];
  *(_OWORD *)(v8 + 32) = a1[2];
  *(_OWORD *)(v8 + 48) = v13;
  *(_OWORD *)(v8 + 324) = *(_OWORD *)&v18[144];
  *(_OWORD *)(v8 + 340) = *(_OWORD *)&v18[160];
  *(_OWORD *)(v8 + 244) = *(_OWORD *)&v18[64];
  *(_OWORD *)(v8 + 260) = *(_OWORD *)&v18[80];
  *(_OWORD *)(v8 + 276) = *(_OWORD *)&v18[96];
  *(_OWORD *)(v8 + 292) = *(_OWORD *)&v18[112];
  *(_OWORD *)(v8 + 308) = *(_OWORD *)&v18[128];
  *(_OWORD *)(v8 + 180) = *(_OWORD *)v18;
  *(_OWORD *)(v8 + 196) = *(_OWORD *)&v18[16];
  *(_OWORD *)(v8 + 212) = *(_OWORD *)&v18[32];
  *(void *)uint64_t v3 = v8;
  *(_DWORD *)(v8 + 176) = v6;
  *(_DWORD *)(v8 + 356) = *(_DWORD *)&v18[176];
  *(_OWORD *)(v8 + 228) = *(_OWORD *)&v18[48];
  *(_DWORD *)(v8 + 360) = v7;
  int v14 = BNNS.ArithmeticUnaryFunction.bnnsArithmeticFunction.getter();
  type metadata accessor for BNNSLayerParametersArithmetic(0);
  a3[3] = v15;
  a3[4] = (uint64_t)&protocol witness table for BNNSLayerParametersArithmetic;
  uint64_t v16 = swift_allocObject();
  *a3 = v16;
  *(_DWORD *)(v16 + 16) = v14;
  *(void *)(v16 + 24) = v8;
  *(_DWORD *)(v16 + 32) = 0;
  *(int32x2_t *)(v16 + 36) = vdup_n_s32(0x7FC00000u);
  *(_DWORD *)(v16 + 44) = 1;
  double result = 0.0;
  *(_OWORD *)(v16 + 48) = 0u;
  *(_OWORD *)(v16 + 64) = 0u;
  return result;
}

void BNNS.FusedUnaryArithmeticParameters.inputDescriptorType.getter(unsigned char *a1@<X8>)
{
  *a1 = *(unsigned char *)(v1 + 8);
}

unsigned char *BNNS.FusedUnaryArithmeticParameters.inputDescriptorType.setter(unsigned char *result)
{
  *(unsigned char *)(v1 + 8) = *result;
  return result;
}

uint64_t (*BNNS.FusedUnaryArithmeticParameters.inputDescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedUnaryArithmeticParameters.outputDescriptorType.getter(unsigned char *a1@<X8>)
{
  *a1 = *(unsigned char *)(v1 + 9);
}

unsigned char *BNNS.FusedUnaryArithmeticParameters.outputDescriptorType.setter(unsigned char *result)
{
  *(unsigned char *)(v1 + 9) = *result;
  return result;
}

uint64_t (*BNNS.FusedUnaryArithmeticParameters.outputDescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedUnaryArithmeticParameters.function.getter(unsigned char *a1@<X8>)
{
  *a1 = *(unsigned char *)(v1 + 10);
}

unsigned char *BNNS.FusedUnaryArithmeticParameters.function.setter(unsigned char *result)
{
  *(unsigned char *)(v1 + 10) = *result;
  return result;
}

uint64_t (*BNNS.FusedUnaryArithmeticParameters.function.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

char *BNNS.FusedUnaryArithmeticParameters.init(inputDescriptorType:outputDescriptorType:function:)@<X0>(char *result@<X0>, char *a2@<X1>, char *a3@<X2>, uint64_t a4@<X8>)
{
  char v4 = *result;
  char v5 = *a2;
  char v6 = *a3;
  *(void *)a4 = 0;
  *(unsigned char *)(a4 + 8) = v4;
  *(unsigned char *)(a4 + 9) = v5;
  *(unsigned char *)(a4 + 10) = v6;
  return result;
}

uint64_t protocol witness for FusableLayerParametersWrapper.filterType.getter in conformance BNNS.FusedUnaryArithmeticParameters()
{
  return 8;
}

uint64_t __swift_memcpy11_8(uint64_t result, uint64_t *a2)
{
  uint64_t v2 = *a2;
  *(_DWORD *)(result + 7) = *(_DWORD *)((char *)a2 + 7);
  *(void *)double result = v2;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.FusedUnaryArithmeticParameters(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFE && *(unsigned char *)(a1 + 11)) {
    return (*(_DWORD *)a1 + 254);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 8);
  BOOL v4 = v3 >= 3;
  int v5 = v3 - 3;
  if (!v4) {
    int v5 = -1;
  }
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.FusedUnaryArithmeticParameters(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFD)
  {
    *(unsigned char *)(result + 10) = 0;
    *(_WORD *)(result + 8) = 0;
    *(void *)double result = a2 - 254;
    if (a3 >= 0xFE) {
      *(unsigned char *)(result + 11) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFE) {
      *(unsigned char *)(result + 11) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 8) = a2 + 2;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.FusedUnaryArithmeticParameters()
{
  return &type metadata for BNNS.FusedUnaryArithmeticParameters;
}

uint64_t sub_1D21135B4()
{
  return MEMORY[0x1F4186498](v0, 80, 7);
}

uint64_t static vImage.Planar8.channelCount.getter()
{
  return 1;
}

uint64_t static vImage.Interleaved8x2.channelCount.getter()
{
  return 2;
}

uint64_t static vImage.Interleaved8x3.channelCount.getter()
{
  return 3;
}

uint64_t static vImage.Interleaved8x4.channelCount.getter()
{
  return 4;
}

uint64_t static vImage.PlanarF.channelCount.getter()
{
  return 1;
}

uint64_t static vImage.InterleavedFx2.channelCount.getter()
{
  return 2;
}

uint64_t static vImage.InterleavedFx3.channelCount.getter()
{
  return 3;
}

uint64_t static vImage.InterleavedFx4.channelCount.getter()
{
  return 4;
}

uint64_t _ss15withUnsafeBytes2of_q0_x_q0_SWq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v11 = *(void *)(a5 - 8);
  uint64_t v12 = MEMORY[0x1F4188790](a1);
  int v14 = (char *)&v18 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t result = v16(v12, v12 + *(void *)(*(void *)(v15 - 8) + 64), v14);
  if (v8) {
    return (*(uint64_t (**)(uint64_t, char *, uint64_t))(v11 + 32))(a8, v14, a5);
  }
  return result;
}

uint64_t static vImage.Planar8x2.bitCountPerPlanarPixel.getter()
{
  return 8;
}

uint64_t static vImage.Planar8x2.planeCount.getter()
{
  return 2;
}

uint64_t protocol witness for static MultiplePlanePixelFormat.planeCount.getter in conformance vImage.Planar8x2()
{
  return 2;
}

uint64_t static vImage.Planar8x3.bitCountPerPlanarPixel.getter()
{
  return 8;
}

uint64_t static vImage.Planar8x3.planeCount.getter()
{
  return 3;
}

uint64_t protocol witness for static MultiplePlanePixelFormat.planeCount.getter in conformance vImage.Planar8x3()
{
  return 3;
}

uint64_t static vImage.Planar8x4.bitCountPerPlanarPixel.getter()
{
  return 8;
}

uint64_t static vImage.Planar8x4.planeCount.getter()
{
  return 4;
}

uint64_t protocol witness for static MultiplePlanePixelFormat.planeCount.getter in conformance vImage.Planar8x4()
{
  return 4;
}

uint64_t static vImage.PlanarFx2.bitCountPerPlanarPixel.getter()
{
  return 32;
}

uint64_t static vImage.PlanarFx2.planeCount.getter()
{
  return 2;
}

uint64_t protocol witness for static MultiplePlanePixelFormat.bitCountPerPlanarPixel.getter in conformance vImage.PlanarFx2()
{
  return 32;
}

uint64_t static vImage.PlanarFx3.bitCountPerPlanarPixel.getter()
{
  return 32;
}

uint64_t static vImage.PlanarFx3.planeCount.getter()
{
  return 3;
}

uint64_t static vImage.PlanarFx4.bitCountPerPlanarPixel.getter()
{
  return 32;
}

uint64_t static vImage.PlanarFx4.planeCount.getter()
{
  return 4;
}

uint64_t static vImage.Planar8.bitCountPerComponent.getter()
{
  return 8;
}

uint64_t static vImage.Planar8.bitCountPerPixel.getter()
{
  return 8;
}

uint64_t static vImage.Interleaved8x2.bitCountPerPixel.getter()
{
  return 16;
}

uint64_t protocol witness for static StaticPixelFormat.bitCountPerPixel.getter in conformance vImage.Interleaved8x2()
{
  return 16;
}

uint64_t static vImage.Interleaved8x3.bitCountPerComponent.getter()
{
  return 8;
}

uint64_t static vImage.Interleaved8x3.bitCountPerPixel.getter()
{
  return 24;
}

uint64_t protocol witness for static StaticPixelFormat.bitCountPerPixel.getter in conformance vImage.Interleaved8x3()
{
  return 24;
}

uint64_t static vImage.Interleaved8x4.bitCountPerComponent.getter()
{
  return 8;
}

uint64_t static vImage.Interleaved8x4.bitCountPerPixel.getter()
{
  return 32;
}

uint64_t static vImage.Planar16F.channelCount.getter()
{
  return 1;
}

uint64_t static vImage.Planar16F.bitCountPerComponent.getter()
{
  return 16;
}

uint64_t static vImage.Planar16F.bitCountPerPixel.getter()
{
  return 16;
}

uint64_t static vImage.Interleaved16Fx2.channelCount.getter()
{
  return 2;
}

uint64_t static vImage.Interleaved16Fx2.bitCountPerPixel.getter()
{
  return 32;
}

uint64_t static vImage.Interleaved16Fx4.channelCount.getter()
{
  return 4;
}

uint64_t static vImage.Interleaved16Fx4.bitCountPerComponent.getter()
{
  return 16;
}

uint64_t static vImage.Interleaved16Fx4.bitCountPerPixel.getter()
{
  return 64;
}

uint64_t protocol witness for static StaticPixelFormat.bitCountPerPixel.getter in conformance vImage.Interleaved16Fx4()
{
  return 64;
}

uint64_t static vImage.PlanarF.bitCountPerComponent.getter()
{
  return 32;
}

uint64_t static vImage.PlanarF.bitCountPerPixel.getter()
{
  return 32;
}

uint64_t static vImage.InterleavedFx2.bitCountPerPixel.getter()
{
  return 64;
}

uint64_t static vImage.InterleavedFx3.bitCountPerComponent.getter()
{
  return 32;
}

uint64_t static vImage.InterleavedFx3.bitCountPerPixel.getter()
{
  return 96;
}

uint64_t protocol witness for static StaticPixelFormat.bitCountPerPixel.getter in conformance vImage.InterleavedFx3()
{
  return 96;
}

uint64_t static vImage.InterleavedFx4.bitCountPerComponent.getter()
{
  return 32;
}

uint64_t static vImage.InterleavedFx4.bitCountPerPixel.getter()
{
  return 128;
}

uint64_t protocol witness for static StaticPixelFormat.bitCountPerPixel.getter in conformance vImage.InterleavedFx4()
{
  return 128;
}

uint64_t static vImage.Planar16U.channelCount.getter()
{
  return 1;
}

uint64_t static vImage.Planar16U.bitCountPerPixel.getter()
{
  return 16;
}

uint64_t static vImage.Interleaved16Ux2.channelCount.getter()
{
  return 2;
}

uint64_t static vImage.Interleaved16Ux2.bitCountPerComponent.getter()
{
  return 16;
}

uint64_t static vImage.Interleaved16Ux2.bitCountPerPixel.getter()
{
  return 32;
}

uint64_t static vImage.Interleaved16Ux4.channelCount.getter()
{
  return 4;
}

uint64_t static vImage.Interleaved16Ux4.bitCountPerComponent.getter()
{
  return 16;
}

uint64_t static vImage.Interleaved16Ux4.bitCountPerPixel.getter()
{
  return 64;
}

uint64_t static vImage.Planar16F.makePixel(_:)(float a1)
{
  uint64_t v6 = *MEMORY[0x1E4F143B8];
  unsigned __int16 v3 = 0;
  float v2 = a1;
  src.char data = &v2;
  *(int64x2_t *)&src.height = vdupq_n_s64(1uLL);
  src.rowBytes = 4;
  dest.char data = &v3;
  *(_OWORD *)&dest.height = *(_OWORD *)&src.height;
  dest.rowBytes = 2;
  vImageConvert_PlanarFtoPlanar16F(&src, &dest, 0);
  return v3;
}

uint64_t convert<A, B>(src:dest:convertFunc:channelCount:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  if (a5 < 0)
  {
    __break(1u);
  }
  else
  {
    uint64_t v5 = MEMORY[0x1F4188790](result);
    v14[2] = v6;
    _OWORD v14[3] = v7;
    v14[4] = v8;
    v14[5] = v9;
    v14[6] = v5;
    v14[7] = v10;
    void v14[8] = v11;
    v14[9] = v12;
    return _ss15withUnsafeBytes2of_q0_x_q0_SWq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(v5, (uint64_t)partial apply for closure #1 in convert<A, B>(src:dest:convertFunc:channelCount:), (uint64_t)v14, v6, MEMORY[0x1E4FBC248], MEMORY[0x1E4FBC848] + 8, MEMORY[0x1E4FBC278], v13);
  }
  return result;
}

uint64_t static vImage.Interleaved16Fx2.makePixel(_:)(float a1, float a2)
{
  uint64_t v7 = *MEMORY[0x1E4F143B8];
  unsigned int v4 = 0;
  *(float *)unsigned __int16 v3 = a1;
  *(float *)&v3[1] = a2;
  src.char data = v3;
  *(_OWORD *)&src.height = xmmword_1D2135280;
  src.rowBytes = 8;
  dest.char data = &v4;
  *(_OWORD *)&dest.height = xmmword_1D2135280;
  dest.rowBytes = 4;
  vImageConvert_PlanarFtoPlanar16F(&src, &dest, 0);
  return v4;
}

uint64_t static vImage.Interleaved16Fx4.makePixel(_:)(float a1, float a2, float a3, float a4)
{
  uint64_t v9 = *MEMORY[0x1E4F143B8];
  *(float *)uint64_t v5 = a1;
  *(float *)&v5[1] = a2;
  *(float *)&void v5[2] = a3;
  *(float *)&v5[3] = a4;
  src.char data = v5;
  *(_OWORD *)&src.height = xmmword_1D2139130;
  src.rowBytes = 16;
  uint64_t v6 = 0;
  dest.char data = &v6;
  *(_OWORD *)&dest.height = xmmword_1D2139130;
  dest.rowBytes = 8;
  vImageConvert_PlanarFtoPlanar16F(&src, &dest, 0);
  return v6;
}

float static vImage.PlanarF.makePixel(_:)(__int16 a1)
{
  uint64_t v6 = *MEMORY[0x1E4F143B8];
  float v3 = 0.0;
  __int16 v2 = a1;
  src.char data = &v2;
  *(int64x2_t *)&src.height = vdupq_n_s64(1uLL);
  src.rowBytes = 2;
  dest.char data = &v3;
  *(_OWORD *)&dest.height = *(_OWORD *)&src.height;
  dest.rowBytes = 4;
  vImageConvert_Planar16FtoPlanarF(&src, &dest, 0);
  return v3;
}

float static vImage.InterleavedFx2.makePixel(_:)(__int16 a1, __int16 a2)
{
  uint64_t v7 = *MEMORY[0x1E4F143B8];
  v3[0] = a1;
  v3[1] = a2;
  src.char data = v3;
  *(_OWORD *)&src.height = xmmword_1D2135280;
  src.rowBytes = 4;
  uint64_t v4 = 0;
  dest.char data = &v4;
  *(_OWORD *)&dest.height = xmmword_1D2135280;
  dest.rowBytes = 8;
  vImageConvert_Planar16FtoPlanarF(&src, &dest, 0);
  return *(float *)&v4;
}

float static vImage.InterleavedFx4.makePixel(_:)(__int16 a1, __int16 a2, __int16 a3, __int16 a4)
{
  uint64_t v9 = *MEMORY[0x1E4F143B8];
  v6[0] = 0;
  v6[1] = 0;
  v5[0] = a1;
  v5[1] = a2;
  void v5[2] = a3;
  v5[3] = a4;
  src.char data = v5;
  *(_OWORD *)&src.height = xmmword_1D2139130;
  src.rowBytes = 8;
  dest.char data = v6;
  *(_OWORD *)&dest.height = xmmword_1D2139130;
  dest.rowBytes = 16;
  vImageConvert_Planar16FtoPlanarF(&src, &dest, 0);
  return *(float *)v6;
}

uint64_t partial apply for closure #1 in convert<A, B>(src:dest:convertFunc:channelCount:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v9 = *(void *)(v8 + 24);
  uint64_t v10 = *(void *)(v8 + 32);
  long long v11 = *(_OWORD *)(v8 + 40);
  uint64_t v12 = *(void *)(v8 + 56);
  v14[2] = *(void *)(v8 + 16);
  _OWORD v14[3] = v9;
  v14[4] = a1;
  v14[5] = a2;
  long long v15 = v11;
  uint64_t v16 = v12;
  long long v17 = *(_OWORD *)(v8 + 64);
  return _ss22withUnsafeMutableBytes2of_q0_xz_q0_Swq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(v10, (uint64_t)partial apply for closure #1 in closure #1 in convert<A, B>(src:dest:convertFunc:channelCount:), (uint64_t)v14, v9, MEMORY[0x1E4FBC248], MEMORY[0x1E4FBC848] + 8, MEMORY[0x1E4FBC278], a8);
}

uint64_t associated type witness table accessor for PixelFormat.ComponentType : Equatable in vImage.Planar8x2()
{
  return MEMORY[0x1E4FBC370];
}

_UNKNOWN **associated type witness table accessor for MultiplePlanePixelFormat.PlanarPixelFormat : StaticPixelFormat in vImage.Planar8x2()
{
  return &protocol witness table for vImage.Planar8;
}

uint64_t associated type witness table accessor for PixelFormat.ComponentType : Equatable in vImage.PlanarFx2()
{
  return MEMORY[0x1E4FBB4A0];
}

_UNKNOWN **associated type witness table accessor for MultiplePlanePixelFormat.PlanarPixelFormat : StaticPixelFormat in vImage.PlanarFx2()
{
  return &protocol witness table for vImage.PlanarF;
}

uint64_t associated type witness table accessor for PixelFormat.ComponentType : Equatable in vImage.DynamicPixelFormat()
{
  return MEMORY[0x1E4FBC260];
}

uint64_t associated type witness table accessor for PixelFormat.ComponentType : Equatable in vImage.Planar16F()
{
  return MEMORY[0x1E4FBC470];
}

uint64_t dispatch thunk of static MultiplePlanePixelFormat.planeCount.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 32))();
}

uint64_t dispatch thunk of static MultiplePlanePixelFormat.bitCountPerPlanarPixel.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 40))();
}

uint64_t dispatch thunk of static StaticPixelFormat.bitCountPerPixel.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

uint64_t dispatch thunk of static StaticPixelFormat.channelCount.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 24))();
}

uint64_t dispatch thunk of static InitializableFromCGImage.bitCountPerComponent.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

ValueMetadata *type metadata accessor for vImage.Planar8x2()
{
  return &type metadata for vImage.Planar8x2;
}

ValueMetadata *type metadata accessor for vImage.Planar8x3()
{
  return &type metadata for vImage.Planar8x3;
}

ValueMetadata *type metadata accessor for vImage.Planar8x4()
{
  return &type metadata for vImage.Planar8x4;
}

ValueMetadata *type metadata accessor for vImage.PlanarFx2()
{
  return &type metadata for vImage.PlanarFx2;
}

ValueMetadata *type metadata accessor for vImage.PlanarFx3()
{
  return &type metadata for vImage.PlanarFx3;
}

ValueMetadata *type metadata accessor for vImage.PlanarFx4()
{
  return &type metadata for vImage.PlanarFx4;
}

ValueMetadata *type metadata accessor for vImage.DynamicPixelFormat()
{
  return &type metadata for vImage.DynamicPixelFormat;
}

ValueMetadata *type metadata accessor for vImage.Planar8()
{
  return &type metadata for vImage.Planar8;
}

ValueMetadata *type metadata accessor for vImage.Interleaved8x2()
{
  return &type metadata for vImage.Interleaved8x2;
}

ValueMetadata *type metadata accessor for vImage.Interleaved8x3()
{
  return &type metadata for vImage.Interleaved8x3;
}

ValueMetadata *type metadata accessor for vImage.Interleaved8x4()
{
  return &type metadata for vImage.Interleaved8x4;
}

ValueMetadata *type metadata accessor for vImage.Planar16F()
{
  return &type metadata for vImage.Planar16F;
}

ValueMetadata *type metadata accessor for vImage.Interleaved16Fx2()
{
  return &type metadata for vImage.Interleaved16Fx2;
}

ValueMetadata *type metadata accessor for vImage.Interleaved16Fx4()
{
  return &type metadata for vImage.Interleaved16Fx4;
}

ValueMetadata *type metadata accessor for vImage.PlanarF()
{
  return &type metadata for vImage.PlanarF;
}

ValueMetadata *type metadata accessor for vImage.InterleavedFx2()
{
  return &type metadata for vImage.InterleavedFx2;
}

ValueMetadata *type metadata accessor for vImage.InterleavedFx3()
{
  return &type metadata for vImage.InterleavedFx3;
}

ValueMetadata *type metadata accessor for vImage.InterleavedFx4()
{
  return &type metadata for vImage.InterleavedFx4;
}

ValueMetadata *type metadata accessor for vImage.Planar16U()
{
  return &type metadata for vImage.Planar16U;
}

ValueMetadata *type metadata accessor for vImage.Interleaved16Ux2()
{
  return &type metadata for vImage.Interleaved16Ux2;
}

ValueMetadata *type metadata accessor for vImage.Interleaved16Ux4()
{
  return &type metadata for vImage.Interleaved16Ux4;
}

uint64_t partial apply for closure #1 in closure #1 in convert<A, B>(src:dest:convertFunc:channelCount:)(uint64_t result)
{
  if (v1[4])
  {
    uint64_t v2 = v1[6];
    uint64_t v4 = v1[8];
    float v3 = (uint64_t (*)(void *, void *, void))v1[9];
    uint64_t v5 = *(void *)(*(void *)(v1[2] - 8) + 72);
    v7[0] = v1[4];
    v7[1] = 1;
    _OWORD v7[2] = v2;
    void v7[3] = v5;
    if (result)
    {
      v6[0] = result;
      v6[1] = 1;
      void v6[2] = v2;
      v6[3] = v4;
      return v3(v7, v6, 0);
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t _ss22withUnsafeMutableBytes2of_q0_xz_q0_Swq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v11 = *(void *)(a5 - 8);
  uint64_t v12 = MEMORY[0x1F4188790](a1);
  int v14 = (char *)&v18 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t result = v16(v12, v12 + *(void *)(*(void *)(v15 - 8) + 64), v14);
  if (v8) {
    return (*(uint64_t (**)(uint64_t, char *, uint64_t))(v11 + 32))(a8, v14, a5);
  }
  return result;
}

uint64_t static BNNS.copy(_:to:filterParameters:)(_OWORD *a1, _OWORD *a2, uint32_t a3, size_t a4, int (__cdecl *a5)(void **, size_t, size_t), void (__cdecl *a6)(void *))
{
  uint64_t v31 = *MEMORY[0x1E4F143B8];
  if (a5 != (int (__cdecl *)(void **, size_t, size_t))1)
  {
    v28.flags = a3;
    v28.n_threads = a4;
    v28.alloc_memory = a5;
    v28.free_memory = a6;
    long long v17 = a1[9];
    *(_OWORD *)&src.stride[7] = a1[8];
    *(_OWORD *)&src.BNNSDataType data_type = v17;
    *(_OWORD *)&src.table_BNNSDataType data_type = a1[10];
    long long v18 = a1[5];
    *(_OWORD *)&src.size[7] = a1[4];
    *(_OWORD *)&src.stride[1] = v18;
    long long v19 = a1[7];
    *(_OWORD *)&src.stride[3] = a1[6];
    *(_OWORD *)&src.stride[5] = v19;
    long long v20 = a1[1];
    *(_OWORD *)&src.flags = *a1;
    *(_OWORD *)&src.size[1] = v20;
    long long v21 = a1[3];
    *(_OWORD *)&src.size[3] = a1[2];
    *(_OWORD *)&src.size[5] = v21;
    long long v22 = a2[9];
    *(_OWORD *)&dest.stride[7] = a2[8];
    *(_OWORD *)&dest.BNNSDataType data_type = v22;
    *(_OWORD *)&dest.table_BNNSDataType data_type = a2[10];
    long long v23 = a2[5];
    *(_OWORD *)&dest.size[7] = a2[4];
    *(_OWORD *)&dest.stride[1] = v23;
    long long v24 = a2[7];
    *(_OWORD *)&dest.stride[3] = a2[6];
    *(_OWORD *)&dest.stride[5] = v24;
    long long v25 = a2[1];
    *(_OWORD *)&dest.flags = *a2;
    *(_OWORD *)&dest.size[1] = v25;
    long long v26 = a2[3];
    *(_OWORD *)&dest.size[3] = a2[2];
    *(_OWORD *)&dest.size[5] = v26;
    uint64_t result = BNNSCopy(&dest, &src, &v28);
    if (!result) {
      return result;
    }
    goto LABEL_5;
  }
  long long v6 = a1[9];
  *(_OWORD *)&src.stride[7] = a1[8];
  *(_OWORD *)&src.BNNSDataType data_type = v6;
  *(_OWORD *)&src.table_BNNSDataType data_type = a1[10];
  long long v7 = a1[5];
  *(_OWORD *)&src.size[7] = a1[4];
  *(_OWORD *)&src.stride[1] = v7;
  long long v8 = a1[7];
  *(_OWORD *)&src.stride[3] = a1[6];
  *(_OWORD *)&src.stride[5] = v8;
  long long v9 = a1[1];
  *(_OWORD *)&src.flags = *a1;
  *(_OWORD *)&src.size[1] = v9;
  long long v10 = a1[3];
  *(_OWORD *)&src.size[3] = a1[2];
  *(_OWORD *)&src.size[5] = v10;
  long long v11 = a2[9];
  *(_OWORD *)&dest.stride[7] = a2[8];
  *(_OWORD *)&dest.BNNSDataType data_type = v11;
  *(_OWORD *)&dest.table_BNNSDataType data_type = a2[10];
  long long v12 = a2[5];
  *(_OWORD *)&dest.size[7] = a2[4];
  *(_OWORD *)&dest.stride[1] = v12;
  long long v13 = a2[7];
  *(_OWORD *)&dest.stride[3] = a2[6];
  *(_OWORD *)&dest.stride[5] = v13;
  long long v14 = a2[1];
  *(_OWORD *)&dest.flags = *a2;
  *(_OWORD *)&dest.size[1] = v14;
  long long v15 = a2[3];
  *(_OWORD *)&dest.size[3] = a2[2];
  *(_OWORD *)&dest.size[5] = v15;
  uint64_t result = BNNSCopy(&dest, &src, 0);
  if (result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    unsigned char *v27 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t vImage.PixelBuffer<>.applyMorphology(operation:destination:)(uint64_t a1, void **a2)
{
  return specialized vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(*(void *)a1, *(void *)(a1 + 8), *(void *)(a1 + 16), *(unsigned char *)(a1 + 24), *a2, (void (*)(void *, void *, void, void, uint64_t, uint64_t, uint64_t, void))@nonobjc vImageErode_ARGB8888(_:_:_:_:_:_:_:_:), 0, (void (*)(void *, void *, void, void, uint64_t, uint64_t, uint64_t, void))@nonobjc vImageDilate_ARGB8888(_:_:_:_:_:_:_:_:), 0, (uint64_t (*)(void *, void *, void, void, void, uint64_t, uint64_t, void))@nonobjc vImageMin_ARGB8888(_:_:_:_:_:_:_:_:), 0, (uint64_t (*)(void *, void *, void, void, void, uint64_t, uint64_t, void))@nonobjc vImageMax_ARGB8888(_:_:_:_:_:_:_:_:), 0, *v2, (uint64_t (*)(uint64_t, uint64_t, uint64_t, uint64_t))outlined consume of vImage.MorphologyOperation<UInt8>);
}

{
  void **v2;

  return specialized vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(*(void *)a1, *(void *)(a1 + 8), *(void *)(a1 + 16), *(unsigned char *)(a1 + 24), *a2, (void (*)(void *, void *, void, void, uint64_t, uint64_t, uint64_t, void))@nonobjc vImageErode_Planar8(_:_:_:_:_:_:_:_:), 0, (void (*)(void *, void *, void, void, uint64_t, uint64_t, uint64_t, void))@nonobjc vImageDilate_Planar8(_:_:_:_:_:_:_:_:), 0, (uint64_t (*)(void *, void *, void, void, void, uint64_t, uint64_t, void))@nonobjc vImageMin_Planar8(_:_:_:_:_:_:_:_:), 0, (uint64_t (*)(void *, void *, void, void, void, uint64_t, uint64_t, void))@nonobjc vImageMax_Planar8(_:_:_:_:_:_:_:_:), 0, *v2, (uint64_t (*)(uint64_t, uint64_t, uint64_t, uint64_t))outlined consume of vImage.MorphologyOperation<UInt8>);
}

{
  void **v2;

  return specialized vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(*(void *)a1, *(void *)(a1 + 8), *(void *)(a1 + 16), *(unsigned char *)(a1 + 24), *a2, (void (*)(void *, void *, void, void, uint64_t, uint64_t, uint64_t, void))@nonobjc vImageErode_ARGBFFFF(_:_:_:_:_:_:_:_:), 0, (void (*)(void *, void *, void, void, uint64_t, uint64_t, uint64_t, void))@nonobjc vImageDilate_ARGBFFFF(_:_:_:_:_:_:_:_:), 0, (uint64_t (*)(void *, void *, void, void, void, uint64_t, uint64_t, void))@nonobjc vImageMin_ARGBFFFF(_:_:_:_:_:_:_:_:), 0, (uint64_t (*)(void *, void *, void, void, void, uint64_t, uint64_t, void))@nonobjc vImageMax_ARGBFFFF(_:_:_:_:_:_:_:_:), 0, *v2, (uint64_t (*)(uint64_t, uint64_t, uint64_t, uint64_t))outlined consume of vImage.MorphologyOperation<Float>);
}

{
  void **v2;

  return specialized vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(*(void *)a1, *(void *)(a1 + 8), *(void *)(a1 + 16), *(unsigned char *)(a1 + 24), *a2, (void (*)(void *, void *, void, void, uint64_t, uint64_t, uint64_t, void))@nonobjc vImageErode_PlanarF(_:_:_:_:_:_:_:_:), 0, (void (*)(void *, void *, void, void, uint64_t, uint64_t, uint64_t, void))@nonobjc vImageDilate_PlanarF(_:_:_:_:_:_:_:_:), 0, (uint64_t (*)(void *, void *, void, void, void, uint64_t, uint64_t, void))@nonobjc vImageMin_PlanarF(_:_:_:_:_:_:_:_:), 0, (uint64_t (*)(void *, void *, void, void, void, uint64_t, uint64_t, void))@nonobjc vImageMax_PlanarF(_:_:_:_:_:_:_:_:), 0, *v2, (uint64_t (*)(uint64_t, uint64_t, uint64_t, uint64_t))outlined consume of vImage.MorphologyOperation<Float>);
}

uint64_t specialized vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, char a4, void *a5, void (*a6)(void *, void *, void, void, uint64_t, uint64_t, uint64_t, void), uint64_t a7, void (*a8)(void *, void *, void, void, uint64_t, uint64_t, uint64_t, void), uint64_t a9, uint64_t (*a10)(void *, void *, void, void, void, uint64_t, uint64_t, void), uint64_t a11, uint64_t (*a12)(void *, void *, void, void, void, uint64_t, uint64_t, void), uint64_t a13, void *a14, uint64_t (*a15)(uint64_t, uint64_t, uint64_t, uint64_t))
{
  v36[4] = *MEMORY[0x1E4F143B8];
  if (!a14[2])
  {
    __break(1u);
    goto LABEL_33;
  }
  if (!a5[2])
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  uint64_t v19 = a14[4];
  uint64_t v20 = a5[4];
  if (v19)
  {
    if (!v20 || v19 != v20) {
      goto LABEL_8;
    }
    __break(1u);
  }
  if (!v20)
  {
    __break(1u);
    JUMPOUT(0x1D21145A0);
  }
LABEL_8:
  uint64_t v21 = a14[6];
  if (v21 < 0)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  uint64_t v22 = a14[5];
  if (v22 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (!v21)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v22)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  uint64_t v23 = a5[6];
  if (v23 < 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  uint64_t v24 = a5[5];
  if (v24 < 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v23)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v24)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (v21 != v23)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (v22 != v24)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if ((a4 & 0xFE) != 0) {
    uint64_t v25 = a2;
  }
  else {
    uint64_t v25 = a2 & ~(a2 >> 63);
  }
  if ((a4 & 0xFE) != 0) {
    uint64_t v26 = a1;
  }
  else {
    uint64_t v26 = a1 & ~(a1 >> 63);
  }
  if ((v25 & v26 & 1) == 0) {
LABEL_44:
  }
    __break(1u);
  uint64_t v27 = a14[7];
  v36[0] = v19;
  v36[1] = v22;
  v36[2] = v21;
  v36[3] = v27;
  uint64_t v28 = a5[7];
  v35[0] = v20;
  v35[1] = v22;
  v35[2] = v21;
  v35[3] = v28;
  switch(a4)
  {
    case 1:
      uint64_t result = a10(v36, v35, 0, 0, 0, v25, v26, 0);
      break;
    case 2:
      swift_bridgeObjectRetain();
      a8(v36, v35, 0, 0, a3 + 32, v25, v26, 0);
      uint64_t v31 = a1;
      uint64_t v32 = a2;
      uint64_t v33 = a3;
      uint64_t v34 = 2;
      goto LABEL_30;
    case 3:
      swift_bridgeObjectRetain();
      a6(v36, v35, 0, 0, a3 + 32, v25, v26, 0);
      uint64_t v31 = a1;
      uint64_t v32 = a2;
      uint64_t v33 = a3;
      uint64_t v34 = 3;
LABEL_30:
      uint64_t result = a15(v31, v32, v33, v34);
      break;
    default:
      uint64_t result = a12(v36, v35, 0, 0, 0, v25, v26, 0);
      break;
  }
  return result;
}

uint64_t vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(uint64_t a1, uint64_t *a2, void (*a3)(uint64_t, uint64_t, void, void, unint64_t, uint64_t, uint64_t, void), uint64_t a4, void (*a5)(uint64_t, uint64_t, void, void, unint64_t, uint64_t, uint64_t, void), uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, void, void, void, uint64_t, uint64_t, void), uint64_t a8, uint64_t (*a9)(uint64_t, uint64_t, void, void, void, uint64_t, uint64_t, void), uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13)
{
  uint64_t v42 = *MEMORY[0x1E4F143B8];
  uint64_t v15 = *(void *)a1;
  uint64_t v36 = *(void *)(a1 + 8);
  unint64_t v32 = *(void *)(a1 + 16);
  unsigned int v16 = *(unsigned __int8 *)(a1 + 24);
  uint64_t v17 = *a2;
  uint64_t v18 = *v13;
  uint64_t v19 = vImage.PixelBuffer<>.vImageBuffer.getter();
  uint64_t v38 = v17;
  type metadata accessor for vImage.PixelBuffer();
  uint64_t result = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (v19)
  {
    if (result) {
      BOOL v21 = v19 == result;
    }
    else {
      BOOL v21 = 0;
    }
    if (!v21)
    {
LABEL_9:
      uint64_t v29 = a6;
      v37[0] = v18;
      swift_bridgeObjectRetain();
      vImage.PixelBuffer.size.getter(&v38);
      uint64_t v23 = v38;
      uint64_t v22 = v39;
      vImage.PixelBuffer.size.getter(v37);
      swift_bridgeObjectRelease();
      if (v23 == v37[0] && v22 == v37[1])
      {
        if (v16 >= 2) {
          uint64_t v24 = v15;
        }
        else {
          uint64_t v24 = v15 & ~(v15 >> 63);
        }
        if (v16 >= 2) {
          uint64_t v25 = v36;
        }
        else {
          uint64_t v25 = v36 & ~(v36 >> 63);
        }
        if (v25 & v24)
        {
          uint64_t v38 = v18;
          uint64_t v38 = vImage.PixelBuffer<>.vImageBuffer.getter();
          uint64_t v39 = v26;
          uint64_t v40 = v27;
          uint64_t v41 = v28;
          return closure #1 in vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)((uint64_t)&v38, v17, v15, v36, v32, v16, a9, a10, v25, v24, a7, a8, a5, v29, a3, a4, *(void *)(a11 + 16), a12, a13);
        }
      }
      else
      {
        __break(1u);
      }
      __break(1u);
    }
    __break(1u);
  }
  if (result) {
    goto LABEL_9;
  }
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.applyMorphology(operation:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t result = (*(uint64_t (**)(void, uint64_t))(a4 + 32))(*(void *)(a3 + 16), a4);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    uint64_t v7 = result;
    if (result)
    {
      unint64_t v8 = 0;
      do
      {
        uint64_t v9 = v4;
        vImage.PixelBuffer<>.subscript.getter(v8, &v13);
        vImage.PixelBuffer<>.subscript.getter(v8, &v12);
        uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
        swift_getAssociatedConformanceWitness();
        uint64_t v11 = type metadata accessor for vImage.PixelBuffer();
        vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(a1, &v12, (void (*)(uint64_t, uint64_t, void, void, unint64_t, uint64_t, uint64_t, void))@nonobjc vImageErode_Planar8(_:_:_:_:_:_:_:_:), 0, (void (*)(uint64_t, uint64_t, void, void, unint64_t, uint64_t, uint64_t, void))@nonobjc vImageDilate_Planar8(_:_:_:_:_:_:_:_:), 0, (uint64_t (*)(uint64_t, uint64_t, void, void, void, uint64_t, uint64_t, void))@nonobjc vImageMin_Planar8(_:_:_:_:_:_:_:_:), 0, (uint64_t (*)(uint64_t, uint64_t, void, void, void, uint64_t, uint64_t, void))@nonobjc vImageMax_Planar8(_:_:_:_:_:_:_:_:), 0, v11, AssociatedTypeWitness, MEMORY[0x1E4FBC358]);
        uint64_t v4 = v9;
        swift_bridgeObjectRelease();
        uint64_t result = swift_bridgeObjectRelease();
        ++v8;
      }
      while (v7 != v8);
    }
  }
  return result;
}

{
  uint64_t v4;
  uint64_t result;
  uint64_t v7;
  unint64_t v8;
  uint64_t v9;
  uint64_t AssociatedTypeWitness;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;

  uint64_t result = (*(uint64_t (**)(void, uint64_t))(a4 + 32))(*(void *)(a3 + 16), a4);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    uint64_t v7 = result;
    if (result)
    {
      unint64_t v8 = 0;
      do
      {
        uint64_t v9 = v4;
        vImage.PixelBuffer<>.subscript.getter(v8, &v13);
        vImage.PixelBuffer<>.subscript.getter(v8, &v12);
        uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
        swift_getAssociatedConformanceWitness();
        uint64_t v11 = type metadata accessor for vImage.PixelBuffer();
        vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(a1, &v12, (void (*)(uint64_t, uint64_t, void, void, unint64_t, uint64_t, uint64_t, void))@nonobjc vImageErode_PlanarF(_:_:_:_:_:_:_:_:), 0, (void (*)(uint64_t, uint64_t, void, void, unint64_t, uint64_t, uint64_t, void))@nonobjc vImageDilate_PlanarF(_:_:_:_:_:_:_:_:), 0, (uint64_t (*)(uint64_t, uint64_t, void, void, void, uint64_t, uint64_t, void))@nonobjc vImageMin_PlanarF(_:_:_:_:_:_:_:_:), 0, (uint64_t (*)(uint64_t, uint64_t, void, void, void, uint64_t, uint64_t, void))@nonobjc vImageMax_PlanarF(_:_:_:_:_:_:_:_:), 0, v11, AssociatedTypeWitness, MEMORY[0x1E4FBB470]);
        uint64_t v4 = v9;
        swift_bridgeObjectRelease();
        uint64_t result = swift_bridgeObjectRelease();
        ++v8;
      }
      while (v7 != v8);
    }
  }
  return result;
}

uint64_t vImage.MorphologyOperation.width.getter()
{
  if (v0[24] >= 2u) {
    return *(void *)v0;
  }
  else {
    return *(void *)v0 & ~(*(uint64_t *)v0 >> 63);
  }
}

uint64_t type metadata accessor for vImage.MorphologyOperation()
{
  return __swift_instantiateGenericMetadata();
}

uint64_t vImage.MorphologyOperation.height.getter()
{
  if (*(unsigned __int8 *)(v0 + 24) >= 2u) {
    return *(void *)(v0 + 8);
  }
  else {
    return *(void *)(v0 + 8) & ~(*(uint64_t *)(v0 + 8) >> 63);
  }
}

uint64_t closure #1 in vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, unint64_t a5, char a6, uint64_t (*a7)(uint64_t, uint64_t, void, void, void, uint64_t, uint64_t, void), uint64_t a8, uint64_t a9, uint64_t a10, uint64_t (*a11)(uint64_t, uint64_t, void, void, void, uint64_t, uint64_t, void), uint64_t a12, void (*a13)(uint64_t, uint64_t, void, void, unint64_t, uint64_t, uint64_t, void), uint64_t a14, void (*a15)(uint64_t, uint64_t, void, void, unint64_t, uint64_t, uint64_t, void), uint64_t a16, uint64_t a17, uint64_t a18, uint64_t a19)
{
  v30[4] = *MEMORY[0x1E4F143B8];
  type metadata accessor for vImage.PixelBuffer();
  v30[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v30[1] = v20;
  v30[2] = v21;
  v30[3] = v22;
  return closure #1 in closure #1 in vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)((uint64_t)v30, a3, a4, a5, a6, a7, a8, a1, a9, a10, a11, a12, a13, a14, a15, a16, a17, a18, a19);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>._applyMorphology<A, B>(operation:destination:erodeFunc:dilateFunc:minFunc:maxFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, unint64_t a4, char a5, uint64_t (*a6)(uint64_t, uint64_t, void, void, void, uint64_t, uint64_t, void), uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t (*a11)(uint64_t, uint64_t, void, void, void, uint64_t, uint64_t, void), uint64_t a12, void (*a13)(uint64_t, uint64_t, void, void, unint64_t, uint64_t, uint64_t, void), uint64_t a14, void (*a15)(uint64_t, uint64_t, void, void, unint64_t, uint64_t, uint64_t, void), uint64_t a16, uint64_t a17, uint64_t a18, uint64_t a19)
{
  switch(a5)
  {
    case 1:
      return a11(a8, a1, 0, 0, 0, a9, a10, 0);
    case 2:
      swift_bridgeObjectRetain();
      if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && a4 >> 62)
      {
        if (MEMORY[0x1D25FF9C0](a4, a19))
        {
          type metadata accessor for _ArrayBuffer();
          swift_getWitnessTable();
          Array.init<A>(_:)();
          swift_unknownObjectRetain();
          unint64_t v24 = _ContiguousArrayBuffer.firstElementAddress.getter();
          swift_release();
          uint64_t v28 = a8;
          uint64_t v30 = a10;
          uint64_t v29 = a1;
          uint64_t v31 = a9;
LABEL_18:
          a13(v28, v29, 0, 0, v24, v31, v30, 0);
          return swift_unknownObjectRelease();
        }
        outlined consume of vImage.MorphologyOperation<Float>(a2, a3, a4, 2);
        unint64_t v24 = 0;
      }
      else
      {
        outlined consume of vImage.MorphologyOperation<Float>(a2, a3, a4, 2);
        if (_swift_isClassOrObjCExistentialType()) {
          unint64_t v24 = (a4 & 0xFFFFFFFFFFFFFF8)
        }
              + ((*(unsigned __int8 *)(*(void *)(a19 - 8) + 80) + 32) & ~(unint64_t)*(unsigned __int8 *)(*(void *)(a19 - 8) + 80));
        else {
          unint64_t v24 = a4
        }
              + ((*(unsigned __int8 *)(*(void *)(a19 - 8) + 80) + 32) & ~(unint64_t)*(unsigned __int8 *)(*(void *)(a19 - 8) + 80));
      }
      if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && a4 >> 62)
      {
        swift_unknownObjectRetain();
      }
      else
      {
        _swift_isClassOrObjCExistentialType();
        swift_bridgeObjectRetain();
      }
      uint64_t v28 = a8;
      uint64_t v30 = a10;
      uint64_t v29 = a1;
      uint64_t v31 = a9;
      if (!v24) {
        unint64_t v24 = ~*(_DWORD *)(*(void *)(a19 - 8) + 80) | 0xFFFFFFFFFFFFFF00;
      }
      goto LABEL_18;
    case 3:
      swift_bridgeObjectRetain();
      unint64_t v26 = a4 >> 62;
      if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && v26)
      {
        if (MEMORY[0x1D25FF9C0](a4, a19))
        {
          type metadata accessor for _ArrayBuffer();
          swift_getWitnessTable();
          Array.init<A>(_:)();
          swift_unknownObjectRetain();
          unint64_t v27 = _ContiguousArrayBuffer.firstElementAddress.getter();
          swift_release();
          uint64_t v32 = a8;
          uint64_t v34 = a10;
          uint64_t v33 = a1;
          uint64_t v35 = a9;
          goto LABEL_25;
        }
        outlined consume of vImage.MorphologyOperation<Float>(a2, a3, a4, 3);
        unint64_t v27 = 0;
      }
      else
      {
        outlined consume of vImage.MorphologyOperation<Float>(a2, a3, a4, 3);
        if (_swift_isClassOrObjCExistentialType()) {
          unint64_t v27 = (a4 & 0xFFFFFFFFFFFFFF8)
        }
              + ((*(unsigned __int8 *)(*(void *)(a19 - 8) + 80) + 32) & ~(unint64_t)*(unsigned __int8 *)(*(void *)(a19 - 8) + 80));
        else {
          unint64_t v27 = a4
        }
              + ((*(unsigned __int8 *)(*(void *)(a19 - 8) + 80) + 32) & ~(unint64_t)*(unsigned __int8 *)(*(void *)(a19 - 8) + 80));
      }
      if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && v26)
      {
        swift_unknownObjectRetain();
      }
      else
      {
        _swift_isClassOrObjCExistentialType();
        swift_bridgeObjectRetain();
      }
      uint64_t v32 = a8;
      uint64_t v34 = a10;
      uint64_t v33 = a1;
      uint64_t v35 = a9;
      if (!v27) {
        unint64_t v27 = ~*(_DWORD *)(*(void *)(a19 - 8) + 80) | 0xFFFFFFFFFFFFFF00;
      }
LABEL_25:
      a15(v32, v33, 0, 0, v27, v35, v34, 0);
      return swift_unknownObjectRelease();
    default:
      return a6(a8, a1, 0, 0, 0, a9, a10, 0);
  }
}

uint64_t vImage.MorphologyOperation.structuringElement.getter@<X0>(uint64_t *a1@<X8>)
{
  unsigned int v3 = *(unsigned __int8 *)(v1 + 24);
  if (v3 >= 2)
  {
    uint64_t v5 = *(void *)(v1 + 8);
    uint64_t v6 = *(void *)(v1 + 16);
    uint64_t v4 = *(void *)v1;
    uint64_t result = outlined copy of vImage.MorphologyOperation<A><A>(*(void *)v1, v5, v6, v3);
  }
  else
  {
    uint64_t v4 = 0;
    uint64_t v5 = 0;
    uint64_t v6 = 0;
  }
  *a1 = v4;
  a1[1] = v5;
  a1[2] = v6;
  return result;
}

uint64_t outlined copy of vImage.MorphologyOperation<A><A>(uint64_t a1, uint64_t a2, uint64_t a3, char a4)
{
  if ((a4 & 0xFE) == 2) {
    return swift_bridgeObjectRetain();
  }
  return result;
}

uint64_t destroy for vImage.MorphologyOperation(uint64_t a1)
{
  return outlined consume of vImage.MorphologyOperation<Float>(*(void *)a1, *(void *)(a1 + 8), *(void *)(a1 + 16), *(unsigned char *)(a1 + 24));
}

uint64_t initializeWithCopy for vImage.MorphologyOperation(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)a2;
  uint64_t v4 = *(void *)(a2 + 8);
  uint64_t v5 = *(void *)(a2 + 16);
  char v6 = *(unsigned char *)(a2 + 24);
  outlined copy of vImage.MorphologyOperation<A><A>(*(void *)a2, v4, v5, v6);
  *(void *)a1 = v3;
  *(void *)(a1 + 8) = v4;
  *(void *)(a1 + 16) = v5;
  *(unsigned char *)(a1 + 24) = v6;
  return a1;
}

uint64_t assignWithCopy for vImage.MorphologyOperation(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)a2;
  uint64_t v4 = *(void *)(a2 + 8);
  uint64_t v5 = *(void *)(a2 + 16);
  char v6 = *(unsigned char *)(a2 + 24);
  outlined copy of vImage.MorphologyOperation<A><A>(*(void *)a2, v4, v5, v6);
  uint64_t v7 = *(void *)a1;
  uint64_t v8 = *(void *)(a1 + 8);
  uint64_t v9 = *(void *)(a1 + 16);
  *(void *)a1 = v3;
  *(void *)(a1 + 8) = v4;
  *(void *)(a1 + 16) = v5;
  char v10 = *(unsigned char *)(a1 + 24);
  *(unsigned char *)(a1 + 24) = v6;
  outlined consume of vImage.MorphologyOperation<Float>(v7, v8, v9, v10);
  return a1;
}

__n128 __swift_memcpy25_8(uint64_t a1, uint64_t a2)
{
  __n128 result = *(__n128 *)a2;
  *(_OWORD *)(a1 + 9) = *(_OWORD *)(a2 + 9);
  *(__n128 *)a1 = result;
  return result;
}

uint64_t assignWithTake for vImage.MorphologyOperation(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(a2 + 16);
  char v4 = *(unsigned char *)(a2 + 24);
  uint64_t v5 = *(void *)a1;
  uint64_t v7 = *(void *)(a1 + 8);
  uint64_t v6 = *(void *)(a1 + 16);
  *(_OWORD *)a1 = *(_OWORD *)a2;
  *(void *)(a1 + 16) = v3;
  char v8 = *(unsigned char *)(a1 + 24);
  *(unsigned char *)(a1 + 24) = v4;
  outlined consume of vImage.MorphologyOperation<Float>(v5, v7, v6, v8);
  return a1;
}

uint64_t getEnumTagSinglePayload for vImage.MorphologyOperation(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFD && *(unsigned char *)(a1 + 25)) {
    return (*(_DWORD *)a1 + 253);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 24);
  if (v3 <= 3) {
    int v4 = -1;
  }
  else {
    int v4 = v3 ^ 0xFF;
  }
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for vImage.MorphologyOperation(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFC)
  {
    *(void *)(result + 8) = 0;
    *(void *)(result + 16) = 0;
    *(unsigned char *)(result + 24) = 0;
    *(void *)__n128 result = a2 - 253;
    if (a3 >= 0xFD) {
      *(unsigned char *)(result + 25) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFD) {
      *(unsigned char *)(result + 25) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 24) = -(char)a2;
    }
  }
  return result;
}

uint64_t getEnumTag for vImage.MorphologyOperation(uint64_t a1)
{
  return *(unsigned __int8 *)(a1 + 24);
}

uint64_t destructiveInjectEnumTag for vImage.MorphologyOperation(uint64_t result, char a2)
{
  *(unsigned char *)(result + 24) = a2;
  return result;
}

uint64_t outlined consume of vImage.MorphologyOperation<Float>(uint64_t a1, uint64_t a2, uint64_t a3, char a4)
{
  if ((a4 & 0xFE) == 2) {
    return swift_bridgeObjectRelease();
  }
  return result;
}

uint64_t specialized _ArrayBuffer._nonNative.getter(uint64_t result)
{
  if (result >= 0) {
    return result & 0xFFFFFFFFFFFFFF8;
  }
  return result;
}

unint64_t static BNNS.matrixMultiplicationWorkspaceSize(inputA:transposed:inputB:transposed:output:alpha:filterParameters:)(_OWORD *a1, char a2, _OWORD *a3, char a4, _OWORD *a5, int a6, uint64_t a7, uint64_t a8, float a9, uint64_t a10)
{
  uint64_t v51 = *MEMORY[0x1E4F143B8];
  if (a8 == 1)
  {
    long long v10 = a1[9];
    *(_OWORD *)&inputA.stride[7] = a1[8];
    *(_OWORD *)&inputA.BNNSDataType data_type = v10;
    *(_OWORD *)&inputA.table_BNNSDataType data_type = a1[10];
    long long v11 = a1[5];
    *(_OWORD *)&inputA.size[7] = a1[4];
    *(_OWORD *)&inputA.stride[1] = v11;
    long long v12 = a1[7];
    *(_OWORD *)&inputA.stride[3] = a1[6];
    *(_OWORD *)&inputA.stride[5] = v12;
    long long v13 = a1[1];
    *(_OWORD *)&inputA.flags = *a1;
    *(_OWORD *)&inputA.size[1] = v13;
    long long v14 = a1[3];
    *(_OWORD *)&inputA.size[3] = a1[2];
    *(_OWORD *)&inputA.size[5] = v14;
    long long v15 = a3[9];
    *(_OWORD *)&inputB.stride[7] = a3[8];
    *(_OWORD *)&inputB.BNNSDataType data_type = v15;
    *(_OWORD *)&inputB.table_BNNSDataType data_type = a3[10];
    long long v16 = a3[5];
    *(_OWORD *)&inputB.size[7] = a3[4];
    *(_OWORD *)&inputB.stride[1] = v16;
    long long v17 = a3[7];
    *(_OWORD *)&inputB.stride[3] = a3[6];
    *(_OWORD *)&inputB.stride[5] = v17;
    long long v18 = a3[1];
    *(_OWORD *)&inputB.flags = *a3;
    *(_OWORD *)&inputB.size[1] = v18;
    long long v19 = a3[3];
    *(_OWORD *)&inputB.size[3] = a3[2];
    *(_OWORD *)&inputB.size[5] = v19;
    long long v20 = a5[9];
    *(_OWORD *)&output.stride[7] = a5[8];
    *(_OWORD *)&output.BNNSDataType data_type = v20;
    *(_OWORD *)&output.table_BNNSDataType data_type = a5[10];
    long long v21 = a5[5];
    *(_OWORD *)&output.size[7] = a5[4];
    *(_OWORD *)&output.stride[1] = v21;
    long long v22 = a5[7];
    *(_OWORD *)&output.stride[3] = a5[6];
    *(_OWORD *)&output.stride[5] = v22;
    long long v23 = a5[1];
    *(_OWORD *)&output.flags = *a5;
    *(_OWORD *)&output.size[1] = v23;
    long long v24 = a5[3];
    *(_OWORD *)&output.size[3] = a5[2];
    *(_OWORD *)&output.size[5] = v24;
    BOOL v25 = a2 & 1;
    BOOL v26 = a4 & 1;
    unint64_t v27 = 0;
  }
  else
  {
    int v44 = a6;
    uint64_t v45 = a7;
    uint64_t v46 = a8;
    uint64_t v47 = a10;
    long long v28 = a1[9];
    *(_OWORD *)&inputA.stride[7] = a1[8];
    *(_OWORD *)&inputA.BNNSDataType data_type = v28;
    *(_OWORD *)&inputA.table_BNNSDataType data_type = a1[10];
    long long v29 = a1[5];
    *(_OWORD *)&inputA.size[7] = a1[4];
    *(_OWORD *)&inputA.stride[1] = v29;
    long long v30 = a1[7];
    *(_OWORD *)&inputA.stride[3] = a1[6];
    *(_OWORD *)&inputA.stride[5] = v30;
    long long v31 = a1[1];
    *(_OWORD *)&inputA.flags = *a1;
    *(_OWORD *)&inputA.size[1] = v31;
    long long v32 = a1[3];
    *(_OWORD *)&inputA.size[3] = a1[2];
    *(_OWORD *)&inputA.size[5] = v32;
    long long v33 = a3[9];
    *(_OWORD *)&inputB.stride[7] = a3[8];
    *(_OWORD *)&inputB.BNNSDataType data_type = v33;
    *(_OWORD *)&inputB.table_BNNSDataType data_type = a3[10];
    long long v34 = a3[5];
    *(_OWORD *)&inputB.size[7] = a3[4];
    *(_OWORD *)&inputB.stride[1] = v34;
    long long v35 = a3[7];
    *(_OWORD *)&inputB.stride[3] = a3[6];
    *(_OWORD *)&inputB.stride[5] = v35;
    long long v36 = a3[1];
    *(_OWORD *)&inputB.flags = *a3;
    *(_OWORD *)&inputB.size[1] = v36;
    long long v37 = a3[3];
    *(_OWORD *)&inputB.size[3] = a3[2];
    *(_OWORD *)&inputB.size[5] = v37;
    long long v38 = a5[9];
    *(_OWORD *)&output.stride[7] = a5[8];
    *(_OWORD *)&output.BNNSDataType data_type = v38;
    *(_OWORD *)&output.table_BNNSDataType data_type = a5[10];
    long long v39 = a5[5];
    *(_OWORD *)&output.size[7] = a5[4];
    *(_OWORD *)&output.stride[1] = v39;
    long long v40 = a5[7];
    *(_OWORD *)&output.stride[3] = a5[6];
    *(_OWORD *)&output.stride[5] = v40;
    long long v41 = a5[1];
    *(_OWORD *)&output.flags = *a5;
    *(_OWORD *)&output.size[1] = v41;
    long long v42 = a5[3];
    *(_OWORD *)&output.size[3] = a5[2];
    *(_OWORD *)&output.size[5] = v42;
    BOOL v25 = a2 & 1;
    BOOL v26 = a4 & 1;
    unint64_t v27 = (const BNNSFilterParameters *)&v44;
  }
  unint64_t result = BNNSMatMulWorkspaceSize(v25, v26, a9, &inputA, &inputB, &output, v27);
  if (result > 0x7FFFFFFFFFFFFFFELL) {
    return 0;
  }
  return result;
}

uint64_t static BNNS.applyMatrixMultiplication(inputA:transposed:inputB:transposed:output:alpha:workspace:filterParameters:)(_OWORD *a1, char a2, _OWORD *a3, char a4, _OWORD *a5, void *a6, float a7, uint64_t a8, char a9, uint32_t a10, size_t a11, int (__cdecl *a12)(void **, size_t, size_t), void (__cdecl *a13)(void *))
{
  uint64_t v49 = *MEMORY[0x1E4F143B8];
  if (a12 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    long long v13 = a1[9];
    *(_OWORD *)&inputA.stride[7] = a1[8];
    *(_OWORD *)&inputA.BNNSDataType data_type = v13;
    *(_OWORD *)&inputA.table_BNNSDataType data_type = a1[10];
    long long v14 = a1[5];
    *(_OWORD *)&inputA.size[7] = a1[4];
    *(_OWORD *)&inputA.stride[1] = v14;
    long long v15 = a1[7];
    *(_OWORD *)&inputA.stride[3] = a1[6];
    *(_OWORD *)&inputA.stride[5] = v15;
    long long v16 = a1[1];
    *(_OWORD *)&inputA.flags = *a1;
    *(_OWORD *)&inputA.size[1] = v16;
    long long v17 = a1[3];
    *(_OWORD *)&inputA.size[3] = a1[2];
    *(_OWORD *)&inputA.size[5] = v17;
    long long v18 = a3[9];
    *(_OWORD *)&inputB.stride[7] = a3[8];
    *(_OWORD *)&inputB.BNNSDataType data_type = v18;
    *(_OWORD *)&inputB.table_BNNSDataType data_type = a3[10];
    long long v19 = a3[5];
    *(_OWORD *)&inputB.size[7] = a3[4];
    *(_OWORD *)&inputB.stride[1] = v19;
    long long v20 = a3[7];
    *(_OWORD *)&inputB.stride[3] = a3[6];
    *(_OWORD *)&inputB.stride[5] = v20;
    long long v21 = a3[1];
    *(_OWORD *)&inputB.flags = *a3;
    *(_OWORD *)&inputB.size[1] = v21;
    long long v22 = a3[3];
    *(_OWORD *)&inputB.size[3] = a3[2];
    *(_OWORD *)&inputB.size[5] = v22;
    long long v23 = a5[9];
    *(_OWORD *)&output.stride[7] = a5[8];
    *(_OWORD *)&output.BNNSDataType data_type = v23;
    *(_OWORD *)&output.table_BNNSDataType data_type = a5[10];
    long long v24 = a5[5];
    *(_OWORD *)&output.size[7] = a5[4];
    *(_OWORD *)&output.stride[1] = v24;
    long long v25 = a5[7];
    *(_OWORD *)&output.stride[3] = a5[6];
    *(_OWORD *)&output.stride[5] = v25;
    long long v26 = a5[1];
    *(_OWORD *)&output.flags = *a5;
    *(_OWORD *)&output.size[1] = v26;
    long long v27 = a5[3];
    if (a9) {
      a6 = 0;
    }
    *(_OWORD *)&output.size[3] = a5[2];
    *(_OWORD *)&output.size[5] = v27;
    uint64_t result = BNNSMatMul(a2 & 1, a4 & 1, a7, &inputA, &inputB, &output, a6, 0);
    if (!result) {
      return result;
    }
LABEL_9:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *int v44 = 0;
    return swift_willThrow();
  }
  v45.flags = a10;
  v45.n_threads = a11;
  v45.alloc_memory = a12;
  v45.free_memory = a13;
  long long v29 = a1[9];
  *(_OWORD *)&inputA.stride[7] = a1[8];
  *(_OWORD *)&inputA.BNNSDataType data_type = v29;
  *(_OWORD *)&inputA.table_BNNSDataType data_type = a1[10];
  long long v30 = a1[5];
  *(_OWORD *)&inputA.size[7] = a1[4];
  *(_OWORD *)&inputA.stride[1] = v30;
  long long v31 = a1[7];
  *(_OWORD *)&inputA.stride[3] = a1[6];
  *(_OWORD *)&inputA.stride[5] = v31;
  long long v32 = a1[1];
  *(_OWORD *)&inputA.flags = *a1;
  *(_OWORD *)&inputA.size[1] = v32;
  long long v33 = a1[3];
  *(_OWORD *)&inputA.size[3] = a1[2];
  *(_OWORD *)&inputA.size[5] = v33;
  long long v34 = a3[9];
  *(_OWORD *)&inputB.stride[7] = a3[8];
  *(_OWORD *)&inputB.BNNSDataType data_type = v34;
  *(_OWORD *)&inputB.table_BNNSDataType data_type = a3[10];
  long long v35 = a3[5];
  *(_OWORD *)&inputB.size[7] = a3[4];
  *(_OWORD *)&inputB.stride[1] = v35;
  long long v36 = a3[7];
  *(_OWORD *)&inputB.stride[3] = a3[6];
  *(_OWORD *)&inputB.stride[5] = v36;
  long long v37 = a3[1];
  *(_OWORD *)&inputB.flags = *a3;
  *(_OWORD *)&inputB.size[1] = v37;
  long long v38 = a3[3];
  *(_OWORD *)&inputB.size[3] = a3[2];
  *(_OWORD *)&inputB.size[5] = v38;
  long long v39 = a5[9];
  *(_OWORD *)&output.stride[7] = a5[8];
  *(_OWORD *)&output.BNNSDataType data_type = v39;
  *(_OWORD *)&output.table_BNNSDataType data_type = a5[10];
  long long v40 = a5[5];
  *(_OWORD *)&output.size[7] = a5[4];
  *(_OWORD *)&output.stride[1] = v40;
  long long v41 = a5[7];
  *(_OWORD *)&output.stride[3] = a5[6];
  *(_OWORD *)&output.stride[5] = v41;
  long long v42 = a5[1];
  *(_OWORD *)&output.flags = *a5;
  *(_OWORD *)&output.size[1] = v42;
  long long v43 = a5[3];
  if (a9) {
    a6 = 0;
  }
  *(_OWORD *)&output.size[3] = a5[2];
  *(_OWORD *)&output.size[5] = v43;
  uint64_t result = BNNSMatMul(a2 & 1, a4 & 1, a7, &inputA, &inputB, &output, a6, &v45);
  if (result) {
    goto LABEL_9;
  }
  return result;
}

double BNNS.FusedQuantizationParameters.layerParameters(input:output:)@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, uint64_t *a3@<X8>)
{
  *(void *)&double result = BNNS.FusedQuantizationParameters.layerParameters(input:output:)(a1, a2, 0, a3).n128_u64[0];
  return result;
}

uint64_t BNNS.FusedQuantizationParameters.axis.getter()
{
  return *(void *)v0;
}

uint64_t BNNS.FusedQuantizationParameters.axis.setter(uint64_t result, char a2)
{
  *(void *)uint64_t v2 = result;
  *(unsigned char *)(v2 + 8) = a2 & 1;
  return result;
}

uint64_t (*BNNS.FusedQuantizationParameters.axis.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedQuantizationParameters.scale.setter(uint64_t a1)
{
  return outlined init with take of BNNSNDArrayDescriptor?(a1, v1 + 16);
}

uint64_t (*BNNS.FusedQuantizationParameters.scale.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedQuantizationParameters.bias.setter(uint64_t a1)
{
  return outlined init with take of BNNSNDArrayDescriptor?(a1, v1 + 200);
}

uint64_t (*BNNS.FusedQuantizationParameters.bias.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

double protocol witness for FusableLayerParametersWrapper.layerParameters(input:output:) in conformance BNNS.FusedQuantizationParameters@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, uint64_t *a3@<X8>)
{
  *(void *)&double result = BNNS.FusedQuantizationParameters.layerParameters(input:output:)(a1, a2, 0, a3).n128_u64[0];
  return result;
}

uint64_t protocol witness for FusableLayerParametersWrapper.filterType.getter in conformance BNNS.FusedQuantizationParameters()
{
  return 7;
}

double BNNS.FusedDequantizationParameters.layerParameters(input:output:)@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, uint64_t *a3@<X8>)
{
  *(void *)&double result = BNNS.FusedQuantizationParameters.layerParameters(input:output:)(a1, a2, 1, a3).n128_u64[0];
  return result;
}

__n128 BNNS.FusedQuantizationParameters.layerParameters(input:output:)@<Q0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, int a3@<W2>, uint64_t *a4@<X8>)
{
  uint64_t v8 = *(void *)v4;
  int v69 = *(unsigned __int8 *)(v4 + 8);
  outlined init with take of BNNSNDArrayDescriptor?(v4 + 16, (uint64_t)v82);
  outlined init with take of BNNSNDArrayDescriptor?(v4 + 200, (uint64_t)v83);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v82, (uint64_t)v81);
  uint64_t v9 = 0;
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v81) == 1)
  {
    uint64_t v66 = 0;
    uint64_t v67 = 0;
    uint64_t v64 = 0;
    uint64_t v65 = 0;
    uint64_t v62 = 0;
    uint64_t v63 = 0;
    uint64_t v60 = 0;
    uint64_t v61 = 0;
    uint64_t v58 = 0;
    uint64_t v59 = 0;
    uint64_t v56 = 0;
    uint64_t v57 = 0;
    uint64_t v75 = 0;
    uint64_t v76 = 0;
    uint64_t v73 = 0;
    uint64_t v74 = 0;
    uint64_t v71 = 0;
    uint64_t v72 = 0;
    int v70 = 0;
    uint64_t v77 = 0;
    uint64_t v10 = 0;
    uint64_t v68 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v82, (uint64_t)v79);
    uint64_t v77 = *(void *)v79;
    uint64_t v66 = *(void *)&v79[16];
    uint64_t v67 = *(void *)&v79[8];
    uint64_t v64 = *(void *)&v79[32];
    uint64_t v65 = *(void *)&v79[24];
    uint64_t v62 = *(void *)&v79[48];
    uint64_t v63 = *(void *)&v79[40];
    uint64_t v60 = *(void *)&v79[64];
    uint64_t v61 = *(void *)&v79[56];
    uint64_t v58 = *(void *)&v79[80];
    uint64_t v59 = *(void *)&v79[72];
    uint64_t v56 = *(void *)&v79[96];
    uint64_t v57 = *(void *)&v79[88];
    uint64_t v75 = *(void *)&v79[112];
    uint64_t v76 = *(void *)&v79[104];
    uint64_t v73 = *(void *)&v79[128];
    uint64_t v74 = *(void *)&v79[120];
    uint64_t v71 = *(void *)&v79[152];
    uint64_t v68 = *(void *)&v79[144];
    uint64_t v72 = *(void *)&v79[136];
    uint64_t v10 = *(void *)&v79[164];
    int v70 = *(_DWORD *)&v79[160];
    int v40 = *(_DWORD *)&v79[172];
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v83, (uint64_t)v80);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v80) == 1)
  {
    int v11 = 0;
    uint64_t v12 = 0;
    uint64_t v13 = 0;
    uint64_t v14 = 0;
    uint64_t v15 = 0;
    uint64_t v16 = 0;
    uint64_t v42 = 0;
    uint64_t v43 = 0;
    uint64_t v44 = 0;
    uint64_t v45 = 0;
    uint64_t v46 = 0;
    uint64_t v47 = 0;
    uint64_t v48 = 0;
    uint64_t v49 = 0;
    uint64_t v50 = 0;
    uint64_t v51 = 0;
    uint64_t v52 = 0;
    uint64_t v53 = 0;
    uint64_t v55 = 0;
    uint64_t v54 = 0;
    __n128 v41 = 0u;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v83, (uint64_t)v79);
    uint64_t v55 = *(void *)&v79[8];
    uint64_t v54 = *(void *)v79;
    uint64_t v52 = *(void *)&v79[24];
    uint64_t v53 = *(void *)&v79[16];
    uint64_t v50 = *(void *)&v79[40];
    uint64_t v51 = *(void *)&v79[32];
    uint64_t v48 = *(void *)&v79[56];
    uint64_t v49 = *(void *)&v79[48];
    uint64_t v46 = *(void *)&v79[72];
    uint64_t v47 = *(void *)&v79[64];
    uint64_t v44 = *(void *)&v79[88];
    uint64_t v45 = *(void *)&v79[80];
    uint64_t v16 = *(void *)&v79[112];
    uint64_t v42 = *(void *)&v79[104];
    uint64_t v43 = *(void *)&v79[96];
    uint64_t v15 = *(void *)&v79[120];
    uint64_t v13 = *(void *)&v79[136];
    uint64_t v14 = *(void *)&v79[128];
    v17.n128_u64[0] = *(void *)&v79[144];
    __n128 v41 = v17;
    uint64_t v12 = *(void *)&v79[152];
    uint64_t v9 = *(void *)&v79[164];
    int v11 = *(_DWORD *)&v79[160];
    int v39 = *(_DWORD *)&v79[172];
  }
  long long v18 = a1[6];
  *(_OWORD *)&v79[116] = a1[7];
  long long v19 = a1[9];
  *(_OWORD *)&v79[132] = a1[8];
  *(_OWORD *)&v79[148] = v19;
  *(_OWORD *)&v79[164] = a1[10];
  long long v20 = a1[2];
  *(_OWORD *)&v79[52] = a1[3];
  long long v21 = a1[5];
  *(_OWORD *)&v79[68] = a1[4];
  *(_OWORD *)&v79[84] = v21;
  *(_OWORD *)&v79[100] = v18;
  long long v22 = a1[1];
  *(_OWORD *)&v79[4] = *a1;
  *(_OWORD *)&v79[20] = v22;
  int v23 = v69;
  if ((unint64_t)(v8 - 65) < 0xFFFFFFFFFFFFFF7FLL) {
    int v23 = 1;
  }
  if (v8 < 0) {
    int v23 = 1;
  }
  uint64_t v24 = 1 << v8;
  if ((unint64_t)v8 >= 0x40) {
    uint64_t v24 = 0;
  }
  if (v23) {
    uint64_t v25 = 0;
  }
  else {
    uint64_t v25 = v24;
  }
  *(_OWORD *)&v79[36] = v20;
  type metadata accessor for BNNSLayerParametersQuantization(0);
  a4[3] = v26;
  a4[4] = (uint64_t)&protocol witness table for BNNSLayerParametersQuantization;
  uint64_t v27 = swift_allocObject();
  *a4 = v27;
  *(void *)(v27 + 16) = v25;
  *(_DWORD *)(v27 + 24) = a3;
  *(void *)(v27 + 392) = v67;
  *(void *)(v27 + 400) = v66;
  *(void *)(v27 + 408) = v65;
  *(void *)(v27 + 416) = v64;
  *(void *)(v27 + 424) = v63;
  *(void *)(v27 + 432) = v62;
  *(void *)(v27 + 440) = v61;
  *(void *)(v27 + 448) = v60;
  *(void *)(v27 + 456) = v59;
  *(void *)(v27 + 464) = v58;
  *(void *)(v27 + 472) = v57;
  *(void *)(v27 + 480) = v56;
  *(_DWORD *)(v27 + 204) = *(_DWORD *)&v79[176];
  long long v28 = *(_OWORD *)&v79[144];
  *(_OWORD *)(v27 + 156) = *(_OWORD *)&v79[128];
  *(_OWORD *)(v27 + 172) = v28;
  *(_OWORD *)(v27 + 188) = *(_OWORD *)&v79[160];
  long long v29 = *(_OWORD *)&v79[80];
  *(_OWORD *)(v27 + 92) = *(_OWORD *)&v79[64];
  *(_OWORD *)(v27 + 108) = v29;
  long long v30 = *(_OWORD *)&v79[112];
  *(_OWORD *)(v27 + 124) = *(_OWORD *)&v79[96];
  *(_OWORD *)(v27 + 140) = v30;
  long long v31 = *(_OWORD *)&v79[16];
  *(_OWORD *)(v27 + 28) = *(_OWORD *)v79;
  *(_OWORD *)(v27 + 44) = v31;
  long long v32 = *(_OWORD *)&v79[48];
  *(_OWORD *)(v27 + 60) = *(_OWORD *)&v79[32];
  *(_OWORD *)(v27 + 76) = v32;
  long long v33 = a2[9];
  *(_OWORD *)(v27 + 336) = a2[8];
  *(_OWORD *)(v27 + 352) = v33;
  *(_OWORD *)(v27 + 368) = a2[10];
  long long v34 = a2[5];
  *(_OWORD *)(v27 + 272) = a2[4];
  *(_OWORD *)(v27 + 288) = v34;
  long long v35 = a2[7];
  *(_OWORD *)(v27 + 304) = a2[6];
  *(_OWORD *)(v27 + 320) = v35;
  long long v36 = a2[1];
  *(_OWORD *)(v27 + 208) = *a2;
  *(_OWORD *)(v27 + 224) = v36;
  long long v37 = a2[3];
  *(_OWORD *)(v27 + 240) = a2[2];
  *(_OWORD *)(v27 + 256) = v37;
  *(void *)(v27 + 384) = v77;
  *(void *)(v27 + 488) = v76;
  *(void *)(v27 + 496) = v75;
  *(void *)(v27 + 504) = v74;
  *(void *)(v27 + 512) = v73;
  *(void *)(v27 + 520) = v72;
  *(void *)(v27 + 528) = v68;
  *(void *)(v27 + 536) = v71;
  *(_DWORD *)(v27 + 544) = v70;
  *(void *)(v27 + 548) = v10;
  *(_DWORD *)(v27 + 556) = v40;
  *(void *)(v27 + 560) = v54;
  *(void *)(v27 + 568) = v55;
  *(void *)(v27 + 576) = v53;
  *(void *)(v27 + 584) = v52;
  *(void *)(v27 + 592) = v51;
  *(void *)(v27 + 600) = v50;
  *(void *)(v27 + 608) = v49;
  *(void *)(v27 + 616) = v48;
  *(void *)(v27 + 624) = v47;
  *(void *)(v27 + 632) = v46;
  *(void *)(v27 + 640) = v45;
  *(void *)(v27 + 648) = v44;
  *(void *)(v27 + 656) = v43;
  *(void *)(v27 + 664) = v42;
  *(void *)(v27 + 672) = v16;
  *(void *)(v27 + 680) = v15;
  *(void *)(v27 + 688) = v14;
  *(void *)(v27 + 696) = v13;
  __n128 result = v41;
  *(void *)(v27 + 704) = v41.n128_u64[0];
  *(void *)(v27 + 712) = v12;
  *(_DWORD *)(v27 + 720) = v11;
  *(void *)(v27 + 724) = v9;
  *(_DWORD *)(v27 + 732) = v39;
  return result;
}

void *BNNS.FusedQuantizationParameters.init(scale:bias:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, void *a3@<X8>)
{
  outlined init with take of BNNSNDArrayDescriptor?(a2, (uint64_t)v7);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v7, (uint64_t)v8);
  outlined init with take of BNNSNDArrayDescriptor?(a1, (uint64_t)v6);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v6, (uint64_t)v9);
  __src[0] = 0;
  LOBYTE(__src[1]) = 1;
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v9, (uint64_t)&__src[2]);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v8, (uint64_t)&__src[25]);
  return memcpy(a3, __src, 0x179uLL);
}

uint64_t (*BNNS.FusedDequantizationParameters.axis.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedQuantizationParameters.scale.getter@<X0>(uint64_t a1@<X8>)
{
  outlined init with take of BNNSNDArrayDescriptor?(v1 + 16, (uint64_t)v4);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v4, a1);
}

uint64_t (*BNNS.FusedDequantizationParameters.scale.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedQuantizationParameters.bias.getter@<X0>(uint64_t a1@<X8>)
{
  outlined init with take of BNNSNDArrayDescriptor?(v1 + 200, (uint64_t)v4);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v4, a1);
}

uint64_t (*BNNS.FusedDequantizationParameters.bias.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

double protocol witness for FusableLayerParametersWrapper.layerParameters(input:output:) in conformance BNNS.FusedDequantizationParameters@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, uint64_t *a3@<X8>)
{
  *(void *)&double result = BNNS.FusedQuantizationParameters.layerParameters(input:output:)(a1, a2, 1, a3).n128_u64[0];
  return result;
}

void *__swift_memcpy377_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x179uLL);
}

uint64_t getEnumTagSinglePayload for BNNS.FusedQuantizationParameters(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 377)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for BNNS.FusedQuantizationParameters(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(unsigned char *)(result + 376) = 0;
    *(_OWORD *)(result + 360) = 0u;
    *(_OWORD *)(result + 344) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(void *)double result = (a2 - 1);
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 377) = v3;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.FusedQuantizationParameters()
{
  return &type metadata for BNNS.FusedQuantizationParameters;
}

ValueMetadata *type metadata accessor for BNNS.FusedDequantizationParameters()
{
  return &type metadata for BNNS.FusedDequantizationParameters;
}

uint64_t sub_1D21162D4()
{
  return MEMORY[0x1F4186498](v0, 736, 7);
}

uint64_t vImage.PixelBuffer<>.histogram()()
{
  uint64_t v10 = *MEMORY[0x1E4F143B8];
  uint64_t v1 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(void *)(v1 + 16) = 256;
  bzero((void *)(v1 + 32), 0x800uLL);
  uint64_t v2 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(void *)(v2 + 16) = 256;
  bzero((void *)(v2 + 32), 0x800uLL);
  uint64_t v3 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(void *)(v3 + 16) = 256;
  bzero((void *)(v3 + 32), 0x800uLL);
  uint64_t v4 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(void *)(v4 + 16) = 256;
  bzero((void *)(v4 + 32), 0x800uLL);
  uint64_t v5 = *v0;
  if (!*(void *)(*v0 + 16)) {
    __break(1u);
  }
  long long v6 = *(_OWORD *)(v5 + 48);
  *(_OWORD *)&src.char data = *(_OWORD *)(v5 + 32);
  *(_OWORD *)&src.width = v6;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<UInt>?>);
  uint64_t inited = swift_initStackObject();
  *(void *)(inited + 32) = v1 + 32;
  *(void *)(inited + 40) = v2 + 32;
  *(void *)(inited + 48) = v3 + 32;
  *(void *)(inited + 56) = v4 + 32;
  vImageHistogramCalculation_ARGB8888(&src, (vImagePixelCount **)(inited + 32), 0);
  swift_setDeallocating();
  return v1;
}

uint64_t vImage.PixelBuffer<>.histogram(binCount:)@<X0>(unint64_t a1@<X0>, unint64_t *a2@<X8>)
{
  uint64_t v14 = *MEMORY[0x1E4F143B8];
  if ((a1 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
  }
  if (a1)
  {
    uint64_t v5 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v5 + 16) = a1;
    bzero((void *)(v5 + 32), 8 * a1);
    uint64_t v6 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v6 + 16) = a1;
    bzero((void *)(v6 + 32), 8 * a1);
    uint64_t v7 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v7 + 16) = a1;
    bzero((void *)(v7 + 32), 8 * a1);
    uint64_t v8 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v8 + 16) = a1;
    bzero((void *)(v8 + 32), 8 * a1);
  }
  else
  {
    uint64_t v8 = MEMORY[0x1E4FBC860];
    uint64_t v7 = MEMORY[0x1E4FBC860];
    uint64_t v6 = MEMORY[0x1E4FBC860];
    uint64_t v5 = MEMORY[0x1E4FBC860];
  }
  uint64_t v9 = *v2;
  if (!*(void *)(*v2 + 16)) {
    goto LABEL_9;
  }
  long long v10 = *(_OWORD *)(v9 + 48);
  *(_OWORD *)&src.char data = *(_OWORD *)(v9 + 32);
  *(_OWORD *)&src.width = v10;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<UInt>?>);
  uint64_t inited = swift_initStackObject();
  *(void *)(inited + 32) = v5 + 32;
  *(void *)(inited + 40) = v6 + 32;
  *(void *)(inited + 48) = v7 + 32;
  *(void *)(inited + 56) = v8 + 32;
  if (HIDWORD(a1)) {
    goto LABEL_10;
  }
  vImageHistogramCalculation_ARGBFFFF(&src, (vImagePixelCount **)(inited + 32), a1, 0.0, 1.0, 0);
  uint64_t result = swift_setDeallocating();
  *a2 = a1;
  a2[1] = v5;
  a2[2] = v6;
  a2[3] = v7;
  a2[4] = v8;
  return result;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(unint64_t a1, unint64_t a2, unint64_t a3, unint64_t a4, unint64_t a5, uint64_t a6)
{
  uint64_t v25 = *MEMORY[0x1E4F143B8];
  uint64_t v7 = *(void **)v6;
  if (!*(void *)(*(void *)v6 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  vImagePixelCount v8 = v7[6];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v9 = v7[5];
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v8)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v9)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  long long v10 = *(void **)a6;
  if (!*(void *)(*(void *)a6 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v11 = v10[6];
  if (v11 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v12 = v10[5];
  if (v12 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v11)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v12)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v8 != v11)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v9 != v12) {
    goto LABEL_25;
  }
  uint64_t v13 = (void *)v7[4];
  size_t v14 = v7[7];
  v24.char data = v13;
  v24.height = v9;
  v24.width = v8;
  v24.rowBytes = v14;
  uint64_t v15 = (void *)v10[4];
  size_t v16 = v10[7];
  v23.char data = v15;
  v23.height = v9;
  v23.width = v8;
  v23.rowBytes = v16;
  uint64_t v17 = *(void *)(a2 + 16);
  uint64_t v18 = *(void *)(a3 + 16);
  uint64_t v19 = *(void *)(a4 + 16);
  uint64_t v20 = *(void *)(a5 + 16);
  v22[0] = a1;
  v22[1] = a2;
  void v22[2] = a3;
  _OWORD v22[3] = a4;
  v22[4] = a5;
  return closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in vImage.PixelBuffer<>.specifyHistogram(_:destination:)(a5 + 32, v20, a2 + 32, v17, a3 + 32, v18, a4 + 32, v19, &v24, &v23, v22);
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in vImage.PixelBuffer<>.specifyHistogram(_:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, const vImage_Buffer *a9, const vImage_Buffer *a10, unint64_t *a11)
{
  unint64_t v15 = *a11;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<UInt>?>);
  uint64_t result = swift_initStackObject();
  *(void *)(result + 32) = a3;
  *(void *)(result + 40) = a5;
  *(void *)(result + 48) = a7;
  *(void *)(result + 56) = a1;
  if ((v15 & 0x8000000000000000) != 0)
  {
    __break(1u);
  }
  else if (!HIDWORD(v15))
  {
    vImageHistogramSpecification_ARGBFFFF(a9, a10, 0, (const vImagePixelCount **)(result + 32), v15, 0.0, 1.0, 0);
    return swift_setDeallocating();
  }
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v24 = *MEMORY[0x1E4F143B8];
  uint64_t v6 = *(void **)v5;
  if (!*(void *)(*(void *)v5 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  vImagePixelCount v7 = v6[6];
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v8 = v6[5];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v7)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v8)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  vImagePixelCount v9 = *(void **)a5;
  if (!*(void *)(*(void *)a5 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v10 = v9[6];
  if (v10 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v11 = v9[5];
  if (v11 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v10)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v11)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v7 != v10)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v8 != v11) {
    goto LABEL_25;
  }
  uint64_t v12 = (void *)v6[4];
  size_t v13 = v6[7];
  src.char data = v12;
  src.height = v8;
  src.width = v7;
  src.rowBytes = v13;
  size_t v14 = (void *)v9[4];
  size_t v15 = v9[7];
  dest.char data = v14;
  dest.height = v8;
  uint64_t v16 = a1 + 32;
  uint64_t v17 = a2 + 32;
  uint64_t v18 = a3 + 32;
  uint64_t v19 = a4 + 32;
  dest.width = v7;
  dest.rowBytes = v15;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<UInt>?>);
  uint64_t inited = swift_initStackObject();
  *(void *)(inited + 32) = v16;
  *(void *)(inited + 40) = v17;
  *(void *)(inited + 48) = v18;
  *(void *)(inited + 56) = v19;
  vImageHistogramSpecification_ARGB8888(&src, &dest, (const vImagePixelCount **)(inited + 32), 0);
  return swift_setDeallocating();
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1)
{
  uint64_t v1 = (uint64_t (*)(void *, void *, void))MEMORY[0x1E4F16FF8];

  return vImage.PixelBuffer<>.equalizeHistogram(destination:)(a1, v1);
}

{
  uint64_t (*v1)(void *, void *, void);
  uint64_t vars8;

  uint64_t v1 = (uint64_t (*)(void *, void *, void))MEMORY[0x1E4F16FE8];

  return vImage.PixelBuffer<>.equalizeHistogram(destination:)(a1, v1);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3 = (uint64_t (*)(uint64_t, uint64_t, void))MEMORY[0x1E4F16FF8];

  return vImage.PixelBuffer<>.equalizeHistogram(destination:)(a1, a2, a3, v3);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t a2)
{
  uint64_t v2 = (uint64_t (*)(void *, void *, void, unint64_t, void, double, float))MEMORY[0x1E4F17000];

  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, v2);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4 = (uint64_t (*)(uint64_t, uint64_t, void, unint64_t, void, double, float))MEMORY[0x1E4F17000];

  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, a3, a4, v4);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t *a2)
{
  uint64_t v2 = (uint64_t (*)(_OWORD *, _OWORD *, void, unint64_t, void, double, float))MEMORY[0x1E4F16FF0];

  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, v2);
}

uint64_t vImage.PixelBuffer<>.contrastStretch(destination:)(uint64_t a1)
{
  uint64_t v1 = (uint64_t (*)(void *, void *, void))MEMORY[0x1E4F16FA0];

  return vImage.PixelBuffer<>.equalizeHistogram(destination:)(a1, v1);
}

{
  uint64_t (*v1)(void *, void *, void);
  uint64_t vars8;

  uint64_t v1 = (uint64_t (*)(void *, void *, void))MEMORY[0x1E4F16F90];

  return vImage.PixelBuffer<>.equalizeHistogram(destination:)(a1, v1);
}

uint64_t vImage.PixelBuffer<>.contrastStretch(destination:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3 = (uint64_t (*)(uint64_t, uint64_t, void))MEMORY[0x1E4F16FA0];

  return vImage.PixelBuffer<>.equalizeHistogram(destination:)(a1, a2, a3, v3);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(uint64_t, uint64_t, void))
{
  uint64_t v18 = *MEMORY[0x1E4F143B8];
  uint64_t v7 = *(void *)(a2 + 16);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 32))(v7, a3);
  if (result < 0) {
    goto LABEL_8;
  }
  uint64_t v9 = result;
  if (result)
  {
    uint64_t v10 = 0;
    unint64_t v11 = 0;
    while (1)
    {
      uint64_t v12 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v11 >= *(void *)(v12 + 16)) {
        break;
      }
      long long v15 = *(_OWORD *)(v12 + v10 + 48);
      long long v16 = *(_OWORD *)(v12 + v10 + 32);
      swift_bridgeObjectRelease();
      v17[0] = v16;
      v17[1] = v15;
      uint64_t result = closure #1 in vImage.PixelBuffer<>.equalizeHistogram(destination:)((uint64_t)v17, a1, v11, v4, v7, a3, v13, a4);
      v10 += 32;
      if (v9 == ++v11) {
        return result;
      }
    }
    __break(1u);
LABEL_8:
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1, uint64_t a2, unint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, void))
{
  uint64_t v22 = *MEMORY[0x1E4F143B8];
  type metadata accessor for vImage.PixelBuffer();
  uint64_t v15 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  if (*(void *)(v15 + 16) <= a3) {
    goto LABEL_5;
  }
  unint64_t v16 = v15 + 32 * a3;
  long long v19 = *(_OWORD *)(v16 + 48);
  long long v20 = *(_OWORD *)(v16 + 32);
  swift_bridgeObjectRelease();
  v21[0] = v20;
  v21[1] = v19;
  return closure #1 in closure #1 in vImage.PixelBuffer<>.equalizeHistogram(destination:)((uint64_t)v21, a4, a3, a2, a1, a5, a6, v17, a8);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1, uint64_t a2, unint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t (*a9)(uint64_t, uint64_t, void))
{
  type metadata accessor for vImage.PixelBuffer();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  uint64_t v12 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_10;
  }
  if (*(void *)(v12 + 16) <= a3)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  uint64_t v19 = a1;
  uint64_t v13 = *(void *)(v12 + 32 * a3 + 48);
  swift_bridgeObjectRelease();
  uint64_t v14 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(void *)(v14 + 16) <= a3)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  uint64_t v15 = *(void *)(v14 + 32 * a3 + 48);
  swift_bridgeObjectRelease();
  if (v13 != v15)
  {
LABEL_12:
    swift_bridgeObjectRelease();
    uint64_t result = swift_bridgeObjectRelease();
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  uint64_t result = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(void *)(result + 16) <= a3)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v17 = *(void *)(result + 32 * a3 + 40);
  swift_bridgeObjectRelease();
  uint64_t result = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(void *)(result + 16) > a3)
  {
    uint64_t v18 = *(void *)(result + 32 * a3 + 40);
    swift_bridgeObjectRelease_n();
    swift_bridgeObjectRelease_n();
    uint64_t result = swift_bridgeObjectRelease();
    if (v17 == v18) {
      return a9(a5, v19, 0);
    }
    goto LABEL_13;
  }
LABEL_15:
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(destination:)(uint64_t a1, uint64_t (*a2)(void *, void *, void))
{
  void v15[4] = *MEMORY[0x1E4F143B8];
  uint64_t v3 = *(void **)v2;
  if (!*(void *)(*(void *)v2 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v4 = v3[6];
  if (v4 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v5 = v3[5];
  if (v5 < 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v4)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v5)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v6 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v7 = v6[6];
  if (v7 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v8 = v6[5];
  if (v8 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v7)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v8)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v4 != v7)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v5 != v8) {
    goto LABEL_25;
  }
  uint64_t v9 = v3[4];
  uint64_t v10 = v3[7];
  v15[0] = v9;
  v15[1] = v5;
  void v15[2] = v4;
  v15[3] = v10;
  uint64_t v11 = v6[4];
  uint64_t v12 = v6[7];
  v14[0] = v11;
  v14[1] = v5;
  v14[2] = v4;
  _OWORD v14[3] = v12;
  return a2(v15, v14, 0);
}

uint64_t vImage.PixelBuffer<>.contrastStretch(binCount:destination:)(unint64_t a1, uint64_t a2)
{
  uint64_t v2 = (uint64_t (*)(void *, void *, void, unint64_t, void, double, float))MEMORY[0x1E4F16FA8];

  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, v2);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t a2, uint64_t (*a3)(void *, void *, void, unint64_t, void, double, float))
{
  v16[4] = *MEMORY[0x1E4F143B8];
  uint64_t v4 = *(void **)v3;
  if (!*(void *)(*(void *)v3 + 16))
  {
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v5 = v4[6];
  if (v5 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v6 = v4[5];
  if (v6 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v5)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v6)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = *(void **)a2;
  if (!*(void *)(*(void *)a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v8 = v7[6];
  if (v8 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v9 = v7[5];
  if (v9 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v8)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v9)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v5 != v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v6 != v9)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  uint64_t v10 = v4[4];
  uint64_t v11 = v4[7];
  v16[0] = v10;
  v16[1] = v6;
  _OWORD v16[2] = v5;
  v16[3] = v11;
  uint64_t v12 = v7[4];
  uint64_t v13 = v7[7];
  v15[0] = v12;
  v15[1] = v6;
  void v15[2] = v5;
  v15[3] = v13;
  if ((a1 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
LABEL_29:
    __break(1u);
  }
  if (HIDWORD(a1)) {
    goto LABEL_29;
  }
  return a3(v16, v15, 0, a1, 0, 0.0, 1.0);
}

uint64_t vImage.PixelBuffer<>.contrastStretch(binCount:destination:)(unint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4 = (uint64_t (*)(uint64_t, uint64_t, void, unint64_t, void, double, float))MEMORY[0x1E4F16FA8];

  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, a3, a4, v4);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(uint64_t, uint64_t, void, unint64_t, void, double, float))
{
  uint64_t v20 = *MEMORY[0x1E4F143B8];
  uint64_t v8 = *(void *)(a3 + 16);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 32))(v8, a4);
  if (result < 0) {
    goto LABEL_8;
  }
  uint64_t v10 = result;
  if (result)
  {
    uint64_t v11 = 0;
    unint64_t v12 = 0;
    while (1)
    {
      uint64_t v13 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v12 >= *(void *)(v13 + 16)) {
        break;
      }
      long long v17 = *(_OWORD *)(v13 + v11 + 48);
      long long v18 = *(_OWORD *)(v13 + v11 + 32);
      swift_bridgeObjectRelease();
      v19[0] = v18;
      v19[1] = v17;
      uint64_t result = closure #1 in vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)((uint64_t)v19, a2, v12, v5, a1, v8, a4, v14, a5);
      v11 += 32;
      if (v10 == ++v12) {
        return result;
      }
    }
    __break(1u);
LABEL_8:
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(uint64_t a1, uint64_t a2, unint64_t a3, uint64_t a4, unint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t (*a9)(uint64_t, uint64_t, void, unint64_t, void, double, float))
{
  uint64_t v23 = *MEMORY[0x1E4F143B8];
  type metadata accessor for vImage.PixelBuffer();
  uint64_t v16 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  if (*(void *)(v16 + 16) <= a3) {
    goto LABEL_5;
  }
  unint64_t v17 = v16 + 32 * a3;
  long long v20 = *(_OWORD *)(v17 + 48);
  long long v21 = *(_OWORD *)(v17 + 32);
  swift_bridgeObjectRelease();
  v22[0] = v21;
  v22[1] = v20;
  return closure #1 in closure #1 in vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)((uint64_t)v22, a4, a3, a2, a1, a5, a6, a7, v19, a9);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(uint64_t a1, uint64_t a2, unint64_t a3, uint64_t a4, uint64_t a5, unint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t (*a10)(uint64_t, uint64_t, void, unint64_t, void, double, float))
{
  type metadata accessor for vImage.PixelBuffer();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  uint64_t v13 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_12;
  }
  if (*(void *)(v13 + 16) <= a3)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  uint64_t v20 = a5;
  uint64_t v14 = *(void *)(v13 + 32 * a3 + 48);
  swift_bridgeObjectRelease();
  uint64_t v15 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(void *)(v15 + 16) <= a3)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  uint64_t v16 = *(void *)(v15 + 32 * a3 + 48);
  swift_bridgeObjectRelease();
  if (v14 != v16)
  {
LABEL_14:
    swift_bridgeObjectRelease();
    uint64_t result = swift_bridgeObjectRelease();
    goto LABEL_15;
  }
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  uint64_t result = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(void *)(result + 16) <= a3)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v18 = *(void *)(result + 32 * a3 + 40);
  swift_bridgeObjectRelease();
  uint64_t result = vImage.PixelBuffer<>.vImageBuffers.getter();
  if (*(void *)(result + 16) <= a3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v19 = *(void *)(result + 32 * a3 + 40);
  swift_bridgeObjectRelease_n();
  swift_bridgeObjectRelease_n();
  uint64_t result = swift_bridgeObjectRelease();
  if (v18 != v19)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  if ((a6 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!HIDWORD(a6)) {
    return a10(v20, a1, 0, a6, 0, 0.0, 1.0);
  }
LABEL_19:
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.contrastStretch(binCount:destination:)(unint64_t a1, uint64_t *a2)
{
  uint64_t v2 = (uint64_t (*)(_OWORD *, _OWORD *, void, unint64_t, void, double, float))MEMORY[0x1E4F16F98];

  return vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(a1, a2, v2);
}

uint64_t vImage.PixelBuffer<>.equalizeHistogram(binCount:destination:)(unint64_t a1, uint64_t *a2, uint64_t (*a3)(_OWORD *, _OWORD *, void, unint64_t, void, double, float))
{
  uint64_t v11 = *MEMORY[0x1E4F143B8];
  uint64_t v4 = *v3;
  if (!*(void *)(*v3 + 16))
  {
    __break(1u);
    goto LABEL_7;
  }
  long long v5 = *(_OWORD *)(v4 + 48);
  v10[0] = *(_OWORD *)(v4 + 32);
  v10[1] = v5;
  uint64_t v6 = *a2;
  if (!*(void *)(*a2 + 16))
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  long long v7 = *(_OWORD *)(v6 + 48);
  v9[0] = *(_OWORD *)(v6 + 32);
  v9[1] = v7;
  if ((a1 & 0x8000000000000000) != 0)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (HIDWORD(a1)) {
    goto LABEL_9;
  }
  return a3(v10, v9, 0, a1, 0, 0.0, 1.0);
}

void *vImage.PixelBuffer<>.histogram()()
{
  uint64_t result = specialized vImage.PixelBuffer<>._calculateHistogram()(*v0);
  unint64_t v2 = result[2];
  if (!v2)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (v2 == 1)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (v2 >= 3)
  {
    uint64_t v3 = result[4];
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRelease();
    return (void *)v3;
  }
LABEL_7:
  __break(1u);
  return result;
}

{
  uint64_t *v0;
  void *result;
  unint64_t v2;
  uint64_t v3;

  uint64_t result = specialized vImage.PixelBuffer<>._calculateHistogram()(*v0);
  unint64_t v2 = result[2];
  if (!v2)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (v2 == 1)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (v2 < 3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (v2 != 3)
  {
    uint64_t v3 = result[4];
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRelease();
    return (void *)v3;
  }
LABEL_9:
  __break(1u);
  return result;
}

void *specialized vImage.PixelBuffer<>._calculateHistogram()(uint64_t a1)
{
  uint64_t v30 = *MEMORY[0x1E4F143B8];
  src.char data = (void *)MEMORY[0x1E4FBC860];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 3, 0);
  char data = src.data;
  uint64_t v3 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(void *)(v3 + 16) = 256;
  bzero((void *)(v3 + 32), 0x800uLL);
  unint64_t v5 = data[2];
  unint64_t v4 = data[3];
  int64_t v6 = v5 + 1;
  if (v5 >= v4 >> 1)
  {
LABEL_24:
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v4 > 1), v6, 1);
    char data = src.data;
  }
  data[2] = v6;
  data[v5 + 4] = v3;
  uint64_t v7 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(void *)(v7 + 16) = 256;
  bzero((void *)(v7 + 32), 0x800uLL);
  src.char data = data;
  unint64_t v9 = data[2];
  unint64_t v8 = data[3];
  if (v9 >= v8 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v8 > 1), v9 + 1, 1);
    char data = src.data;
  }
  data[2] = v9 + 1;
  data[v9 + 4] = v7;
  uint64_t v3 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(void *)(v3 + 16) = 256;
  bzero((void *)(v3 + 32), 0x800uLL);
  src.char data = data;
  unint64_t v11 = data[2];
  unint64_t v10 = data[3];
  int64_t v6 = v11 + 1;
  if (v11 >= v10 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v10 > 1), v11 + 1, 1);
    char data = src.data;
  }
  unint64_t v12 = 0;
  data[2] = v6;
  data[v11 + 4] = v3;
  unint64_t v5 = *(void *)(a1 + 16);
  do
  {
    if (v5)
    {
      src.char data = (void *)MEMORY[0x1E4FBC860];
      swift_bridgeObjectRetain();
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v5, 0);
      uint64_t v13 = src.data;
      unint64_t v14 = *((void *)src.data + 2);
      uint64_t v15 = 4 * v14;
      uint64_t v16 = (long long *)(a1 + 48);
      uint64_t v3 = v5;
      do
      {
        long long v17 = *(v16 - 1);
        long long v18 = *v16;
        src.char data = v13;
        unint64_t v19 = v13[3];
        int64_t v6 = v14 + 1;
        if (v14 >= v19 >> 1)
        {
          long long v25 = v18;
          long long v27 = v17;
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v19 > 1), v14 + 1, 1);
          long long v18 = v25;
          long long v17 = v27;
          uint64_t v13 = src.data;
        }
        v13[2] = v6;
        uint64_t v20 = &v13[v15];
        *((_OWORD *)v20 + 2) = v17;
        *((_OWORD *)v20 + 3) = v18;
        v15 += 4;
        uint64_t v16 = (long long *)((char *)v16 + 40);
        unint64_t v14 = v6;
        --v3;
      }
      while (v3);
      swift_bridgeObjectRelease();
    }
    else
    {
      uint64_t v13 = (void *)MEMORY[0x1E4FBC860];
    }
    unint64_t v4 = v13[2];
    if (v12 >= v4)
    {
      __break(1u);
LABEL_23:
      __break(1u);
      goto LABEL_24;
    }
    long long v21 = &v13[4 * v12];
    long long v26 = *((_OWORD *)v21 + 3);
    long long v28 = *((_OWORD *)v21 + 2);
    swift_bridgeObjectRelease();
    *(_OWORD *)&src.char data = v28;
    *(_OWORD *)&src.width = v26;
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
      char data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
    }
    unint64_t v4 = data[2];
    if (v12 >= v4) {
      goto LABEL_23;
    }
    uint64_t v3 = (uint64_t)(data + 4);
    uint64_t v22 = (char *)data[v12 + 4];
    char isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
    data[v12 + 4] = v22;
    if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    {
      uint64_t v22 = specialized _ArrayBuffer._consumeAndCreateNew()((uint64_t)v22);
      *(void *)(v3 + 8 * v12) = v22;
    }
    vImageHistogramCalculation_Planar8(&src, (vImagePixelCount *)v22 + 4, 0);
    *(void *)(v3 + 8 * v12++) = v22;
    int64_t v6 = v12;
  }
  while (v12 != 3);
  return data;
}

{
  void *data;
  uint64_t v3;
  unint64_t v4;
  unint64_t v5;
  int64_t v6;
  uint64_t v7;
  unint64_t v8;
  unint64_t v9;
  uint64_t v10;
  unint64_t v11;
  unint64_t v12;
  unint64_t v13;
  unint64_t v14;
  unint64_t v15;
  void *v16;
  unint64_t v17;
  uint64_t v18;
  long long *v19;
  long long v20;
  long long v21;
  unint64_t v22;
  void *v23;
  void *v24;
  char *v25;
  char isUniquelyReferenced_nonNull_native;
  long long v28;
  long long v29;
  long long v30;
  long long v31;
  vImage_Buffer src;
  uint64_t v33;

  long long v33 = *MEMORY[0x1E4F143B8];
  src.char data = (void *)MEMORY[0x1E4FBC860];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 4, 0);
  char data = src.data;
  uint64_t v3 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(void *)(v3 + 16) = 256;
  bzero((void *)(v3 + 32), 0x800uLL);
  unint64_t v5 = data[2];
  unint64_t v4 = data[3];
  int64_t v6 = v5 + 1;
  if (v5 >= v4 >> 1)
  {
LABEL_26:
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v4 > 1), v6, 1);
    char data = src.data;
  }
  data[2] = v6;
  data[v5 + 4] = v3;
  uint64_t v7 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(void *)(v7 + 16) = 256;
  bzero((void *)(v7 + 32), 0x800uLL);
  src.char data = data;
  unint64_t v9 = data[2];
  unint64_t v8 = data[3];
  if (v9 >= v8 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v8 > 1), v9 + 1, 1);
    char data = src.data;
  }
  data[2] = v9 + 1;
  data[v9 + 4] = v7;
  unint64_t v10 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(void *)(v10 + 16) = 256;
  bzero((void *)(v10 + 32), 0x800uLL);
  src.char data = data;
  unint64_t v12 = data[2];
  unint64_t v11 = data[3];
  if (v12 >= v11 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v11 > 1), v12 + 1, 1);
    char data = src.data;
  }
  data[2] = v12 + 1;
  data[v12 + 4] = v10;
  uint64_t v3 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(void *)(v3 + 16) = 256;
  bzero((void *)(v3 + 32), 0x800uLL);
  src.char data = data;
  unint64_t v14 = data[2];
  uint64_t v13 = data[3];
  int64_t v6 = v14 + 1;
  if (v14 >= v13 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v13 > 1), v14 + 1, 1);
    char data = src.data;
  }
  uint64_t v15 = 0;
  data[2] = v6;
  data[v14 + 4] = v3;
  unint64_t v5 = *(void *)(a1 + 16);
  do
  {
    if (v5)
    {
      src.char data = (void *)MEMORY[0x1E4FBC860];
      swift_bridgeObjectRetain();
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v5, 0);
      uint64_t v16 = src.data;
      long long v17 = *((void *)src.data + 2);
      long long v18 = 4 * v17;
      unint64_t v19 = (long long *)(a1 + 48);
      uint64_t v3 = v5;
      do
      {
        uint64_t v20 = *(v19 - 1);
        long long v21 = *v19;
        src.char data = v16;
        uint64_t v22 = v16[3];
        int64_t v6 = v17 + 1;
        if (v17 >= v22 >> 1)
        {
          long long v28 = v21;
          uint64_t v30 = v20;
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v22 > 1), v17 + 1, 1);
          long long v21 = v28;
          uint64_t v20 = v30;
          uint64_t v16 = src.data;
        }
        _OWORD v16[2] = v6;
        uint64_t v23 = &v16[v18];
        *((_OWORD *)v23 + 2) = v20;
        *((_OWORD *)v23 + 3) = v21;
        v18 += 4;
        unint64_t v19 = (long long *)((char *)v19 + 40);
        long long v17 = v6;
        --v3;
      }
      while (v3);
      swift_bridgeObjectRelease();
    }
    else
    {
      uint64_t v16 = (void *)MEMORY[0x1E4FBC860];
    }
    unint64_t v4 = v16[2];
    if (v15 >= v4)
    {
      __break(1u);
LABEL_25:
      __break(1u);
      goto LABEL_26;
    }
    uint64_t v24 = &v16[4 * v15];
    long long v29 = *((_OWORD *)v24 + 3);
    long long v31 = *((_OWORD *)v24 + 2);
    swift_bridgeObjectRelease();
    *(_OWORD *)&src.char data = v31;
    *(_OWORD *)&src.width = v29;
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
      char data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
    }
    unint64_t v4 = data[2];
    if (v15 >= v4) {
      goto LABEL_25;
    }
    uint64_t v3 = (uint64_t)(data + 4);
    long long v25 = (char *)data[v15 + 4];
    char isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
    data[v15 + 4] = v25;
    if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    {
      long long v25 = specialized _ArrayBuffer._consumeAndCreateNew()((uint64_t)v25);
      *(void *)(v3 + 8 * v15) = v25;
    }
    vImageHistogramCalculation_Planar8(&src, (vImagePixelCount *)v25 + 4, 0);
    *(void *)(v3 + 8 * v15++) = v25;
    int64_t v6 = v15;
  }
  while (v15 != 4);
  return data;
}

void *vImage.PixelBuffer<>._calculateHistogram()(uint64_t a1, uint64_t a2)
{
  uint64_t v25 = *MEMORY[0x1E4F143B8];
  unint64_t v4 = *v2;
  uint64_t v5 = *(void *)(a1 + 16);
  int64_t v6 = *(uint64_t (**)(uint64_t))(a2 + 32);
  int64_t v7 = v6(v5);
  if (v7 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  int64_t v8 = v7;
  char data = (void *)MEMORY[0x1E4FBC860];
  if (v7)
  {
    src.char data = (void *)MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v7, 0);
    char data = src.data;
    do
    {
      uint64_t v10 = static Array._allocateBufferUninitialized(minimumCapacity:)();
      *(void *)(v10 + 16) = 256;
      bzero((void *)(v10 + 32), 0x800uLL);
      src.char data = data;
      unint64_t v12 = data[2];
      unint64_t v11 = data[3];
      if (v12 >= v11 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v11 > 1), v12 + 1, 1);
        char data = src.data;
      }
      data[2] = v12 + 1;
      data[v12 + 4] = v10;
      --v8;
    }
    while (v8);
  }
  uint64_t v13 = ((uint64_t (*)(uint64_t, uint64_t))v6)(v5, a2);
  if (v13 < 0) {
LABEL_21:
  }
    __break(1u);
  uint64_t v14 = v13;
  if (v13)
  {
    uint64_t v15 = 0;
    unint64_t v16 = 0;
    while (1)
    {
      src.char data = v4;
      uint64_t v17 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v16 >= *(void *)(v17 + 16)) {
        break;
      }
      long long v22 = *(_OWORD *)(v17 + v15 + 48);
      long long v23 = *(_OWORD *)(v17 + v15 + 32);
      swift_bridgeObjectRelease();
      *(_OWORD *)&src.char data = v23;
      *(_OWORD *)&src.width = v22;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
        char data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
      }
      if (v16 >= data[2]) {
        goto LABEL_19;
      }
      long long v18 = &data[v16];
      unint64_t v19 = (char *)v18[4];
      char isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
      v18[4] = v19;
      if ((isUniquelyReferenced_nonNull_native & 1) == 0)
      {
        unint64_t v19 = specialized _ArrayBuffer._consumeAndCreateNew()((uint64_t)v19);
        v18[4] = v19;
      }
      ++v16;
      vImageHistogramCalculation_Planar8(&src, (vImagePixelCount *)v19 + 4, 0);
      v18[4] = v19;
      v15 += 32;
      if (v14 == v16) {
        return data;
      }
    }
    __break(1u);
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  return data;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, void **a4)
{
  int64_t v8 = *a4;
  unint64_t v9 = *v4;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<[UInt]>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135DC0;
  *(void *)(inited + 32) = a1;
  *(void *)(inited + 40) = a2;
  *(void *)(inited + 48) = a3;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  specialized vImage.PixelBuffer<>._specifyHistogram(_:destination:)(inited, v8, v9);
  swift_setDeallocating();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [UInt]);
  return swift_arrayDestroy();
}

uint64_t specialized vImage.PixelBuffer<>._specifyHistogram(_:destination:)(uint64_t a1, void *a2, void *a3)
{
  uint64_t v51 = *MEMORY[0x1E4F143B8];
  int64_t v40 = a3[2];
  if (!v40)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  uint64_t v3 = a3[6];
  if (v3 < 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  uint64_t v4 = a3[5];
  if (v4 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v3)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v4)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  int64_t v38 = a2[2];
  if (!v38)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  uint64_t v5 = a2[6];
  if (v5 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v6 = a2[5];
  if (v6 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v5)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v6)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v3 != v5)
  {
LABEL_40:
    __break(1u);
LABEL_41:
    __break(1u);
  }
  if (v4 != v6) {
    goto LABEL_41;
  }
  unint64_t v7 = 0;
  uint64_t v35 = a1 + 32;
  uint64_t v36 = *(void *)(a1 + 16);
  int v39 = (long long *)(a3 + 6);
  long long v37 = (long long *)(a2 + 6);
  int64_t v8 = (void *)MEMORY[0x1E4FBC860];
  do
  {
    src.char data = v8;
    swift_bridgeObjectRetain();
    int64_t v9 = v40;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v40, 0);
    char data = src.data;
    unint64_t v11 = *((void *)src.data + 2);
    uint64_t v12 = 4 * v11;
    uint64_t v13 = v39;
    do
    {
      long long v14 = *(v13 - 1);
      long long v15 = *v13;
      src.char data = data;
      unint64_t v16 = data[3];
      unint64_t v17 = v11 + 1;
      if (v11 >= v16 >> 1)
      {
        long long v41 = v15;
        long long v45 = v14;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v16 > 1), v11 + 1, 1);
        long long v15 = v41;
        long long v14 = v45;
        char data = src.data;
      }
      data[2] = v17;
      long long v18 = &data[v12];
      *((_OWORD *)v18 + 2) = v14;
      *((_OWORD *)v18 + 3) = v15;
      v12 += 4;
      uint64_t v13 = (long long *)((char *)v13 + 40);
      unint64_t v11 = v17;
      --v9;
    }
    while (v9);
    swift_bridgeObjectRelease();
    if (v7 >= data[2])
    {
      __break(1u);
LABEL_28:
      __break(1u);
LABEL_29:
      __break(1u);
      goto LABEL_30;
    }
    unint64_t v19 = v7 + 1;
    uint64_t v20 = (long long *)&data[4 * v7 + 4];
    long long v42 = v20[1];
    long long v46 = *v20;
    swift_bridgeObjectRelease();
    *(_OWORD *)&src.char data = v46;
    *(_OWORD *)&src.width = v42;
    long long v21 = v8;
    dest.char data = v8;
    swift_bridgeObjectRetain();
    int64_t v22 = v38;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v38, 0);
    long long v23 = dest.data;
    unint64_t v24 = *((void *)dest.data + 2);
    uint64_t v25 = 4 * v24;
    long long v26 = v37;
    do
    {
      long long v27 = *(v26 - 1);
      long long v28 = *v26;
      dest.char data = v23;
      unint64_t v29 = v23[3];
      unint64_t v30 = v24 + 1;
      if (v24 >= v29 >> 1)
      {
        long long v43 = v28;
        long long v47 = v27;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v29 > 1), v24 + 1, 1);
        long long v28 = v43;
        long long v27 = v47;
        long long v23 = dest.data;
      }
      _OWORD v23[2] = v30;
      long long v31 = &v23[v25];
      *((_OWORD *)v31 + 2) = v27;
      *((_OWORD *)v31 + 3) = v28;
      v25 += 4;
      long long v26 = (long long *)((char *)v26 + 40);
      unint64_t v24 = v30;
      --v22;
    }
    while (v22);
    swift_bridgeObjectRelease();
    if (v7 >= v23[2]) {
      goto LABEL_28;
    }
    long long v32 = (long long *)&v23[4 * v7 + 4];
    long long v44 = v32[1];
    long long v48 = *v32;
    swift_bridgeObjectRelease();
    *(_OWORD *)&dest.char data = v48;
    *(_OWORD *)&dest.width = v44;
    if (v7 == v36) {
      goto LABEL_29;
    }
    uint64_t v33 = *(void *)(v35 + 8 * v7);
    swift_bridgeObjectRetain();
    vImageHistogramSpecification_Planar8(&src, &dest, (const vImagePixelCount *)(v33 + 32), 0);
    uint64_t result = swift_bridgeObjectRelease();
    ++v7;
    int64_t v8 = v21;
  }
  while (v19 != 3);
  return result;
}

{
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  unint64_t v7;
  void *v8;
  int64_t v9;
  void *data;
  unint64_t v11;
  uint64_t v12;
  long long *v13;
  long long v14;
  long long v15;
  unint64_t v16;
  unint64_t v17;
  void *v18;
  unint64_t v19;
  long long *v20;
  void *v21;
  int64_t v22;
  void *v23;
  unint64_t v24;
  uint64_t v25;
  long long *v26;
  long long v27;
  long long v28;
  unint64_t v29;
  unint64_t v30;
  void *v31;
  long long *v32;
  uint64_t v33;
  uint64_t result;
  uint64_t v35;
  uint64_t v36;
  long long *v37;
  int64_t v38;
  long long *v39;
  int64_t v40;
  long long v41;
  long long v42;
  long long v43;
  long long v44;
  long long v45;
  long long v46;
  long long v47;
  long long v48;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v51;

  uint64_t v51 = *MEMORY[0x1E4F143B8];
  int64_t v40 = a3[2];
  if (!v40)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  uint64_t v3 = a3[6];
  if (v3 < 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  uint64_t v4 = a3[5];
  if (v4 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v3)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v4)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  int64_t v38 = a2[2];
  if (!v38)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  uint64_t v5 = a2[6];
  if (v5 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v6 = a2[5];
  if (v6 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v5)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v6)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v3 != v5)
  {
LABEL_40:
    __break(1u);
LABEL_41:
    __break(1u);
  }
  if (v4 != v6) {
    goto LABEL_41;
  }
  unint64_t v7 = 0;
  uint64_t v35 = a1 + 32;
  uint64_t v36 = *(void *)(a1 + 16);
  int v39 = (long long *)(a3 + 6);
  long long v37 = (long long *)(a2 + 6);
  int64_t v8 = (void *)MEMORY[0x1E4FBC860];
  do
  {
    src.char data = v8;
    swift_bridgeObjectRetain();
    int64_t v9 = v40;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v40, 0);
    char data = src.data;
    unint64_t v11 = *((void *)src.data + 2);
    uint64_t v12 = 4 * v11;
    uint64_t v13 = v39;
    do
    {
      long long v14 = *(v13 - 1);
      long long v15 = *v13;
      src.char data = data;
      unint64_t v16 = data[3];
      unint64_t v17 = v11 + 1;
      if (v11 >= v16 >> 1)
      {
        long long v41 = v15;
        long long v45 = v14;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v16 > 1), v11 + 1, 1);
        long long v15 = v41;
        long long v14 = v45;
        char data = src.data;
      }
      data[2] = v17;
      long long v18 = &data[v12];
      *((_OWORD *)v18 + 2) = v14;
      *((_OWORD *)v18 + 3) = v15;
      v12 += 4;
      uint64_t v13 = (long long *)((char *)v13 + 40);
      unint64_t v11 = v17;
      --v9;
    }
    while (v9);
    swift_bridgeObjectRelease();
    if (v7 >= data[2])
    {
      __break(1u);
LABEL_28:
      __break(1u);
LABEL_29:
      __break(1u);
      goto LABEL_30;
    }
    unint64_t v19 = v7 + 1;
    uint64_t v20 = (long long *)&data[4 * v7 + 4];
    long long v42 = v20[1];
    long long v46 = *v20;
    swift_bridgeObjectRelease();
    *(_OWORD *)&src.char data = v46;
    *(_OWORD *)&src.width = v42;
    long long v21 = v8;
    dest.char data = v8;
    swift_bridgeObjectRetain();
    int64_t v22 = v38;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v38, 0);
    long long v23 = dest.data;
    unint64_t v24 = *((void *)dest.data + 2);
    uint64_t v25 = 4 * v24;
    long long v26 = v37;
    do
    {
      long long v27 = *(v26 - 1);
      long long v28 = *v26;
      dest.char data = v23;
      unint64_t v29 = v23[3];
      unint64_t v30 = v24 + 1;
      if (v24 >= v29 >> 1)
      {
        long long v43 = v28;
        long long v47 = v27;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v29 > 1), v24 + 1, 1);
        long long v28 = v43;
        long long v27 = v47;
        long long v23 = dest.data;
      }
      _OWORD v23[2] = v30;
      long long v31 = &v23[v25];
      *((_OWORD *)v31 + 2) = v27;
      *((_OWORD *)v31 + 3) = v28;
      v25 += 4;
      long long v26 = (long long *)((char *)v26 + 40);
      unint64_t v24 = v30;
      --v22;
    }
    while (v22);
    swift_bridgeObjectRelease();
    if (v7 >= v23[2]) {
      goto LABEL_28;
    }
    long long v32 = (long long *)&v23[4 * v7 + 4];
    long long v44 = v32[1];
    long long v48 = *v32;
    swift_bridgeObjectRelease();
    *(_OWORD *)&dest.char data = v48;
    *(_OWORD *)&dest.width = v44;
    if (v7 == v36) {
      goto LABEL_29;
    }
    uint64_t v33 = *(void *)(v35 + 8 * v7);
    swift_bridgeObjectRetain();
    vImageHistogramSpecification_Planar8(&src, &dest, (const vImagePixelCount *)(v33 + 32), 0);
    uint64_t result = swift_bridgeObjectRelease();
    ++v7;
    int64_t v8 = v21;
  }
  while (v19 != 4);
  return result;
}

uint64_t vImage.PixelBuffer<>._specifyHistogram(_:destination:)(uint64_t a1, void **a2, uint64_t a3, uint64_t a4)
{
  uint64_t v21 = *MEMORY[0x1E4F143B8];
  unint64_t v7 = *a2;
  int64_t v8 = *v4;
  v19[0] = *v4;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&v20);
  long long v9 = *(_OWORD *)&v20.data;
  vImage.PixelBuffer.size.getter(v19);
  swift_bridgeObjectRelease();
  if ((void)v9 != v19[0] || *((void *)&v9 + 1) != v19[1])
  {
LABEL_13:
    __break(1u);
LABEL_14:
    __break(1u);
  }
  uint64_t result = (*(uint64_t (**)(void, uint64_t))(a4 + 32))(*(void *)(a3 + 16), a4);
  if (result < 0) {
    goto LABEL_14;
  }
  uint64_t v12 = result;
  if (result)
  {
    uint64_t v13 = 0;
    unint64_t v14 = 0;
    while (1)
    {
      v20.char data = v8;
      uint64_t v15 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v14 >= *(void *)(v15 + 16)) {
        break;
      }
      long long v17 = *(_OWORD *)(v15 + v13 + 48);
      long long v18 = *(_OWORD *)(v15 + v13 + 32);
      swift_bridgeObjectRelease();
      *(_OWORD *)&v20.char data = v18;
      *(_OWORD *)&v20.width = v17;
      uint64_t result = closure #1 in vImage.PixelBuffer<>._specifyHistogram(_:destination:)(&v20, v7, v14, a1);
      v13 += 32;
      if (v12 == ++v14) {
        return result;
      }
    }
    __break(1u);
    goto LABEL_13;
  }
  return result;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, void **a5)
{
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<[UInt]>);
  uint64_t inited = swift_initStackObject();
  *(void *)(inited + 32) = a1;
  *(_OWORD *)(inited + 16) = xmmword_1D2135FC0;
  *(void *)(inited + 40) = a2;
  *(void *)(inited + 48) = a3;
  *(void *)(inited + 56) = a4;
  uint64_t v12 = *a5;
  uint64_t v13 = *v5;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  specialized vImage.PixelBuffer<>._specifyHistogram(_:destination:)(inited, v12, v13);
  swift_setDeallocating();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [UInt]);
  return swift_arrayDestroy();
}

uint64_t closure #1 in vImage.PixelBuffer<>._specifyHistogram(_:destination:)(const vImage_Buffer *a1, void *a2, unint64_t a3, uint64_t a4)
{
  uint64_t v14 = *MEMORY[0x1E4F143B8];
  dest.char data = a2;
  type metadata accessor for vImage.PixelBuffer();
  uint64_t v7 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (*(void *)(v7 + 16) <= a3)
  {
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  uint64_t v8 = v7 + 32 * a3;
  long long v11 = *(_OWORD *)(v8 + 48);
  long long v12 = *(_OWORD *)(v8 + 32);
  swift_bridgeObjectRelease();
  *(_OWORD *)&dest.char data = v12;
  *(_OWORD *)&dest.width = v11;
  if (*(void *)(a4 + 16) <= a3) {
    goto LABEL_7;
  }
  uint64_t v9 = *(void *)(a4 + 8 * a3 + 32);
  swift_bridgeObjectRetain();
  vImageHistogramSpecification_Planar8(a1, &dest, (const vImagePixelCount *)(v9 + 32), 0);
  return swift_bridgeObjectRelease();
}

void *vImage.PixelBuffer<>.histogram(binCount:)(unint64_t a1)
{
  uint64_t result = specialized vImage.PixelBuffer<>._calculateHistogram(binCount:)(a1, *v1);
  unint64_t v4 = result[2];
  if (!v4)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (v4 == 1)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (v4 >= 3)
  {
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRelease();
    return (void *)a1;
  }
LABEL_7:
  __break(1u);
  return result;
}

void *specialized vImage.PixelBuffer<>._calculateHistogram(binCount:)(unint64_t a1, uint64_t a2)
{
  uint64_t v36 = *MEMORY[0x1E4F143B8];
  src.char data = (void *)MEMORY[0x1E4FBC860];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 3, 0);
  if ((a1 & 0x8000000000000000) != 0)
  {
LABEL_35:
    __break(1u);
  }
  else
  {
    char data = src.data;
    uint64_t v2 = 8 * a1;
    if (a1)
    {
      unint64_t v4 = static Array._allocateBufferUninitialized(minimumCapacity:)();
      *(void *)(v4 + 16) = a1;
      bzero((void *)(v4 + 32), 8 * a1);
    }
    else
    {
      unint64_t v4 = MEMORY[0x1E4FBC860];
    }
    unint64_t v5 = data[2];
    unint64_t v8 = data[3];
    unint64_t v3 = v5 + 1;
    if (v5 < v8 >> 1) {
      goto LABEL_6;
    }
  }
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v8 > 1), v3, 1);
  char data = src.data;
LABEL_6:
  data[2] = v3;
  data[v5 + 4] = v4;
  if (a1)
  {
    uint64_t v10 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v10 + 16) = a1;
    bzero((void *)(v10 + 32), v2);
    unint64_t v3 = data[2];
  }
  else
  {
    uint64_t v10 = MEMORY[0x1E4FBC860];
  }
  src.char data = data;
  unint64_t v11 = data[3];
  if (v3 >= v11 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v11 > 1), v3 + 1, 1);
    char data = src.data;
  }
  data[2] = v3 + 1;
  data[v3 + 4] = v10;
  long long v12 = (void *)MEMORY[0x1E4FBC860];
  if (a1)
  {
    unint64_t v3 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v3 + 16) = a1;
    bzero((void *)(v3 + 32), v2);
  }
  else
  {
    unint64_t v3 = MEMORY[0x1E4FBC860];
  }
  src.char data = data;
  unint64_t v4 = data[2];
  unint64_t v13 = data[3];
  if (v4 >= v13 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v13 > 1), v4 + 1, 1);
    char data = src.data;
  }
  data[2] = v4 + 1;
  data[v4 + 4] = v3;
  if (HIDWORD(a1)) {
    __break(1u);
  }
  unint64_t v5 = 0;
  uint64_t v2 = *(void *)(a2 + 16);
  unint64_t v29 = (long long *)(a2 + 48);
  uint64_t v30 = v2;
  do
  {
    if (v2)
    {
      unint64_t v4 = (unint64_t)data;
      uint64_t v14 = v12;
      src.char data = v12;
      uint64_t v15 = a2;
      swift_bridgeObjectRetain();
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v2, 0);
      unint64_t v16 = src.data;
      unint64_t v17 = *((void *)src.data + 2);
      uint64_t v18 = 4 * v17;
      unint64_t v19 = v29;
      do
      {
        long long v20 = *(v19 - 1);
        long long v21 = *v19;
        src.char data = v16;
        unint64_t v22 = v16[3];
        unint64_t v3 = v17 + 1;
        if (v17 >= v22 >> 1)
        {
          long long v31 = v21;
          long long v33 = v20;
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v22 > 1), v17 + 1, 1);
          long long v21 = v31;
          long long v20 = v33;
          unint64_t v16 = src.data;
        }
        _OWORD v16[2] = v3;
        long long v23 = &v16[v18];
        *((_OWORD *)v23 + 2) = v20;
        *((_OWORD *)v23 + 3) = v21;
        v18 += 4;
        unint64_t v19 = (long long *)((char *)v19 + 40);
        unint64_t v17 = v3;
        --v2;
      }
      while (v2);
      a2 = v15;
      swift_bridgeObjectRelease();
      long long v12 = v14;
      char data = (void *)v4;
    }
    else
    {
      unint64_t v16 = v12;
    }
    unint64_t v8 = v16[2];
    if (v5 >= v8)
    {
      __break(1u);
LABEL_34:
      __break(1u);
      goto LABEL_35;
    }
    unint64_t v24 = &v16[4 * v5];
    long long v32 = *((_OWORD *)v24 + 3);
    long long v34 = *((_OWORD *)v24 + 2);
    swift_bridgeObjectRelease();
    *(_OWORD *)&src.char data = v34;
    *(_OWORD *)&src.width = v32;
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
      char data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
    }
    unint64_t v8 = data[2];
    if (v5 >= v8) {
      goto LABEL_34;
    }
    uint64_t v25 = data + 4;
    long long v26 = (char *)data[v5 + 4];
    char isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
    data[v5 + 4] = v26;
    if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    {
      long long v26 = specialized _ArrayBuffer._consumeAndCreateNew()((uint64_t)v26);
      v25[v5] = v26;
    }
    vImageHistogramCalculation_PlanarF(&src, (vImagePixelCount *)v26 + 4, a1, 0.0, 1.0, 0);
    v25[v5++] = v26;
    unint64_t v3 = v5;
    uint64_t v2 = v30;
  }
  while (v5 != 3);
  return data;
}

{
  unint64_t v2;
  uint64_t v3;
  unint64_t v4;
  unint64_t v5;
  unint64_t v8;
  void *data;
  uint64_t v10;
  unint64_t v11;
  uint64_t v12;
  unint64_t v13;
  void *v14;
  uint64_t v15;
  unint64_t v16;
  unint64_t v17;
  void *v18;
  uint64_t v19;
  void *v20;
  unint64_t v21;
  uint64_t v22;
  long long *v23;
  long long v24;
  long long v25;
  unint64_t v26;
  void *v27;
  void *v28;
  void *v29;
  char *v30;
  char isUniquelyReferenced_nonNull_native;
  long long *v33;
  uint64_t v34;
  long long v35;
  long long v36;
  long long v37;
  long long v38;
  vImage_Buffer src;
  uint64_t v40;

  int64_t v40 = *MEMORY[0x1E4F143B8];
  src.char data = (void *)MEMORY[0x1E4FBC860];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 4, 0);
  if ((a1 & 0x8000000000000000) != 0)
  {
LABEL_40:
    __break(1u);
  }
  else
  {
    char data = src.data;
    unint64_t v3 = 8 * a1;
    if (a1)
    {
      unint64_t v5 = static Array._allocateBufferUninitialized(minimumCapacity:)();
      *(void *)(v5 + 16) = a1;
      bzero((void *)(v5 + 32), 8 * a1);
    }
    else
    {
      unint64_t v5 = MEMORY[0x1E4FBC860];
    }
    uint64_t v2 = data[2];
    unint64_t v8 = data[3];
    unint64_t v4 = v2 + 1;
    if (v2 < v8 >> 1) {
      goto LABEL_6;
    }
  }
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v8 > 1), v4, 1);
  char data = src.data;
LABEL_6:
  data[2] = v4;
  data[v2 + 4] = v5;
  if (a1)
  {
    uint64_t v10 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v10 + 16) = a1;
    bzero((void *)(v10 + 32), v3);
    unint64_t v4 = data[2];
  }
  else
  {
    uint64_t v10 = MEMORY[0x1E4FBC860];
  }
  src.char data = data;
  unint64_t v11 = data[3];
  if (v4 >= v11 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v11 > 1), v4 + 1, 1);
    char data = src.data;
  }
  data[2] = v4 + 1;
  data[v4 + 4] = v10;
  if (a1)
  {
    long long v12 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v12 + 16) = a1;
    bzero((void *)(v12 + 32), v3);
  }
  else
  {
    long long v12 = MEMORY[0x1E4FBC860];
  }
  src.char data = data;
  uint64_t v2 = data[2];
  unint64_t v13 = data[3];
  unint64_t v4 = v2 + 1;
  if (v2 >= v13 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v13 > 1), v2 + 1, 1);
    char data = src.data;
  }
  data[2] = v4;
  data[v2 + 4] = v12;
  uint64_t v14 = (void *)MEMORY[0x1E4FBC860];
  if (a1)
  {
    uint64_t v15 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v15 + 16) = a1;
    bzero((void *)(v15 + 32), v3);
    unint64_t v4 = data[2];
  }
  else
  {
    uint64_t v15 = MEMORY[0x1E4FBC860];
  }
  src.char data = data;
  unint64_t v16 = data[3];
  if (v4 >= v16 >> 1)
  {
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v16 > 1), v4 + 1, 1);
    char data = src.data;
  }
  data[2] = v4 + 1;
  data[v4 + 4] = v15;
  if (HIDWORD(a1)) {
    __break(1u);
  }
  unint64_t v5 = 0;
  unint64_t v3 = *(void *)(a2 + 16);
  long long v33 = (long long *)(a2 + 48);
  long long v34 = v3;
  do
  {
    if (v3)
    {
      uint64_t v2 = (unint64_t)data;
      unint64_t v17 = a1;
      uint64_t v18 = v14;
      src.char data = v14;
      unint64_t v19 = a2;
      swift_bridgeObjectRetain();
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v3, 0);
      long long v20 = src.data;
      long long v21 = *((void *)src.data + 2);
      unint64_t v22 = 4 * v21;
      long long v23 = v33;
      do
      {
        unint64_t v24 = *(v23 - 1);
        uint64_t v25 = *v23;
        src.char data = v20;
        long long v26 = v20[3];
        unint64_t v4 = v21 + 1;
        if (v21 >= v26 >> 1)
        {
          uint64_t v35 = v25;
          long long v37 = v24;
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v26 > 1), v21 + 1, 1);
          uint64_t v25 = v35;
          unint64_t v24 = v37;
          long long v20 = src.data;
        }
        _OWORD v20[2] = v4;
        long long v27 = &v20[v22];
        *((_OWORD *)v27 + 2) = v24;
        *((_OWORD *)v27 + 3) = v25;
        v22 += 4;
        long long v23 = (long long *)((char *)v23 + 40);
        long long v21 = v4;
        --v3;
      }
      while (v3);
      a2 = v19;
      swift_bridgeObjectRelease();
      a1 = v17;
      uint64_t v14 = v18;
      char data = (void *)v2;
    }
    else
    {
      long long v20 = v14;
    }
    unint64_t v8 = v20[2];
    if (v5 >= v8)
    {
      __break(1u);
LABEL_39:
      __break(1u);
      goto LABEL_40;
    }
    long long v28 = &v20[4 * v5];
    uint64_t v36 = *((_OWORD *)v28 + 3);
    int64_t v38 = *((_OWORD *)v28 + 2);
    swift_bridgeObjectRelease();
    *(_OWORD *)&src.char data = v38;
    *(_OWORD *)&src.width = v36;
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
      char data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
    }
    unint64_t v8 = data[2];
    if (v5 >= v8) {
      goto LABEL_39;
    }
    unint64_t v29 = data + 4;
    uint64_t v30 = (char *)data[v5 + 4];
    char isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
    data[v5 + 4] = v30;
    if ((isUniquelyReferenced_nonNull_native & 1) == 0)
    {
      uint64_t v30 = specialized _ArrayBuffer._consumeAndCreateNew()((uint64_t)v30);
      v29[v5] = v30;
    }
    vImageHistogramCalculation_PlanarF(&src, (vImagePixelCount *)v30 + 4, a1, 0.0, 1.0, 0);
    v29[v5++] = v30;
    unint64_t v4 = v5;
    unint64_t v3 = v34;
  }
  while (v5 != 4);
  return data;
}

void *vImage.PixelBuffer<>._calculateHistogram(binCount:)(unint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v29 = *MEMORY[0x1E4F143B8];
  uint64_t v25 = *v3;
  uint64_t v6 = *(void *)(a2 + 16);
  uint64_t v7 = *(uint64_t (**)(uint64_t, uint64_t))(a3 + 32);
  int64_t v8 = v7(v6, a3);
  if (v8 < 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  int64_t v9 = v8;
  char data = (void *)MEMORY[0x1E4FBC860];
  if (v8)
  {
    src.char data = (void *)MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v8, 0);
    if ((a1 & 0x8000000000000000) != 0)
    {
LABEL_29:
      __break(1u);
      goto LABEL_30;
    }
    unint64_t v24 = v7;
    char data = src.data;
    uint64_t v11 = MEMORY[0x1E4FBC860];
    do
    {
      if (a1)
      {
        uint64_t v12 = static Array._allocateBufferUninitialized(minimumCapacity:)();
        *(void *)(v12 + 16) = a1;
        bzero((void *)(v12 + 32), 8 * a1);
      }
      else
      {
        uint64_t v12 = v11;
      }
      src.char data = data;
      unint64_t v14 = data[2];
      unint64_t v13 = data[3];
      if (v14 >= v13 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v13 > 1), v14 + 1, 1);
        char data = src.data;
      }
      data[2] = v14 + 1;
      data[v14 + 4] = v12;
      --v9;
    }
    while (v9);
    uint64_t v7 = v24;
  }
  uint64_t v15 = v7(v6, a3);
  if (v15 < 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  uint64_t v16 = v15;
  if (v15)
  {
    if ((a1 & 0x8000000000000000) != 0)
    {
LABEL_30:
      __break(1u);
LABEL_31:
      __break(1u);
    }
    if (HIDWORD(a1)) {
      goto LABEL_31;
    }
    uint64_t v17 = 0;
    unint64_t v18 = 0;
    while (1)
    {
      src.char data = v25;
      uint64_t v19 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v18 >= *(void *)(v19 + 16)) {
        break;
      }
      long long v26 = *(_OWORD *)(v19 + v17 + 48);
      long long v27 = *(_OWORD *)(v19 + v17 + 32);
      swift_bridgeObjectRelease();
      *(_OWORD *)&src.char data = v27;
      *(_OWORD *)&src.width = v26;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
        char data = specialized _ArrayBuffer._consumeAndCreateNew()(data);
      }
      if (v18 >= data[2]) {
        goto LABEL_26;
      }
      long long v20 = &data[v18];
      long long v21 = (char *)v20[4];
      char isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
      v20[4] = v21;
      if ((isUniquelyReferenced_nonNull_native & 1) == 0)
      {
        long long v21 = specialized _ArrayBuffer._consumeAndCreateNew()((uint64_t)v21);
        v20[4] = v21;
      }
      ++v18;
      vImageHistogramCalculation_PlanarF(&src, (vImagePixelCount *)v21 + 4, a1, 0.0, 1.0, 0);
      v20[4] = v21;
      v17 += 32;
      if (v16 == v18) {
        return data;
      }
    }
    __break(1u);
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  return data;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(unint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, void **a5)
{
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<[UInt]>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135DC0;
  *(void *)(inited + 32) = a2;
  *(void *)(inited + 40) = a3;
  *(void *)(inited + 48) = a4;
  uint64_t v12 = *a5;
  unint64_t v13 = *v5;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  specialized vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(inited, a1, v12, v13);
  swift_setDeallocating();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [UInt]);
  return swift_arrayDestroy();
}

uint64_t specialized vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(uint64_t a1, unint64_t a2, void *a3, void *a4)
{
  uint64_t v52 = *MEMORY[0x1E4F143B8];
  int64_t v41 = a4[2];
  if (!v41)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  uint64_t v4 = a4[6];
  if (v4 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  uint64_t v5 = a4[5];
  if (v5 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v4)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (!v5)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  int64_t v39 = a3[2];
  if (!v39)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v6 = a3[6];
  if (v6 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  uint64_t v7 = a3[5];
  if (v7 < 1)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v4 != v6)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (v5 != v7)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  unint64_t v8 = a2;
  if ((a2 & 0x8000000000000000) != 0)
  {
LABEL_42:
    __break(1u);
LABEL_43:
    __break(1u);
  }
  if (HIDWORD(a2)) {
    goto LABEL_43;
  }
  unint64_t v9 = 0;
  uint64_t v36 = a1 + 32;
  uint64_t v37 = *(void *)(a1 + 16);
  int64_t v40 = (long long *)(a4 + 6);
  int64_t v38 = (long long *)(a3 + 6);
  do
  {
    src.char data = (void *)MEMORY[0x1E4FBC860];
    swift_bridgeObjectRetain();
    int64_t v10 = v41;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v41, 0);
    char data = src.data;
    unint64_t v12 = *((void *)src.data + 2);
    uint64_t v13 = 4 * v12;
    unint64_t v14 = v40;
    do
    {
      long long v15 = *(v14 - 1);
      long long v16 = *v14;
      src.char data = data;
      unint64_t v17 = data[3];
      unint64_t v18 = v12 + 1;
      if (v12 >= v17 >> 1)
      {
        long long v42 = v16;
        long long v46 = v15;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v17 > 1), v12 + 1, 1);
        long long v16 = v42;
        long long v15 = v46;
        char data = src.data;
      }
      data[2] = v18;
      uint64_t v19 = &data[v13];
      *((_OWORD *)v19 + 2) = v15;
      *((_OWORD *)v19 + 3) = v16;
      v13 += 4;
      unint64_t v14 = (long long *)((char *)v14 + 40);
      unint64_t v12 = v18;
      --v10;
    }
    while (v10);
    swift_bridgeObjectRelease();
    if (v9 >= data[2])
    {
      __break(1u);
LABEL_29:
      __break(1u);
LABEL_30:
      __break(1u);
      goto LABEL_31;
    }
    unint64_t v20 = v8;
    unint64_t v21 = v9 + 1;
    unint64_t v22 = (long long *)&data[4 * v9 + 4];
    long long v43 = v22[1];
    long long v47 = *v22;
    swift_bridgeObjectRelease();
    *(_OWORD *)&src.char data = v47;
    *(_OWORD *)&src.width = v43;
    dest.char data = (void *)MEMORY[0x1E4FBC860];
    swift_bridgeObjectRetain();
    int64_t v23 = v39;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v39, 0);
    unint64_t v24 = dest.data;
    unint64_t v25 = *((void *)dest.data + 2);
    uint64_t v26 = 4 * v25;
    long long v27 = v38;
    do
    {
      long long v28 = *(v27 - 1);
      long long v29 = *v27;
      dest.char data = v24;
      unint64_t v30 = v24[3];
      unint64_t v31 = v25 + 1;
      if (v25 >= v30 >> 1)
      {
        long long v44 = v29;
        long long v48 = v28;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v30 > 1), v25 + 1, 1);
        long long v29 = v44;
        long long v28 = v48;
        unint64_t v24 = dest.data;
      }
      void v24[2] = v31;
      long long v32 = &v24[v26];
      *((_OWORD *)v32 + 2) = v28;
      *((_OWORD *)v32 + 3) = v29;
      v26 += 4;
      long long v27 = (long long *)((char *)v27 + 40);
      unint64_t v25 = v31;
      --v23;
    }
    while (v23);
    swift_bridgeObjectRelease();
    if (v9 >= v24[2]) {
      goto LABEL_29;
    }
    long long v33 = (long long *)&v24[4 * v9 + 4];
    long long v45 = v33[1];
    long long v49 = *v33;
    swift_bridgeObjectRelease();
    *(_OWORD *)&dest.char data = v49;
    *(_OWORD *)&dest.width = v45;
    if (v9 == v37) {
      goto LABEL_30;
    }
    uint64_t v34 = *(void *)(v36 + 8 * v9);
    swift_bridgeObjectRetain();
    unint64_t v8 = v20;
    vImageHistogramSpecification_PlanarF(&src, &dest, 0, (const vImagePixelCount *)(v34 + 32), v20, 0.0, 1.0, 0);
    uint64_t result = swift_bridgeObjectRelease();
    ++v9;
  }
  while (v21 != 3);
  return result;
}

{
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  unint64_t v8;
  unint64_t v9;
  int64_t v10;
  void *data;
  unint64_t v12;
  uint64_t v13;
  long long *v14;
  long long v15;
  long long v16;
  unint64_t v17;
  unint64_t v18;
  void *v19;
  unint64_t v20;
  unint64_t v21;
  long long *v22;
  int64_t v23;
  void *v24;
  unint64_t v25;
  uint64_t v26;
  long long *v27;
  long long v28;
  long long v29;
  unint64_t v30;
  unint64_t v31;
  void *v32;
  long long *v33;
  uint64_t v34;
  uint64_t result;
  uint64_t v36;
  uint64_t v37;
  long long *v38;
  int64_t v39;
  long long *v40;
  int64_t v41;
  long long v42;
  long long v43;
  long long v44;
  long long v45;
  long long v46;
  long long v47;
  long long v48;
  long long v49;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v52;

  uint64_t v52 = *MEMORY[0x1E4F143B8];
  int64_t v41 = a4[2];
  if (!v41)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  uint64_t v4 = a4[6];
  if (v4 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  uint64_t v5 = a4[5];
  if (v5 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v4)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (!v5)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  int64_t v39 = a3[2];
  if (!v39)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v6 = a3[6];
  if (v6 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  uint64_t v7 = a3[5];
  if (v7 < 1)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v4 != v6)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (v5 != v7)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  unint64_t v8 = a2;
  if ((a2 & 0x8000000000000000) != 0)
  {
LABEL_42:
    __break(1u);
LABEL_43:
    __break(1u);
  }
  if (HIDWORD(a2)) {
    goto LABEL_43;
  }
  unint64_t v9 = 0;
  uint64_t v36 = a1 + 32;
  uint64_t v37 = *(void *)(a1 + 16);
  int64_t v40 = (long long *)(a4 + 6);
  int64_t v38 = (long long *)(a3 + 6);
  do
  {
    src.char data = (void *)MEMORY[0x1E4FBC860];
    swift_bridgeObjectRetain();
    int64_t v10 = v41;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v41, 0);
    char data = src.data;
    unint64_t v12 = *((void *)src.data + 2);
    uint64_t v13 = 4 * v12;
    unint64_t v14 = v40;
    do
    {
      long long v15 = *(v14 - 1);
      long long v16 = *v14;
      src.char data = data;
      unint64_t v17 = data[3];
      unint64_t v18 = v12 + 1;
      if (v12 >= v17 >> 1)
      {
        long long v42 = v16;
        long long v46 = v15;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v17 > 1), v12 + 1, 1);
        long long v16 = v42;
        long long v15 = v46;
        char data = src.data;
      }
      data[2] = v18;
      uint64_t v19 = &data[v13];
      *((_OWORD *)v19 + 2) = v15;
      *((_OWORD *)v19 + 3) = v16;
      v13 += 4;
      unint64_t v14 = (long long *)((char *)v14 + 40);
      unint64_t v12 = v18;
      --v10;
    }
    while (v10);
    swift_bridgeObjectRelease();
    if (v9 >= data[2])
    {
      __break(1u);
LABEL_29:
      __break(1u);
LABEL_30:
      __break(1u);
      goto LABEL_31;
    }
    unint64_t v20 = v8;
    unint64_t v21 = v9 + 1;
    unint64_t v22 = (long long *)&data[4 * v9 + 4];
    long long v43 = v22[1];
    long long v47 = *v22;
    swift_bridgeObjectRelease();
    *(_OWORD *)&src.char data = v47;
    *(_OWORD *)&src.width = v43;
    dest.char data = (void *)MEMORY[0x1E4FBC860];
    swift_bridgeObjectRetain();
    int64_t v23 = v39;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v39, 0);
    unint64_t v24 = dest.data;
    unint64_t v25 = *((void *)dest.data + 2);
    uint64_t v26 = 4 * v25;
    long long v27 = v38;
    do
    {
      long long v28 = *(v27 - 1);
      long long v29 = *v27;
      dest.char data = v24;
      unint64_t v30 = v24[3];
      unint64_t v31 = v25 + 1;
      if (v25 >= v30 >> 1)
      {
        long long v44 = v29;
        long long v48 = v28;
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v30 > 1), v25 + 1, 1);
        long long v29 = v44;
        long long v28 = v48;
        unint64_t v24 = dest.data;
      }
      void v24[2] = v31;
      long long v32 = &v24[v26];
      *((_OWORD *)v32 + 2) = v28;
      *((_OWORD *)v32 + 3) = v29;
      v26 += 4;
      long long v27 = (long long *)((char *)v27 + 40);
      unint64_t v25 = v31;
      --v23;
    }
    while (v23);
    swift_bridgeObjectRelease();
    if (v9 >= v24[2]) {
      goto LABEL_29;
    }
    long long v33 = (long long *)&v24[4 * v9 + 4];
    long long v45 = v33[1];
    long long v49 = *v33;
    swift_bridgeObjectRelease();
    *(_OWORD *)&dest.char data = v49;
    *(_OWORD *)&dest.width = v45;
    if (v9 == v37) {
      goto LABEL_30;
    }
    uint64_t v34 = *(void *)(v36 + 8 * v9);
    swift_bridgeObjectRetain();
    unint64_t v8 = v20;
    vImageHistogramSpecification_PlanarF(&src, &dest, 0, (const vImagePixelCount *)(v34 + 32), v20, 0.0, 1.0, 0);
    uint64_t result = swift_bridgeObjectRelease();
    ++v9;
  }
  while (v21 != 4);
  return result;
}

uint64_t vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(uint64_t a1, unint64_t a2, void **a3, uint64_t a4, uint64_t a5)
{
  uint64_t v23 = *MEMORY[0x1E4F143B8];
  unint64_t v8 = *a3;
  unint64_t v9 = *v5;
  v21[0] = *v5;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&v22);
  long long v10 = *(_OWORD *)&v22.data;
  vImage.PixelBuffer.size.getter(v21);
  swift_bridgeObjectRelease();
  if ((void)v10 != v21[0] || *((void *)&v10 + 1) != v21[1])
  {
LABEL_13:
    __break(1u);
LABEL_14:
    __break(1u);
  }
  uint64_t result = (*(uint64_t (**)(void, uint64_t))(a5 + 32))(*(void *)(a4 + 16), a5);
  if (result < 0) {
    goto LABEL_14;
  }
  uint64_t v13 = result;
  if (result)
  {
    uint64_t v14 = 0;
    unint64_t v15 = 0;
    while (1)
    {
      v22.char data = v9;
      uint64_t v16 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v15 >= *(void *)(v16 + 16)) {
        break;
      }
      long long v19 = *(_OWORD *)(v16 + v14 + 48);
      long long v20 = *(_OWORD *)(v16 + v14 + 32);
      swift_bridgeObjectRelease();
      *(_OWORD *)&v22.char data = v20;
      *(_OWORD *)&v22.width = v19;
      uint64_t result = closure #1 in vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(&v22, v8, v15, a1, a2);
      v14 += 32;
      if (v13 == ++v15) {
        return result;
      }
    }
    __break(1u);
    goto LABEL_13;
  }
  return result;
}

void *vImage.PixelBuffer<>.histogram(binCount:)@<X0>(unint64_t a1@<X0>, unint64_t *a2@<X8>)
{
  uint64_t result = specialized vImage.PixelBuffer<>._calculateHistogram(binCount:)(a1, *v2);
  unint64_t v6 = result[2];
  if (!v6)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (v6 == 1)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (v6 < 3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (v6 != 3)
  {
    unint64_t v7 = result[4];
    unint64_t v8 = result[5];
    unint64_t v9 = result[6];
    unint64_t v10 = result[7];
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    swift_bridgeObjectRetain();
    uint64_t result = (void *)swift_bridgeObjectRelease();
    *a2 = a1;
    a2[1] = v7;
    a2[2] = v8;
    a2[3] = v9;
    a2[4] = v10;
    return result;
  }
LABEL_9:
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.specifyHistogram(_:destination:)(unint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, void **a6)
{
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<[UInt]>);
  uint64_t inited = swift_initStackObject();
  *(void *)(inited + 32) = a2;
  *(_OWORD *)(inited + 16) = xmmword_1D2135FC0;
  *(void *)(inited + 40) = a3;
  *(void *)(inited + 48) = a4;
  *(void *)(inited + 56) = a5;
  uint64_t v14 = *a6;
  unint64_t v15 = *v6;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  specialized vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(inited, a1, v14, v15);
  swift_setDeallocating();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [UInt]);
  return swift_arrayDestroy();
}

uint64_t closure #1 in vImage.PixelBuffer<>._specifyHistogram(_:binCount:destination:)(const vImage_Buffer *a1, void *a2, unint64_t a3, uint64_t a4, unint64_t a5)
{
  uint64_t v16 = *MEMORY[0x1E4F143B8];
  dest.char data = a2;
  type metadata accessor for vImage.PixelBuffer();
  uint64_t v9 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_8;
  }
  if (*(void *)(v9 + 16) <= a3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v10 = v9 + 32 * a3;
  long long v13 = *(_OWORD *)(v10 + 48);
  long long v14 = *(_OWORD *)(v10 + 32);
  swift_bridgeObjectRelease();
  *(_OWORD *)&dest.char data = v14;
  *(_OWORD *)&dest.width = v13;
  if (*(void *)(a4 + 16) <= a3)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if ((a5 & 0x8000000000000000) != 0)
  {
LABEL_10:
    __break(1u);
LABEL_11:
    __break(1u);
  }
  if (HIDWORD(a5)) {
    goto LABEL_11;
  }
  uint64_t v11 = *(void *)(a4 + 8 * a3 + 32);
  swift_bridgeObjectRetain();
  vImageHistogramSpecification_PlanarF(a1, &dest, 0, (const vImagePixelCount *)(v11 + 32), a5, 0.0, 1.0, 0);
  return swift_bridgeObjectRelease();
}

void vImage.PixelBuffer.size.getter(void *a1@<X8>)
{
  uint64_t v2 = *v1;
  if (!*(void *)(*v1 + 16))
  {
    __break(1u);
    goto LABEL_10;
  }
  uint64_t v3 = *(void *)(v2 + 48);
  if (v3 < 0)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  uint64_t v4 = *(void *)(v2 + 40);
  if (v4 < 0)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  if (v3) {
    BOOL v5 = v4 == 0;
  }
  else {
    BOOL v5 = 1;
  }
  if (!v5)
  {
    *a1 = v3;
    a1[1] = v4;
    return;
  }
LABEL_12:
  __break(1u);
}

BOOL static vImage.Size.== infix(_:_:)(void *a1, void *a2)
{
  return *a1 == *a2 && a1[1] == a2[1];
}

void *vImage.PixelBuffer<>.init(size:pixelFormat:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X2>, uint64_t a3@<X3>, uint64_t *a4@<X8>)
{
  unint64_t v7 = *(void **)a1;
  uint64_t v8 = *(void *)(a1 + 8);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v9 = swift_allocObject();
  *(_OWORD *)(v9 + 16) = xmmword_1D2135280;
  uint64_t v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  if (v10 < 1)
  {
    __break(1u);
  }
  else if (!HIDWORD(v10))
  {
    uint64_t v11 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)((uint64_t)v7, v8);
    uint64_t v13 = v12;
    uint64_t v15 = v14;
    uint64_t v17 = v16;
    type metadata accessor for vImage.BufferReference();
    uint64_t result = (void *)swift_allocObject();
    result[2] = v11;
    result[3] = v13;
    result[4] = v15;
    result[5] = v17;
    *(void *)(v9 + 32) = v11;
    *(void *)(v9 + 40) = v13;
    *(void *)(v9 + 48) = v15;
    *(void *)(v9 + 56) = v17;
    *(void *)(v9 + 64) = result;
    *a4 = v9;
    return result;
  }
  __break(1u);

  uint64_t result = (void *)_assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.vImageBuffer.getter()
{
  if (*(void *)(*(void *)v0 + 16)) {
    return *(void *)(*(void *)v0 + 32);
  }
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer.width.getter()
{
  if (!*(void *)(*(void *)v0 + 16))
  {
    __break(1u);
    goto LABEL_5;
  }
  uint64_t result = *(void *)(*(void *)v0 + 48);
  if (result < 0) {
LABEL_5:
  }
    __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer.height.getter()
{
  if (!*(void *)(*(void *)v0 + 16))
  {
    __break(1u);
    goto LABEL_5;
  }
  uint64_t result = *(void *)(*(void *)v0 + 40);
  if (result < 0) {
LABEL_5:
  }
    __break(1u);
  return result;
}

uint64_t _ss17withUnsafePointer2to_q0_x_q0_SPyxGq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v11 = *(void *)(a5 - 8);
  uint64_t v12 = MEMORY[0x1F4188790](a1);
  uint64_t v14 = (char *)&v17 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t result = v15(v12, v14);
  if (v8) {
    return (*(uint64_t (**)(uint64_t, char *, uint64_t))(v11 + 32))(a8, v14, a5);
  }
  return result;
}

uint64_t vImage.PixelBuffer<>.vImageBuffers.getter()
{
  uint64_t v1 = *v0;
  int64_t v2 = *(void *)(*v0 + 16);
  uint64_t v3 = MEMORY[0x1E4FBC860];
  if (v2)
  {
    uint64_t v15 = MEMORY[0x1E4FBC860];
    swift_bridgeObjectRetain();
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v2, 0);
    uint64_t v3 = v15;
    unint64_t v4 = *(void *)(v15 + 16);
    uint64_t v5 = 32 * v4;
    unint64_t v6 = (long long *)(v1 + 48);
    do
    {
      long long v7 = *(v6 - 1);
      long long v8 = *v6;
      unint64_t v9 = *(void *)(v15 + 24);
      unint64_t v10 = v4 + 1;
      if (v4 >= v9 >> 1)
      {
        long long v13 = *v6;
        long long v14 = *(v6 - 1);
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v9 > 1), v4 + 1, 1);
        long long v8 = v13;
        long long v7 = v14;
      }
      *(void *)(v15 + 16) = v10;
      uint64_t v11 = v15 + v5;
      *(_OWORD *)(v11 + 32) = v7;
      *(_OWORD *)(v11 + 48) = v8;
      v5 += 32;
      unint64_t v6 = (long long *)((char *)v6 + 40);
      unint64_t v4 = v10;
      --v2;
    }
    while (v2);
    swift_bridgeObjectRelease();
  }
  return v3;
}

uint64_t vImage.PixelBuffer<>.withUnsafePixelBuffers<A>(_:)(void (*a1)(void), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.init(data:width:height:byteCountPerRow:pixelFormat:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t *a5@<X8>)
{
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t result = swift_allocObject();
  *(_OWORD *)(result + 16) = xmmword_1D2135280;
  if ((a3 | a2) < 0)
  {
    __break(1u);
  }
  else
  {
    *(void *)(result + 32) = a1;
    *(void *)(result + 40) = a3;
    *(void *)(result + 48) = a2;
    *(void *)(result + 56) = a4;
    *(void *)(result + 64) = 0;
    *a5 = result;
  }
  return result;
}

uint64_t vImage.PixelBuffer<>.withUnsafePointerToVImageBuffer<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v20 = *MEMORY[0x1E4F143B8];
  uint64_t v5 = *v4;
  if (!*(void *)(*v4 + 16)) {
    __break(1u);
  }
  long long v7 = *(_OWORD *)(v5 + 48);
  v19[0] = *(_OWORD *)(v5 + 32);
  v19[1] = v7;
  uint64_t v8 = MEMORY[0x1F4188790](a1);
  _OWORD v17[2] = *(void *)(v9 + 16);
  v17[3] = v10;
  v17[4] = v11;
  v17[5] = v8;
  v17[6] = v12;
  type metadata accessor for vImage_Buffer(0);
  uint64_t v14 = v13;
  uint64_t v15 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Error);
  return _ss17withUnsafePointer2to_q0_x_q0_SPyxGq_YKXEtq_YKs5ErrorR_Ri_zRi_0_r1_lF((uint64_t)v19, (uint64_t)partial apply for closure #1 in vImage.PixelBuffer<>.withUnsafePointerToVImageBuffer<A>(_:), (uint64_t)v17, v14, v15, a4, MEMORY[0x1E4FBC0F0], (uint64_t)&v18);
}

uint64_t vImage.Size.width.getter()
{
  return *(void *)v0;
}

uint64_t vImage.Size.height.getter()
{
  return *(void *)(v0 + 8);
}

unint64_t vImage.PixelBuffer<>.subscript.getter@<X0>(unint64_t result@<X0>, uint64_t *a2@<X8>)
{
  if ((result & 0x8000000000000000) != 0)
  {
    __break(1u);
  }
  else if (*(void *)(*(void *)v2 + 16) > result)
  {
    uint64_t v4 = *(void *)v2 + 40 * result;
    long long v5 = *(_OWORD *)(v4 + 32);
    long long v6 = *(_OWORD *)(v4 + 48);
    uint64_t v10 = *(void *)(v4 + 64);
    long long v8 = v5;
    long long v9 = v6;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    uint64_t v7 = swift_allocObject();
    *(_OWORD *)(v7 + 16) = xmmword_1D2135280;
    *(_OWORD *)(v7 + 32) = v8;
    *(_OWORD *)(v7 + 48) = v9;
    *(void *)(v7 + 64) = v10;
    *a2 = v7;
    outlined init with take of vImage.BufferReference?((uint64_t)&v10, (uint64_t)v11);
    return outlined retain of vImage.BufferReference?((uint64_t)v11);
  }
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.count.getter(uint64_t a1, uint64_t a2)
{
  uint64_t v4 = *v2;
  uint64_t result = vImage.PixelBuffer<>.rowStride.getter(a1, a2);
  if (!*(void *)(v4 + 16))
  {
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v6 = *(void *)(v4 + 40);
  if (v6 < 0)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  uint64_t v7 = result * v6;
  if ((unsigned __int128)(result * (__int128)v6) >> 64 != (result * v6) >> 63)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v8 = (*(uint64_t (**)(void))(a2 + 24))();
  uint64_t result = v7 * v8;
  if ((unsigned __int128)(v7 * (__int128)v8) >> 64 != (v7 * v8) >> 63) {
LABEL_9:
  }
    __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.rowStride.getter(uint64_t result, uint64_t a2)
{
  if (!*(void *)(*(void *)v2 + 16))
  {
    __break(1u);
    goto LABEL_14;
  }
  uint64_t v4 = *(void *)(*(void *)v2 + 56);
  uint64_t v5 = *(void *)(result + 16);
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  uint64_t v7 = *(void *)(AssociatedTypeWitness - 8);
  uint64_t result = AssociatedTypeWitness - 8;
  uint64_t v8 = *(void *)(v7 + 72);
  if (!v8)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  if (v4 == 0x8000000000000000 && v8 == -1) {
    goto LABEL_16;
  }
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a2 + 24))(v5, a2);
  if (!result)
  {
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (v4 / v8 != 0x8000000000000000 || result != -1) {
    return v4 / v8 / result;
  }
LABEL_17:
  __break(1u);
  return result;
}

uint64_t protocol witness for AccelerateMatrixBuffer.withUnsafeBufferPointer<A>(_:) in conformance <> vImage.PixelBuffer<A>(uint64_t (*a1)(uint64_t), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return vImage.PixelBuffer<>.withUnsafeBufferPointer<A>(_:)(a1, a2, a4, a4, *(void *)(a5 - 8));
}

uint64_t vImage.PixelBuffer<>.withUnsafeBufferPointer<A>(_:)(uint64_t (*a1)(uint64_t), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  long long v9 = (void *)*v5;
  uint64_t result = vImage.PixelBuffer<>.rowStride.getter(a3, a5);
  if (!v9[2])
  {
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v11 = v9[5];
  if (v11 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  uint64_t v12 = result * v11;
  if ((unsigned __int128)(result * (__int128)v11) >> 64 != (result * v11) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  uint64_t result = (*(uint64_t (**)(void, uint64_t))(a5 + 24))(*(void *)(a3 + 16), a5);
  if ((unsigned __int128)(v12 * (__int128)result) >> 64 != (v12 * result) >> 63)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  if (!v9[2])
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  if (v9[4])
  {
    swift_getAssociatedTypeWitness();
    uint64_t v13 = UnsafeBufferPointer.init(start:count:)();
    return a1(v13);
  }
LABEL_13:
  __break(1u);
  return result;
}

uint64_t protocol witness for AccelerateMutableMatrixBuffer.withUnsafeMutableBufferPointer<A>(_:) in conformance <> vImage.PixelBuffer<A>(uint64_t (*a1)(void *), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return vImage.PixelBuffer<>.withUnsafeMutableBufferPointer<A>(_:)(a1, a2, a4, a4, *(void *)(a5 - 8));
}

uint64_t vImage.PixelBuffer<>.withUnsafeMutableBufferPointer<A>(_:)(uint64_t (*a1)(void *), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  v13[0] = *v5;
  uint64_t result = vImage.PixelBuffer<>.rowStride.getter(a3, a5);
  if (!*(void *)(v13[0] + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v10 = *(void *)(v13[0] + 40);
  if (v10 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  uint64_t v11 = result * v10;
  if ((unsigned __int128)(result * (__int128)v10) >> 64 != (result * v10) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  uint64_t result = (*(uint64_t (**)(void, uint64_t))(a5 + 24))(*(void *)(a3 + 16), a5);
  if ((unsigned __int128)(v11 * (__int128)result) >> 64 != (v11 * result) >> 63)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  if (!*(void *)(v13[0] + 16))
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  if (*(void *)(v13[0] + 32))
  {
    swift_getAssociatedTypeWitness();
    v13[0] = UnsafeMutableBufferPointer.init(start:count:)();
    v13[1] = v12;
    return a1(v13);
  }
LABEL_13:
  __break(1u);
  return result;
}

uint64_t vImage.Size.init(width:height:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X1>, uint64_t *a3@<X8>)
{
  if (result < 1 || a2 < 1)
  {
    __break(1u);
  }
  else
  {
    *a3 = result;
    a3[1] = a2;
  }
  return result;
}

uint64_t (*vImage.PixelBuffer<>.withUnsafeVImageBuffer<A>(_:)(uint64_t (*result)(void, void, void, void)))(void, void, void, void)
{
  if (*(void *)(*(void *)v1 + 16)) {
    return (uint64_t (*)(void, void, void, void))result(*(void *)(*(void *)v1 + 32), *(void *)(*(void *)v1 + 40), *(void *)(*(void *)v1 + 48), *(void *)(*(void *)v1 + 56));
  }
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.withUnsafeVImageBuffers<A>(_:)(void (*a1)(void))
{
  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.pixelBuffers.getter(uint64_t a1, uint64_t a2)
{
  void v9[5] = *v2;
  _OWORD v9[2] = *(void *)(a1 + 16);
  _OWORD v9[3] = a2;
  swift_bridgeObjectRetain();
  uint64_t v3 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [vImage.BufferWrapper]);
  swift_getAssociatedTypeWitness();
  swift_getAssociatedConformanceWitness();
  uint64_t v4 = type metadata accessor for vImage.PixelBuffer();
  unint64_t v5 = lazy protocol witness table accessor for type [vImage.BufferWrapper] and conformance [A]();
  uint64_t v7 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in vImage.PixelBuffer<>.pixelBuffers.getter, (uint64_t)v9, v3, v4, MEMORY[0x1E4FBC248], v5, MEMORY[0x1E4FBC278], v6);
  swift_bridgeObjectRelease();
  return v7;
}

uint64_t vImage.PixelBuffer<>.withUnsafePixelBuffer<A>(at:_:)(unint64_t a1, void (*a2)(uint64_t *))
{
  vImage.PixelBuffer<>.subscript.getter(a1, &v5);
  uint64_t v4 = v5;
  a2(&v4);
  return swift_bridgeObjectRelease();
}

uint64_t closure #1 in vImage.PixelBuffer<>.pixelBuffers.getter@<X0>(long long *a1@<X0>, uint64_t *a2@<X8>)
{
  uint64_t v8 = *((void *)a1 + 4);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v4 = swift_allocObject();
  long long v5 = *a1;
  long long v6 = a1[1];
  *(_OWORD *)(v4 + 16) = xmmword_1D2135280;
  *(_OWORD *)(v4 + 32) = v5;
  *(_OWORD *)(v4 + 48) = v6;
  *(void *)(v4 + 64) = *((void *)a1 + 4);
  *a2 = v4;
  outlined init with take of vImage.BufferReference?((uint64_t)&v8, (uint64_t)v9);
  return outlined retain of vImage.BufferReference?((uint64_t)v9);
}

double vImage.PixelBuffer<>.init(bufferWrapper:pixelFormat:)@<D0>(long long *a1@<X0>, uint64_t *a2@<X8>)
{
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v4 = swift_allocObject();
  *(void *)&double result = 1;
  long long v6 = *a1;
  long long v7 = a1[1];
  *(_OWORD *)(v4 + 16) = xmmword_1D2135280;
  *(_OWORD *)(v4 + 32) = v6;
  *(_OWORD *)(v4 + 48) = v7;
  *(void *)(v4 + 64) = *((void *)a1 + 4);
  *a2 = v4;
  return result;
}

uint64_t vImage.PixelBuffer<>.init(data:width:height:byteCountPerRow:pixelFormat:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, char a5@<W4>, uint64_t a6@<X6>, uint64_t a7@<X7>, uint64_t *a8@<X8>)
{
  if ((a5 & 1) == 0) {
    goto LABEL_4;
  }
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  uint64_t v17 = *(void *)(AssociatedTypeWitness - 8);
  uint64_t result = AssociatedTypeWitness - 8;
  uint64_t v18 = *(void *)(v17 + 72);
  uint64_t v19 = a2 * v18;
  if ((unsigned __int128)(a2 * (__int128)v18) >> 64 != (a2 * v18) >> 63)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 24))(a6, a7);
  a4 = v19 * result;
  if ((unsigned __int128)(v19 * (__int128)result) >> 64 == (v19 * result) >> 63)
  {
LABEL_4:
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    uint64_t result = swift_allocObject();
    *(_OWORD *)(result + 16) = xmmword_1D2135280;
    if (((a3 | a2) & 0x8000000000000000) == 0)
    {
      *(void *)(result + 32) = a1;
      *(void *)(result + 40) = a3;
      *(void *)(result + 48) = a2;
      *(void *)(result + 56) = a4;
      *(void *)(result + 64) = 0;
      *a8 = result;
      return result;
    }
    __break(1u);
    goto LABEL_7;
  }
LABEL_8:
  __break(1u);
  return result;
}

int64_t vImage.PixelBuffer<>.init(size:pixelFormat:)@<X0>(uint64_t *a1@<X0>, void *a2@<X2>, uint64_t a3@<X3>, void *a4@<X8>)
{
  uint64_t v7 = *a1;
  uint64_t v8 = a1[1];
  int64_t result = (*(uint64_t (**)(void *, uint64_t))(a3 + 32))(a2, a3);
  if ((result & 0x8000000000000000) == 0)
  {
    int64_t v10 = result;
    uint64_t v11 = MEMORY[0x1E4FBC860];
    if (!result)
    {
LABEL_9:
      *a4 = v11;
      return result;
    }
    uint64_t v20 = a4;
    uint64_t v24 = MEMORY[0x1E4FBC860];
    int64_t result = (int64_t)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, result, 0);
    uint64_t v11 = v24;
    while (v10)
    {
      int64_t result = (int64_t)closure #1 in vImage.PixelBuffer<>.init(size:pixelFormat:)(v7, v8, a2, a3, (uint64_t *)&v21);
      long long v12 = v21;
      long long v13 = v22;
      uint64_t v14 = v23;
      uint64_t v24 = v11;
      unint64_t v16 = *(void *)(v11 + 16);
      unint64_t v15 = *(void *)(v11 + 24);
      if (v16 >= v15 >> 1)
      {
        long long v18 = v22;
        long long v19 = v21;
        int64_t result = (int64_t)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v15 > 1), v16 + 1, 1);
        long long v13 = v18;
        long long v12 = v19;
        uint64_t v11 = v24;
      }
      *(void *)(v11 + 16) = v16 + 1;
      uint64_t v17 = v11 + 40 * v16;
      *(_OWORD *)(v17 + 32) = v12;
      *(_OWORD *)(v17 + 48) = v13;
      *(void *)(v17 + 64) = v14;
      if (!--v10)
      {
        a4 = v20;
        goto LABEL_9;
      }
    }
    __break(1u);
  }
  __break(1u);
  return result;
}

void *closure #1 in vImage.PixelBuffer<>.init(size:pixelFormat:)@<X0>(uint64_t a1@<X1>, uint64_t a2@<X2>, void *a3@<X3>, uint64_t a4@<X4>, uint64_t *a5@<X8>)
{
  long long v9 = a3;
  uint64_t v10 = (*(uint64_t (**)(void *, uint64_t))(a4 + 40))(a3, a4);
  if (v10 < 1)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (HIDWORD(v10))
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v11 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(a1, a2);
  long long v9 = v5;
  if (!v5)
  {
    uint64_t v15 = v11;
    uint64_t v16 = v12;
    uint64_t v17 = v13;
    uint64_t v18 = v14;
    type metadata accessor for vImage.BufferReference();
    int64_t result = (void *)swift_allocObject();
    result[2] = v15;
    result[3] = v16;
    result[4] = v17;
    result[5] = v18;
    *a5 = v15;
    a5[1] = v16;
    a5[2] = v17;
    a5[3] = v18;
    a5[4] = (uint64_t)result;
    return result;
  }
LABEL_7:

  int64_t result = (void *)_assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X2>, uint64_t a3@<X3>, char **a4@<X8>)
{
  swift_getAssociatedTypeWitness();
  swift_getAssociatedConformanceWitness();
  uint64_t v8 = type metadata accessor for vImage.PixelBuffer();
  uint64_t v9 = MEMORY[0x1D25FF9B0](a1, v8);
  uint64_t v10 = *(uint64_t (**)(uint64_t, uint64_t))(a3 + 32);
  uint64_t v11 = v10(a2, a3);
  if (v9 != v11)
  {
    __break(1u);
    goto LABEL_7;
  }
  long long v32 = a4;
  long long v33 = v10;
  uint64_t v34 = a1;
  MEMORY[0x1F4188790](v11);
  uint64_t v29 = a2;
  uint64_t v30 = a3;
  uint64_t v12 = type metadata accessor for Array();
  uint64_t WitnessTable = swift_getWitnessTable();
  uint64_t v15 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:), (uint64_t)v28, v12, MEMORY[0x1E4FBB550], MEMORY[0x1E4FBC248], WitnessTable, MEMORY[0x1E4FBC278], v14);
  uint64_t v16 = specialized Set.init<A>(_:)(v15);
  uint64_t v17 = swift_bridgeObjectRelease();
  uint64_t v31 = a1;
  uint64_t v34 = a1;
  MEMORY[0x1F4188790](v17);
  uint64_t v29 = a2;
  uint64_t v30 = a3;
  uint64_t v19 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lF((void (*)(char *, char *))partial apply for closure #1 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:), (uint64_t)v28, v12, MEMORY[0x1E4FBB550], MEMORY[0x1E4FBC248], WitnessTable, MEMORY[0x1E4FBC278], v18);
  uint64_t v20 = specialized Set.init<A>(_:)(v19);
  swift_bridgeObjectRelease();
  uint64_t v21 = *(void *)(v16 + 16);
  swift_bridgeObjectRelease();
  if (v21 == 1)
  {
    uint64_t v22 = *(void *)(v20 + 16);
    swift_bridgeObjectRelease();
    if (v22 == 1)
    {
      uint64_t v23 = v33(a2, a3);
      if ((v23 & 0x8000000000000000) == 0)
      {
        MEMORY[0x1F4188790](v23);
        v28[0] = a2;
        v28[1] = a3;
        uint64_t v29 = v31;
        unint64_t v25 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO13BufferWrapperVs5NeverOTg5((char *)partial apply for closure #3 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:), (uint64_t)&v27, 0, v24);
        uint64_t result = swift_bridgeObjectRelease();
        *long long v32 = v25;
        return result;
      }
      goto LABEL_8;
    }
LABEL_7:
    __break(1u);
LABEL_8:
    __break(1u);
  }
  uint64_t result = swift_bridgeObjectRelease();
  __break(1u);
  return result;
}

void *closure #3 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)@<X0>(uint64_t a1@<X2>, uint64_t a2@<X3>, uint64_t a3@<X8>)
{
  swift_getAssociatedTypeWitness();
  swift_getAssociatedConformanceWitness();
  type metadata accessor for vImage.PixelBuffer();
  Array.subscript.getter();
  uint64_t result = v13;
  if (v13[2])
  {
    uint64_t v7 = (void *)v13[4];
    vImagePixelCount v8 = v13[5];
    vImagePixelCount v9 = v13[6];
    size_t v10 = v13[7];
    swift_bridgeObjectRelease();
    unint64_t v11 = (*(uint64_t (**)(uint64_t, uint64_t))(a2 + 40))(a1, a2);
    uint64_t result = specialized vImage.BufferWrapper.init(copying:bitsPerPixel:)(v7, v8, v9, v10, v11, v14);
    long long v12 = v14[1];
    *(_OWORD *)a3 = v14[0];
    *(_OWORD *)(a3 + 16) = v12;
    *(void *)(a3 + 32) = v15;
  }
  else
  {
    __break(1u);
  }
  return result;
}

void *vImage.PixelBuffer<>.init(width:height:pixelFormat:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X3>, uint64_t a4@<X4>, uint64_t *a5@<X8>)
{
  if (a1 < 1 || (long long v5 = (void *)a2, a2 < 1))
  {
    __break(1u);
    goto LABEL_7;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v10 = swift_allocObject();
  *(_OWORD *)(v10 + 16) = xmmword_1D2135280;
  uint64_t v11 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  if (v11 < 1)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (!HIDWORD(v11))
  {
    uint64_t v12 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(a1, (uint64_t)v5);
    uint64_t v14 = v13;
    uint64_t v16 = v15;
    uint64_t v18 = v17;
    type metadata accessor for vImage.BufferReference();
    uint64_t result = (void *)swift_allocObject();
    result[2] = v12;
    result[3] = v14;
    result[4] = v16;
    result[5] = v18;
    *(void *)(v10 + 32) = v12;
    *(void *)(v10 + 40) = v14;
    *(void *)(v10 + 48) = v16;
    *(void *)(v10 + 56) = v18;
    *(void *)(v10 + 64) = result;
    *a5 = v10;
    return result;
  }
LABEL_8:
  __break(1u);

  uint64_t result = (void *)_assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.channelCount.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 24))();
}

uint64_t vImage.PixelBuffer<>.byteCountPerPixel.getter(uint64_t a1, uint64_t a2)
{
  return specialized vImage.PixelBuffer<>.byteCountPerPixel.getter(*(void *)(a1 + 16), a2);
}

vImage_Error vImage.PixelBuffer<>.copy(to:)(uint64_t *a1, uint64_t a2, uint64_t a3)
{
  uint64_t v10 = *MEMORY[0x1E4F143B8];
  uint64_t v5 = *v3;
  if (!*(void *)(*v3 + 16)) {
    __break(1u);
  }
  uint64_t v6 = *a1;
  long long v7 = *(_OWORD *)(v5 + 48);
  *(_OWORD *)&v9.char data = *(_OWORD *)(v5 + 32);
  *(_OWORD *)&v9.width = v7;
  return closure #1 in vImage.PixelBuffer<>.copy(to:)(&v9, v6, v5, *(void *)(a2 + 16), a3);
}

vImage_Error closure #1 in vImage.PixelBuffer<>.copy(to:)(const vImage_Buffer *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v10 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a2 + 16)) {
    __break(1u);
  }
  long long v6 = *(_OWORD *)(a2 + 48);
  *(_OWORD *)&v9.char data = *(_OWORD *)(a2 + 32);
  *(_OWORD *)&v9.width = v6;
  uint64_t v7 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  return vImageCopyBuffer(a1, &v9, v7 / 8, 0);
}

uint64_t vImage.PixelBuffer<>.cropped(to:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X1>, uint64_t *a3@<X8>, double a4@<D0>)
{
  uint64_t v9 = *v7;
  if (!*(void *)(*v7 + 16))
  {
    __break(1u);
    goto LABEL_14;
  }
  uint64_t v10 = *(void *)(v9 + 48);
  if (v10 < 0)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v11 = *(void *)(v9 + 40);
  if (v11 < 0)
  {
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
LABEL_17:
    __break(1u);
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
    return result;
  }
  uint64_t v13 = result;
  CGFloat v14 = (double)v10;
  CGFloat v15 = (double)v11;
  CGRect v27 = CGRectIntegral(*(CGRect *)&a4);
  v25.origin.double x = 0.0;
  v25.origin.double y = 0.0;
  v25.size.width = v14;
  v25.size.double height = v15;
  CGRect v26 = CGRectIntersection(v25, v27);
  double x = v26.origin.x;
  double y = v26.origin.y;
  double height = v26.size.height;
  uint64_t result = specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&v21, v26.size.width);
  if (result & 1) == 0 || (v22) {
    goto LABEL_18;
  }
  uint64_t v19 = v21;
  if (v21 < 1) {
    goto LABEL_16;
  }
  uint64_t result = specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&v21, height);
  if ((result & 1) == 0 || v22 == 1) {
    goto LABEL_19;
  }
  uint64_t v20 = v21;
  if (v21 < 1) {
    goto LABEL_17;
  }
  uint64_t v21 = v19;
  uint64_t v22 = v20;
  char v23 = 0;
  vImage.PixelBuffer<>.init(size:pixelFormat:)((uint64_t)&v21, *(void *)(v13 + 16), a2, a3);

  return vImage.PixelBuffer<>.crop(at:destination:)(a3, v13, a2, x, y);
}

BOOL vImage.Size.init(exactly:)@<W0>(uint64_t a1@<X8>, double a2@<D0>, double a3@<D1>)
{
  BOOL result = specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&v8, a2);
  if (!result
    || v9 == 1
    || (uint64_t v6 = v8, v8 < 1)
    || !(BOOL result = specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&v8, a3))
    || v9 == 1
    || (uint64_t v7 = v8, v8 < 1))
  {
    *(void *)a1 = 0;
    *(void *)(a1 + 8) = 0;
    *(unsigned char *)(a1 + 16) = 1;
  }
  else
  {
    *(void *)a1 = v6;
    *(void *)(a1 + 8) = v7;
    *(unsigned char *)(a1 + 16) = 0;
  }
  return result;
}

uint64_t vImage.PixelBuffer<>.crop(at:destination:)(uint64_t *a1, uint64_t a2, uint64_t a3, double a4, double a5)
{
  uint64_t v6 = v5;
  uint64_t v38 = *MEMORY[0x1E4F143B8];
  uint64_t v7 = *(void **)v5;
  if (!*(void *)(*(void *)v5 + 16))
  {
    __break(1u);
    goto LABEL_30;
  }
  uint64_t v8 = v7[6];
  if (v8 < 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  uint64_t v9 = v7[5];
  if (v9 < 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  uint64_t v11 = *a1;
  if (!*(void *)(*a1 + 16))
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  uint64_t v12 = *(void *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  uint64_t v13 = *(void *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  CGFloat v16 = (double)v8;
  CGFloat v17 = (double)v9;
  double v18 = (double)v12;
  double v19 = (double)v13;
  CGRect v43 = CGRectIntegral(*(CGRect *)&a4);
  v39.origin.double x = 0.0;
  v39.origin.double y = 0.0;
  v39.size.CGFloat width = v16;
  v39.size.CGFloat height = v17;
  CGRect v40 = CGRectIntersection(v39, v43);
  double x = v40.origin.x;
  double y = v40.origin.y;
  CGFloat width = v40.size.width;
  CGFloat height = v40.size.height;
  if (CGRectIsEmpty(v40))
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if ((~*(void *)&y & 0x7FF0000000000000) == 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (y <= -9.22337204e18)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (y >= 9.22337204e18)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v7[2])
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  uint64_t v24 = v7[7];
  uint64_t v25 = (uint64_t)y * v24;
  if ((unsigned __int128)((uint64_t)y * (__int128)v24) >> 64 != v25 >> 63)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if ((~*(void *)&x & 0x7FF0000000000000) == 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (x <= -9.22337204e18)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (x >= 9.22337204e18)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  uint64_t v26 = *(void *)(a2 + 16);
  uint64_t v27 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(v26, a3);
  uint64_t v28 = (uint64_t)x * (v27 / 8);
  if ((unsigned __int128)((uint64_t)x * (__int128)(v27 / 8)) >> 64 != v28 >> 63)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  BOOL v29 = __OFADD__(v25, v28);
  uint64_t v30 = v25 + v28;
  if (v29)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7[2])
  {
LABEL_46:
    __break(1u);
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  uint64_t v31 = v7[4];
  if (!v31) {
    goto LABEL_55;
  }
  v41.origin.double x = x;
  v41.origin.double y = y;
  v41.size.CGFloat width = width;
  v41.size.CGFloat height = height;
  double v32 = CGRectGetWidth(v41);
  if ((~*(void *)&v32 & 0x7FF0000000000000) == 0) {
    goto LABEL_47;
  }
  if (v32 <= -9.22337204e18)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (v32 >= 9.22337204e18)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  v42.origin.double x = x;
  v42.origin.double y = y;
  v42.size.CGFloat width = width;
  v42.size.CGFloat height = height;
  double v33 = CGRectGetHeight(v42);
  if ((~*(void *)&v33 & 0x7FF0000000000000) == 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (v33 <= -9.22337204e18)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v33 >= 9.22337204e18)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v7[2])
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  vImage.PixelBuffer<>.init(data:width:height:byteCountPerRow:pixelFormat:)(v31 + v30, (uint64_t)v32, (uint64_t)v33, v7[7], &v36);
  if (!*(void *)(v36 + 16))
  {
LABEL_54:
    __break(1u);
LABEL_55:
    __break(1u);
  }
  long long v34 = *(_OWORD *)(v36 + 48);
  *(_OWORD *)&v37.char data = *(_OWORD *)(v36 + 32);
  *(_OWORD *)&v37.CGFloat width = v34;
  closure #1 in vImage.PixelBuffer<>.crop(at:destination:)(&v37, a1, v6, v26, a3);
  return swift_bridgeObjectRelease();
}

vImage_Error closure #1 in vImage.PixelBuffer<>.crop(at:destination:)(const vImage_Buffer *a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v11 = *MEMORY[0x1E4F143B8];
  uint64_t v5 = *a2;
  if (!*(void *)(*a2 + 16)) {
    __break(1u);
  }
  long long v7 = *(_OWORD *)(v5 + 48);
  *(_OWORD *)&v10.char data = *(_OWORD *)(v5 + 32);
  *(_OWORD *)&v10.CGFloat width = v7;
  uint64_t v8 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  return vImageCopyBuffer(a1, &v10, v8 / 8, 0);
}

void vImage.PixelBuffer<>.withUnsafeRegionOfInterest<A>(_:_:)(void (*a1)(uint64_t *), double a2, double a3, double a4, double a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  vImage_Buffer v10 = *(void **)v9;
  if (!*(void *)(*(void *)v9 + 16))
  {
    __break(1u);
    goto LABEL_26;
  }
  uint64_t v11 = v10[6];
  if (v11 < 0)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  uint64_t v12 = v10[5];
  if (v12 < 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  CGFloat v15 = (double)v11;
  CGFloat v16 = (double)v12;
  CGRect v34 = CGRectIntegral(*(CGRect *)&a2);
  v30.origin.double x = 0.0;
  v30.origin.double y = 0.0;
  v30.size.CGFloat width = v15;
  v30.size.CGFloat height = v16;
  CGRect v31 = CGRectIntersection(v30, v34);
  double x = v31.origin.x;
  double y = v31.origin.y;
  CGFloat width = v31.size.width;
  CGFloat height = v31.size.height;
  if (CGRectIsEmpty(v31))
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if ((~*(void *)&y & 0x7FF0000000000000) == 0)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (y <= -9.22337204e18)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (y >= 9.22337204e18)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v10[2])
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  uint64_t v21 = v10[7];
  uint64_t v22 = (uint64_t)y * v21;
  if ((unsigned __int128)((uint64_t)y * (__int128)v21) >> 64 != v22 >> 63)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if ((~*(void *)&x & 0x7FF0000000000000) == 0)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (x <= -9.22337204e18)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (x >= 9.22337204e18)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v23 = (*(uint64_t (**)(void))(a9 + 16))();
  uint64_t v24 = (uint64_t)x * (v23 / 8);
  if ((unsigned __int128)((uint64_t)x * (__int128)(v23 / 8)) >> 64 != v24 >> 63)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  uint64_t v25 = v22 + v24;
  if (__OFADD__(v22, v24))
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v10[2])
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  uint64_t v26 = v10[4];
  if (v26)
  {
    v32.origin.double x = x;
    v32.origin.double y = y;
    v32.size.CGFloat width = width;
    v32.size.CGFloat height = height;
    double v27 = CGRectGetWidth(v32);
    if ((~*(void *)&v27 & 0x7FF0000000000000) != 0)
    {
      if (v27 > -9.22337204e18)
      {
        if (v27 < 9.22337204e18)
        {
          v33.origin.double x = x;
          v33.origin.double y = y;
          v33.size.CGFloat width = width;
          v33.size.CGFloat height = height;
          double v28 = CGRectGetHeight(v33);
          if ((~*(void *)&v28 & 0x7FF0000000000000) != 0)
          {
            if (v28 > -9.22337204e18)
            {
              if (v28 < 9.22337204e18)
              {
                if (v10[2])
                {
                  vImage.PixelBuffer<>.init(data:width:height:byteCountPerRow:pixelFormat:)(v26 + v25, (uint64_t)v27, (uint64_t)v28, v10[7], &v29);
                  a1(&v29);
                  swift_bridgeObjectRelease();
                  return;
                }
                goto LABEL_46;
              }
LABEL_45:
              __break(1u);
LABEL_46:
              __break(1u);
              goto LABEL_47;
            }
LABEL_44:
            __break(1u);
            goto LABEL_45;
          }
LABEL_43:
          __break(1u);
          goto LABEL_44;
        }
LABEL_42:
        __break(1u);
        goto LABEL_43;
      }
LABEL_41:
      __break(1u);
      goto LABEL_42;
    }
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
LABEL_47:
  __break(1u);
}

double static vImage.PixelBuffer<>.makeDynamicPixelBufferAndCGImageFormat(cgImage:)@<D0>(uint64_t *a1@<X0>, void *a2@<X1>, uint64_t a3@<X8>)
{
  uint64_t v19 = *MEMORY[0x1E4F143B8];
  int v16 = 0;
  long long v14 = 0u;
  long long v15 = 0u;
  id v6 = a2;
  specialized vImage.BufferWrapper.init(cgImage:format:)((uint64_t)v6, (uint64_t)&v14, (uint64_t)v17);

  if (!v3)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    uint64_t v8 = swift_allocObject();
    long long v9 = v17[0];
    long long v10 = v17[1];
    *(_OWORD *)(v8 + 16) = xmmword_1D2135280;
    *(_OWORD *)(v8 + 32) = v9;
    *(_OWORD *)(v8 + 48) = v10;
    *(void *)(v8 + 64) = v18;
    *a1 = v8;
    uint64_t v11 = *((void *)&v14 + 1);
    uint64_t v12 = *((void *)&v15 + 1);
    int v13 = v16;
    double result = *(double *)&v14;
    *(void *)&long long v9 = v15;
    *(void *)a3 = v14;
    *(void *)(a3 + 8) = v11;
    *(void *)(a3 + 16) = v9;
    *(void *)(a3 + 24) = v12;
    *(_DWORD *)(a3 + 32) = v13;
  }
  return result;
}

double static vImage.PixelBuffer<>.makePixelBufferAndCGImageFormat(cgImage:pixelFormat:)@<D0>(void *a1@<X0>, void *a2@<X1>, uint64_t a3@<X3>, uint64_t a4@<X4>, size_t a5@<X5>, uint64_t a6@<X8>)
{
  uint64_t v18 = *MEMORY[0x1E4F143B8];
  int v17 = 0;
  long long v15 = 0u;
  long long v16 = 0u;
  vImage.PixelBuffer<>.init(cgImage:cgImageFormat:pixelFormat:)((CGImage *)a2, (unsigned int *)&v15, a3, a4, a5, &v14);
  if (!v6)
  {
    *a1 = v14;
    uint64_t v10 = *((void *)&v15 + 1);
    uint64_t v11 = *((void *)&v16 + 1);
    int v12 = v17;
    double result = *(double *)&v15;
    uint64_t v13 = v16;
    *(void *)a6 = v15;
    *(void *)(a6 + 8) = v10;
    *(void *)(a6 + 16) = v13;
    *(void *)(a6 + 24) = v11;
    *(_DWORD *)(a6 + 32) = v12;
  }
  return result;
}

void vImage.PixelBuffer<>.init(cgImage:cgImageFormat:pixelFormat:)(CGImage *a1@<X0>, unsigned int *a2@<X1>, uint64_t a3@<X3>, uint64_t a4@<X4>, size_t BitsPerComponent@<X5>, uint64_t *a6@<X8>)
{
  uint64_t v31 = *MEMORY[0x1E4F143B8];
  size_t BitsPerPixel = *a2;
  uint64_t v14 = a2[1];
  uint64_t v15 = *((void *)a2 + 1);
  uint64_t v16 = *((void *)a2 + 3);
  unsigned int v17 = a2[8];
  v24[0] = *a2;
  v24[1] = v14;
  uint64_t v25 = v15;
  uint64_t v26 = *((void *)a2 + 2);
  uint64_t v27 = v16;
  unsigned int v28 = v17;
  int v23 = 0;
  memset(v22, 0, sizeof(v22));
  if (MEMORY[0x1D2600F50](v24, v22))
  {
    size_t BitsPerPixel = CGImageGetBitsPerPixel(a1);
    if (BitsPerPixel != (*(uint64_t (**)(uint64_t, size_t))(BitsPerComponent + 16))(a3, BitsPerComponent))
    {
      __break(1u);
      goto LABEL_12;
    }
    BitsPerComponent = CGImageGetBitsPerComponent(a1);
    if (BitsPerComponent == (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4)) {
      goto LABEL_7;
    }
    __break(1u);
  }
  if ((*(uint64_t (**)(uint64_t, size_t))(BitsPerComponent + 16))(a3, BitsPerComponent) != v14)
  {
LABEL_12:
    __break(1u);
LABEL_13:
    __break(1u);
  }
  if ((*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4) != BitsPerPixel) {
    goto LABEL_13;
  }
LABEL_7:
  uint64_t v18 = a1;
  specialized vImage.BufferWrapper.init(cgImage:format:)((uint64_t)v18, (uint64_t)a2, (uint64_t)v29);

  if (v6)
  {
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    uint64_t v19 = swift_allocObject();
    long long v20 = v29[0];
    long long v21 = v29[1];
    *(_OWORD *)(v19 + 16) = xmmword_1D2135280;
    *(_OWORD *)(v19 + 32) = v20;
    *(_OWORD *)(v19 + 48) = v21;
    *(void *)(v19 + 64) = v30;

    *a6 = v19;
  }
}

void *vImage.PixelBuffer<>.makeCGImage(cgImageFormat:)(void *result)
{
  uint64_t v2 = *(void **)v1;
  if (*(void *)(*(void *)v1 + 16))
  {
    uint64_t v3 = v2[4];
    uint64_t v4 = v2[5];
    uint64_t v5 = v2[6];
    uint64_t v6 = v2[7];
    unsigned int v7 = 0;
    return vImage_Buffer.createCGImage(format:flags:)((uint64_t)result, &v7, v3, v4, v5, v6);
  }
  else
  {
    __break(1u);
  }
  return result;
}

void vImage.PixelBuffer<>.init(referencing:planeIndex:overrideSize:pixelFormat:)(__CVBuffer *a1@<X0>, size_t a2@<X1>, uint64_t a3@<X2>, void *a4@<X8>)
{
  size_t WidthOfPlane = *(void *)a3;
  size_t HeightOfPlane = *(void *)(a3 + 8);
  int v9 = *(unsigned __int8 *)(a3 + 16);
  BaseAddressOfPlane = CVPixelBufferGetBaseAddressOfPlane(a1, a2);
  size_t BytesPerRowOfPlane = CVPixelBufferGetBytesPerRowOfPlane(a1, a2);
  if (v9 == 1)
  {
    size_t HeightOfPlane = CVPixelBufferGetHeightOfPlane(a1, a2);
    size_t WidthOfPlane = CVPixelBufferGetWidthOfPlane(a1, a2);
  }
  if (BaseAddressOfPlane)
  {
    vImage.PixelBuffer<>.init(data:width:height:byteCountPerRow:pixelFormat:)((uint64_t)BaseAddressOfPlane, WidthOfPlane, HeightOfPlane, BytesPerRowOfPlane, &v12);

    *a4 = v12;
  }
  else
  {
    __break(1u);
  }
}

void vImage.PixelBuffer<>.init(copying:cvImageFormat:cgImageFormat:pixelFormat:)(void *a1@<X0>, void *a2@<X1>, uint64_t a3@<X2>, uint64_t *a4@<X8>)
{
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v9 = swift_allocObject();
  *(_OWORD *)(v9 + 16) = xmmword_1D2135280;
  specialized vImage.BufferWrapper.init(cvPixelBuffer:cvImageFormat:cgImageFormat:)((uint64_t)a1, (uint64_t)a2, a3, (uint64_t)v11);
  if (v4)
  {
    *(void *)(v9 + 16) = 0;

    swift_release();
  }
  else
  {
    long long v10 = v11[1];
    *(_OWORD *)(v9 + 32) = v11[0];
    *(_OWORD *)(v9 + 48) = v10;
    *(void *)(v9 + 64) = v12;

    *a4 = v9;
  }
}

void vImage.PixelBuffer<>.init(referencing:converter:destinationPixelFormat:)(void *a1@<X0>, void *a2@<X1>, uint64_t *a3@<X8>)
{
  uint64_t v10 = *MEMORY[0x1E4F143B8];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v6 = swift_allocObject();
  *(_OWORD *)(v6 + 16) = xmmword_1D2135280;
  long long v8 = 0u;
  long long v9 = 0u;
  MEMORY[0x1D2600F10](&v8, a2, a1, 512);
  long long v7 = v9;
  *(_OWORD *)(v6 + 32) = v8;
  *(_OWORD *)(v6 + 48) = v7;
  *(void *)(v6 + 64) = 0;

  *a3 = v6;
}

uint64_t vImage.PixelBuffer<>.copy(to:cvImageFormat:cgImageFormat:)(__CVBuffer *a1, uint64_t a2, uint64_t a3)
{
  uint64_t v23 = *MEMORY[0x1E4F143B8];
  uint64_t v7 = *v3;
  if (CVPixelBufferGetPlaneCount(a1))
  {
    __break(1u);
    goto LABEL_15;
  }
  swift_bridgeObjectRetain();
  size_t Width = CVPixelBufferGetWidth(a1);
  if (!*(void *)(v7 + 16))
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v9 = *(void *)(v7 + 48);
  if (v9 < 0)
  {
LABEL_16:
    __break(1u);
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (Width != v9) {
    goto LABEL_21;
  }
  size_t Height = CVPixelBufferGetHeight(a1);
  if (!*(void *)(v7 + 16)) {
    goto LABEL_17;
  }
  uint64_t v11 = *(void *)(v7 + 40);
  if (v11 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  size_t v12 = Height;
  swift_bridgeObjectRelease();
  if (v12 != v11)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!*(void *)(v7 + 16))
  {
LABEL_20:
    __break(1u);
LABEL_21:
    swift_bridgeObjectRelease();
    __break(1u);
  }
  long long v13 = *(_OWORD *)(v7 + 48);
  v22[0] = *(_OWORD *)(v7 + 32);
  v22[1] = v13;
  long long v14 = *(_OWORD *)(a3 + 16);
  v20[0] = *(_OWORD *)a3;
  v20[1] = v14;
  uint64_t v21 = *(void *)(a3 + 32);
  uint64_t result = MEMORY[0x1D2600EE0](v22, v20, a1, a2, 0, 0);
  if (result)
  {
    uint64_t v16 = result;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    uint64_t v18 = v17;
    vImage.Error.init(rawValue:)(v16, (char *)v20);
    char v19 = v20[0];
    if (LOBYTE(v20[0]) == 20) {
      char v19 = 11;
    }
    *uint64_t v18 = v19;
    return swift_willThrow();
  }
  return result;
}

uint64_t vImage.PixelBuffer<>.init<A>(pixelValues:size:pixelFormat:)@<X0>(uint64_t a1@<X0>, void (*a2)(char *, void *)@<X3>, void *a3@<X4>, uint64_t a4@<X5>, uint64_t a5@<X6>, uint64_t *a6@<X8>)
{
  uint64_t v52 = a6;
  uint64_t v57 = *MEMORY[0x1E4F143B8];
  uint64_t v11 = *(a3 - 1);
  MEMORY[0x1F4188790](a1);
  long long v13 = (char *)&v47 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v16 = *v14;
  uint64_t v15 = v14[1];
  (*(void (**)(char *, uint64_t, uint64_t))(v11 + 16))(v13, a1, v17);
  uint64_t v18 = *(uint64_t (**)(void *, uint64_t))(a5 + 16);
  uint64_t v51 = a5;
  uint64_t v19 = v18(a3, a5);
  uint64_t v20 = v16 * v15;
  uint64_t v49 = v15;
  uint64_t v50 = v16;
  if ((unsigned __int128)(v16 * (__int128)v15) >> 64 != (v16 * v15) >> 63)
  {
    __break(1u);
    goto LABEL_10;
  }
  uint64_t v21 = a2;
  uint64_t v22 = v19;
  uint64_t v48 = a1;
  uint64_t v23 = v21;
  uint64_t v24 = (void (*)(char *, void *))a4;
  uint64_t v25 = a4;
  uint64_t v26 = *(uint64_t (**)(void, uint64_t))(a4 + 24);
  uint64_t v27 = v26(v21, v25);
  long long v47 = *(void (**)(char *, void *))(v11 + 8);
  v47(v13, a3);
  if ((unsigned __int128)(v20 * (__int128)v27) >> 64 != (v20 * v27) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (v22 != v20 * v27)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  uint64_t v28 = *(void *)(*(void *)(swift_getAssociatedTypeWitness() - 8) + 72);
  if ((unint64_t)(v28 - 0x1000000000000000) >> 61 != 7)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  uint64_t v29 = 8 * v28;
  uint64_t v30 = v26(v23, (uint64_t)v24);
  unint64_t v31 = v29 * v30;
  if ((unsigned __int128)(v29 * (__int128)v30) >> 64 != (v29 * v30) >> 63)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  if ((v31 & 0x8000000000000000) != 0)
  {
LABEL_14:
    __break(1u);
LABEL_15:
    __break(1u);
  }
  if (HIDWORD(v31)) {
    goto LABEL_15;
  }
  CGRect v33 = (void (*)(char *, void *))v49;
  CGRect v32 = (void (*)(char *, void *))v50;
  uint64_t v53 = specialized vImage_Buffer.init(size:bitsPerPixel:)((double)v50, (double)v49);
  uint64_t v54 = v34;
  uint64_t v55 = v35;
  uint64_t v56 = v36;
  MEMORY[0x1F4188790](v53);
  *(&v47 - 8) = v23;
  *(&v47 - 7) = (void (*)(char *, void *))a3;
  uint64_t v37 = v51;
  *(&v47 - 6) = v24;
  *(&v47 - 5) = (void (*)(char *, void *))v37;
  *(&v47 - 4) = v33;
  *(&v47 - 3) = v32;
  long long v46 = &v53;
  uint64_t v38 = v48;
  (*(void (**)(vImage_Error (*)(uint64_t, uint64_t)))(v37 + 24))(partial apply for closure #1 in vImage.PixelBuffer<>.init<A>(pixelValues:size:pixelFormat:));
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v39 = swift_allocObject();
  *(_OWORD *)(v39 + 16) = xmmword_1D2135280;
  uint64_t v40 = v53;
  uint64_t v41 = v54;
  uint64_t v42 = v55;
  uint64_t v43 = v56;
  type metadata accessor for vImage.BufferReference();
  long long v44 = (void *)swift_allocObject();
  v44[2] = v40;
  v44[3] = v41;
  v44[4] = v42;
  v44[5] = v43;
  *(void *)(v39 + 32) = v40;
  *(void *)(v39 + 40) = v41;
  *(void *)(v39 + 48) = v42;
  *(void *)(v39 + 56) = v43;
  *(void *)(v39 + 64) = v44;
  uint64_t result = ((uint64_t (*)(uint64_t, void *))v47)(v38, a3);
  *uint64_t v52 = v39;
  return result;
}

vImage_Error closure #1 in vImage.PixelBuffer<>.init<A>(pixelValues:size:pixelFormat:)(uint64_t a1, uint64_t a2, vImagePixelCount a3, int64_t a4, vImage_Buffer *a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v24 = *MEMORY[0x1E4F143B8];
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  uint64_t v14 = UnsafeBufferPointer.baseAddress.getter();
  if (!v14) {
LABEL_11:
  }
    __break(1u);
  if (((a4 | a3) & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_8;
  }
  vImage_Buffer dest = a5;
  uint64_t v15 = *(void *)(*(void *)(AssociatedTypeWitness - 8) + 72);
  int64_t v16 = a4 * v15;
  if ((unsigned __int128)(a4 * (__int128)v15) >> 64 != (a4 * v15) >> 63)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v17 = (void *)v14;
  uint64_t v18 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 24);
  uint64_t v19 = v18(a6, a8);
  if ((unsigned __int128)(v16 * (__int128)v19) >> 64 != (v16 * v19) >> 63)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  src.char data = v17;
  src.CGFloat height = a3;
  src.CGFloat width = a4;
  src.rowBytes = v16 * v19;
  uint64_t v20 = v18(a6, a8);
  if ((unsigned __int128)(v15 * (__int128)v20) >> 64 != (v15 * v20) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  return vImageCopyBuffer(&src, dest, v15 * v20, 0);
}

uint64_t vImage.PixelBuffer<>.array.getter(uint64_t result, uint64_t a2)
{
  uint64_t v3 = *v2;
  if (!*(void *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  uint64_t v4 = *(void *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v5 = *(void *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  uint64_t v6 = v4 * v5;
  if ((unsigned __int128)(v4 * (__int128)v5) >> 64 != (v4 * v5) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  uint64_t result = (*(uint64_t (**)(void))(a2 + 24))(*(void *)(result + 16));
  if ((unsigned __int128)(v6 * (__int128)result) >> 64 == (v6 * result) >> 63)
  {
    MEMORY[0x1F4188790](result);
    swift_getAssociatedTypeWitness();
    return Array.init(unsafeUninitializedCapacity:initializingWith:)();
  }
LABEL_11:
  __break(1u);
  return result;
}

vImage_Error closure #1 in vImage.PixelBuffer<>.array.getter(uint64_t a1, void *a2, void *a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v30 = *MEMORY[0x1E4F143B8];
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  uint64_t v12 = UnsafeBufferPointer.baseAddress.getter();
  if (!a3[2])
  {
    __break(1u);
    goto LABEL_11;
  }
  vImagePixelCount v13 = a3[5];
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  int64_t v14 = a3[6];
  if (v14 < 0)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  uint64_t v26 = a2;
  uint64_t v15 = *(void *)(*(void *)(AssociatedTypeWitness - 8) + 72);
  int64_t v16 = v14 * v15;
  if ((unsigned __int128)(v14 * (__int128)v15) >> 64 != (v14 * v15) >> 63)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  uint64_t v17 = (void *)v12;
  uint64_t v18 = *(uint64_t (**)(uint64_t))(a6 + 24);
  uint64_t v19 = v18(a5);
  if ((unsigned __int128)(v16 * (__int128)v19) >> 64 != (v16 * v19) >> 63)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  dest.char data = v17;
  dest.CGFloat height = v13;
  dest.CGFloat width = v14;
  dest.rowBytes = v16 * v19;
  if (!a3[2])
  {
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
  }
  uint64_t v20 = (void *)a3[4];
  vImagePixelCount v21 = a3[5];
  vImagePixelCount v23 = a3[6];
  size_t v22 = a3[7];
  uint64_t v24 = ((uint64_t (*)(uint64_t, uint64_t))v18)(a5, a6);
  if ((unsigned __int128)(v15 * (__int128)v24) >> 64 != (v15 * v24) >> 63) {
    goto LABEL_16;
  }
  vImage_Flags v28 = 0;
  vImage_Error result = vImage_Buffer.copy(destinationBuffer:pixelSize:flags:)(&dest, v15 * v24, &v28, v20, v21, v23, v22);
  if (v6)
  {
    vImage_Error result = swift_unexpectedError();
    __break(1u);
  }
  else
  {
    void *v26 = a4;
  }
  return result;
}

uint64_t protocol witness for AccelerateBuffer.count.getter in conformance <> vImage.PixelBuffer<A>(uint64_t a1, uint64_t a2)
{
  return vImage.PixelBuffer<>.count.getter(a1, *(void *)(a2 - 8));
}

void static vImageCVImageFormatRef.make(format:colorSpace:alphaIsOpaqueHint:)()
{
}

uint64_t vImage.Size.init<A>(exactWidth:height:)@<X0>(uint64_t a1@<X0>, char *a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X8>)
{
  long long v141 = a2;
  uint64_t v139 = a5;
  uint64_t v128 = *(void *)(*(void *)(a4 + 24) + 16);
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  uint64_t v7 = MEMORY[0x1F4188790](AssociatedTypeWitness);
  long long v126 = (char *)&v124 - v8;
  uint64_t v9 = *(void *)(a3 - 8);
  uint64_t v10 = MEMORY[0x1F4188790](v7);
  uint64_t v12 = (char *)&v124 - ((v11 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v13 = MEMORY[0x1F4188790](v10);
  uint64_t v134 = (char *)&v124 - v14;
  uint64_t v15 = MEMORY[0x1F4188790](v13);
  uint64_t v131 = (char *)&v124 - v16;
  uint64_t v17 = MEMORY[0x1F4188790](v15);
  long long v135 = (char *)&v124 - v18;
  uint64_t v19 = MEMORY[0x1F4188790](v17);
  int v132 = (char *)&v124 - v20;
  uint64_t v21 = MEMORY[0x1F4188790](v19);
  uint64_t v133 = (char *)&v124 - v22;
  uint64_t v23 = MEMORY[0x1F4188790](v21);
  long long v125 = (char *)&v124 - v24;
  uint64_t v25 = MEMORY[0x1F4188790](v23);
  uint64_t v129 = (char *)&v124 - v26;
  uint64_t v27 = MEMORY[0x1F4188790](v25);
  long long v136 = (char *)&v124 - v28;
  uint64_t v29 = MEMORY[0x1F4188790](v27);
  long long v137 = (char *)&v124 - v30;
  uint64_t v31 = MEMORY[0x1F4188790](v29);
  long long v138 = (char *)&v124 - v32;
  uint64_t v33 = MEMORY[0x1F4188790](v31);
  uint64_t v35 = (char *)&v124 - v34;
  uint64_t v36 = MEMORY[0x1F4188790](v33);
  uint64_t v38 = (char *)&v124 - v37;
  MEMORY[0x1F4188790](v36);
  uint64_t v40 = (char *)&v124 - v39;
  uint64_t v41 = v9 + 16;
  uint64_t v42 = *(void (**)(char *, uint64_t, uint64_t))(v9 + 16);
  uint64_t v140 = a1;
  v42((char *)&v124 - v39, a1, a3);
  LOBYTE(a1) = dispatch thunk of static BinaryInteger.isSigned.getter();
  int v144 = (void (*)(char *, char *, uint64_t))v42;
  v42(v38, (uint64_t)v40, a3);
  BOOL v43 = (a1 & 1) != 0 && dispatch thunk of BinaryInteger.bitWidth.getter() > 64;
  uint64_t v130 = v9;
  long long v44 = *(void (**)(char *, uint64_t))(v9 + 8);
  v44(v38, a3);
  v144(v35, v40, a3);
  uint64_t v142 = v41;
  uint64_t v143 = v44;
  long long v124 = v12;
  if (!v43)
  {
    v44(v35, a3);
    uint64_t v50 = v139;
    uint64_t v51 = v141;
    goto LABEL_10;
  }
  int64_t v146 = 0x8000000000000000;
  if (dispatch thunk of static BinaryInteger.isSigned.getter())
  {
    uint64_t v45 = dispatch thunk of BinaryInteger.bitWidth.getter();
    long long v46 = v138;
    uint64_t v47 = v139;
    if (v45 < 64)
    {
      uint64_t v48 = dispatch thunk of BinaryInteger._lowWord.getter();
      uint64_t v49 = v143;
      v143(v35, a3);
      uint64_t v50 = v47;
      uint64_t v51 = v141;
      if (v48 < v146) {
        goto LABEL_85;
      }
      goto LABEL_10;
    }
    lazy protocol witness table accessor for type Int and conformance Int();
    dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
    char v68 = dispatch thunk of static Comparable.< infix(_:_:)();
    uint64_t v49 = v143;
    v143(v46, a3);
    v49(v35, a3);
    uint64_t v50 = v47;
LABEL_45:
    uint64_t v51 = v141;
    if (v68) {
      goto LABEL_85;
    }
    goto LABEL_10;
  }
  char v65 = dispatch thunk of static BinaryInteger.isSigned.getter();
  uint64_t v66 = dispatch thunk of BinaryInteger.bitWidth.getter();
  if ((v65 & 1) == 0)
  {
    uint64_t v51 = v141;
    if (v66 < 64)
    {
      uint64_t v93 = dispatch thunk of BinaryInteger._lowWord.getter();
      uint64_t v49 = v143;
      v143(v35, a3);
      uint64_t v50 = v139;
      if (v93 < v146) {
        goto LABEL_85;
      }
    }
    else
    {
      v143(v35, a3);
      uint64_t v50 = v139;
    }
    goto LABEL_10;
  }
  if (v66 > 64)
  {
    lazy protocol witness table accessor for type Int and conformance Int();
    uint64_t v67 = v138;
    dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
    char v68 = dispatch thunk of static Comparable.< infix(_:_:)();
    uint64_t v49 = v143;
    v143(v67, a3);
    v49(v35, a3);
    uint64_t v50 = v139;
    goto LABEL_45;
  }
  swift_getAssociatedConformanceWitness();
  dispatch thunk of _ExpressibleByBuiltinIntegerLiteral.init(_builtinIntegerLiteral:)();
  int v102 = v138;
  dispatch thunk of ExpressibleByIntegerLiteral.init(integerLiteral:)();
  char v103 = dispatch thunk of static Comparable.< infix(_:_:)();
  int v104 = v143;
  v143(v102, a3);
  uint64_t v105 = v125;
  (*(void (**)(char *, char *, uint64_t))(v130 + 32))(v125, v35, a3);
  if (v103)
  {
    v104(v105, a3);
    uint64_t v50 = v139;
    uint64_t v49 = v104;
    uint64_t v51 = v141;
    v49(v40, a3);
    goto LABEL_86;
  }
  int64_t v113 = v146;
  uint64_t v114 = dispatch thunk of BinaryInteger._lowWord.getter();
  v104(v105, a3);
  BOOL v115 = v114 < v113;
  uint64_t v50 = v139;
  uint64_t v49 = v104;
  uint64_t v51 = v141;
  if (v115) {
    goto LABEL_85;
  }
LABEL_10:
  uint64_t v52 = v50;
  uint64_t v53 = v51;
  uint64_t v54 = dispatch thunk of BinaryInteger.bitWidth.getter();
  uint64_t v55 = v137;
  uint64_t v56 = v144;
  v144(v137, v40, a3);
  if (v54 >= 65)
  {
    v143(v55, a3);
    uint64_t v57 = v136;
    v56(v136, v40, a3);
    uint64_t v51 = v53;
    uint64_t v50 = v52;
    goto LABEL_12;
  }
  uint64_t v63 = dispatch thunk of BinaryInteger.bitWidth.getter();
  v143(v55, a3);
  if (v63 != 64)
  {
    uint64_t v57 = v136;
    v144(v136, v40, a3);
    uint64_t v51 = v53;
    uint64_t v50 = v52;
LABEL_26:
    uint64_t v49 = v143;
    v143(v57, a3);
    goto LABEL_27;
  }
  char v64 = dispatch thunk of static BinaryInteger.isSigned.getter();
  uint64_t v57 = v136;
  v144(v136, v40, a3);
  uint64_t v51 = v53;
  uint64_t v50 = v52;
  if (v64) {
    goto LABEL_26;
  }
LABEL_12:
  int64_t v146 = 0x7FFFFFFFFFFFFFFFLL;
  char v58 = dispatch thunk of static BinaryInteger.isSigned.getter();
  uint64_t v59 = dispatch thunk of BinaryInteger.bitWidth.getter();
  if ((v58 & 1) == 0)
  {
    if (v59 > 63)
    {
      uint64_t v145 = 0x7FFFFFFFFFFFFFFFLL;
      long long v94 = v57;
      long long v95 = v138;
      (*(void (**)(char *, char *, uint64_t))(v130 + 32))(v138, v94, a3);
      lazy protocol witness table accessor for type Int and conformance Int();
      uint64_t v96 = v50;
      long long v97 = v129;
      dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
      char v98 = dispatch thunk of static Comparable.< infix(_:_:)();
      long long v99 = v97;
      uint64_t v50 = v96;
      uint64_t v51 = v141;
      uint64_t v49 = v143;
      v143(v99, a3);
      v49(v95, a3);
      if (v98) {
        goto LABEL_85;
      }
      goto LABEL_27;
    }
    goto LABEL_23;
  }
  if (v59 <= 64)
  {
LABEL_23:
    uint64_t v69 = dispatch thunk of BinaryInteger._lowWord.getter();
    uint64_t v49 = v143;
    v143(v57, a3);
    if (v146 < v69) {
      goto LABEL_85;
    }
    goto LABEL_27;
  }
  lazy protocol witness table accessor for type Int and conformance Int();
  uint64_t v60 = v57;
  uint64_t v61 = v138;
  dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
  char v62 = dispatch thunk of static Comparable.< infix(_:_:)();
  uint64_t v49 = v143;
  v143(v61, a3);
  v49(v60, a3);
  if (v62) {
    goto LABEL_85;
  }
LABEL_27:
  uint64_t v70 = dispatch thunk of BinaryInteger._lowWord.getter();
  v49(v40, a3);
  if (v70 <= 0)
  {
LABEL_86:
    v49(v51, a3);
    uint64_t result = ((uint64_t (*)(uint64_t, uint64_t))v49)(v140, a3);
    goto LABEL_87;
  }
  uint64_t v142 = v70;
  uint64_t v40 = v133;
  uint64_t v71 = v51;
  uint64_t v72 = v144;
  v144(v133, v71, a3);
  char v73 = dispatch thunk of static BinaryInteger.isSigned.getter();
  uint64_t v74 = v132;
  v72(v132, v40, a3);
  if ((v73 & 1) == 0)
  {
    v49(v74, a3);
    uint64_t v76 = v135;
    v72(v135, v40, a3);
    goto LABEL_34;
  }
  uint64_t v75 = dispatch thunk of BinaryInteger.bitWidth.getter();
  v49(v74, a3);
  uint64_t v76 = v135;
  v72(v135, v40, a3);
  if (v75 < 65)
  {
LABEL_34:
    long long v80 = v76;
LABEL_35:
    v49(v80, a3);
    goto LABEL_36;
  }
  int64_t v146 = 0x8000000000000000;
  if (dispatch thunk of static BinaryInteger.isSigned.getter())
  {
    uint64_t v77 = v135;
    if (dispatch thunk of BinaryInteger.bitWidth.getter() < 64)
    {
      uint64_t v78 = dispatch thunk of BinaryInteger._lowWord.getter();
      long long v79 = v77;
      goto LABEL_78;
    }
    lazy protocol witness table accessor for type Int and conformance Int();
    long long v112 = v138;
    dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
    goto LABEL_73;
  }
  char v110 = dispatch thunk of static BinaryInteger.isSigned.getter();
  uint64_t v111 = dispatch thunk of BinaryInteger.bitWidth.getter();
  if (v110)
  {
    if (v111 <= 64)
    {
      swift_getAssociatedConformanceWitness();
      dispatch thunk of _ExpressibleByBuiltinIntegerLiteral.init(_builtinIntegerLiteral:)();
      int v118 = v138;
      dispatch thunk of ExpressibleByIntegerLiteral.init(integerLiteral:)();
      uint64_t v119 = v135;
      LODWORD(v139) = dispatch thunk of static Comparable.< infix(_:_:)();
      v49(v118, a3);
      uint64_t v120 = v124;
      (*(void (**)(char *, char *, uint64_t))(v130 + 32))(v124, v119, a3);
      if (v139)
      {
        v49(v120, a3);
      }
      else
      {
        long long v121 = v49;
        int64_t v122 = v146;
        uint64_t v123 = dispatch thunk of BinaryInteger._lowWord.getter();
        v121(v120, a3);
        BOOL v115 = v123 < v122;
        uint64_t v49 = v121;
        if (!v115) {
          goto LABEL_36;
        }
      }
LABEL_84:
      uint64_t v51 = v141;
LABEL_85:
      v49(v40, a3);
      goto LABEL_86;
    }
    lazy protocol witness table accessor for type Int and conformance Int();
    long long v112 = v138;
    dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
    uint64_t v77 = v135;
LABEL_73:
    char v116 = dispatch thunk of static Comparable.< infix(_:_:)();
    v49(v112, a3);
    v49(v77, a3);
    if ((v116 & 1) == 0) {
      goto LABEL_36;
    }
    goto LABEL_84;
  }
  if (v111 >= 64)
  {
    long long v80 = v135;
    goto LABEL_35;
  }
  uint64_t v117 = v135;
  uint64_t v78 = dispatch thunk of BinaryInteger._lowWord.getter();
  long long v79 = v117;
LABEL_78:
  v49(v79, a3);
  if (v78 < v146) {
    goto LABEL_84;
  }
LABEL_36:
  uint64_t v81 = dispatch thunk of BinaryInteger.bitWidth.getter();
  long long v82 = v131;
  long long v83 = v144;
  v144(v131, v40, a3);
  if (v81 < 65)
  {
    uint64_t v90 = dispatch thunk of BinaryInteger.bitWidth.getter();
    v49(v82, a3);
    if (v90 != 64)
    {
      long long v100 = v134;
      v144(v134, v40, a3);
      long long v85 = v100;
      uint64_t v51 = v141;
      goto LABEL_56;
    }
    char v91 = dispatch thunk of static BinaryInteger.isSigned.getter();
    uint64_t v92 = v134;
    v144(v134, v40, a3);
    long long v85 = v92;
    uint64_t v51 = v141;
    if (v91) {
      goto LABEL_56;
    }
  }
  else
  {
    v49(v82, a3);
    long long v84 = v134;
    v83(v134, v40, a3);
    long long v85 = v84;
    uint64_t v51 = v141;
  }
  int64_t v146 = 0x7FFFFFFFFFFFFFFFLL;
  char v86 = dispatch thunk of static BinaryInteger.isSigned.getter();
  uint64_t v87 = dispatch thunk of BinaryInteger.bitWidth.getter();
  if (v86)
  {
    int v88 = v138;
    if (v87 > 64)
    {
      lazy protocol witness table accessor for type Int and conformance Int();
      dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
      uint64_t v89 = v134;
      goto LABEL_64;
    }
    uint64_t v106 = v134;
    dispatch thunk of BinaryInteger._lowWord.getter();
    uint64_t v101 = v106;
LABEL_60:
    v49(v101, a3);
    goto LABEL_61;
  }
  uint64_t v89 = v138;
  if (v87 <= 63)
  {
    dispatch thunk of BinaryInteger._lowWord.getter();
LABEL_56:
    uint64_t v101 = v85;
    goto LABEL_60;
  }
  uint64_t v145 = 0x7FFFFFFFFFFFFFFFLL;
  (*(void (**)(char *, char *, uint64_t))(v130 + 32))(v138, v85, a3);
  lazy protocol witness table accessor for type Int and conformance Int();
  int v88 = v129;
  dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)();
LABEL_64:
  char v109 = dispatch thunk of static Comparable.< infix(_:_:)();
  v49(v88, a3);
  v49(v89, a3);
  if (v109) {
    goto LABEL_85;
  }
LABEL_61:
  uint64_t v107 = dispatch thunk of BinaryInteger._lowWord.getter();
  v49(v40, a3);
  v49(v51, a3);
  uint64_t result = ((uint64_t (*)(uint64_t, uint64_t))v49)(v140, a3);
  if (v107 > 0)
  {
    *(void *)uint64_t v50 = v142;
    *(void *)(v50 + 8) = v107;
    *(unsigned char *)(v50 + 16) = 0;
    return result;
  }
LABEL_87:
  *(void *)uint64_t v50 = 0;
  *(void *)(v50 + 8) = 0;
  *(unsigned char *)(v50 + 16) = 1;
  return result;
}

uint64_t vImage.Size.init<A>(exactWidth:height:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t *a4@<X8>)
{
  uint64_t v17 = a4;
  uint64_t v7 = *(void *)(a3 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v9 = (char *)&v16 - ((v8 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v10 = *(void (**)(char *, uint64_t))(v7 + 16);
  v10(v9, a1);
  lazy protocol witness table accessor for type Int and conformance Int();
  FixedWidthInteger.init<A>(exactly:)();
  if (v19 == 1 || v18 < 1)
  {
    uint64_t v15 = *(void (**)(uint64_t, uint64_t))(v7 + 8);
    v15(a2, a3);
    uint64_t result = ((uint64_t (*)(uint64_t, uint64_t))v15)(a1, a3);
    uint64_t v13 = v17;
  }
  else
  {
    uint64_t v16 = v18;
    ((void (*)(char *, uint64_t, uint64_t))v10)(v9, a2, a3);
    FixedWidthInteger.init<A>(exactly:)();
    uint64_t v11 = *(void (**)(uint64_t, uint64_t))(v7 + 8);
    v11(a2, a3);
    uint64_t result = ((uint64_t (*)(uint64_t, uint64_t))v11)(a1, a3);
    uint64_t v13 = v17;
    if ((v19 & 1) == 0)
    {
      uint64_t v14 = v18;
      if (v18 >= 1)
      {
        *uint64_t v17 = v16;
        v13[1] = v14;
        *((unsigned char *)v13 + 16) = 0;
        return result;
      }
    }
  }
  *uint64_t v13 = 0;
  v13[1] = 0;
  *((unsigned char *)v13 + 16) = 1;
  return result;
}

uint64_t vImage.Size.init(width:height:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X1>, void *a3@<X8>)
{
  if ((a2 | result) < 0)
  {
    __break(1u);
  }
  else
  {
    *a3 = result;
    a3[1] = a2;
  }
  return result;
}

void vImage.Size.init(cvPixelBuffer:)(__CVBuffer *a1@<X0>, int64_t *a2@<X8>)
{
  int64_t Width = CVPixelBufferGetWidth(a1);
  int64_t Height = CVPixelBufferGetHeight(a1);

  if (Width < 1 || Height < 1)
  {
    __break(1u);
  }
  else
  {
    *a2 = Width;
    a2[1] = Height;
  }
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance vImage.Size(void *a1, void *a2)
{
  return *a1 == *a2 && a1[1] == a2[1];
}

char *specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(char *a1, int64_t a2, char a3)
{
  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<Float>?>);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<vImage_Buffer>?>);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<UInt32>);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>?>);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<UInt64>);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<BNNSNDArrayDescriptor>>);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>>);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<Int32>);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<UInt>);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3);
  *uint64_t v3 = result;
  return result;
}

{
  char **v3;
  char *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3);
  *uint64_t v3 = result;
  return result;
}

void *specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(void *a1, int64_t a2, char a3)
{
  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<[Int16]>, &demangling cache variable for type metadata for [Int16]);
  *uint64_t v3 = result;
  return result;
}

{
  void **v3;
  void *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.PixelBuffer<vImage.DynamicPixelFormat>>, &demangling cache variable for type metadata for vImage.PixelBuffer<vImage.DynamicPixelFormat>);
  *uint64_t v3 = result;
  return result;
}

{
  void **v3;
  void *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<[UInt]>, &demangling cache variable for type metadata for [UInt]);
  *uint64_t v3 = result;
  return result;
}

{
  void **v3;
  void *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.PixelBuffer<vImage.Planar16U>>, &demangling cache variable for type metadata for vImage.PixelBuffer<vImage.Planar16U>);
  *uint64_t v3 = result;
  return result;
}

{
  void **v3;
  void *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.PixelBuffer<vImage.PlanarF>>, &demangling cache variable for type metadata for vImage.PixelBuffer<vImage.PlanarF>);
  *uint64_t v3 = result;
  return result;
}

{
  void **v3;
  void *result;

  uint64_t result = specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, *v3, &demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.PixelBuffer<vImage.Planar8>>, &demangling cache variable for type metadata for vImage.PixelBuffer<vImage.Planar8>);
  *uint64_t v3 = result;
  return result;
}

char *specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(char *result, int64_t a2, char a3, char *a4)
{
  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int16>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    int64_t v12 = v11 - 32;
    if (v11 < 32) {
      int64_t v12 = v11 - 31;
    }
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = v12 & 0xFFFFFFFFFFFFFFFELL;
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v13 = v10 + 32;
  uint64_t v14 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v13 >= &v14[2 * v8]) {
      memmove(v13, v14, 2 * v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 2 * v8);
  }
  swift_release();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<BNNSGraph.Shape>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    int64_t v12 = v11 - 32;
    if (v11 < 32) {
      int64_t v12 = v11 - 17;
    }
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * (v12 >> 4);
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v13 = v10 + 32;
  uint64_t v14 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v13 >= &v14[16 * v8]) {
      memmove(v13, v14, 16 * v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 16 * v8);
  }
  swift_release();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<bnns_graph_shape_t>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    int64_t v12 = v11 - 32;
    if (v11 < 32) {
      int64_t v12 = v11 - 17;
    }
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * (v12 >> 4);
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v13 = v10 + 32;
  uint64_t v14 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v13 >= &v14[16 * v8]) {
      memmove(v13, v14, 16 * v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 16 * v8);
  }
  swift_release();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<String>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    int64_t v12 = v11 - 32;
    if (v11 < 32) {
      int64_t v12 = v11 - 17;
    }
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * (v12 >> 4);
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v13 = v10 + 32;
  uint64_t v14 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v13 >= &v14[16 * v8]) {
      memmove(v13, v14, 16 * v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    swift_arrayInitWithCopy();
  }
  swift_release();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  int64_t v8;
  int64_t v9;
  char *v10;
  size_t v11;
  char *v12;
  char *v13;

  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * v11 - 64;
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  int64_t v12 = v10 + 32;
  uint64_t v13 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v12 >= &v13[v8]) {
      memmove(v12, v13, v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v12, v13, v8);
  }
  swift_release();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  int64_t v8;
  int64_t v9;
  char *v10;
  size_t v11;
  char *v12;
  char *v13;

  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferType?>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * v11 - 64;
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  int64_t v12 = v10 + 32;
  uint64_t v13 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v12 >= &v13[v8]) {
      memmove(v12, v13, v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v12, v13, v8);
  }
  swift_release();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  size_t v11;
  char *v12;
  char *v13;

  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * ((uint64_t)(v11 - 32) / 40);
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  int64_t v12 = v10 + 32;
  uint64_t v13 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v12 >= &v13[40 * v8]) {
      memmove(v12, v13, 40 * v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    swift_arrayInitWithCopy();
  }
  swift_release();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage_Buffer>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    int64_t v12 = v11 - 32;
    if (v11 < 32) {
      int64_t v12 = v11 - 1;
    }
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * (v12 >> 5);
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v13 = v10 + 32;
  uint64_t v14 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v13 >= &v14[32 * v8]) {
      memmove(v13, v14, 32 * v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 32 * v8);
  }
  swift_release();
  return v10;
}

char *specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(char *result, int64_t a2, char a3, char *a4, uint64_t *a5)
{
  char v6 = (char)result;
  if (a3)
  {
    unint64_t v7 = *((void *)a4 + 3);
    int64_t v8 = v7 >> 1;
    if ((uint64_t)(v7 >> 1) < a2)
    {
      if (v8 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v8 = v7 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v7 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v8 = a2;
      }
    }
  }
  else
  {
    int64_t v8 = a2;
  }
  uint64_t v9 = *((void *)a4 + 2);
  if (v8 <= v9) {
    uint64_t v10 = *((void *)a4 + 2);
  }
  else {
    uint64_t v10 = v8;
  }
  if (v10)
  {
    __swift_instantiateConcreteTypeFromMangledName(a5);
    int64_t v11 = (char *)swift_allocObject();
    int64_t v12 = _swift_stdlib_malloc_size(v11);
    uint64_t v13 = v12 - 32;
    if (v12 < 32) {
      uint64_t v13 = v12 - 25;
    }
    *((void *)v11 + 2) = v9;
    *((void *)v11 + 3) = 2 * (v13 >> 3);
  }
  else
  {
    int64_t v11 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v14 = v11 + 32;
  uint64_t v15 = a4 + 32;
  if (v6)
  {
    if (v11 != a4 || v14 >= &v15[8 * v9]) {
      memmove(v14, v15, 8 * v9);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v14, v15, 8 * v9);
  }
  swift_release();
  return v11;
}

{
  char v6;
  unint64_t v7;
  int64_t v8;
  uint64_t v9;
  uint64_t v10;
  char *v11;
  int64_t v12;
  uint64_t v13;
  char *v14;
  char *v15;

  char v6 = (char)result;
  if (a3)
  {
    unint64_t v7 = *((void *)a4 + 3);
    int64_t v8 = v7 >> 1;
    if ((uint64_t)(v7 >> 1) < a2)
    {
      if (v8 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v8 = v7 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v7 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v8 = a2;
      }
    }
  }
  else
  {
    int64_t v8 = a2;
  }
  uint64_t v9 = *((void *)a4 + 2);
  if (v8 <= v9) {
    uint64_t v10 = *((void *)a4 + 2);
  }
  else {
    uint64_t v10 = v8;
  }
  if (v10)
  {
    __swift_instantiateConcreteTypeFromMangledName(a5);
    int64_t v11 = (char *)swift_allocObject();
    int64_t v12 = _swift_stdlib_malloc_size(v11);
    uint64_t v13 = v12 - 32;
    if (v12 < 32) {
      uint64_t v13 = v12 - 29;
    }
    *((void *)v11 + 2) = v9;
    *((void *)v11 + 3) = 2 * (v13 >> 2);
  }
  else
  {
    int64_t v11 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v14 = v11 + 32;
  uint64_t v15 = a4 + 32;
  if (v6)
  {
    if (v11 != a4 || v14 >= &v15[4 * v9]) {
      memmove(v14, v15, 4 * v9);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v14, v15, 4 * v9);
  }
  swift_release();
  return v11;
}

void *specialized _ContiguousArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(void *result, int64_t a2, char a3, void *a4, uint64_t *a5, uint64_t *a6)
{
  char v8 = (char)result;
  if (a3)
  {
    unint64_t v9 = a4[3];
    int64_t v10 = v9 >> 1;
    if ((uint64_t)(v9 >> 1) < a2)
    {
      if (v10 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v10 = v9 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v9 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v10 = a2;
      }
    }
  }
  else
  {
    int64_t v10 = a2;
  }
  uint64_t v11 = a4[2];
  if (v10 <= v11) {
    uint64_t v12 = a4[2];
  }
  else {
    uint64_t v12 = v10;
  }
  if (v12)
  {
    __swift_instantiateConcreteTypeFromMangledName(a5);
    uint64_t v13 = (void *)swift_allocObject();
    int64_t v14 = _swift_stdlib_malloc_size(v13);
    uint64_t v15 = v14 - 32;
    if (v14 < 32) {
      uint64_t v15 = v14 - 25;
    }
    void v13[2] = v11;
    v13[3] = 2 * (v15 >> 3);
  }
  else
  {
    uint64_t v13 = (void *)MEMORY[0x1E4FBC860];
  }
  if (v8)
  {
    if (v13 != a4 || v13 + 4 >= &a4[v11 + 4]) {
      memmove(v13 + 4, a4 + 4, 8 * v11);
    }
    a4[2] = 0;
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(a6);
    swift_arrayInitWithCopy();
  }
  swift_release();
  return v13;
}

uint64_t partial apply for closure #1 in vImage.PixelBuffer<>.withUnsafePointerToVImageBuffer<A>(_:)(uint64_t a1, void *a2)
{
  uint64_t result = (*(uint64_t (**)(void))(v2 + 40))();
  if (v3) {
    *a2 = v3;
  }
  return result;
}

uint64_t outlined init with take of vImage.BufferReference?(uint64_t a1, uint64_t a2)
{
  uint64_t v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for vImage.BufferReference?);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(void *)(v4 - 8) + 32))(a2, a1, v4);
  return a2;
}

uint64_t outlined retain of vImage.BufferReference?(uint64_t a1)
{
  return a1;
}

uint64_t partial apply for closure #1 in vImage.PixelBuffer<>.pixelBuffers.getter@<X0>(long long *a1@<X0>, uint64_t *a2@<X8>)
{
  return closure #1 in vImage.PixelBuffer<>.pixelBuffers.getter(a1, a2);
}

uint64_t type metadata accessor for vImage.PixelBuffer()
{
  return __swift_instantiateGenericMetadata();
}

unint64_t lazy protocol witness table accessor for type [vImage.BufferWrapper] and conformance [A]()
{
  unint64_t result = lazy protocol witness table cache variable for type [vImage.BufferWrapper] and conformance [A];
  if (!lazy protocol witness table cache variable for type [vImage.BufferWrapper] and conformance [A])
  {
    __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for [vImage.BufferWrapper]);
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type [vImage.BufferWrapper] and conformance [A]);
  }
  return result;
}

uint64_t partial apply for closure #1 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)@<X0>(uint64_t a1@<X0>, void *a2@<X8>)
{
  return partial apply for closure #1 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)(a1, a2);
}

uint64_t partial apply for closure #1 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)@<X0>(uint64_t result@<X0>, void *a2@<X8>)
{
  if (*(void *)(*(void *)result + 16))
  {
    uint64_t v2 = *(void *)(*(void *)result + 48);
    if ((v2 & 0x8000000000000000) == 0)
    {
      *a2 = v2;
      return result;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

void *partial apply for closure #3 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)@<X0>(uint64_t a1@<X8>)
{
  return closure #3 in vImage.PixelBuffer<>.init(planarBuffers:pixelFormat:)(*(void *)(v1 + 16), *(void *)(v1 + 24), a1);
}

uint64_t specialized vImage.PixelBuffer<>.byteCountPerPixel.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))() / 8;
}

vImage_Error partial apply for closure #1 in vImage.PixelBuffer<>.init<A>(pixelValues:size:pixelFormat:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in vImage.PixelBuffer<>.init<A>(pixelValues:size:pixelFormat:)(a1, a2, *(void *)(v2 + 48), *(void *)(v2 + 56), *(vImage_Buffer **)(v2 + 64), *(void *)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32));
}

vImage_Error partial apply for closure #1 in vImage.PixelBuffer<>.array.getter(uint64_t a1, void *a2)
{
  return closure #1 in vImage.PixelBuffer<>.array.getter(a1, a2, *(void **)(v2 + 32), *(void *)(v2 + 40), *(void *)(v2 + 16), *(void *)(v2 + 24));
}

ValueMetadata *type metadata accessor for vImage.Size()
{
  return &type metadata for vImage.Size;
}

uint64_t protocol witness for AccelerateBuffer.withUnsafeBufferPointer<A>(_:) in conformance <> vImage.PixelBuffer<A>(uint64_t (*a1)(uint64_t), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return protocol witness for AccelerateMatrixBuffer.withUnsafeBufferPointer<A>(_:) in conformance <> vImage.PixelBuffer<A>(a1, a2, a3, a4, a5);
}

uint64_t protocol witness for AccelerateMutableBuffer.withUnsafeMutableBufferPointer<A>(_:) in conformance <> vImage.PixelBuffer<A>(uint64_t (*a1)(void *), uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return protocol witness for AccelerateMutableMatrixBuffer.withUnsafeMutableBufferPointer<A>(_:) in conformance <> vImage.PixelBuffer<A>(a1, a2, a3, a4, a5);
}

uint64_t vImage.MultidimensionalLookupTable.LookupTableReference.lookupTable.getter()
{
  return *(void *)(v0 + 16);
}

uint64_t vImage.MultidimensionalLookupTable.LookupTableReference.deinit()
{
  vImageMultidimensionalTable_Release(*(vImage_MultidimensionalTable *)(v0 + 16));
  return v0;
}

uint64_t vImage.MultidimensionalLookupTable.LookupTableReference.__deallocating_deinit()
{
  vImageMultidimensionalTable_Release(*(vImage_MultidimensionalTable *)(v0 + 16));

  return swift_deallocClassInstance();
}

uint64_t vImage.MultidimensionalLookupTable.lookupTableReference.getter()
{
  return swift_retain();
}

uint64_t vImage.MultidimensionalLookupTable.lookupTableReference.setter(uint64_t a1)
{
  uint64_t result = swift_release();
  *uint64_t v1 = a1;
  return result;
}

uint64_t (*vImage.MultidimensionalLookupTable.lookupTableReference.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t vImage.MultidimensionalLookupTable.entryCountPerSourceChannel.getter()
{
  return swift_bridgeObjectRetain();
}

uint64_t vImage.MultidimensionalLookupTable.sourceChannelCount.getter()
{
  return *(void *)(v0 + 16);
}

uint64_t vImage.MultidimensionalLookupTable.destinationChannelCount.getter()
{
  return *(void *)(v0 + 24);
}

uint64_t vImage.MultidimensionalLookupTable.init<A>(entryCountPerSourceChannel:destinationChannelCount:data:)@<X0>(uint64_t a1@<X0>, uint64_t *a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t *a6@<X8>)
{
  void (*v34)(uint64_t *__return_ptr, const uint16_t *(*)@<X0>(const uint16_t *@<X0>, const uint16_t **@<X8>), uint64_t **, uint64_t, uint64_t, uint64_t);
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t result;
  uint64_t v39;
  uint64_t *v40;
  uint64_t v41;
  int v42;
  uint64_t *v43;
  uint64_t v44;
  uint64_t **v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t v49;
  long long v50;

  uint64_t v45 = *(uint64_t ***)(a4 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v13 = (char *)&v43 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v48 = 0;
  if (v14 < 1)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  int64_t v15 = *(void *)(a1 + 16);
  *(void *)&uint64_t v50 = v15;
  *((void *)&v50 + 1) = a2;
  if (!v15)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  __int16 v16 = specialized Sequence<>.min()(a1);
  if ((v16 & 0x100) != 0) {
    goto LABEL_22;
  }
  if ((_BYTE)v16)
  {
    BOOL v43 = a6;
    long long v44 = a5;
    long long v46 = a3;
    uint64_t v49 = a1;
    uint64_t v47 = MEMORY[0x1E4FBC860];
    swift_bridgeObjectRetain();
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v15, 0);
    uint64_t v17 = 0;
    uint64_t v18 = v47;
    unint64_t v19 = *(void *)(v47 + 16);
    do
    {
      uint64_t v20 = *(unsigned __int8 *)(a1 + v17 + 32);
      uint64_t v47 = v18;
      unint64_t v21 = *(void *)(v18 + 24);
      unint64_t v22 = v19 + 1;
      if (v19 >= v21 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v21 > 1), v19 + 1, 1);
        uint64_t v18 = v47;
      }
      ++v17;
      *(void *)(v18 + 16) = v22;
      *(void *)(v18 + 8 * v19++ + 32) = v20;
    }
    while (v15 != v17);
    uint64_t v23 = 0;
    uint64_t v24 = 1;
    uint64_t v25 = v46;
    do
    {
      uint64_t v26 = *(void *)(v18 + 8 * v23 + 32);
      uint64_t v27 = v24 * v26;
      if ((unsigned __int128)(v24 * (__int128)v26) >> 64 != (v24 * v26) >> 63)
      {
        __break(1u);
        goto LABEL_17;
      }
      ++v23;
      v24 *= v26;
    }
    while (v22 != v23);
    swift_bridgeObjectRelease();
    uint64_t v28 = v27 * (void)a2;
    if ((unsigned __int128)(v27 * (__int128)(uint64_t)a2) >> 64 != (v27 * (uint64_t)a2) >> 63) {
      goto LABEL_20;
    }
    uint64_t v29 = v45;
    ((void (*)(char *, uint64_t, uint64_t))v45[2])(v13, v25, a4);
    uint64_t v30 = v44;
    uint64_t v31 = (*(uint64_t (**)(uint64_t, uint64_t))(v44 + 16))(a4, v44);
    uint64_t v32 = v29[1];
    uint64_t v33 = ((uint64_t (*)(char *, uint64_t))v32)(v13, a4);
    if (v28 == v31)
    {
      uint64_t v45 = &v43;
      MEMORY[0x1F4188790](v33);
      *(&v43 - 4) = &v48;
      *(&v43 - 3) = a2;
      uint64_t v41 = a1;
      uint64_t v34 = *(void (**)(uint64_t *__return_ptr, const uint16_t *(*)@<X0>(const uint16_t *@<X0>, const uint16_t **@<X8>), uint64_t **, uint64_t, uint64_t, uint64_t))(v30 + 24);
      uint64_t v35 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for OpaquePointer?);
      v34(&v47, partial apply for closure #2 in vImage.MultidimensionalLookupTable.init<A>(entryCountPerSourceChannel:destinationChannelCount:data:), &v43 - 6, v35, a4, v30);
      swift_bridgeObjectRelease();
      uint64_t v36 = v47;
      if (!v47) {
        goto LABEL_23;
      }
      ((void (*)(uint64_t, uint64_t))v32)(v25, a4);
      type metadata accessor for vImage.MultidimensionalLookupTable.LookupTableReference();
      uint64_t v37 = swift_allocObject();
      *(void *)(v37 + 16) = v36;
      uint64_t result = swift_release();
      uint64_t v39 = v49;
      uint64_t v40 = v43;
      *BOOL v43 = v37;
      v40[1] = v39;
      *((_OWORD *)v40 + 1) = v50;
      return result;
    }
    goto LABEL_21;
  }
LABEL_19:
  __break(1u);
LABEL_20:
  __break(1u);
LABEL_21:
  __break(1u);
LABEL_22:
  __break(1u);
LABEL_23:
  uint64_t v42 = 0;
  uint64_t v41 = 158;
  uint64_t result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t specialized Sequence<>.min()(uint64_t a1)
{
  unint64_t v1 = *(void *)(a1 + 16);
  if (!v1)
  {
    LOBYTE(v2) = 0;
    return v2 | ((v1 == 0) << 8);
  }
  unsigned int v2 = *(unsigned __int8 *)(a1 + 32);
  unint64_t v3 = v1 - 1;
  if (v1 != 1)
  {
    if (v1 < 9)
    {
      unint64_t v4 = 1;
      goto LABEL_17;
    }
    if (v1 >= 0x21)
    {
      unint64_t v5 = v3 & 0xFFFFFFFFFFFFFFE0;
      uint8x16_t v6 = (uint8x16_t)vdupq_n_s8(v2);
      unint64_t v7 = (uint8x16_t *)(a1 + 49);
      unint64_t v8 = v3 & 0xFFFFFFFFFFFFFFE0;
      uint8x16_t v9 = v6;
      do
      {
        uint8x16_t v6 = vminq_u8(v7[-1], v6);
        uint8x16_t v9 = vminq_u8(*v7, v9);
        v7 += 2;
        v8 -= 32;
      }
      while (v8);
      uint8x16_t v10 = vminq_u8(v6, v9);
      v10.i8[0] = vminvq_u8(v10);
      unsigned int v2 = v10.i32[0];
      if (v3 == v5) {
        return v2 | ((v1 == 0) << 8);
      }
      if ((v3 & 0x18) == 0)
      {
        unint64_t v4 = v5 | 1;
LABEL_17:
        unint64_t v15 = v1 - v4;
        __int16 v16 = (unsigned __int8 *)(v4 + a1 + 32);
        do
        {
          unsigned int v18 = *v16++;
          char v17 = v18;
          if (v18 < v2) {
            LOBYTE(v2) = v17;
          }
          --v15;
        }
        while (v15);
        return v2 | ((v1 == 0) << 8);
      }
    }
    else
    {
      unint64_t v5 = 0;
    }
    unint64_t v4 = v3 & 0xFFFFFFFFFFFFFFF8 | 1;
    uint8x8_t v11 = (uint8x8_t)vdup_n_s8(v2);
    uint64_t v12 = (uint8x8_t *)(v5 + a1 + 33);
    unint64_t v13 = v5 - (v3 & 0xFFFFFFFFFFFFFFF8);
    do
    {
      uint8x8_t v14 = *v12++;
      uint8x8_t v11 = vmin_u8(v14, v11);
      v13 += 8;
    }
    while (v13);
    LOBYTE(v2) = vminv_u8(v11);
    if (v3 == (v3 & 0xFFFFFFFFFFFFFFF8)) {
      return v2 | ((v1 == 0) << 8);
    }
    goto LABEL_17;
  }
  return v2 | ((v1 == 0) << 8);
}

const uint16_t *closure #2 in vImage.MultidimensionalLookupTable.init<A>(entryCountPerSourceChannel:destinationChannelCount:data:)@<X0>(uint64_t numDestChannels@<X3>, const uint16_t *result@<X0>, uint64_t a3@<X2>, uint64_t a4@<X4>, const uint16_t **a5@<X8>)
{
  if (!result) {
    goto LABEL_9;
  }
  uint64_t v6 = *(void *)(a3 + 16);
  if (v6 > 0xFFFFFFFFLL)
  {
    __break(1u);
  }
  else if (((v6 | numDestChannels) & 0x8000000000000000) == 0)
  {
    if (numDestChannels <= 0xFFFFFFFFLL)
    {
      uint64_t result = (const uint16_t *)vImageMultidimensionalTable_Create(result, v6, numDestChannels, (const uint8_t *)(a4 + 32), kvImageMDTableHint_Float, 0, 0);
      *a5 = result;
      return result;
    }
    goto LABEL_8;
  }
  __break(1u);
LABEL_8:
  __break(1u);
LABEL_9:
  __break(1u);
  return result;
}

uint64_t vImage.MultidimensionalLookupTable.InterpolationMethod.vImageInterpolationMethod.getter()
{
  return *v0;
}

BOOL static vImage.MultidimensionalLookupTable.InterpolationMethod.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void vImage.MultidimensionalLookupTable.InterpolationMethod.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int vImage.MultidimensionalLookupTable.InterpolationMethod.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t vImage.MultidimensionalLookupTable.apply(sources:destinations:interpolation:)(uint64_t result, uint64_t a2, unsigned __int8 *a3)
{
  uint64_t v4 = *(void *)(v3 + 16);
  if (*(void *)(result + 16) != v4) {
    goto LABEL_21;
  }
  uint64_t v5 = v3;
  uint64_t v7 = *(void *)(v3 + 24);
  if (*(void *)(a2 + 16) != v7)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint8x16_t v9 = (const vImage_Buffer *)MEMORY[0x1E4FBC860];
  if (v4)
  {
    uint64_t v10 = result;
    uint64_t v30 = (const vImage_Buffer *)MEMORY[0x1E4FBC860];
    uint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v4, 0);
    uint8x16_t v9 = v30;
    uint64_t v11 = v10 + 32;
    while (1)
    {
      uint64_t v12 = *(void *)v11;
      if (!*(void *)(*(void *)v11 + 16)) {
        break;
      }
      long long v13 = *(_OWORD *)(v12 + 32);
      long long v14 = *(_OWORD *)(v12 + 48);
      unint64_t width = v30->width;
      unint64_t rowBytes = v30->rowBytes;
      if (width >= rowBytes >> 1)
      {
        long long v26 = v14;
        long long v28 = v13;
        uint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(rowBytes > 1, width + 1, 1);
        long long v14 = v26;
        long long v13 = v28;
      }
      v30->unint64_t width = width + 1;
      char v17 = &v30[width];
      *(_OWORD *)&v17[1].char data = v13;
      *(_OWORD *)&v17[1].unint64_t width = v14;
      v11 += 8;
      if (!--v4) {
        goto LABEL_9;
      }
    }
    __break(1u);
    goto LABEL_20;
  }
LABEL_9:
  unsigned int v18 = (const vImage_Buffer *)MEMORY[0x1E4FBC860];
  if (v7)
  {
    uint64_t v31 = (const vImage_Buffer *)MEMORY[0x1E4FBC860];
    uint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v7, 0);
    unsigned int v18 = v31;
    unint64_t v19 = (uint64_t *)(a2 + 32);
    while (1)
    {
      uint64_t v20 = *v19;
      if (!*(void *)(*v19 + 16)) {
        break;
      }
      long long v21 = *(_OWORD *)(v20 + 32);
      long long v22 = *(_OWORD *)(v20 + 48);
      unint64_t v24 = v31->width;
      unint64_t v23 = v31->rowBytes;
      if (v24 >= v23 >> 1)
      {
        long long v27 = v22;
        long long v29 = v21;
        uint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v23 > 1, v24 + 1, 1);
        long long v22 = v27;
        long long v21 = v29;
      }
      v31->unint64_t width = v24 + 1;
      uint64_t v25 = &v31[v24];
      *(_OWORD *)&v25[1].char data = v21;
      *(_OWORD *)&v25[1].unint64_t width = v22;
      ++v19;
      if (!--v7) {
        goto LABEL_15;
      }
    }
LABEL_20:
    __break(1u);
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
LABEL_15:
  if (!*(void *)v5)
  {
LABEL_23:
    __break(1u);
    return result;
  }
  vImageMultiDimensionalInterpolatedLookupTable_PlanarF(v9 + 1, v18 + 1, 0, *(vImage_MultidimensionalTable *)(*(void *)v5 + 16), (vImage_InterpolationMethod)*a3, 0);
  swift_bridgeObjectRelease();

  return swift_bridgeObjectRelease();
}

const vImage_Buffer *vImage.MultidimensionalLookupTable.apply<A, B>(source:destination:interpolation:)(uint64_t a1, uint64_t a2, unsigned __int8 *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v11 = (const vImage_Buffer *)v7[2];
  uint64_t result = (const vImage_Buffer *)(*(uint64_t (**)(uint64_t, uint64_t))(a6 + 32))(a4, a6);
  if (result != v11)
  {
    __break(1u);
    goto LABEL_8;
  }
  long long v13 = (const vImage_Buffer *)v7[3];
  uint64_t result = (const vImage_Buffer *)(*(uint64_t (**)(uint64_t, uint64_t))(a7 + 32))(a5, a7);
  if (result != v13)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  type metadata accessor for vImage.PixelBuffer();
  long long v14 = (const vImage_Buffer *)vImage.PixelBuffer<>.vImageBuffers.getter();
  type metadata accessor for vImage.PixelBuffer();
  uint64_t result = (const vImage_Buffer *)vImage.PixelBuffer<>.vImageBuffers.getter();
  if (!*v7)
  {
LABEL_9:
    __break(1u);
    return result;
  }
  vImageMultiDimensionalInterpolatedLookupTable_PlanarF(v14 + 1, result + 1, 0, *(vImage_MultidimensionalTable *)(*v7 + 16), (vImage_InterpolationMethod)*a3, 0);
  swift_bridgeObjectRelease();

  return (const vImage_Buffer *)swift_bridgeObjectRelease();
}

const uint16_t *partial apply for closure #2 in vImage.MultidimensionalLookupTable.init<A>(entryCountPerSourceChannel:destinationChannelCount:data:)@<X0>(const uint16_t *a1@<X0>, const uint16_t **a2@<X8>)
{
  return closure #2 in vImage.MultidimensionalLookupTable.init<A>(entryCountPerSourceChannel:destinationChannelCount:data:)(v2[3], a1, v2[2], v2[4], a2);
}

uint64_t type metadata accessor for vImage.MultidimensionalLookupTable.LookupTableReference()
{
  return self;
}

unint64_t lazy protocol witness table accessor for type vImage.MultidimensionalLookupTable.InterpolationMethod and conformance vImage.MultidimensionalLookupTable.InterpolationMethod()
{
  unint64_t result = lazy protocol witness table cache variable for type vImage.MultidimensionalLookupTable.InterpolationMethod and conformance vImage.MultidimensionalLookupTable.InterpolationMethod;
  if (!lazy protocol witness table cache variable for type vImage.MultidimensionalLookupTable.InterpolationMethod and conformance vImage.MultidimensionalLookupTable.InterpolationMethod)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.MultidimensionalLookupTable.InterpolationMethod and conformance vImage.MultidimensionalLookupTable.InterpolationMethod);
  }
  return result;
}

uint64_t destroy for vImage.MultidimensionalLookupTable()
{
  swift_release();

  return swift_bridgeObjectRelease();
}

uint64_t initializeWithCopy for vImage.MultidimensionalLookupTable(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(a2 + 8);
  *(void *)a1 = *(void *)a2;
  *(void *)(a1 + 8) = v3;
  *(_OWORD *)(a1 + 16) = *(_OWORD *)(a2 + 16);
  swift_retain();
  swift_bridgeObjectRetain();
  return a1;
}

void *assignWithCopy for vImage.MultidimensionalLookupTable(void *a1, void *a2)
{
  *a1 = *a2;
  swift_retain();
  swift_release();
  a1[1] = a2[1];
  swift_bridgeObjectRetain();
  swift_bridgeObjectRelease();
  a1[2] = a2[2];
  a1[3] = a2[3];
  return a1;
}

_OWORD *assignWithTake for vImage.MultidimensionalLookupTable(_OWORD *a1, _OWORD *a2)
{
  *a1 = *a2;
  swift_bridgeObjectRelease();
  a1[1] = a2[1];
  return a1;
}

uint64_t getEnumTagSinglePayload for vImage.MultidimensionalLookupTable(uint64_t a1, int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0 && *(unsigned char *)(a1 + 32)) {
    return *(_DWORD *)a1 + 0x80000000;
  }
  unint64_t v2 = *(void *)(a1 + 8);
  if (v2 >= 0xFFFFFFFF) {
    LODWORD(v2) = -1;
  }
  return (v2 + 1);
}

uint64_t storeEnumTagSinglePayload for vImage.MultidimensionalLookupTable(uint64_t result, int a2, int a3)
{
  if (a2 < 0)
  {
    *(void *)(result + 16) = 0;
    *(void *)(result + 24) = 0;
    *(void *)unint64_t result = a2 ^ 0x80000000;
    *(void *)(result + 8) = 0;
    if (a3 < 0) {
      *(unsigned char *)(result + 32) = 1;
    }
  }
  else
  {
    if ((a3 & 0x80000000) == 0)
    {
      if (!a2) {
        return result;
      }
LABEL_8:
      *(void *)(result + 8) = (a2 - 1);
      return result;
    }
    *(unsigned char *)(result + 32) = 0;
    if (a2) {
      goto LABEL_8;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vImage.MultidimensionalLookupTable()
{
  return &type metadata for vImage.MultidimensionalLookupTable;
}

uint64_t method lookup function for vImage.MultidimensionalLookupTable.LookupTableReference(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for vImage.MultidimensionalLookupTable.LookupTableReference);
}

unsigned char *storeEnumTagSinglePayload for vImage.MultidimensionalLookupTable.InterpolationMethod(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 2 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 2) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFE) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFD)
  {
    unsigned int v6 = ((a2 - 254) >> 8) + 1;
    *unint64_t result = a2 + 2;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D2120A98);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 2;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vImage.MultidimensionalLookupTable.InterpolationMethod()
{
  return &type metadata for vImage.MultidimensionalLookupTable.InterpolationMethod;
}

uint64_t BNNS.FusedParametersLayer.__allocating_init(input:output:fusedLayerParameters:filterParameters:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint32_t a4, size_t a5, int (__cdecl *a6)(void **, size_t, size_t), void (__cdecl *a7)(void *))
{
  uint64_t v65 = *MEMORY[0x1E4F143B8];
  if (*(void *)(a3 + 16) != 2)
  {
    __break(1u);
    goto LABEL_40;
  }
  outlined init with copy of BNNSOptimizer(a3 + 32, (uint64_t)__src);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParameters);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParametersWrapper);
  if ((swift_dynamicCast() & 1) == 0)
  {
    memset(__dst, 0, 40);
    swift_bridgeObjectRelease();
    outlined destroy of FusableTernaryInputLayerParametersWrapper?((uint64_t)__dst, &demangling cache variable for type metadata for FusableLayerParametersWrapper?);
    return 0;
  }
  outlined init with take of FusableLayerParametersWrapper(__dst, (uint64_t)v48);
  if (*(void *)(a3 + 16) < 2uLL) {
LABEL_40:
  }
    __break(1u);
  outlined init with copy of BNNSOptimizer(a3 + 72, (uint64_t)v44);
  swift_bridgeObjectRelease();
  if ((swift_dynamicCast() & 1) == 0)
  {
    uint64_t v43 = 0;
    memset(v42, 0, sizeof(v42));
    outlined destroy of FusableTernaryInputLayerParametersWrapper?((uint64_t)v42, &demangling cache variable for type metadata for FusableLayerParametersWrapper?);
LABEL_20:
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v48);
    return 0;
  }
  outlined init with take of FusableLayerParametersWrapper(v42, (uint64_t)v45);
  uint64_t v14 = a2[17];
  uint64_t v15 = a2[19];
  int v16 = *((_DWORD *)a2 + 40);
  int v17 = *(_DWORD *)(a1 + 144);
  uint64_t v51 = *a2;
  long long v52 = *(_OWORD *)(a2 + 1);
  long long v53 = *(_OWORD *)(a2 + 3);
  long long v54 = *(_OWORD *)(a2 + 5);
  long long v55 = *(_OWORD *)(a2 + 7);
  long long v56 = *(_OWORD *)(a2 + 9);
  long long v57 = *(_OWORD *)(a2 + 11);
  long long v58 = *(_OWORD *)(a2 + 13);
  long long v59 = *(_OWORD *)(a2 + 15);
  uint64_t v60 = v14;
  int v61 = v17;
  uint64_t v62 = v15;
  int v63 = v16;
  uint64_t v64 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v18 = v49;
  uint64_t v19 = v50;
  __swift_mutable_project_boxed_opaque_existential_1((uint64_t)v48, v49);
  (*(void (**)(void *__return_ptr, uint64_t, uint64_t *, uint64_t, uint64_t))(v19 + 8))(v41, a1, &v51, v18, v19);
  uint64_t v20 = v46;
  uint64_t v21 = v47;
  __swift_mutable_project_boxed_opaque_existential_1((uint64_t)v45, v46);
  (*(void (**)(void *__return_ptr, uint64_t *, uint64_t *, uint64_t, uint64_t))(v21 + 8))(v40, &v51, a2, v20, v21);
  uint64_t v22 = v49;
  uint64_t v23 = v50;
  __swift_project_boxed_opaque_existential_1(v48, v49);
  int v24 = (*(uint64_t (**)(uint64_t, uint64_t))(v23 + 16))(v22, v23);
  uint64_t v25 = v46;
  uint64_t v26 = v47;
  __swift_project_boxed_opaque_existential_1(v45, v46);
  int v27 = (*(uint64_t (**)(uint64_t, uint64_t))(v26 + 16))(v25, v26);
  int v28 = v27;
  unsigned int v29 = v27 - 2;
  if ((v24 || v29 >= 4) && (v24 != 6 || v29 > 3))
  {
    if (v24 == 1 && v29 <= 3)
    {
      outlined init with copy of BNNSOptimizer((uint64_t)v41, (uint64_t)v39);
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
      type metadata accessor for BNNSLayerParametersFullyConnected(0);
      swift_dynamicCast();
      memcpy(__dst, __src, 0x2F0uLL);
      uint64_t v30 = specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)((uint64_t)__dst, (uint64_t)v40, a4, a5, a6, a7, 1, v28);
    }
    else
    {
      BOOL v36 = v24 == 6 || v24 == 0;
      if (v36 && v27 == 7)
      {
        outlined init with copy of BNNSOptimizer((uint64_t)v41, (uint64_t)v39);
        __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
        type metadata accessor for BNNSLayerParametersConvolution(0);
        swift_dynamicCast();
        memcpy(__dst, __src, 0x348uLL);
        uint64_t v30 = specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)((uint64_t)__dst, (uint64_t)v40, a4, a5, a6, a7, v24, 7);
      }
      else if (v24 == 1 && v27 == 7)
      {
        outlined init with copy of BNNSOptimizer((uint64_t)v41, (uint64_t)v39);
        __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
        type metadata accessor for BNNSLayerParametersFullyConnected(0);
        swift_dynamicCast();
        memcpy(__dst, __src, 0x2F0uLL);
        uint64_t v30 = specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)((uint64_t)__dst, (uint64_t)v40, a4, a5, a6, a7, 1, 7);
      }
      else
      {
        if (v24 != 8 || v29 > 3) {
          goto LABEL_19;
        }
        outlined init with copy of BNNSOptimizer((uint64_t)v41, (uint64_t)v39);
        __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
        type metadata accessor for BNNSLayerParametersArithmetic(0);
        swift_dynamicCast();
        LODWORD(__dst[0]) = __src[0];
        *((void *)&__dst[0] + 1) = __src[1];
        LODWORD(__dst[1]) = __src[2];
        *(long long *)((char *)&__dst[1] + 4) = *(_OWORD *)((char *)&__src[2] + 4);
        DWORD1(__dst[2]) = HIDWORD(__src[4]);
        *(long long *)((char *)&__dst[2] + 8) = *(_OWORD *)&__src[5];
        *((void *)&__dst[3] + 1) = __src[7];
        uint64_t v30 = specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)((uint64_t)__dst, (uint64_t)v40, a4, a5, a6, a7, 8, v28);
      }
    }
  }
  else
  {
    outlined init with copy of BNNSOptimizer((uint64_t)v41, (uint64_t)v39);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
    type metadata accessor for BNNSLayerParametersConvolution(0);
    swift_dynamicCast();
    memcpy(__dst, __src, 0x348uLL);
    uint64_t v30 = specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)((uint64_t)__dst, (uint64_t)v40, a4, a5, a6, a7, v24, v28);
  }
  uint64_t v31 = v30;
  type metadata accessor for BNNS.FusedParametersLayer();
  uint64_t v32 = swift_allocObject();
  uint64_t v33 = v32;
  *(void *)(v32 + 24) = MEMORY[0x1E4FBC860];
  if (!v31)
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
LABEL_19:
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v40);
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v41);
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v45);
    goto LABEL_20;
  }
  *(void *)(v32 + 16) = v31;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<FusableLayerParametersWrapperDeallocatable?>);
  uint64_t v34 = swift_allocObject();
  *(_OWORD *)(v34 + 16) = xmmword_1D2135290;
  outlined init with copy of BNNSOptimizer((uint64_t)v48, (uint64_t)__src);
  swift_retain();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParametersWrapperDeallocatable);
  if ((swift_dynamicCast() & 1) == 0)
  {
    *(void *)(v34 + 64) = 0;
    *(_OWORD *)(v34 + 32) = 0u;
    *(_OWORD *)(v34 + 48) = 0u;
  }
  outlined init with copy of BNNSOptimizer((uint64_t)v45, (uint64_t)__dst);
  if ((swift_dynamicCast() & 1) == 0)
  {
    *(void *)(v34 + 104) = 0;
    *(_OWORD *)(v34 + 72) = 0u;
    *(_OWORD *)(v34 + 88) = 0u;
  }
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v40);
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v41);
  *(void *)(v33 + 24) = v34;
  swift_release();
  swift_bridgeObjectRelease();
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v45);
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v48);
  return v33;
}

uint64_t BNNS.FusedParametersLayer.__ivar_destroyer()
{
  return swift_bridgeObjectRelease();
}

uint64_t BNNS.FusedParametersLayer.deinit()
{
  uint64_t v1 = *(void *)(v0 + 24);
  uint64_t v2 = *(void *)(v1 + 16);
  if (v2)
  {
    uint64_t v3 = v1 + 32;
    swift_bridgeObjectRetain();
    do
    {
      outlined init with copy of FusableLayerParametersWrapperDeallocatable?(v3, (uint64_t)v10);
      outlined init with copy of FusableLayerParametersWrapperDeallocatable?((uint64_t)v10, (uint64_t)v7);
      uint64_t v4 = v8;
      if (v8)
      {
        uint64_t v5 = v9;
        __swift_project_boxed_opaque_existential_1(v7, v8);
        (*(void (**)(uint64_t, uint64_t))(v5 + 8))(v4, v5);
        __swift_destroy_boxed_opaque_existential_1((uint64_t)v7);
      }
      else
      {
        outlined destroy of FusableTernaryInputLayerParametersWrapper?((uint64_t)v7, &demangling cache variable for type metadata for FusableLayerParametersWrapperDeallocatable?);
      }
      outlined destroy of FusableTernaryInputLayerParametersWrapper?((uint64_t)v10, &demangling cache variable for type metadata for FusableLayerParametersWrapperDeallocatable?);
      v3 += 40;
      --v2;
    }
    while (v2);
    swift_bridgeObjectRelease();
  }
  BNNSFilterDestroy(*(void **)(v0 + 16));
  swift_bridgeObjectRelease();
  return v0;
}

uint64_t BNNS.FusedParametersLayer.__deallocating_deinit()
{
  BNNS.FusedParametersLayer.deinit();

  return swift_deallocClassInstance();
}

uint64_t BNNS.FusedParametersLayer.__allocating_init(bnnsFilter:)(uint64_t a1)
{
  uint64_t v2 = swift_allocObject();
  uint64_t v3 = v2;
  *(void *)(v2 + 24) = MEMORY[0x1E4FBC860];
  if (a1)
  {
    *(void *)(v2 + 16) = a1;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v3;
}

uint64_t type metadata accessor for BNNS.FusedParametersLayer()
{
  return self;
}

uint64_t outlined init with copy of FusableLayerParametersWrapperDeallocatable?(uint64_t a1, uint64_t a2)
{
  uint64_t v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParametersWrapperDeallocatable?);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(void *)(v4 - 8) + 16))(a2, a1, v4);
  return a2;
}

uint64_t dispatch thunk of FusableLayerParametersWrapper.layerParameters(input:output:)(uint64_t *a1, uint64_t *a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4 = a1[17];
  int v5 = *((_DWORD *)a1 + 36);
  uint64_t v6 = a1[19];
  int v7 = *((_DWORD *)a1 + 40);
  uint64_t v8 = a2[17];
  int v9 = *((_DWORD *)a2 + 36);
  uint64_t v10 = a2[19];
  int v11 = *((_DWORD *)a2 + 40);
  uint64_t v12 = *(uint64_t (**)(uint64_t *, uint64_t *))(a4 + 8);
  uint64_t v28 = *a1;
  long long v29 = *(_OWORD *)(a1 + 1);
  long long v30 = *(_OWORD *)(a1 + 3);
  long long v31 = *(_OWORD *)(a1 + 5);
  long long v32 = *(_OWORD *)(a1 + 7);
  long long v33 = *(_OWORD *)(a1 + 9);
  long long v34 = *(_OWORD *)(a1 + 11);
  long long v35 = *(_OWORD *)(a1 + 13);
  long long v36 = *(_OWORD *)(a1 + 15);
  uint64_t v37 = v4;
  int v38 = v5;
  uint64_t v39 = v6;
  int v40 = v7;
  uint64_t v41 = *(uint64_t *)((char *)a1 + 164);
  uint64_t v14 = *a2;
  long long v15 = *(_OWORD *)(a2 + 1);
  long long v16 = *(_OWORD *)(a2 + 3);
  long long v17 = *(_OWORD *)(a2 + 5);
  long long v18 = *(_OWORD *)(a2 + 7);
  long long v19 = *(_OWORD *)(a2 + 9);
  long long v20 = *(_OWORD *)(a2 + 11);
  long long v21 = *(_OWORD *)(a2 + 13);
  long long v22 = *(_OWORD *)(a2 + 15);
  uint64_t v23 = v8;
  int v24 = v9;
  uint64_t v25 = v10;
  int v26 = v11;
  uint64_t v27 = *(uint64_t *)((char *)a2 + 164);
  return v12(&v28, &v14);
}

uint64_t dispatch thunk of FusableLayerParametersWrapper.filterType.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

uint64_t dispatch thunk of FusableLayerParametersWrapperDeallocatable.deallocate()(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 8))();
}

uint64_t method lookup function for BNNS.FusedParametersLayer(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.FusedParametersLayer);
}

uint64_t BNNS.PaddingMode.bnnsPaddingMode.getter()
{
  if (*(unsigned __int8 *)(v0 + 4) < 2u) {
    return 0;
  }
  if (*(_DWORD *)v0) {
    return 2;
  }
  return 1;
}

uint64_t BNNS.PaddingMode.paddingBitPattern.getter()
{
  if (v0[4] >= 2u) {
    return 0;
  }
  else {
    return *(unsigned int *)v0;
  }
}

uint64_t BNNS.PaddingLayer.__allocating_init(input:output:mode:size:filterParameters:)(_OWORD *a1, long long *a2, int *a3, char *a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v62 = *MEMORY[0x1E4F143B8];
  long long v12 = a2[8];
  long long v13 = a2[9];
  long long v14 = a2[6];
  __src[18] = a2[7];
  __src[19] = v12;
  long long v15 = a2[10];
  __src[20] = v13;
  __src[21] = v15;
  long long v16 = a2[4];
  long long v17 = a2[5];
  long long v18 = a2[2];
  __src[14] = a2[3];
  __src[15] = v16;
  __src[16] = v17;
  __src[17] = v14;
  long long v19 = *a2;
  __src[12] = a2[1];
  __src[13] = v18;
  long long v20 = a1[9];
  __src[8] = a1[8];
  __src[9] = v20;
  __src[10] = a1[10];
  __src[11] = v19;
  long long v21 = a1[5];
  __src[4] = a1[4];
  __src[5] = v21;
  long long v22 = a1[7];
  __src[6] = a1[6];
  __src[7] = v22;
  long long v23 = a1[1];
  __src[0] = *a1;
  __src[1] = v23;
  long long v24 = a1[3];
  __src[2] = a1[2];
  __src[3] = v24;
  int v25 = *a3;
  unsigned int v26 = *((unsigned __int8 *)a3 + 4);
  unint64_t v27 = specialized static BNNS.arrayToTuple<A>(_:fillValue:)(&v44, &v43, &v42, &v41, &v40, &v39, &v38, a4, 0, 0);
  uint64_t v29 = v28;
  swift_bridgeObjectRelease();
  if (v25) {
    int v30 = 2;
  }
  else {
    int v30 = 1;
  }
  if (v26 >= 2) {
    int v31 = 0;
  }
  else {
    int v31 = v25;
  }
  if (v26 >= 2) {
    int v32 = v30;
  }
  else {
    int v32 = 0;
  }
  memcpy(__dst, __src, sizeof(__dst));
  unint64_t v51 = v27;
  uint64_t v52 = v29;
  long long v53 = v44;
  long long v54 = v43;
  long long v55 = v42;
  long long v56 = v41;
  long long v57 = v40;
  long long v58 = v39;
  long long v59 = v38;
  int v60 = v32;
  int v61 = v31;
  if (a7 == 1)
  {
    long long v33 = 0;
  }
  else
  {
    int v45 = a5;
    uint64_t v46 = a6;
    uint64_t v47 = a7;
    uint64_t v48 = a8;
    long long v33 = &v45;
  }
  uint64_t v34 = MEMORY[0x1D2600040](__dst, v33);
  type metadata accessor for BNNS.PaddingLayer();
  uint64_t v35 = swift_allocObject();
  uint64_t v36 = v35;
  if (v34)
  {
    *(void *)(v35 + 16) = v34;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v36;
}

uint64_t type metadata accessor for BNNS.PaddingLayer()
{
  return self;
}

uint64_t BNNS.PaddingLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.PaddingLayer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

ValueMetadata *type metadata accessor for BNNS.PaddingMode()
{
  return &type metadata for BNNS.PaddingMode;
}

uint64_t static vDSP.slidingWindowSum<A>(_:usingWindowLength:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return static vDSP.slidingWindowSum<A>(_:usingWindowLength:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.slidingWindowSum<A>(_:usingWindowLength:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.slidingWindowSum<A>(_:usingWindowLength:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.slidingWindowSum<A>(_:usingWindowLength:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vDSP.slidingWindowSum<A>(_:usingWindowLength:)(uint64_t a1, void *a2)
{
  return partial apply for closure #1 in static vDSP.slidingWindowSum<A>(_:usingWindowLength:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:));
}

{
  return partial apply for closure #1 in static vDSP.slidingWindowSum<A>(_:usingWindowLength:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:));
}

uint64_t static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  return static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:));
}

{
  return static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:));
}

uint64_t static vDSP.slidingWindowSum<A>(_:usingWindowLength:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t))
{
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  uint64_t v13 = result - a2;
  if (__OFSUB__(result, a2))
  {
    __break(1u);
  }
  else
  {
    uint64_t result = v13 + 1;
    if (!__OFADD__(v13, 1))
    {
      uint64_t v14 = a3;
      uint64_t v15 = a4;
      uint64_t v16 = a1;
      uint64_t v17 = a2;
      uint64_t v18 = MEMORY[0x1F4188790](result);
      return a6(v18, a5);
    }
  }
  __break(1u);
  return result;
}

uint64_t static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v28 = a8;
  uint64_t v14 = *(void *)(a4 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v16 = (char *)v27 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = *(uint64_t (**)(uint64_t))(*(void *)(v17 + 8) + 16);
  v27[1] = v19;
  uint64_t v21 = v18(v20);
  (*(void (**)(char *, uint64_t, uint64_t))(v14 + 16))(v16, a1, a4);
  uint64_t v22 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v14 + 8))(v16, a4);
  uint64_t v24 = v21 + a2;
  if (__OFADD__(v21, a2))
  {
    __break(1u);
    goto LABEL_6;
  }
  BOOL v25 = __OFSUB__(v24, 1);
  uint64_t v26 = v24 - 1;
  if (v25)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (v22 == v26)
  {
    MEMORY[0x1F4188790](result);
    v27[-8] = a4;
    v27[-7] = a5;
    v27[-6] = a6;
    v27[-5] = a7;
    v27[-4] = a1;
    v27[-3] = v21;
    v27[-2] = a2;
    return (*(uint64_t (**)(uint64_t))(a7 + 16))(v28);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(uint64_t result, uint64_t a2, void *a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(void))
{
  if (!result)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*a3)
  {
    if (((a5 | a4) & 0x8000000000000000) == 0) {
      return a6();
    }
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:));
}

{
  return partial apply for closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:));
}

uint64_t partial apply for closure #1 in static vDSP.slidingWindowSum<A>(_:usingWindowLength:)(uint64_t a1, void *a2, uint64_t *a3, unint64_t *a4, uint64_t (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v9 = v5[2];
  uint64_t v10 = v5[3];
  uint64_t v11 = v5[4];
  uint64_t v12 = v5[5];
  uint64_t v13 = v5[6];
  uint64_t v14 = __swift_instantiateConcreteTypeFromMangledName(a3);
  uint64_t v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a4, a3);
  uint64_t result = a5(v11, v12, a1, v9, v14, v10, v15);
  *a2 = v13;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(v2 + 16);
  uint64_t v4 = *(void *)(v2 + 32);
  void v6[2] = a1;
  long long v7 = *(_OWORD *)(v2 + 56);
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v4 + 24))(a2, v6, MEMORY[0x1E4FBC848] + 8, v3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(a1, a2, *(void **)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16E80]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.slidingWindowSum<A, B>(_:usingWindowLength:result:)(a1, a2, *(void **)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16E78]);
}

uint64_t static vForce.log<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.rectangularToPolar<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.log<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.rectangularToPolar<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.log<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vForce.log<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.log<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.log<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.log<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.log<A, B>(_:result:));
}

uint64_t static vForce.log<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.log<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.log<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.log<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.log<A, B>(_:result:));
}

uint64_t closure #1 in static vForce.log<A>(_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t *a6, unint64_t *a7, void (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(a6);
  uint64_t v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a7, a6);
  a8(a3, a1, a4, v16, a5, v17);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vForce.log1p<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.rectangularToPolar<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.log1p<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.rectangularToPolar<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vForce.log1p<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vForce.log1p<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.log<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.log1p<A, B>(_:result:));
}

{
  uint64_t vars8;

  return static vForce.log<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.log1p<A, B>(_:result:));
}

uint64_t static vForce.log<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v25 = a7;
  uint64_t v27 = a4;
  uint64_t v29 = *MEMORY[0x1E4F143B8];
  uint64_t v11 = *(void *)(a3 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v13 = (char *)v24 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v14 = *(void (**)(char *))(v11 + 16);
  uint64_t v26 = v15;
  v14(v13);
  uint64_t v16 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  uint64_t v17 = v16(a3, a5);
  v24[0] = a6;
  v24[1] = a2;
  uint64_t v18 = (*(uint64_t (**)(uint64_t))(*(void *)(a6 + 8) + 16))(v27);
  (*(void (**)(char *, uint64_t))(v11 + 8))(v13, a3);
  if (v17 != v18)
  {
    __break(1u);
    goto LABEL_6;
  }
  uint64_t v19 = v26;
  uint64_t v20 = v16(a3, a5);
  if (v20 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  if (v20 > 0x7FFFFFFF) {
    goto LABEL_7;
  }
  int v28 = v20;
  MEMORY[0x1F4188790](v20);
  uint64_t v21 = v27;
  v24[-6] = a3;
  v24[-5] = v21;
  uint64_t v22 = v24[0];
  v24[-4] = a5;
  v24[-3] = v22;
  v24[-2] = v19;
  v24[-1] = &v28;
  return (*(uint64_t (**)(uint64_t))(v22 + 16))(v25);
}

uint64_t static vForce.atan2<A, B, C>(x:y:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vForce.atan2<A, B, C>(x:y:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vForce.atan2<A, B, C>(x:y:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  void (*v22)(char *);
  uint64_t v23;
  void (*v24)(void);
  uint64_t (*v25)(uint64_t, uint64_t);
  uint64_t v26;
  uint64_t v27;
  uint64_t (*v28)(uint64_t, uint64_t);
  uint64_t v29;
  char *v30;
  uint64_t v31;
  uint64_t v32;
  BOOL v33;
  void (*v34)(char *, uint64_t);
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  char *v47;
  char *v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  int v55;
  uint64_t v56;

  uint64_t v52 = a8;
  long long v53 = a6;
  long long v54 = a3;
  long long v56 = *MEMORY[0x1E4F143B8];
  uint64_t v13 = *(void *)(a5 - 8);
  uint64_t v14 = MEMORY[0x1F4188790](a1);
  uint64_t v47 = (char *)&v42 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v16 = MEMORY[0x1F4188790](v14);
  uint64_t v18 = (char *)&v42 - v17;
  MEMORY[0x1F4188790](v16);
  uint64_t v20 = (char *)&v42 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v49 = v21;
  uint64_t v22 = *(void (**)(char *))(v21 + 16);
  long long v43 = v23;
  v22(v20);
  uint64_t v50 = v13;
  uint64_t v24 = *(void (**)(void))(v13 + 16);
  long long v44 = a2;
  ((void (*)(char *, uint64_t, uint64_t))v24)(v18, a2, a5);
  uint64_t v25 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  uint64_t v48 = v20;
  unint64_t v51 = a4;
  int v45 = a7;
  uint64_t v26 = v25(a4, a7);
  uint64_t v46 = a9;
  uint64_t v27 = *(void *)(a9 + 8);
  int v28 = *(uint64_t (**)(uint64_t, uint64_t))(v27 + 16);
  uint64_t v29 = v28(v53, v27);
  int v30 = v47;
  v24();
  if (v26 == v29)
  {
    int v31 = (*(uint64_t (**)(uint64_t))(v52 + 16))(a5);
    int v32 = v53;
    long long v33 = v31 != v28(v53, v27);
  }
  else
  {
    long long v33 = 1;
    int v32 = v53;
  }
  uint64_t v34 = *(void (**)(char *, uint64_t))(v50 + 8);
  v34(v30, a5);
  v34(v18, a5);
  uint64_t v35 = v51;
  (*(void (**)(char *, uint64_t))(v49 + 8))(v48, v51);
  if (v33)
  {
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v36 = v28(v32, v27);
  if (v36 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
  }
  if (v36 > 0x7FFFFFFF) {
    goto LABEL_10;
  }
  long long v55 = v36;
  MEMORY[0x1F4188790](v36);
  *(&v42 - 10) = v35;
  *(&v42 - 9) = a5;
  long long v38 = v45;
  uint64_t v37 = v46;
  *(&v42 - 8) = v32;
  *(&v42 - 7) = v38;
  *(&v42 - 6) = v52;
  *(&v42 - 5) = v37;
  long long v39 = v44;
  *(&v42 - 4) = v43;
  *(&v42 - 3) = v39;
  *(&v42 - 2) = (uint64_t)&v55;
  return (*(uint64_t (**)(uint64_t))(v37 + 16))(v40);
}

uint64_t static vForce.atan2<A, B>(x:y:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vForce.atan2<A, B>(x:y:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.atan2<A, B>(x:y:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vForce.atan2<A, B>(x:y:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vForce.atan2<A, B>(x:y:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t closure #1 in closure #1 in closure #1 in static vForce.atan2<A, B, C>(x:y:result:)(uint64_t a1, uint64_t a2, uint64_t *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(void))
{
  uint64_t result = *a3;
  if (!*a3)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!a1)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (a4) {
    return a7();
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vForce.atan2<A, B>(x:y:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t))
{
  uint64_t v38 = a7;
  long long v39 = a8;
  uint64_t v40 = a6;
  uint64_t v12 = *(void *)(a4 - 8);
  uint64_t v13 = MEMORY[0x1F4188790](a1);
  uint64_t v15 = (char *)&v34 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v17 = *(void *)(v16 - 8);
  MEMORY[0x1F4188790](v13);
  uint64_t v19 = (char *)&v34 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v20 = *(void (**)(char *))(v17 + 16);
  uint64_t v35 = v21;
  v20(v19);
  uint64_t v22 = *(void (**)(char *, uint64_t, uint64_t))(v12 + 16);
  uint64_t v37 = a2;
  v22(v15, a2, a4);
  long long v23 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  uint64_t v36 = a5;
  uint64_t v24 = v23(a3, a5);
  uint64_t v25 = (*(uint64_t (**)(uint64_t))(*(void *)(v40 + 8) + 16))(a4);
  (*(void (**)(char *, uint64_t))(v12 + 8))(v15, a4);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v17 + 8))(v19, a3);
  if (v24 == v25)
  {
    uint64_t v27 = v35;
    uint64_t v28 = v36;
    uint64_t v29 = v23(a3, v36);
    uint64_t v30 = MEMORY[0x1F4188790](v29);
    *(&v34 - 6) = a3;
    *(&v34 - 5) = a4;
    uint64_t v31 = v40;
    *(&v34 - 4) = v28;
    *(&v34 - 3) = v31;
    uint64_t v33 = v37;
    uint64_t v32 = v38;
    *(&v34 - 2) = v27;
    *(&v34 - 1) = v33;
    return v39(v30, v32);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in static vForce.atan2<A, B>(x:y:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, unint64_t *a10, void (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v17 = __swift_instantiateConcreteTypeFromMangledName(a9);
  uint64_t v18 = *(void *)(a8 + 8);
  uint64_t v19 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a10, a9);
  a11(a3, a4, a1, a5, a6, v17, a7, v18, v19);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  *a2 = result;
  return result;
}

uint64_t partial apply for closure #1 in static vForce.log<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.log<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.log<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.log1p<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vForce.log<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.log1p<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vForce.log<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.log1p<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.log1p<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.log1p<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.log1p<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vForce.atan2<A, B, C>(x:y:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.atan2<A, B, C>(x:y:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vForce.atan2<A, B, C>(x:y:result:));
}

uint64_t partial apply for closure #1 in static vForce.atan2<A, B>(x:y:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vForce.atan2<A, B>(x:y:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.atan2<A, B, C>(x:y:result:));
}

{
  return partial apply for closure #1 in static vForce.atan2<A, B>(x:y:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vForce.atan2<A, B, C>(x:y:result:));
}

uint64_t partial apply for closure #1 in static vForce.atan2<A, B>(x:y:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  return closure #1 in static vForce.atan2<A, B>(x:y:)(a1, a2, v5[6], v5[7], v5[2], v5[3], v5[4], v5[5], a3, a4, a5);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.atan2<A, B, C>(x:y:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.atan2<A, B, C>(x:y:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vForce.pow<A, B, C>(bases:exponents:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vForce.atan2<A, B, C>(x:y:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vForce.atan2<A, B, C>(x:y:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vForce.atan2<A, B, C>(x:y:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F171F0]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vForce.atan2<A, B, C>(x:y:result:)(a1, a2, *(uint64_t **)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F171F8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.log1p<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.log1p<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F172E8]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.log1p<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F172F0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.log<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vForce.log1p<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F172D0]);
}

{
  return partial apply for closure #1 in closure #1 in static vForce.log1p<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F17318]);
}

uint64_t partial apply for closure #1 in closure #1 in static vForce.log1p<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(void))
{
  uint64_t result = **(void **)(v3 + 16);
  if (result)
  {
    if (a1) {
      return a3();
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t vDSP.FourierTransformDirection.dftDirection.getter()
{
  if (*v0) {
    return 0xFFFFFFFFLL;
  }
  else {
    return 1;
  }
}

BOOL static vDSP.FourierTransformDirection.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void vDSP.FourierTransformDirection.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int vDSP.FourierTransformDirection.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

unint64_t lazy protocol witness table accessor for type vDSP.FourierTransformDirection and conformance vDSP.FourierTransformDirection()
{
  unint64_t result = lazy protocol witness table cache variable for type vDSP.FourierTransformDirection and conformance vDSP.FourierTransformDirection;
  if (!lazy protocol witness table cache variable for type vDSP.FourierTransformDirection and conformance vDSP.FourierTransformDirection)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.FourierTransformDirection and conformance vDSP.FourierTransformDirection);
  }
  return result;
}

unsigned char *storeEnumTagSinglePayload for vDSP.FourierTransformDirection(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 1 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFF) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFE)
  {
    unsigned int v6 = ((a2 - 255) >> 8) + 1;
    *unint64_t result = a2 + 1;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D212325CLL);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 1;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vDSP.FourierTransformDirection()
{
  return &type metadata for vDSP.FourierTransformDirection;
}

uint64_t static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:));
}

{
  uint64_t v3;

  uint64_t v3 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v3, partial apply for closure #1 in static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:));
}

uint64_t closure #1 in static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:)(void **a1, uint64_t *a2, uint64_t a3, float *a4, uint64_t a5, uint64_t a6)
{
  float v11 = *a4;
  float v12 = a4[1];
  float v13 = a4[2];
  float v14 = a4[3];
  float v15 = a4[4];
  **a1 = 0;
  uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(a3, (uint64_t)a1, a5, v16, a6, v17, v11, v12, v13, v14, v15);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:)(void **a1, uint64_t *a2)
{
  uint64_t v3 = *(void *)(v2 + 16);
  uint64_t v4 = *(void *)(v2 + 24);
  uint64_t v5 = *(void *)(v2 + 32);
  int v6 = *(_DWORD *)(v2 + 56);
  long long v8 = *(_OWORD *)(v2 + 40);
  int v9 = v6;
  return closure #1 in static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:)(a1, a2, v5, (float *)&v8, v3, v4);
}

uint64_t static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7, float a8, float a9, float a10, float a11)
{
  v30[1] = a2;
  uint64_t v31 = a4;
  uint64_t v20 = *(void *)(a3 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v22 = (char *)v30 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(char *, uint64_t))(v20 + 16))(v22, a1);
  long long v23 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  uint64_t v24 = v23(a3, a5);
  v30[0] = a6;
  uint64_t v25 = (*(uint64_t (**)(uint64_t))(*(void *)(a6 + 8) + 16))(v31);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v20 + 8))(v22, a3);
  if (v24 != v25)
  {
    __break(1u);
    goto LABEL_6;
  }
  uint64_t result = v23(a3, a5);
  if (__OFSUB__(result, 2))
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (((result - 2) & 0x8000000000000000) == 0)
  {
    MEMORY[0x1F4188790](result);
    uint64_t v27 = v31;
    v30[-10] = a3;
    v30[-9] = v27;
    uint64_t v28 = v30[0];
    v30[-8] = a5;
    v30[-7] = v28;
    v30[-6] = a1;
    *(float *)&v30[-5] = a7;
    *((float *)&v30[-5] + 1) = a8;
    *(float *)&v30[-4] = a9;
    *((float *)&v30[-4] + 1) = a10;
    *(float *)&v30[-3] = a11;
    v30[-2] = v29;
    return (*(uint64_t (**)(void))(v28 + 16))(partial apply for closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:));
  }
LABEL_7:
  __break(1u);
  return result;
}

void closure #1 in closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(const float *a1, int a2, long long *a3, float **a4, vDSP_Length __N)
{
  if (a1)
  {
    int v5 = *((_DWORD *)a3 + 4);
    long long v7 = *a3;
    int v8 = v5;
    int v6 = *a4;
    if (v6)
    {
      vDSP_deq22(a1, 1, (const float *)&v7, v6, 1, __N);
      return;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
}

uint64_t closure #1 in static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:)(void *a1, uint64_t *a2, uint64_t a3, double *a4, uint64_t a5, uint64_t a6)
{
  double v11 = *a4;
  double v12 = a4[1];
  double v13 = a4[2];
  double v14 = a4[3];
  double v15 = a4[4];
  uint64_t v16 = (void *)*a1;
  *uint64_t v16 = 0;
  v16[1] = 0;
  uint64_t v17 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v18 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(a3, (uint64_t)a1, a5, v17, a6, v18, v11, v12, v13, v14, v15);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7, double a8, double a9, double a10, double a11)
{
  v30[1] = a2;
  uint64_t v31 = a4;
  uint64_t v20 = *(void *)(a3 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v22 = (char *)v30 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(char *, uint64_t))(v20 + 16))(v22, a1);
  long long v23 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  uint64_t v24 = v23(a3, a5);
  v30[0] = a6;
  uint64_t v25 = (*(uint64_t (**)(uint64_t))(*(void *)(a6 + 8) + 16))(v31);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v20 + 8))(v22, a3);
  if (v24 != v25)
  {
    __break(1u);
    goto LABEL_6;
  }
  uint64_t result = v23(a3, a5);
  if (__OFSUB__(result, 2))
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (((result - 2) & 0x8000000000000000) == 0)
  {
    MEMORY[0x1F4188790](result);
    uint64_t v27 = v31;
    v30[-12] = a3;
    v30[-11] = v27;
    uint64_t v28 = v30[0];
    v30[-10] = a5;
    v30[-9] = v28;
    v30[-8] = a1;
    *(double *)&v30[-7] = a7;
    *(double *)&v30[-6] = a8;
    *(double *)&v30[-5] = a9;
    *(double *)&v30[-4] = a10;
    *(double *)&v30[-3] = a11;
    v30[-2] = v29;
    return (*(uint64_t (**)(void))(v28 + 16))(partial apply for closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:));
  }
LABEL_7:
  __break(1u);
  return result;
}

void closure #1 in closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(const double *a1, int a2, uint64_t a3, double **a4, vDSP_Length __N)
{
  if (a1)
  {
    uint64_t v5 = *(void *)(a3 + 32);
    long long v6 = *(_OWORD *)(a3 + 16);
    v8[0] = *(_OWORD *)a3;
    v8[1] = v6;
    uint64_t v9 = v5;
    long long v7 = *a4;
    if (v7)
    {
      vDSP_deq22D(a1, 1, (const double *)v8, v7, 1, __N);
      return;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
}

uint64_t partial apply for closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(uint64_t a1)
{
  uint64_t v2 = *(void *)(v1 + 16);
  uint64_t v3 = *(void *)(v1 + 32);
  int v4 = *(_DWORD *)(v1 + 72);
  uint64_t v5 = *(void *)(v1 + 80);
  v7[1] = *(_OWORD *)(v1 + 56);
  int v8 = v4;
  uint64_t v9 = a1;
  uint64_t v10 = v5;
  return (*(uint64_t (**)(float (*)(const float *, int), _OWORD *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:), v7, MEMORY[0x1E4FBC848] + 8, v2);
}

{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  long long v6;
  _OWORD v8[3];
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;

  uint64_t v2 = *(void *)(v1 + 16);
  uint64_t v3 = *(void *)(v1 + 32);
  int v4 = *(void *)(v1 + 88);
  uint64_t v5 = *(void *)(v1 + 96);
  long long v6 = *(_OWORD *)(v1 + 72);
  v8[1] = *(_OWORD *)(v1 + 56);
  _OWORD v8[2] = v6;
  uint64_t v9 = v4;
  uint64_t v10 = a1;
  double v11 = v5;
  return (*(uint64_t (**)(double (*)(const double *, int), _OWORD *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:), v8, MEMORY[0x1E4FBC848] + 8, v2);
}

uint64_t partial apply for closure #1 in static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:)(void *a1, uint64_t *a2)
{
  uint64_t v3 = *(void *)(v2 + 16);
  uint64_t v4 = *(void *)(v2 + 24);
  uint64_t v5 = *(void *)(v2 + 32);
  uint64_t v6 = *(void *)(v2 + 72);
  long long v7 = *(_OWORD *)(v2 + 56);
  v9[0] = *(_OWORD *)(v2 + 40);
  v9[1] = v7;
  uint64_t v10 = v6;
  return closure #1 in static vDSP.twoPoleTwoZeroFilter<A>(_:coefficients:)(a1, a2, v5, (double *)v9, v3, v4);
}

double partial apply for closure #1 in closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(const double *a1, int a2)
{
  uint64_t v3 = *(void *)(v2 + 48);
  long long v4 = *(_OWORD *)(v2 + 32);
  v6[0] = *(_OWORD *)(v2 + 16);
  v6[1] = v4;
  uint64_t v7 = v3;
  closure #1 in closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(a1, a2, (uint64_t)v6, *(double ***)(v2 + 56), *(void *)(v2 + 64));
  return result;
}

float partial apply for closure #1 in closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(const float *a1, int a2)
{
  int v3 = *(_DWORD *)(v2 + 32);
  long long v5 = *(_OWORD *)(v2 + 16);
  int v6 = v3;
  closure #1 in closure #1 in static vDSP.twoPoleTwoZeroFilter<A, B>(_:coefficients:result:)(a1, a2, &v5, *(float ***)(v2 + 40), *(void *)(v2 + 48));
  return result;
}

uint64_t vImage.PixelBuffer<>.premultiply(alpha:)(uint64_t *a1)
{
  uint64_t v1 = (uint64_t (*)(_OWORD *, _OWORD *, _OWORD *, void))MEMORY[0x1E4F170B8];

  return vImage.PixelBuffer<>.premultiply(alpha:)(a1, v1);
}

{
  uint64_t (*v1)(_OWORD *, _OWORD *, _OWORD *, void);
  uint64_t vars8;

  uint64_t v1 = (uint64_t (*)(_OWORD *, _OWORD *, _OWORD *, void))MEMORY[0x1E4F170C0];

  return vImage.PixelBuffer<>.premultiply(alpha:)(a1, v1);
}

uint64_t vImage.PixelBuffer<>.unpremultiply(alpha:)(uint64_t *a1)
{
  uint64_t v1 = (uint64_t (*)(_OWORD *, _OWORD *, _OWORD *, void))MEMORY[0x1E4F17178];

  return vImage.PixelBuffer<>.premultiply(alpha:)(a1, v1);
}

{
  uint64_t (*v1)(_OWORD *, _OWORD *, _OWORD *, void);
  uint64_t vars8;

  uint64_t v1 = (uint64_t (*)(_OWORD *, _OWORD *, _OWORD *, void))MEMORY[0x1E4F17180];

  return vImage.PixelBuffer<>.premultiply(alpha:)(a1, v1);
}

uint64_t vImage.PixelBuffer<>.premultiply(channelOrdering:)(unsigned char *a1)
{
  uint64_t v1 = (uint64_t (*)(_OWORD *, _OWORD *, void))MEMORY[0x1E4F170A8];
  uint64_t v2 = (uint64_t (*)(_OWORD *, _OWORD *, void))MEMORY[0x1E4F170D8];

  return vImage.PixelBuffer<>.premultiply(channelOrdering:)(a1, v1, v2);
}

{
  uint64_t (*v1)(_OWORD *, _OWORD *, void);
  uint64_t (*v2)(_OWORD *, _OWORD *, void);
  uint64_t vars8;

  uint64_t v1 = (uint64_t (*)(_OWORD *, _OWORD *, void))MEMORY[0x1E4F170A0];
  uint64_t v2 = (uint64_t (*)(_OWORD *, _OWORD *, void))MEMORY[0x1E4F170D0];

  return vImage.PixelBuffer<>.premultiply(channelOrdering:)(a1, v1, v2);
}

{
  uint64_t (*v1)(_OWORD *, _OWORD *, void);
  uint64_t (*v2)(_OWORD *, _OWORD *, void);
  uint64_t vars8;

  uint64_t v1 = (uint64_t (*)(_OWORD *, _OWORD *, void))MEMORY[0x1E4F170B0];
  uint64_t v2 = (uint64_t (*)(_OWORD *, _OWORD *, void))MEMORY[0x1E4F170E0];

  return vImage.PixelBuffer<>.premultiply(channelOrdering:)(a1, v1, v2);
}

uint64_t vImage.PixelBuffer<>.unpremultiply(channelOrdering:)(unsigned char *a1)
{
  uint64_t v1 = (uint64_t (*)(_OWORD *, _OWORD *, void))MEMORY[0x1E4F17168];
  uint64_t v2 = (uint64_t (*)(_OWORD *, _OWORD *, void))MEMORY[0x1E4F17198];

  return vImage.PixelBuffer<>.premultiply(channelOrdering:)(a1, v1, v2);
}

{
  uint64_t (*v1)(_OWORD *, _OWORD *, void);
  uint64_t (*v2)(_OWORD *, _OWORD *, void);
  uint64_t vars8;

  uint64_t v1 = (uint64_t (*)(_OWORD *, _OWORD *, void))MEMORY[0x1E4F17160];
  uint64_t v2 = (uint64_t (*)(_OWORD *, _OWORD *, void))MEMORY[0x1E4F17190];

  return vImage.PixelBuffer<>.premultiply(channelOrdering:)(a1, v1, v2);
}

{
  uint64_t (*v1)(_OWORD *, _OWORD *, void);
  uint64_t (*v2)(_OWORD *, _OWORD *, void);
  uint64_t vars8;

  uint64_t v1 = (uint64_t (*)(_OWORD *, _OWORD *, void))MEMORY[0x1E4F17170];
  uint64_t v2 = (uint64_t (*)(_OWORD *, _OWORD *, void))MEMORY[0x1E4F171A0];

  return vImage.PixelBuffer<>.premultiply(channelOrdering:)(a1, v1, v2);
}

uint64_t vImage.PixelBuffer<>.premultiply()()
{
  uint64_t v0 = (uint64_t (*)(_OWORD *, _OWORD *, void))MEMORY[0x1E4F170C8];

  return vImage.PixelBuffer<>.premultiply()(v0);
}

uint64_t vImage.PixelBuffer<>.unpremultiply()()
{
  uint64_t v0 = (uint64_t (*)(_OWORD *, _OWORD *, void))MEMORY[0x1E4F17188];

  return vImage.PixelBuffer<>.premultiply()(v0);
}

uint64_t vImage.PixelBuffer<>.premultiply()(uint64_t (*a1)(_OWORD *, _OWORD *, void))
{
  uint64_t v6 = *MEMORY[0x1E4F143B8];
  uint64_t v2 = *v1;
  if (!*(void *)(*v1 + 16)) {
    __break(1u);
  }
  long long v3 = *(_OWORD *)(v2 + 48);
  v5[0] = *(_OWORD *)(v2 + 32);
  v5[1] = v3;
  return a1(v5, v5, 0);
}

uint64_t vImage.PixelBuffer<>.premultiply(alpha:)(uint64_t *a1, uint64_t (*a2)(_OWORD *, _OWORD *, _OWORD *, void))
{
  uint64_t v10 = *MEMORY[0x1E4F143B8];
  uint64_t v3 = *v2;
  if (!*(void *)(*v2 + 16))
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  long long v4 = *(_OWORD *)(v3 + 48);
  v9[0] = *(_OWORD *)(v3 + 32);
  v9[1] = v4;
  uint64_t v5 = *a1;
  if (!*(void *)(*a1 + 16)) {
    goto LABEL_5;
  }
  long long v6 = *(_OWORD *)(v5 + 48);
  v8[0] = *(_OWORD *)(v5 + 32);
  v8[1] = v6;
  return a2(v9, v8, v9, 0);
}

uint64_t vImage.PixelBuffer<>.premultiply(channelOrdering:)(unsigned char *a1, uint64_t (*a2)(_OWORD *, _OWORD *, void), uint64_t (*a3)(_OWORD *, _OWORD *, void))
{
  uint64_t v8 = *MEMORY[0x1E4F143B8];
  uint64_t v4 = *v3;
  if (!*(void *)(*v3 + 16)) {
    __break(1u);
  }
  long long v5 = *(_OWORD *)(v4 + 48);
  v7[0] = *(_OWORD *)(v4 + 32);
  v7[1] = v5;
  if (*a1) {
    return a3(v7, v7, 0);
  }
  else {
    return a2(v7, v7, 0);
  }
}

vImage_Error vImage.PixelBuffer<>.premultipliedAlphaBlend(_:topLayer:destination:)(unsigned char *a1, uint64_t a2, uint64_t a3)
{
  uint64_t v23 = *MEMORY[0x1E4F143B8];
  uint64_t v4 = *(void **)a2;
  if (!*(void *)(*(void *)a2 + 16))
  {
    __break(1u);
    goto LABEL_27;
  }
  vImagePixelCount v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  vImagePixelCount v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (!v5)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v6)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  uint64_t v7 = *(void **)a3;
  if (!*(void *)(*(void *)a3 + 16))
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  uint64_t v8 = v7[6];
  if (v8 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  uint64_t v9 = v7[5];
  if (v9 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v8)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (!v9)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (v5 != v8)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (v6 != v9)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  uint64_t v10 = *(void **)v3;
  if (!*(void *)(*(void *)v3 + 16))
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  uint64_t v11 = v10[6];
  if (v11 < 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  uint64_t v12 = v10[5];
  if (v12 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v11)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v12)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (v11 != v5)
  {
LABEL_43:
    __break(1u);
LABEL_44:
    __break(1u);
  }
  if (v12 != v6) {
    goto LABEL_44;
  }
  double v13 = (void *)v10[4];
  size_t v14 = v10[7];
  srcBottom.char data = v13;
  srcBottom.CGFloat height = v6;
  srcBottom.unint64_t width = v5;
  srcBottom.unint64_t rowBytes = v14;
  double v15 = (void *)v4[4];
  size_t v16 = v4[7];
  srcTop.char data = v15;
  srcTop.CGFloat height = v6;
  srcTop.unint64_t width = v5;
  srcTop.unint64_t rowBytes = v16;
  uint64_t v17 = (void *)v7[4];
  size_t v18 = v7[7];
  dest.char data = v17;
  dest.CGFloat height = v6;
  dest.unint64_t width = v5;
  dest.unint64_t rowBytes = v18;
  switch(*a1)
  {
    case 0:
      vImage_Error result = vImagePremultipliedAlphaBlendMultiply_RGBA8888(&srcTop, &srcBottom, &dest, 0);
      break;
    case 1:
      vImage_Error result = vImagePremultipliedAlphaBlendScreen_RGBA8888(&srcTop, &srcBottom, &dest, 0);
      break;
    case 2:
      vImage_Error result = vImagePremultipliedAlphaBlendDarken_RGBA8888(&srcTop, &srcBottom, &dest, 0);
      break;
    case 3:
      vImage_Error result = vImagePremultipliedAlphaBlendLighten_RGBA8888(&srcTop, &srcBottom, &dest, 0);
      break;
    default:
      _assertionFailure(_:_:file:line:flags:)();
      __break(1u);
      JUMPOUT(0x1D212596CLL);
  }
  return result;
}

vImage_Error vImage.PixelBuffer<>.alphaComposite(_:topLayer:destination:)(unsigned __int8 *a1, uint64_t a2, uint64_t a3)
{
  uint64_t v24 = *MEMORY[0x1E4F143B8];
  uint64_t v4 = *(void **)a2;
  if (!*(void *)(*(void *)a2 + 16))
  {
    __break(1u);
    goto LABEL_29;
  }
  vImagePixelCount v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  vImagePixelCount v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (!v5)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v6)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  uint64_t v7 = *(void **)a3;
  if (!*(void *)(*(void *)a3 + 16))
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  uint64_t v8 = v7[6];
  if (v8 < 0)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  uint64_t v9 = v7[5];
  if (v9 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (!v8)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v9)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (v5 != v8)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (v6 != v9)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  uint64_t v10 = *(void **)v3;
  if (!*(void *)(*(void *)v3 + 16))
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  uint64_t v11 = v10[6];
  if (v11 < 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  uint64_t v12 = v10[5];
  if (v12 < 0)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v11)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!v12)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (v11 != v5)
  {
LABEL_45:
    __break(1u);
LABEL_46:
    __break(1u);
  }
  if (v12 != v6) {
    goto LABEL_46;
  }
  double v13 = (void *)v10[4];
  size_t v14 = v10[7];
  srcBottom.char data = v13;
  srcBottom.CGFloat height = v6;
  srcBottom.unint64_t width = v5;
  srcBottom.unint64_t rowBytes = v14;
  double v15 = (void *)v4[4];
  size_t v16 = v4[7];
  srcTop.char data = v15;
  srcTop.CGFloat height = v6;
  srcTop.unint64_t width = v5;
  srcTop.unint64_t rowBytes = v16;
  uint64_t v17 = (void *)v7[4];
  size_t v18 = v7[7];
  dest.char data = v17;
  dest.CGFloat height = v6;
  dest.unint64_t width = v5;
  dest.unint64_t rowBytes = v18;
  int v19 = *a1;
  if (a1[1] != 1) {
    return vImagePremultipliedConstAlphaBlend_ARGB8888(&srcTop, v19, &srcBottom, &dest, 0);
  }
  if (!*a1) {
    return vImageAlphaBlend_ARGB8888(&srcTop, &srcBottom, &dest, 0);
  }
  if (v19 == 1) {
    return vImagePremultipliedAlphaBlend_ARGB8888(&srcTop, &srcBottom, &dest, 0);
  }
  return vImageAlphaBlend_NonpremultipliedToPremultiplied_ARGB8888(&srcTop, &srcBottom, &dest, 0);
}

vImage_Error vImage.PixelBuffer<>.alphaComposite(_:topLayer:destination:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v24 = *MEMORY[0x1E4F143B8];
  uint64_t v4 = *(void **)a2;
  if (!*(void *)(*(void *)a2 + 16))
  {
    __break(1u);
    goto LABEL_29;
  }
  vImagePixelCount v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  vImagePixelCount v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (!v5)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v6)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  uint64_t v7 = *(void **)a3;
  if (!*(void *)(*(void *)a3 + 16))
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  uint64_t v8 = v7[6];
  if (v8 < 0)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  uint64_t v9 = v7[5];
  if (v9 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (!v8)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v9)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (v5 != v8)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (v6 != v9)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  uint64_t v10 = *(void **)v3;
  if (!*(void *)(*(void *)v3 + 16))
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  uint64_t v11 = v10[6];
  if (v11 < 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  uint64_t v12 = v10[5];
  if (v12 < 0)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v11)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!v12)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (v11 != v5)
  {
LABEL_45:
    __break(1u);
LABEL_46:
    __break(1u);
  }
  if (v12 != v6) {
    goto LABEL_46;
  }
  double v13 = (void *)v10[4];
  size_t v14 = v10[7];
  srcBottom.char data = v13;
  srcBottom.CGFloat height = v6;
  srcBottom.unint64_t width = v5;
  srcBottom.unint64_t rowBytes = v14;
  double v15 = (void *)v4[4];
  size_t v16 = v4[7];
  srcTop.char data = v15;
  srcTop.CGFloat height = v6;
  srcTop.unint64_t width = v5;
  srcTop.unint64_t rowBytes = v16;
  uint64_t v17 = (void *)v7[4];
  size_t v18 = v7[7];
  dest.char data = v17;
  dest.CGFloat height = v6;
  dest.unint64_t width = v5;
  dest.unint64_t rowBytes = v18;
  Pixel_F v19 = *(float *)a1;
  if (*(unsigned char *)(a1 + 4) != 1) {
    return vImagePremultipliedConstAlphaBlend_ARGBFFFF(&srcTop, *(Pixel_F *)a1, &srcBottom, &dest, 0);
  }
  if (v19 == 0.0) {
    return vImageAlphaBlend_ARGBFFFF(&srcTop, &srcBottom, &dest, 0);
  }
  if (LODWORD(v19) == 1) {
    return vImagePremultipliedAlphaBlend_ARGBFFFF(&srcTop, &srcBottom, &dest, 0);
  }
  return vImageAlphaBlend_NonpremultipliedToPremultiplied_ARGBFFFF(&srcTop, &srcBottom, &dest, 0);
}

BOOL static vImage.BlendMode.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void vImage.BlendMode.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int vImage.BlendMode.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

unint64_t lazy protocol witness table accessor for type vImage.BlendMode and conformance vImage.BlendMode()
{
  unint64_t result = lazy protocol witness table cache variable for type vImage.BlendMode and conformance vImage.BlendMode;
  if (!lazy protocol witness table cache variable for type vImage.BlendMode and conformance vImage.BlendMode)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.BlendMode and conformance vImage.BlendMode);
  }
  return result;
}

unsigned char *storeEnumTagSinglePayload for vImage.BlendMode(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 3 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 3) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFD) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFC)
  {
    unsigned int v6 = ((a2 - 253) >> 8) + 1;
    *unint64_t result = a2 + 3;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D2125E98);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 3;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vImage.BlendMode()
{
  return &type metadata for vImage.BlendMode;
}

uint64_t *initializeBufferWithCopyOfBuffer for vImage.CompositeMode(uint64_t *a1, uint64_t *a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(a3 + 16);
  uint64_t v6 = *(void *)(v5 - 8);
  unsigned int v7 = *(_DWORD *)(v6 + 84);
  size_t v8 = *(void *)(v6 + 64);
  unint64_t v9 = v8;
  if (v7 <= 2)
  {
    if (v8 <= 3)
    {
      unsigned int v11 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v11 > 0xFFFE)
      {
        uint64_t v10 = 4;
      }
      else
      {
        BOOL v12 = v11 != 0;
        BOOL v13 = v11 >= 0xFF;
        uint64_t v10 = 2;
        if (!v13) {
          uint64_t v10 = v12;
        }
      }
    }
    else
    {
      uint64_t v10 = 1;
    }
    unint64_t v9 = v10 + v8;
  }
  uint64_t v14 = *(_DWORD *)(v6 + 80);
  if (v14 <= 7 && v9 <= 0x18 && (*(_DWORD *)(v6 + 80) & 0x100000) == 0)
  {
    if ((*(unsigned int (**)(uint64_t *, uint64_t, uint64_t))(v6 + 48))(a2, 3, v5))
    {
      if (v7 <= 2)
      {
        if (v8 <= 3)
        {
          unsigned int v19 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
          if (v19 > 0xFFFE)
          {
            uint64_t v18 = 4;
          }
          else
          {
            BOOL v20 = v19 != 0;
            BOOL v13 = v19 >= 0xFF;
            uint64_t v18 = 2;
            if (!v13) {
              uint64_t v18 = v20;
            }
          }
        }
        else
        {
          uint64_t v18 = 1;
        }
        v8 += v18;
      }
      memcpy(a1, a2, v8);
    }
    else
    {
      (*(void (**)(uint64_t *, uint64_t *, uint64_t))(v6 + 16))(a1, a2, v5);
      (*(void (**)(uint64_t *, void, uint64_t, uint64_t))(v6 + 56))(a1, 0, 3, v5);
    }
  }
  else
  {
    uint64_t v17 = *a2;
    *a1 = *a2;
    a1 = (uint64_t *)(v17 + ((v14 + 16) & ~v14));
    swift_retain();
  }
  return a1;
}

uint64_t destroy for vImage.CompositeMode(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(a2 + 16);
  uint64_t v6 = *(void *)(v3 - 8);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t, uint64_t))(v6 + 48))(a1, 3, v3);
  if (!result)
  {
    uint64_t v5 = *(uint64_t (**)(uint64_t, uint64_t))(v6 + 8);
    return v5(a1, v3);
  }
  return result;
}

void *initializeWithCopy for vImage.CompositeMode(void *a1, const void *a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(a3 + 16);
  uint64_t v6 = *(void *)(v5 - 8);
  if ((*(unsigned int (**)(const void *, uint64_t, uint64_t))(v6 + 48))(a2, 3, v5))
  {
    unsigned int v7 = *(_DWORD *)(v6 + 84);
    size_t v8 = *(void *)(v6 + 64);
    if (v7 <= 2)
    {
      if (v8 <= 3)
      {
        unsigned int v10 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
        if (v10 > 0xFFFE)
        {
          uint64_t v9 = 4;
        }
        else
        {
          BOOL v11 = v10 != 0;
          BOOL v12 = v10 >= 0xFF;
          uint64_t v9 = 2;
          if (!v12) {
            uint64_t v9 = v11;
          }
        }
      }
      else
      {
        uint64_t v9 = 1;
      }
      v8 += v9;
    }
    memcpy(a1, a2, v8);
  }
  else
  {
    (*(void (**)(void *, const void *, uint64_t))(v6 + 16))(a1, a2, v5);
    (*(void (**)(void *, void, uint64_t, uint64_t))(v6 + 56))(a1, 0, 3, v5);
  }
  return a1;
}

void *assignWithCopy for vImage.CompositeMode(void *a1, void *a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(a3 + 16);
  uint64_t v6 = *(void *)(v5 - 8);
  unsigned int v7 = *(uint64_t (**)(void *, uint64_t, uint64_t))(v6 + 48);
  int v8 = v7(a1, 3, v5);
  int v9 = v7(a2, 3, v5);
  if (v8)
  {
    if (v9)
    {
      unsigned int v10 = *(_DWORD *)(v6 + 84);
      size_t v11 = *(void *)(v6 + 64);
      if (v10 <= 2)
      {
        if (v11 > 3)
        {
LABEL_5:
          uint64_t v12 = 1;
LABEL_16:
          v11 += v12;
          goto LABEL_17;
        }
LABEL_9:
        unsigned int v15 = (~(-1 << (8 * v11)) - v10 + 3) >> (8 * v11);
        if (v15 > 0xFFFE)
        {
          uint64_t v12 = 4;
        }
        else
        {
          BOOL v16 = v15 != 0;
          BOOL v17 = v15 >= 0xFF;
          uint64_t v12 = 2;
          if (!v17) {
            uint64_t v12 = v16;
          }
        }
        goto LABEL_16;
      }
      goto LABEL_17;
    }
    (*(void (**)(void *, void *, uint64_t))(v6 + 16))(a1, a2, v5);
    (*(void (**)(void *, void, uint64_t, uint64_t))(v6 + 56))(a1, 0, 3, v5);
  }
  else
  {
    if (v9)
    {
      uint64_t v14 = *(void (**)(void *, uint64_t))(v6 + 8);
      uint64_t v13 = v6 + 8;
      v14(a1, v5);
      unsigned int v10 = *(_DWORD *)(v13 + 76);
      size_t v11 = *(void *)(v13 + 56);
      if (v10 <= 2)
      {
        if (v11 > 3) {
          goto LABEL_5;
        }
        goto LABEL_9;
      }
LABEL_17:
      memcpy(a1, a2, v11);
      return a1;
    }
    (*(void (**)(void *, void *, uint64_t))(v6 + 24))(a1, a2, v5);
  }
  return a1;
}

void *initializeWithTake for vImage.CompositeMode(void *a1, const void *a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(a3 + 16);
  uint64_t v6 = *(void *)(v5 - 8);
  if ((*(unsigned int (**)(const void *, uint64_t, uint64_t))(v6 + 48))(a2, 3, v5))
  {
    unsigned int v7 = *(_DWORD *)(v6 + 84);
    size_t v8 = *(void *)(v6 + 64);
    if (v7 <= 2)
    {
      if (v8 <= 3)
      {
        unsigned int v10 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
        if (v10 > 0xFFFE)
        {
          uint64_t v9 = 4;
        }
        else
        {
          BOOL v11 = v10 != 0;
          BOOL v12 = v10 >= 0xFF;
          uint64_t v9 = 2;
          if (!v12) {
            uint64_t v9 = v11;
          }
        }
      }
      else
      {
        uint64_t v9 = 1;
      }
      v8 += v9;
    }
    memcpy(a1, a2, v8);
  }
  else
  {
    (*(void (**)(void *, const void *, uint64_t))(v6 + 32))(a1, a2, v5);
    (*(void (**)(void *, void, uint64_t, uint64_t))(v6 + 56))(a1, 0, 3, v5);
  }
  return a1;
}

void *assignWithTake for vImage.CompositeMode(void *a1, void *a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(a3 + 16);
  uint64_t v6 = *(void *)(v5 - 8);
  unsigned int v7 = *(uint64_t (**)(void *, uint64_t, uint64_t))(v6 + 48);
  int v8 = v7(a1, 3, v5);
  int v9 = v7(a2, 3, v5);
  if (v8)
  {
    if (v9)
    {
      unsigned int v10 = *(_DWORD *)(v6 + 84);
      size_t v11 = *(void *)(v6 + 64);
      if (v10 <= 2)
      {
        if (v11 > 3)
        {
LABEL_5:
          uint64_t v12 = 1;
LABEL_16:
          v11 += v12;
          goto LABEL_17;
        }
LABEL_9:
        unsigned int v15 = (~(-1 << (8 * v11)) - v10 + 3) >> (8 * v11);
        if (v15 > 0xFFFE)
        {
          uint64_t v12 = 4;
        }
        else
        {
          BOOL v16 = v15 != 0;
          BOOL v17 = v15 >= 0xFF;
          uint64_t v12 = 2;
          if (!v17) {
            uint64_t v12 = v16;
          }
        }
        goto LABEL_16;
      }
      goto LABEL_17;
    }
    (*(void (**)(void *, void *, uint64_t))(v6 + 32))(a1, a2, v5);
    (*(void (**)(void *, void, uint64_t, uint64_t))(v6 + 56))(a1, 0, 3, v5);
  }
  else
  {
    if (v9)
    {
      uint64_t v14 = *(void (**)(void *, uint64_t))(v6 + 8);
      uint64_t v13 = v6 + 8;
      v14(a1, v5);
      unsigned int v10 = *(_DWORD *)(v13 + 76);
      size_t v11 = *(void *)(v13 + 56);
      if (v10 <= 2)
      {
        if (v11 > 3) {
          goto LABEL_5;
        }
        goto LABEL_9;
      }
LABEL_17:
      memcpy(a1, a2, v11);
      return a1;
    }
    (*(void (**)(void *, void *, uint64_t))(v6 + 40))(a1, a2, v5);
  }
  return a1;
}

uint64_t getEnumTagSinglePayload for vImage.CompositeMode(unsigned __int16 *a1, unsigned int a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(*(void *)(a3 + 16) - 8);
  unsigned int v5 = *(_DWORD *)(v4 + 84);
  unsigned int v6 = v5 - 3;
  uint64_t v7 = *(void *)(v4 + 64);
  if (v5 <= 2)
  {
    unsigned int v6 = 0;
    if (v7 <= 3)
    {
      unsigned int v9 = (~(-1 << (8 * v7)) - v5 + 3) >> (8 * v7);
      if (v9 > 0xFFFE)
      {
        uint64_t v8 = 4;
      }
      else
      {
        BOOL v10 = v9 != 0;
        BOOL v11 = v9 >= 0xFF;
        uint64_t v8 = 2;
        if (!v11) {
          uint64_t v8 = v10;
        }
      }
    }
    else
    {
      uint64_t v8 = 1;
    }
    v7 += v8;
  }
  if (!a2) {
    return 0;
  }
  int v12 = a2 - v6;
  if (a2 <= v6) {
    goto LABEL_30;
  }
  char v13 = 8 * v7;
  if (v7 <= 3)
  {
    unsigned int v15 = ((v12 + ~(-1 << v13)) >> v13) + 1;
    if (HIWORD(v15))
    {
      int v14 = *(_DWORD *)((char *)a1 + v7);
      if (!v14) {
        goto LABEL_30;
      }
      goto LABEL_20;
    }
    if (v15 > 0xFF)
    {
      int v14 = *(unsigned __int16 *)((char *)a1 + v7);
      if (!*(unsigned __int16 *)((char *)a1 + v7)) {
        goto LABEL_30;
      }
      goto LABEL_20;
    }
    if (v15 < 2)
    {
LABEL_30:
      if (v6)
      {
        unsigned int v19 = (*(uint64_t (**)(void))(v4 + 48))();
        if (v19 >= 4) {
          return v19 - 3;
        }
        else {
          return 0;
        }
      }
      return 0;
    }
  }
  int v14 = *((unsigned __int8 *)a1 + v7);
  if (!*((unsigned char *)a1 + v7)) {
    goto LABEL_30;
  }
LABEL_20:
  int v16 = (v14 - 1) << v13;
  if (v7 > 3) {
    int v16 = 0;
  }
  if (v7)
  {
    if (v7 <= 3) {
      int v17 = v7;
    }
    else {
      int v17 = 4;
    }
    switch(v17)
    {
      case 2:
        int v18 = *a1;
        break;
      case 3:
        int v18 = *a1 | (*((unsigned __int8 *)a1 + 2) << 16);
        break;
      case 4:
        int v18 = *(_DWORD *)a1;
        break;
      default:
        int v18 = *(unsigned __int8 *)a1;
        break;
    }
  }
  else
  {
    int v18 = 0;
  }
  return v6 + (v18 | v16) + 1;
}

void storeEnumTagSinglePayload for vImage.CompositeMode(char *a1, unsigned int a2, unsigned int a3, uint64_t a4)
{
  uint64_t v6 = *(void *)(*(void *)(a4 + 16) - 8);
  unsigned int v7 = *(_DWORD *)(v6 + 84);
  unsigned int v8 = v7 - 3;
  size_t v9 = *(void *)(v6 + 64);
  if (v7 <= 2)
  {
    unsigned int v8 = 0;
    if (v9 <= 3)
    {
      unsigned int v11 = (~(-1 << (8 * v9)) - v7 + 3) >> (8 * v9);
      if (v11 > 0xFFFE)
      {
        uint64_t v10 = 4;
      }
      else
      {
        BOOL v12 = v11 != 0;
        BOOL v13 = v11 >= 0xFF;
        uint64_t v10 = 2;
        if (!v13) {
          uint64_t v10 = v12;
        }
      }
    }
    else
    {
      uint64_t v10 = 1;
    }
    v9 += v10;
  }
  BOOL v13 = a3 >= v8;
  unsigned int v14 = a3 - v8;
  if (v14 != 0 && v13)
  {
    if (v9 <= 3)
    {
      unsigned int v18 = ((v14 + ~(-1 << (8 * v9))) >> (8 * v9)) + 1;
      if (HIWORD(v18))
      {
        int v15 = 4;
      }
      else if (v18 >= 0x100)
      {
        int v15 = 2;
      }
      else
      {
        int v15 = v18 > 1;
      }
    }
    else
    {
      int v15 = 1;
    }
  }
  else
  {
    int v15 = 0;
  }
  if (v8 < a2)
  {
    unsigned int v16 = ~v8 + a2;
    if (v9 < 4)
    {
      int v17 = (v16 >> (8 * v9)) + 1;
      if (v9)
      {
        int v19 = v16 & ~(-1 << (8 * v9));
        bzero(a1, v9);
        if (v9 == 3)
        {
          *(_WORD *)a1 = v19;
          a1[2] = BYTE2(v19);
        }
        else if (v9 == 2)
        {
          *(_WORD *)a1 = v19;
        }
        else
        {
          *a1 = v19;
        }
      }
    }
    else
    {
      bzero(a1, v9);
      *(_DWORD *)a1 = v16;
      int v17 = 1;
    }
    switch(v15)
    {
      case 1:
        a1[v9] = v17;
        return;
      case 2:
        *(_WORD *)&a1[v9] = v17;
        return;
      case 3:
        goto LABEL_43;
      case 4:
        *(_DWORD *)&a1[v9] = v17;
        return;
      default:
        return;
    }
  }
  switch(v15)
  {
    case 1:
      a1[v9] = 0;
      if (!a2) {
        return;
      }
      goto LABEL_30;
    case 2:
      *(_WORD *)&a1[v9] = 0;
      if (!a2) {
        return;
      }
      goto LABEL_30;
    case 3:
LABEL_43:
      __break(1u);
      JUMPOUT(0x1D2126C44);
    case 4:
      *(_DWORD *)&a1[v9] = 0;
      goto LABEL_29;
    default:
LABEL_29:
      if (a2)
      {
LABEL_30:
        BOOL v20 = *(void (**)(void))(v6 + 56);
        v20();
      }
      return;
  }
}

uint64_t getEnumTag for vImage.CompositeMode(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(uint64_t, uint64_t))(*(void *)(*(void *)(a2 + 16) - 8) + 48))(a1, 3);
}

uint64_t destructiveInjectEnumTag for vImage.CompositeMode(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(uint64_t, uint64_t, uint64_t))(*(void *)(*(void *)(a3 + 16) - 8) + 56))(a1, a2, 3);
}

uint64_t type metadata accessor for vImage.CompositeMode()
{
  return __swift_instantiateGenericMetadata();
}

uint64_t static vDSP.minimum<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return static vDSP.minimum<A>(_:_:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.minimum<A>(_:_:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.minimum<A>(_:_:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.minimum<A>(_:_:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vDSP.minimum<A>(_:_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.minimum<A>(_:_:)(a1, a2, v2[4], v2[5], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.minimum<A, B>(_:_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.minimum<A>(_:_:)(a1, a2, v2[4], v2[5], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.minimum<A, B>(_:_:result:));
}

uint64_t static vDSP.minimum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  return static vDSP.minimum<A, B>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.minimum<A, B>(_:_:result:));
}

{
  return static vDSP.minimum<A, B>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.minimum<A, B>(_:_:result:));
}

uint64_t closure #1 in static vDSP.minimum<A>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t *a7, unint64_t *a8, void (*a9)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(a7);
  uint64_t v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a8, a7);
  a9(a3, a4, a1, a5, v16, a6, v17);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t static vDSP.maximum<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return static vDSP.minimum<A>(_:_:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.maximum<A>(_:_:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.minimum<A>(_:_:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.maximum<A>(_:_:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.maximum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  return static vDSP.minimum<A, B>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.maximum<A, B>(_:_:result:));
}

{
  return static vDSP.minimum<A, B>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.maximum<A, B>(_:_:result:));
}

uint64_t static vDSP.minimum<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t))
{
  uint64_t v30 = a5;
  uint64_t v31 = a6;
  uint64_t v9 = *(void *)(a3 - 8);
  uint64_t v10 = MEMORY[0x1F4188790](a1);
  BOOL v12 = (char *)&v27 - ((v11 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1F4188790](v10);
  unsigned int v14 = (char *)&v27 - v13;
  int v15 = *(void (**)(char *))(v9 + 16);
  uint64_t v28 = v16;
  v15((char *)&v27 - v13);
  uint64_t v29 = a2;
  ((void (*)(char *, uint64_t, uint64_t))v15)(v12, a2, a3);
  uint64_t v17 = *(uint64_t (**)(uint64_t, uint64_t))(a4 + 16);
  uint64_t v18 = v17(a3, a4);
  uint64_t v19 = v17(a3, a4);
  BOOL v20 = *(void (**)(char *, uint64_t))(v9 + 8);
  v20(v12, a3);
  uint64_t result = ((uint64_t (*)(char *, uint64_t))v20)(v14, a3);
  if (v18 == v19)
  {
    uint64_t v22 = v28;
    uint64_t v23 = v17(a3, a4);
    uint64_t v24 = MEMORY[0x1F4188790](v23);
    *(&v27 - 4) = a3;
    *(&v27 - 3) = a4;
    uint64_t v26 = v29;
    uint64_t v25 = v30;
    *(&v27 - 2) = v22;
    *(&v27 - 1) = v26;
    return v31(v24, v25);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t static vDSP.minimum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  BOOL v12 = *(uint64_t (**)(uint64_t, uint64_t))(a6 + 16);
  uint64_t v13 = v12(a4, a6);
  uint64_t v14 = v12(a4, a6);
  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a7 + 8) + 16))(a5);
  if (v14 >= v13) {
    uint64_t v16 = v13;
  }
  else {
    uint64_t v16 = v14;
  }
  if (result < v16) {
    uint64_t v16 = result;
  }
  if (v16 < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(uint64_t))(a7 + 16))(a8);
  }
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t *a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  if (!a3)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!result)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v7 = *a5;
  if (v7) {
    return a7(a3, 1, result, 1, v7, 1, a6);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vDSP.absolute<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.absolute<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.absolute<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.absolute<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.absolute<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.absolute<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.absolute<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.absolute<A, B>(_:result:));
}

{
  return static vDSP.absolute<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.absolute<A, B>(_:result:));
}

uint64_t closure #1 in static vDSP.absolute<A>(_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t *a6, unint64_t *a7, void (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(a6);
  uint64_t v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a7, a6);
  a8(a3, a1, a4, v16, a5, v17);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.negativeAbsolute<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.absolute<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.negativeAbsolute<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.absolute<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.negativeAbsolute<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.negativeAbsolute<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.absolute<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.negativeAbsolute<A, B>(_:result:));
}

{
  return static vDSP.absolute<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.negativeAbsolute<A, B>(_:result:));
}

uint64_t static vDSP.negative<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.absolute<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.negative<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.absolute<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.negative<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.negative<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.absolute<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.negative<A, B>(_:result:));
}

{
  return static vDSP.absolute<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.negative<A, B>(_:result:));
}

uint64_t static vDSP.absolute<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  v20[0] = a7;
  uint64_t v12 = *(void *)(a3 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v14 = (char *)v20 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v17 = (*(uint64_t (**)(uint64_t))(*(void *)(v15 + 8) + 16))(v16);
  (*(void (**)(char *, uint64_t, uint64_t))(v12 + 16))(v14, a1, a3);
  uint64_t v18 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v12 + 8))(v14, a3);
  if (v18 == v17)
  {
    MEMORY[0x1F4188790](result);
    v20[-6] = a3;
    v20[-5] = a4;
    v20[-4] = a5;
    v20[-3] = a6;
    v20[-2] = a1;
    v20[-1] = v17;
    return (*(uint64_t (**)(void))(a6 + 16))(v20[0]);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:)(uint64_t result, uint64_t a2, void *a3, uint64_t a4, uint64_t (*a5)(void))
{
  if (!result)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*a3)
  {
    if ((a4 & 0x8000000000000000) == 0) {
      return a5();
    }
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vDSP.reverse<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.reverse<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.reverse<A>(_:));
}

{
  return static vDSP.reverse<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.reverse<A>(_:));
}

uint64_t static vDSP.reverse<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a3 + 8) + 16))(a2);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(uint64_t))(a3 + 16))(a4);
  }
  return result;
}

uint64_t vDSP.SortOrder.init(rawValue:)@<X0>(uint64_t result@<X0>, char *a2@<X8>)
{
  if (result == -1) {
    char v2 = 1;
  }
  else {
    char v2 = 2;
  }
  if (result == 1) {
    char v2 = 0;
  }
  *a2 = v2;
  return result;
}

uint64_t vDSP.SortOrder.rawValue.getter()
{
  if (*v0) {
    return 0xFFFFFFFFLL;
  }
  else {
    return 1;
  }
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance vDSP.SortOrder()
{
  int v1 = *v0;
  Hasher.init(_seed:)();
  if (v1) {
    Swift::UInt32 v2 = -1;
  }
  else {
    Swift::UInt32 v2 = 1;
  }
  Hasher._combine(_:)(v2);
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance vDSP.SortOrder()
{
  if (*v0) {
    Swift::UInt32 v1 = -1;
  }
  else {
    Swift::UInt32 v1 = 1;
  }
  Hasher._combine(_:)(v1);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance vDSP.SortOrder()
{
  int v1 = *v0;
  Hasher.init(_seed:)();
  if (v1) {
    Swift::UInt32 v2 = -1;
  }
  else {
    Swift::UInt32 v2 = 1;
  }
  Hasher._combine(_:)(v2);
  return Hasher._finalize()();
}

_DWORD *protocol witness for RawRepresentable.init(rawValue:) in conformance vDSP.SortOrder@<X0>(_DWORD *result@<X0>, char *a2@<X8>)
{
  if (*result == -1) {
    char v2 = 1;
  }
  else {
    char v2 = 2;
  }
  if (*result == 1) {
    char v3 = 0;
  }
  else {
    char v3 = v2;
  }
  *a2 = v3;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance vDSP.SortOrder(int *a1@<X8>)
{
  if (*v1) {
    int v2 = -1;
  }
  else {
    int v2 = 1;
  }
  *a1 = v2;
}

uint64_t static vDSP.sort<A>(_:sortOrder:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return static vDSP.sort<A>(_:sortOrder:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.sort<A>(_:sortOrder:));
}

{
  return static vDSP.sort<A>(_:sortOrder:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in static vDSP.sort<A>(_:sortOrder:));
}

uint64_t static vDSP.sort<A>(_:sortOrder:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a4 + 8) + 16))(a3);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(uint64_t))(a4 + 16))(a5);
  }
  return result;
}

uint64_t static vDSP.square<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.absolute<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.square<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.absolute<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.square<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.square<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.square<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.square<A, B>(_:result:));
}

{
  return static vDSP.square<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.square<A, B>(_:result:));
}

uint64_t static vDSP.signedSquare<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.absolute<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.signedSquare<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.absolute<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.signedSquare<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.signedSquare<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.square<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.signedSquare<A, B>(_:result:));
}

{
  return static vDSP.square<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.signedSquare<A, B>(_:result:));
}

uint64_t static vDSP.trunc<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.absolute<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.trunc<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.absolute<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.trunc<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.trunc<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.square<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.trunc<A, B>(_:result:));
}

{
  return static vDSP.square<A, B>(_:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.trunc<A, B>(_:result:));
}

uint64_t static vDSP.absolute<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(uint64_t, uint64_t, void *))
{
  uint64_t v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  _OWORD v12[2] = a2;
  v12[3] = a3;
  v12[4] = a1;
  return a5(v10, a4, v12);
}

uint64_t static vDSP.square<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v30 = a7;
  uint64_t v12 = *(void *)(a3 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v14 = (char *)&v27 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v15 = *(void (**)(char *))(v12 + 16);
  uint64_t v32 = v16;
  v15(v14);
  uint64_t v17 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  uint64_t v33 = a5;
  uint64_t v18 = v17(a3, a5);
  uint64_t v28 = a6;
  uint64_t v29 = a2;
  uint64_t v19 = *(uint64_t (**)(uint64_t))(*(void *)(a6 + 8) + 16);
  uint64_t v31 = a4;
  uint64_t v20 = v19(a4);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v12 + 8))(v14, a3);
  if (v18 == v20)
  {
    uint64_t v23 = v32;
    uint64_t v22 = v33;
    uint64_t result = v17(a3, v33);
    if ((result & 0x8000000000000000) == 0)
    {
      uint64_t v24 = MEMORY[0x1F4188790](result);
      uint64_t v25 = v31;
      *(&v27 - 6) = a3;
      *(&v27 - 5) = v25;
      uint64_t v26 = v28;
      *(&v27 - 4) = v22;
      *(&v27 - 3) = v26;
      *(&v27 - 2) = v23;
      *(&v27 - 1) = v24;
      return (*(uint64_t (**)(uint64_t))(v26 + 16))(v30);
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t static vDSP.countZeroCrossings<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.countZeroCrossings<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.countZeroCrossings<A>(_:));
}

{
  uint64_t vars8;

  return static vDSP.countZeroCrossings<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.countZeroCrossings<A>(_:));
}

uint64_t static vDSP.countZeroCrossings<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v6 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  if (v6 < 0) {
    __break(1u);
  }
  MEMORY[0x1F4188790](v6);
  (*(void (**)(uint64_t))(a3 + 24))(a4);
  return 0;
}

uint64_t partial apply for closure #1 in static vDSP.minimum<A, B>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.minimum<A, B>(_:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.minimum<A, B>(_:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.maximum<A>(_:_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.minimum<A>(_:_:)(a1, a2, v2[4], v2[5], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.maximum<A, B>(_:_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.minimum<A>(_:_:)(a1, a2, v2[4], v2[5], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.maximum<A, B>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.maximum<A, B>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.minimum<A, B>(_:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.minimum<A, B>(_:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.minimum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(v2 + 56);
  uint64_t v4 = *(void *)(v2 + 64);
  long long v5 = *(_OWORD *)(v2 + 32);
  long long v8 = *(_OWORD *)(v2 + 16);
  long long v9 = v5;
  uint64_t v10 = v3;
  uint64_t v11 = a1;
  uint64_t v12 = v4;
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t, void))(v5 + 24))(a2, &v7, MEMORY[0x1E4FBC848] + 8, v8);
}

uint64_t partial apply for closure #1 in static vDSP.absolute<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.absolute<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.absolute<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.absolute<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.absolute<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.absolute<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.negativeAbsolute<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.absolute<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.negativeAbsolute<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.absolute<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.negativeAbsolute<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.negativeAbsolute<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.negativeAbsolute<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.negativeAbsolute<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.negative<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.absolute<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.negative<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.absolute<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.negative<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.negative<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.negative<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.negative<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.reverse<A>(_:)(uint64_t *a1)
{
  return partial apply for closure #1 in static vDSP.reverse<A>(_:)(a1, MEMORY[0x1E4F16DA8]);
}

{
  return partial apply for closure #1 in static vDSP.reverse<A>(_:)(a1, MEMORY[0x1E4F16DB0]);
}

uint64_t partial apply for closure #1 in static vDSP.reverse<A>(_:)(uint64_t *a1, uint64_t (*a2)(uint64_t, uint64_t, void))
{
  uint64_t result = *a1;
  if (result) {
    return a2(result, 1, *(void *)(v2 + 16));
  }
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.sort<A>(_:sortOrder:)(uint64_t *a1)
{
  return partial apply for closure #1 in static vDSP.sort<A>(_:sortOrder:)(a1, MEMORY[0x1E4F16E20]);
}

{
  return partial apply for closure #1 in static vDSP.sort<A>(_:sortOrder:)(a1, MEMORY[0x1E4F16E28]);
}

uint64_t partial apply for closure #1 in static vDSP.sort<A>(_:sortOrder:)(uint64_t *a1, uint64_t (*a2)(void))
{
  uint64_t result = *a1;
  if (result) {
    return a2();
  }
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.square<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.absolute<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.square<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.absolute<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.square<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.square<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.square<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.square<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.signedSquare<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.absolute<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.signedSquare<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.absolute<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.signedSquare<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.signedSquare<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.signedSquare<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.signedSquare<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.trunc<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.absolute<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.trunc<A, B>(_:result:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.absolute<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.trunc<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.trunc<A, B>(_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.trunc<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.trunc<A, B>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.countZeroCrossings<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.countZeroCrossings<A>(_:)(a1, a2, MEMORY[0x1E4F169F8]);
}

{
  return partial apply for closure #1 in static vDSP.countZeroCrossings<A>(_:)(a1, a2, MEMORY[0x1E4F16A00]);
}

uint64_t partial apply for closure #1 in static vDSP.countZeroCrossings<A>(_:)(uint64_t result, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, void, void, void, void))
{
  if (result) {
    return a3(result, 1, v3[2], v3[3], v3[4], v3[2]);
  }
  __break(1u);
  return result;
}

unint64_t lazy protocol witness table accessor for type vDSP.SortOrder and conformance vDSP.SortOrder()
{
  unint64_t result = lazy protocol witness table cache variable for type vDSP.SortOrder and conformance vDSP.SortOrder;
  if (!lazy protocol witness table cache variable for type vDSP.SortOrder and conformance vDSP.SortOrder)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.SortOrder and conformance vDSP.SortOrder);
  }
  return result;
}

unsigned char *storeEnumTagSinglePayload for vDSP.SortOrder(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 1 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFF) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFE)
  {
    unsigned int v6 = ((a2 - 255) >> 8) + 1;
    *unint64_t result = a2 + 1;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D2128A50);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 1;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vDSP.SortOrder()
{
  return &type metadata for vDSP.SortOrder;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.trunc<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.trunc<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F16C80]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.trunc<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F16C78]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.signedSquare<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.trunc<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F16E50]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.trunc<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F16E48]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.square<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.trunc<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F16E40]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.trunc<A, B>(_:result:)(a1, a2, MEMORY[0x1E4F16E38]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.trunc<A, B>(_:result:)(uint64_t result, uint64_t a2, uint64_t (*a3)(void))
{
  if (result)
  {
    if (**(void **)(v3 + 16)) {
      return a3();
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.negative<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:)(a1, a2, *(void **)(v2 + 16), *(void *)(v2 + 24), MEMORY[0x1E4F16D70]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:)(a1, a2, *(void **)(v2 + 16), *(void *)(v2 + 24), MEMORY[0x1E4F16D68]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.negativeAbsolute<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:)(a1, a2, *(void **)(v2 + 16), *(void *)(v2 + 24), MEMORY[0x1E4F16D60]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:)(a1, a2, *(void **)(v2 + 16), *(void *)(v2 + 24), MEMORY[0x1E4F16D58]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:)(a1, a2, *(void **)(v2 + 16), *(void *)(v2 + 24), MEMORY[0x1E4F16AA8]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.absolute<A, B>(_:result:)(a1, a2, *(void **)(v2 + 16), *(void *)(v2 + 24), MEMORY[0x1E4F16AA0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(uint64_t **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16CF0]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(uint64_t **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16CE8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(uint64_t **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16D00]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.minimum<A, B>(_:_:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(uint64_t **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16CF8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.maximum<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = v3[2];
  uint64_t v5 = v3[4];
  uint64_t v6 = v3[7];
  _OWORD v8[2] = a1;
  void v8[3] = a2;
  v8[4] = v6;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E4FBC848] + 8, v4);
}

void __swiftcall vImage_CGImageFormat.init(cgImage:)(vImage_CGImageFormat_optional *__return_ptr retstr, CGImageRef cgImage)
{
  specialized vImage_CGImageFormat.init(cgImage:)(cgImage, v3);
  outlined init with take of vImage_CGImageFormat?((uint64_t)v3, (uint64_t)retstr);
}

void vImage_CGImageFormat.init(bitsPerComponent:bitsPerPixel:colorSpace:bitmapInfo:renderingIntent:)(uint64_t a1@<X0>, unint64_t a2@<X1>, uint64_t a3@<X2>, unsigned int a4@<W3>, unsigned int a5@<W4>, uint64_t *a6@<X8>)
{
  if (a1 <= 0 || (a2 & 0x8000000000000000) != 0)
  {

    uint64_t v7 = 0;
    uint64_t v8 = 0;
    uint64_t v9 = 0;
    a3 = 1;
    goto LABEL_7;
  }
  if (HIDWORD(a1))
  {
    __break(1u);
  }
  else if (!HIDWORD(a2))
  {
    uint64_t v7 = a1 | (a2 << 32);
    uint64_t v8 = a4;
    uint64_t v9 = a5;
LABEL_7:
    *a6 = v7;
    a6[1] = a3;
    a6[2] = v8;
    a6[3] = 0;
    a6[4] = v9;
    return;
  }
  __break(1u);
}

uint64_t vImage_CGImageFormat.componentCount.getter()
{
  uint64_t v5 = *MEMORY[0x1E4F143B8];
  long long v1 = *(_OWORD *)(v0 + 16);
  v3[0] = *(_OWORD *)v0;
  v3[1] = v1;
  uint64_t v4 = *(void *)(v0 + 32);
  return MEMORY[0x1D2600F40](v3);
}

void specialized vImage_CGImageFormat.init(cgImage:)(CGImage *a1@<X0>, size_t *a2@<X8>)
{
  uint64_t v4 = CGImageGetColorSpace(a1);
  if (!v4)
  {

    size_t v13 = 0;
    size_t v14 = 0;
    size_t v15 = 0;
    uint64_t v5 = 1;
    goto LABEL_8;
  }
  uint64_t v5 = (uint64_t)v4;
  size_t BitsPerComponent = CGImageGetBitsPerComponent(a1);
  if ((BitsPerComponent & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_10;
  }
  size_t v7 = BitsPerComponent;
  if (HIDWORD(BitsPerComponent))
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  size_t BitsPerPixel = CGImageGetBitsPerPixel(a1);
  if ((BitsPerPixel & 0x8000000000000000) != 0)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  size_t v9 = BitsPerPixel;
  if (!HIDWORD(BitsPerPixel))
  {
    id v10 = (id)v5;
    CGBitmapInfo BitmapInfo = CGImageGetBitmapInfo(a1);
    CGColorRenderingIntent RenderingIntent = CGImageGetRenderingIntent(a1);

    size_t v13 = v7 | (v9 << 32);
    size_t v14 = BitmapInfo;
    size_t v15 = RenderingIntent;
LABEL_8:
    *a2 = v13;
    a2[1] = v5;
    a2[2] = v14;
    a2[3] = 0;
    a2[4] = v15;
    return;
  }
LABEL_12:
  __break(1u);
}

uint64_t static vDSP.phase<A>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return static vDSP.phase<A>(_:result:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in static vDSP.phase<A>(_:result:));
}

{
  return static vDSP.phase<A>(_:result:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in static vDSP.phase<A>(_:result:));
}

void *partial apply for closure #1 in static vDSP.phase<A>(_:result:)(void *a1)
{
  return partial apply for closure #1 in static vDSP.phase<A>(_:result:)(a1, MEMORY[0x1E4F16F38]);
}

{
  return partial apply for closure #1 in static vDSP.phase<A>(_:result:)(a1, MEMORY[0x1E4F16F40]);
}

uint64_t static vDSP.copy(_:to:count:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4 = (uint64_t (*)(void *, uint64_t, uint64_t, uint64_t, uint64_t))MEMORY[0x1E4F16F18];

  return static vDSP.copy(_:to:count:)(a1, a2, a3, a4, v4);
}

{
  uint64_t (*v4)(void *, uint64_t, uint64_t, uint64_t, uint64_t);
  uint64_t vars8;

  uint64_t v4 = (uint64_t (*)(void *, uint64_t, uint64_t, uint64_t, uint64_t))MEMORY[0x1E4F16F20];

  return static vDSP.copy(_:to:count:)(a1, a2, a3, a4, v4);
}

uint64_t static vDSP.copy(_:to:count:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(void *, uint64_t, uint64_t, uint64_t, uint64_t))
{
  _OWORD v6[2] = *MEMORY[0x1E4F143B8];
  if (a4 < 0) {
    __break(1u);
  }
  v6[0] = a1;
  v6[1] = a2;
  return a5(v6, 1, a3, 1, a4);
}

uint64_t static vDSP.conjugate(_:count:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4 = (uint64_t (*)(void *, uint64_t, uint64_t, uint64_t, uint64_t))MEMORY[0x1E4F16EE8];

  return static vDSP.conjugate(_:count:result:)(a1, a2, a3, a4, v4);
}

{
  uint64_t (*v4)(void *, uint64_t, uint64_t, uint64_t, uint64_t);
  uint64_t vars8;

  uint64_t v4 = (uint64_t (*)(void *, uint64_t, uint64_t, uint64_t, uint64_t))MEMORY[0x1E4F16EF0];

  return static vDSP.conjugate(_:count:result:)(a1, a2, a3, a4, v4);
}

uint64_t static vDSP.conjugate(_:count:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(void *, uint64_t, uint64_t, uint64_t, uint64_t))
{
  _OWORD v6[2] = *MEMORY[0x1E4F143B8];
  v6[0] = a1;
  v6[1] = a2;
  if (a3 < 0) {
    __break(1u);
  }
  return a5(v6, 1, a4, 1, a3);
}

uint64_t static vDSP.divide<A>(_:by:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.divide<A>(_:by:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.divide<A>(_:by:result:));
}

{
  uint64_t vars8;

  return static vDSP.divide<A>(_:by:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.divide<A>(_:by:result:));
}

uint64_t static vDSP.multiply<A>(_:by:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.divide<A>(_:by:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A>(_:by:result:));
}

{
  uint64_t vars8;

  return static vDSP.divide<A>(_:by:result:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A>(_:by:result:));
}

uint64_t static vDSP.divide<A>(_:by:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v8[0] = a1;
  v8[1] = a2;
  uint64_t v10 = a5;
  uint64_t v11 = a6;
  uint64_t v12 = v8;
  uint64_t v13 = a4;
  uint64_t v14 = a3;
  return (*(uint64_t (**)(uint64_t, unsigned char *, uint64_t, uint64_t, uint64_t))(a6 + 24))(a7, v9, MEMORY[0x1E4FBC848] + 8, a5, a6);
}

uint64_t closure #1 in closure #1 in static vDSP.divide<A>(_:by:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  if (result)
  {
    uint64_t v11 = result;
    unint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a6, a7);
    if ((result & 0x8000000000000000) == 0) {
      return a8(a3, 1, v11, 1, a4, 1, result);
    }
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t static vDSP.multiply(_:by:count:useConjugate:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, char a6, uint64_t a7)
{
  size_t v7 = (uint64_t (*)(void *, uint64_t, void *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))MEMORY[0x1E4F16F28];

  return static vDSP.multiply(_:by:count:useConjugate:result:)(a1, a2, a3, a4, a5, a6, a7, v7);
}

{
  uint64_t (*v7)(void *, uint64_t, void *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t);
  uint64_t vars8;

  size_t v7 = (uint64_t (*)(void *, uint64_t, void *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))MEMORY[0x1E4F16F30];

  return static vDSP.multiply(_:by:count:useConjugate:result:)(a1, a2, a3, a4, a5, a6, a7, v7);
}

uint64_t static vDSP.multiply(_:by:count:useConjugate:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, char a6, uint64_t a7, uint64_t (*a8)(void *, uint64_t, void *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  _OWORD v12[2] = *MEMORY[0x1E4F143B8];
  v12[0] = a1;
  v12[1] = a2;
  v11[0] = a3;
  v11[1] = a4;
  if (a5 < 0) {
    __break(1u);
  }
  if (a6) {
    uint64_t v9 = 0xFFFFFFFFLL;
  }
  else {
    uint64_t v9 = 1;
  }
  return a8(v12, 1, v11, 1, a7, 1, a5, v9);
}

uint64_t static vDSP.add(_:to:count:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v6 = (uint64_t (*)(void *, uint64_t, void *, uint64_t, uint64_t, uint64_t, uint64_t))MEMORY[0x1E4F16ED8];

  return static vDSP.add(_:to:count:result:)(a1, a2, a3, a4, a5, a6, v6);
}

{
  uint64_t (*v6)(void *, uint64_t, void *, uint64_t, uint64_t, uint64_t, uint64_t);
  uint64_t vars8;

  uint64_t v6 = (uint64_t (*)(void *, uint64_t, void *, uint64_t, uint64_t, uint64_t, uint64_t))MEMORY[0x1E4F16EE0];

  return static vDSP.add(_:to:count:result:)(a1, a2, a3, a4, a5, a6, v6);
}

uint64_t static vDSP.add(_:to:count:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(void *, uint64_t, void *, uint64_t, uint64_t, uint64_t, uint64_t))
{
  _OWORD v9[2] = *MEMORY[0x1E4F143B8];
  v9[0] = a1;
  v9[1] = a2;
  v8[0] = a3;
  v8[1] = a4;
  if (a5 < 0) {
    __break(1u);
  }
  return a7(v9, 1, v8, 1, a6, 1, a5);
}

uint64_t static vDSP.divide(_:by:count:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v6 = (uint64_t (*)(void *, uint64_t, void *, uint64_t, uint64_t, uint64_t, uint64_t))MEMORY[0x1E4F16EF8];

  return static vDSP.divide(_:by:count:result:)(a1, a2, a3, a4, a5, a6, v6);
}

{
  uint64_t (*v6)(void *, uint64_t, void *, uint64_t, uint64_t, uint64_t, uint64_t);
  uint64_t vars8;

  uint64_t v6 = (uint64_t (*)(void *, uint64_t, void *, uint64_t, uint64_t, uint64_t, uint64_t))MEMORY[0x1E4F16F00];

  return static vDSP.divide(_:by:count:result:)(a1, a2, a3, a4, a5, a6, v6);
}

uint64_t static vDSP.divide(_:by:count:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(void *, uint64_t, void *, uint64_t, uint64_t, uint64_t, uint64_t))
{
  _OWORD v9[2] = *MEMORY[0x1E4F143B8];
  v9[0] = a1;
  v9[1] = a2;
  v8[0] = a3;
  v8[1] = a4;
  if (a5 < 0) {
    __break(1u);
  }
  return a7(v8, 1, v9, 1, a6, 1, a5);
}

uint64_t static vDSP.subtract(_:from:count:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v6 = (uint64_t (*)(void *, uint64_t, void *, uint64_t, uint64_t, uint64_t, uint64_t))MEMORY[0x1E4F16F48];

  return static vDSP.subtract(_:from:count:result:)(a1, a2, a3, a4, a5, a6, v6);
}

{
  uint64_t (*v6)(void *, uint64_t, void *, uint64_t, uint64_t, uint64_t, uint64_t);
  uint64_t vars8;

  uint64_t v6 = (uint64_t (*)(void *, uint64_t, void *, uint64_t, uint64_t, uint64_t, uint64_t))MEMORY[0x1E4F16F50];

  return static vDSP.subtract(_:from:count:result:)(a1, a2, a3, a4, a5, a6, v6);
}

uint64_t static vDSP.subtract(_:from:count:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(void *, uint64_t, void *, uint64_t, uint64_t, uint64_t, uint64_t))
{
  _OWORD v9[2] = *MEMORY[0x1E4F143B8];
  v9[0] = a3;
  v9[1] = a4;
  v8[0] = a1;
  v8[1] = a2;
  if (a5 < 0) {
    __break(1u);
  }
  return a7(v8, 1, v9, 1, a6, 1, a5);
}

uint64_t static vDSP.absolute<A>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return static vDSP.absolute<A>(_:result:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in static vDSP.absolute<A>(_:result:));
}

{
  return static vDSP.absolute<A>(_:result:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in static vDSP.absolute<A>(_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.absolute<A>(_:result:)(void *a1)
{
  return closure #1 in static vDSP.absolute<A>(_:result:)(a1, v1[2], v1[3], v1[4], MEMORY[0x1E4F16EC8]);
}

{
  uint64_t *v1;

  return closure #1 in static vDSP.absolute<A>(_:result:)(a1, v1[2], v1[3], v1[4], MEMORY[0x1E4F16ED0]);
}

uint64_t static vDSP.absolute<A>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v11 = (*(uint64_t (**)(uint64_t))(*(void *)(a5 + 8) + 16))(a4);
  void v13[2] = a1;
  v13[3] = a2;
  v13[4] = v11;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t, uint64_t))(a5 + 16))(a6, v13, MEMORY[0x1E4FBC848] + 8, a4, a5);
}

uint64_t closure #1 in static vDSP.absolute<A>(_:result:)(void *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(void *, uint64_t))
{
  _OWORD v6[2] = *MEMORY[0x1E4F143B8];
  v6[0] = a2;
  v6[1] = a3;
  if (!*a1) {
LABEL_5:
  }
    __break(1u);
  if (a4 < 0)
  {
    __break(1u);
    goto LABEL_5;
  }
  return a5(v6, 1);
}

uint64_t static vDSP.squareMagnitudes<A>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return static vDSP.phase<A>(_:result:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in static vDSP.squareMagnitudes<A>(_:result:));
}

{
  return static vDSP.phase<A>(_:result:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in static vDSP.squareMagnitudes<A>(_:result:));
}

void *partial apply for closure #1 in static vDSP.squareMagnitudes<A>(_:result:)(void *a1)
{
  return partial apply for closure #1 in static vDSP.phase<A>(_:result:)(a1, MEMORY[0x1E4F16F08]);
}

{
  return partial apply for closure #1 in static vDSP.phase<A>(_:result:)(a1, MEMORY[0x1E4F16F10]);
}

uint64_t static vDSP.phase<A>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a5 + 8) + 16))(a4);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(uint64_t))(a5 + 16))(a6);
  }
  return result;
}

void *partial apply for closure #1 in static vDSP.phase<A>(_:result:)(void *result, uint64_t (*a2)(long long *, uint64_t, void, uint64_t, uint64_t))
{
  uint64_t v3 = *(void *)(v2 + 32);
  long long v4 = *(_OWORD *)(v2 + 16);
  if (*result) {
    return (void *)a2(&v4, 1, *result, 1, v3);
  }
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A>(_:by:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A>(_:by:result:)(a1, a2, MEMORY[0x1E4F16EB0]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A>(_:by:result:)(a1, a2, MEMORY[0x1E4F16EA8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A>(_:by:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  return closure #1 in closure #1 in static vDSP.divide<A>(_:by:result:)(a1, a2, v3[4], v3[5], v3[6], v3[2], v3[3], a3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.divide<A>(_:by:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A>(_:by:result:)(a1, a2, MEMORY[0x1E4F16EA0]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A>(_:by:result:)(a1, a2, MEMORY[0x1E4F16E98]);
}

uint64_t BNNS.NearestNeighborsRef.init(capacity:dimensionCount:neighborCount:dataType:)(uint64_t result, uint64_t a2, uint64_t a3)
{
  if (result > 0xFFFFFFFFLL)
  {
    __break(1u);
    goto LABEL_8;
  }
  if (a2 > 0xFFFFFFFFLL)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if ((a2 | result | a3) < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (a3 > 0xFFFFFFFFLL)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  uint64_t result = MEMORY[0x1D25FFEB0]();
  if (result)
  {
    *(void *)(v3 + 16) = result;
    return v3;
  }
LABEL_11:
  __break(1u);
  return result;
}

uint64_t BNNS.NearestNeighborsRef.__deallocating_deinit()
{
  MEMORY[0x1D25FFF00](*(void *)(v0 + 16));

  return swift_deallocClassInstance();
}

uint64_t BNNS.NearestNeighbors.init(capacity:dimensionCount:neighborCount:dataType:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, int a4@<W3>, uint64_t a5@<X8>)
{
  type metadata accessor for BNNS.NearestNeighborsRef();
  swift_allocObject();
  uint64_t result = BNNS.NearestNeighborsRef.init(capacity:dimensionCount:neighborCount:dataType:)(a1, a2, a3);
  *(void *)a5 = result;
  *(void *)(a5 + 8) = a2;
  *(void *)(a5 + 16) = a1;
  *(void *)(a5 + 24) = a3;
  *(_DWORD *)(a5 + 32) = a4;
  return result;
}

uint64_t type metadata accessor for BNNS.NearestNeighborsRef()
{
  return self;
}

uint64_t BNNS.NearestNeighbors.append(samples:)(uint64_t result)
{
  uint64_t v2 = (uint64_t *)v1;
  if (*(_DWORD *)(result + 144) != *(_DWORD *)(v1 + 32))
  {
    __break(1u);
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  uint64_t v3 = result;
  uint64_t v4 = *v2;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v8);
  outlined init with take of BNNS.Shape((uint64_t)v8, (uint64_t)v11);
  uint64_t result = _s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v11);
  if ((result - 1) >= 4)
  {
LABEL_12:
    __break(1u);
    return result;
  }
  outlined init with take of UnsafeMutableRawPointer?(v3 + 136, (uint64_t)v9);
  uint64_t result = outlined init with take of UnsafeMutableRawPointer?((uint64_t)v9, (uint64_t)&v10);
  if (!v10) {
    return result;
  }
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v7);
  outlined init with take of BNNS.Shape((uint64_t)v7, (uint64_t)v8);
  outlined init with take of BNNS.Shape((uint64_t)v8, (uint64_t)v6);
  uint64_t result = (uint64_t)BNNS.Shape.size.getter((uint64_t)v5);
  if ((v5[0] & 0x8000000000000000) != 0) {
    goto LABEL_9;
  }
  if (HIDWORD(v5[0]))
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  uint64_t result = MEMORY[0x1D26002E0](*(void *)(v4 + 16));
  if (result)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  return result;
}

uint64_t BNNS.NearestNeighbors.apply(index:outputIndices:outputDistances:)(uint64_t result, char a2, uint64_t a3, uint64_t a4)
{
  if (a2) {
    uint64_t v5 = -1;
  }
  else {
    uint64_t v5 = result;
  }
  if (v5 < (uint64_t)0xFFFFFFFF80000000)
  {
    __break(1u);
    goto LABEL_10;
  }
  if (v5 > 0x7FFFFFFF)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  uint64_t v7 = *(void *)(*(void *)v4 + 16);
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)&v8);
  uint64_t result = outlined init with take of UnsafeMutableRawPointer?((uint64_t)&v8, (uint64_t)&v9);
  if (!v9)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  if (*(void *)(a4 + 136)) {
    return MEMORY[0x1D26002D0](v7, v5);
  }
LABEL_12:
  __break(1u);
  return result;
}

uint64_t destroy for BNNS.NearestNeighbors()
{
  return swift_release();
}

uint64_t initializeWithCopy for BNNS.NearestNeighbors(uint64_t a1, uint64_t a2)
{
  *(void *)a1 = *(void *)a2;
  *(_OWORD *)(a1 + 8) = *(_OWORD *)(a2 + 8);
  *(void *)(a1 + 24) = *(void *)(a2 + 24);
  *(_DWORD *)(a1 + 32) = *(_DWORD *)(a2 + 32);
  swift_retain();
  return a1;
}

uint64_t assignWithCopy for BNNS.NearestNeighbors(uint64_t a1, uint64_t a2)
{
  *(void *)a1 = *(void *)a2;
  swift_retain();
  swift_release();
  *(void *)(a1 + 8) = *(void *)(a2 + 8);
  *(void *)(a1 + 16) = *(void *)(a2 + 16);
  *(void *)(a1 + 24) = *(void *)(a2 + 24);
  *(_DWORD *)(a1 + 32) = *(_DWORD *)(a2 + 32);
  return a1;
}

__n128 __swift_memcpy36_8(uint64_t a1, uint64_t a2)
{
  __n128 result = *(__n128 *)a2;
  long long v3 = *(_OWORD *)(a2 + 16);
  *(_DWORD *)(a1 + 32) = *(_DWORD *)(a2 + 32);
  *(__n128 *)a1 = result;
  *(_OWORD *)(a1 + 16) = v3;
  return result;
}

uint64_t assignWithTake for BNNS.NearestNeighbors(uint64_t a1, uint64_t a2)
{
  *(void *)a1 = *(void *)a2;
  swift_release();
  *(_OWORD *)(a1 + 8) = *(_OWORD *)(a2 + 8);
  *(void *)(a1 + 24) = *(void *)(a2 + 24);
  *(_DWORD *)(a1 + 32) = *(_DWORD *)(a2 + 32);
  return a1;
}

uint64_t getEnumTagSinglePayload for BNNS.NearestNeighbors(uint64_t *a1, int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0 && *((unsigned char *)a1 + 36)) {
    return *(_DWORD *)a1 + 0x80000000;
  }
  uint64_t v2 = *a1;
  if ((unint64_t)*a1 >= 0xFFFFFFFF) {
    LODWORD(v2) = -1;
  }
  return (v2 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.NearestNeighbors(uint64_t result, int a2, int a3)
{
  if (a2 < 0)
  {
    *(void *)(result + 16) = 0;
    *(void *)(result + 24) = 0;
    *(_DWORD *)(result + 32) = 0;
    *(void *)__n128 result = a2 ^ 0x80000000;
    *(void *)(result + 8) = 0;
    if (a3 < 0) {
      *(unsigned char *)(result + 36) = 1;
    }
  }
  else
  {
    if ((a3 & 0x80000000) == 0)
    {
      if (!a2) {
        return result;
      }
LABEL_8:
      *(void *)__n128 result = (a2 - 1);
      return result;
    }
    *(unsigned char *)(result + 36) = 0;
    if (a2) {
      goto LABEL_8;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.NearestNeighbors()
{
  return &type metadata for BNNS.NearestNeighbors;
}

uint64_t vDSP.DCTTransformType.dctType.getter()
{
  return *v0 + 2;
}

BOOL static vDSP.DCTTransformType.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void vDSP.DCTTransformType.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

void *static vDSP.DCTTransformType.allCases.getter()
{
  return &outlined read-only object #0 of static vDSP.DCTTransformType.allCases.getter;
}

Swift::Int vDSP.DCTTransformType.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

void protocol witness for static CaseIterable.allCases.getter in conformance vDSP.DCTTransformType(void *a1@<X8>)
{
  *a1 = &outlined read-only object #0 of protocol witness for static CaseIterable.allCases.getter in conformance vDSP.DCTTransformType;
}

vDSP_DFT_SetupStruct *vDSP.DCT.__allocating_init(previous:count:transformType:)(vDSP_DFT_SetupStruct *a1, vDSP_Length a2, unsigned __int8 *a3)
{
  return vDSP.DCT.init(previous:count:transformType:)(a1, a2, a3);
}

vDSP_DFT_SetupStruct *vDSP.DCT.init(previous:count:transformType:)(vDSP_DFT_SetupStruct *result, vDSP_Length a2, unsigned __int8 *a3)
{
  if (result) {
    __n128 result = (vDSP_DFT_SetupStruct *)*((void *)result + 2);
  }
  if ((a2 & 0x8000000000000000) != 0)
  {
    __break(1u);
  }
  else
  {
    vDSP_DFT_Setup Setup = vDSP_DCT_CreateSetup(result, a2, (vDSP_DCT_Type)(*a3 + 2));
    swift_release();
    if (Setup)
    {
      *(void *)(v3 + 16) = Setup;
    }
    else
    {
      type metadata accessor for vDSP.DCT();
      swift_deallocPartialClassInstance();
      return 0;
    }
    return (vDSP_DFT_SetupStruct *)v3;
  }
  return result;
}

uint64_t type metadata accessor for vDSP.DCT()
{
  return self;
}

uint64_t vDSP.DCT.transform<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in vDSP.DCT.transform<A>(_:));
}

uint64_t closure #1 in vDSP.DCT.transform<A>(_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v11 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v12 = lazy protocol witness table accessor for type [vDSP.DCTTransformType] and conformance [A](&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v13 = *(void *)(a3 + 16);
  void v15[2] = a5;
  v15[3] = v11;
  void v15[4] = a6;
  void v15[5] = v12;
  v15[6] = a4;
  v15[7] = v13;
  (*(void (**)(uint64_t (*)(uint64_t), void *, uint64_t, uint64_t, uint64_t))(v12 + 16))(partial apply for closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dctSetup:source:destination:), v15, MEMORY[0x1E4FBC848] + 8, v11, v12);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t partial apply for closure #1 in vDSP.DCT.transform<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in vDSP.DCT.transform<A>(_:)(a1, a2, v2[4], v2[5], v2[2], v2[3]);
}

uint64_t vDSP.DCT.transform<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v7 = *(void *)(v6 + 16);
  _OWORD v9[2] = a3;
  void v9[3] = a4;
  v9[4] = a5;
  void v9[5] = a6;
  void v9[6] = a1;
  v9[7] = v7;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t), void *, uint64_t, uint64_t, uint64_t))(a6 + 16))(partial apply for closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dctSetup:source:destination:), v9, MEMORY[0x1E4FBC848] + 8, a4, a6);
}

uint64_t vDSP.DCT.deinit()
{
  vDSP_DFT_DestroySetup(*(vDSP_DFT_Setup *)(v0 + 16));
  return v0;
}

uint64_t vDSP.DCT.__deallocating_deinit()
{
  vDSP_DFT_DestroySetup(*(vDSP_DFT_Setup *)(v0 + 16));

  return swift_deallocClassInstance();
}

unint64_t lazy protocol witness table accessor for type vDSP.DCTTransformType and conformance vDSP.DCTTransformType()
{
  unint64_t result = lazy protocol witness table cache variable for type vDSP.DCTTransformType and conformance vDSP.DCTTransformType;
  if (!lazy protocol witness table cache variable for type vDSP.DCTTransformType and conformance vDSP.DCTTransformType)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.DCTTransformType and conformance vDSP.DCTTransformType);
  }
  return result;
}

uint64_t associated type witness table accessor for CaseIterable.AllCases : Collection in vDSP.DCTTransformType()
{
  return lazy protocol witness table accessor for type [vDSP.DCTTransformType] and conformance [A](&lazy protocol witness table cache variable for type [vDSP.DCTTransformType] and conformance [A], &demangling cache variable for type metadata for [vDSP.DCTTransformType]);
}

_UNKNOWN **associated type witness table accessor for vDSP_FloatingPointDiscreteCosineTransformable.DCTFunctions : vDSP_DCTFunctions in Float()
{
  return &protocol witness table for vDSP.VectorizableFloat;
}

unsigned char *storeEnumTagSinglePayload for vDSP.DCTTransformType(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 2 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 2) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFE) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFD)
  {
    unsigned int v6 = ((a2 - 254) >> 8) + 1;
    *unint64_t result = a2 + 2;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D212AAD8);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 2;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vDSP.DCTTransformType()
{
  return &type metadata for vDSP.DCTTransformType;
}

uint64_t method lookup function for vDSP.DCT(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for vDSP.DCT);
}

uint64_t dispatch thunk of vDSP.DCT.__allocating_init(previous:count:transformType:)()
{
  return (*(uint64_t (**)(void))(v0 + 88))();
}

uint64_t dispatch thunk of vDSP.DCT.transform<A>(_:)()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 96))();
}

uint64_t dispatch thunk of vDSP.DCT.transform<A, B>(_:result:)()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 104))();
}

uint64_t partial apply for closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dctSetup:source:destination:)(uint64_t a1)
{
  v5[3] = a1;
  uint64_t v2 = v1[2];
  uint64_t v3 = v1[4];
  _OWORD v5[2] = v1[7];
  return (*(uint64_t (**)(void (*)(const float *), void *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dctSetup:source:destination:), v5, MEMORY[0x1E4FBC848] + 8, v2);
}

void partial apply for closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dctSetup:source:destination:)(const float *__Input)
{
  if (__Input)
  {
    uint64_t v2 = **(float ***)(v1 + 24);
    if (v2)
    {
      vDSP_DCT_Execute(*(const vDSP_DFT_SetupStruct **)(v1 + 16), __Input, v2);
      return;
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
}

uint64_t lazy protocol witness table accessor for type [vDSP.DCTTransformType] and conformance [A](unint64_t *a1, uint64_t *a2)
{
  uint64_t result = *a1;
  if (!result)
  {
    __swift_instantiateConcreteTypeFromMangledNameAbstract(a2);
    uint64_t result = swift_getWitnessTable();
    atomic_store(result, a1);
  }
  return result;
}

uint64_t static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  void (*v22)(char *);
  uint64_t v23;
  void (*v24)(char *, uint64_t, uint64_t);
  uint64_t (*v25)(uint64_t, uint64_t);
  uint64_t v26;
  uint64_t (*v27)(uint64_t, uint64_t);
  uint64_t v28;
  uint64_t result;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t (*v39)(uint64_t, uint64_t);
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;

  long long v42 = a3;
  long long v43 = a6;
  uint64_t v14 = *(void *)(a4 - 8);
  uint64_t v15 = MEMORY[0x1F4188790](a1);
  uint64_t v17 = (char *)&v37 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v19 = *(void *)(v18 - 8);
  MEMORY[0x1F4188790](v15);
  uint64_t v21 = (char *)&v37 - ((v20 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v22 = *(void (**)(char *))(v19 + 16);
  uint64_t v40 = v23;
  v22(v21);
  uint64_t v24 = *(void (**)(char *, uint64_t, uint64_t))(v14 + 16);
  uint64_t v37 = a1;
  v24(v17, a1, a4);
  uint64_t v25 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  long long v41 = a8;
  long long v39 = v25;
  uint64_t v26 = v25(a5, a8);
  uint64_t v27 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  uint64_t v38 = a7;
  uint64_t v28 = v27(a4, a7);
  (*(void (**)(char *, uint64_t))(v14 + 8))(v17, a4);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v19 + 8))(v21, a5);
  if (v26 != v28)
  {
    __break(1u);
    goto LABEL_6;
  }
  uint64_t v30 = v43;
  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a9 + 8) + 16))(v43);
  if (result < 0)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v31 = result;
  uint64_t v32 = v40;
  uint64_t result = v39(a5, v41);
  if ((result & 0x8000000000000000) == 0)
  {
    uint64_t v33 = MEMORY[0x1F4188790](result);
    *(&v37 - 10) = a4;
    *(&v37 - 9) = a5;
    uint64_t v34 = v38;
    *(&v37 - 8) = v30;
    *(&v37 - 7) = v34;
    *(&v37 - 6) = v35;
    *(&v37 - 5) = a9;
    *(&v37 - 4) = v37;
    *(&v37 - 3) = v32;
    *(&v37 - 2) = v31;
    *(&v37 - 1) = v33;
    return (*(uint64_t (**)(uint64_t))(a9 + 16))(v36);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:));
}

{
  return partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:));
}

void static vDSP.linearInterpolate<A, B>(values:atIndices:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  float v6 = static vDSP.maximum<A>(_:)(a2, a4, a6);
  if ((~LODWORD(v6) & 0x7F800000) == 0)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (v6 <= -9.2234e18)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (v6 >= 9.2234e18)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (!__OFADD__((uint64_t)v6, 1))
  {
    uint64_t v7 = MEMORY[0x1F4188790]((uint64_t)v6 + 1);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v7, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(values:atIndices:));
    return;
  }
LABEL_9:
  __break(1u);
}

{
  double v6;
  uint64_t v7;

  float v6 = static vDSP.maximum<A>(_:)(a2, a4, a6);
  if ((~*(void *)&v6 & 0x7FF0000000000000) == 0)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (v6 <= -9.22337204e18)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (v6 >= 9.22337204e18)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (!__OFADD__((uint64_t)v6, 1))
  {
    uint64_t v7 = MEMORY[0x1F4188790]((uint64_t)v6 + 1);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v7, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(values:atIndices:));
    return;
  }
LABEL_9:
  __break(1u);
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(values:atIndices:)(uint64_t a1, void *a2)
{
  return partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(values:atIndices:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:));
}

{
  return partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(values:atIndices:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:));
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t *a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  if (!a3)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!result)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v8 = *a5;
  if (v8) {
    return a8(a3, 1, result, 1, v8, 1, a6, a7);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(uint64_t a1, float a2, float a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  uint64_t v40 = a6;
  uint64_t v41 = a8;
  uint64_t v39 = a1;
  uint64_t v17 = *(void *)(a7 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v19 = (char *)&v35 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v20 = *(void (**)(char *))(v17 + 16);
  uint64_t v35 = v21;
  v20(v19);
  uint64_t v22 = *(uint64_t (**)(uint64_t, uint64_t))(a10 + 16);
  uint64_t v37 = a10;
  uint64_t v23 = v22(a7, a10);
  uint64_t v36 = a11;
  uint64_t v24 = *(void *)(a11 + 8);
  uint64_t v25 = *(uint64_t (**)(uint64_t, uint64_t))(v24 + 16);
  uint64_t v38 = a5;
  uint64_t v26 = v41;
  uint64_t v27 = v25(v41, v24);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v17 + 8))(v19, a7);
  if (v23 != v27)
  {
    __break(1u);
    goto LABEL_6;
  }
  uint64_t v29 = v39;
  uint64_t v30 = v40;
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a9 + 16))(v40, a9);
  if (result < 0)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v31 = result;
  uint64_t result = v25(v26, v24);
  if ((result & 0x8000000000000000) == 0)
  {
    uint64_t v32 = MEMORY[0x1F4188790](result);
    *(&v35 - 12) = v30;
    *(&v35 - 11) = a7;
    *(&v35 - 10) = v26;
    *(&v35 - 9) = a9;
    uint64_t v33 = v36;
    *(&v35 - 8) = v37;
    *(&v35 - 7) = v33;
    uint64_t v34 = v35;
    *(&v35 - 6) = v29;
    *(&v35 - 5) = v34;
    *((float *)&v35 - 8) = a2;
    *((float *)&v35 - 7) = a3;
    *(&v35 - 3) = v31;
    *(&v35 - 2) = v32;
    return (*(uint64_t (**)(void))(v33 + 16))(partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:));
  }
LABEL_7:
  __break(1u);
  return result;
}

void closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(const float *a1, int a2, const float *__C, int a4, vDSP_Length __M, float **a6, vDSP_Length __N, float a8, float a9)
{
  if (!a1)
  {
    __break(1u);
    goto LABEL_6;
  }
  float v10 = a8;
  float v9 = a9;
  if (!__C)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*a6)
  {
    vDSP_vtabi(a1, 1, &v10, &v9, __C, __M, *a6, 1, __N);
    return;
  }
LABEL_7:
  __break(1u);
}

uint64_t static vDSP.linearInterpolate<A, B>(lookupTable:withOffsets:scale:baseOffset:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v6 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(lookupTable:withOffsets:scale:baseOffset:));
}

{
  uint64_t v6;

  uint64_t v6 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(lookupTable:withOffsets:scale:baseOffset:));
}

uint64_t closure #1 in static vDSP.linearInterpolate<A, B>(lookupTable:withOffsets:scale:baseOffset:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, float a10)
{
  uint64_t v19 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v20 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(a3, a9, a10, a4, a1, a5, a6, v19, a7, a8, v20);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
  *a2 = result;
  return result;
}

uint64_t static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(uint64_t a1, double a2, double a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  uint64_t v40 = a6;
  uint64_t v41 = a8;
  uint64_t v39 = a1;
  uint64_t v17 = *(void *)(a7 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v19 = (char *)&v35 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v20 = *(void (**)(char *))(v17 + 16);
  uint64_t v35 = v21;
  v20(v19);
  uint64_t v22 = *(uint64_t (**)(uint64_t, uint64_t))(a10 + 16);
  uint64_t v37 = a10;
  uint64_t v23 = v22(a7, a10);
  uint64_t v36 = a11;
  uint64_t v24 = *(void *)(a11 + 8);
  uint64_t v25 = *(uint64_t (**)(uint64_t, uint64_t))(v24 + 16);
  uint64_t v38 = a5;
  uint64_t v26 = v41;
  uint64_t v27 = v25(v41, v24);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v17 + 8))(v19, a7);
  if (v23 != v27)
  {
    __break(1u);
    goto LABEL_6;
  }
  uint64_t v29 = v39;
  uint64_t v30 = v40;
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a9 + 16))(v40, a9);
  if (result < 0)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v31 = result;
  uint64_t result = v25(v26, v24);
  if ((result & 0x8000000000000000) == 0)
  {
    uint64_t v32 = MEMORY[0x1F4188790](result);
    *(&v35 - 12) = v30;
    *(&v35 - 11) = a7;
    *(&v35 - 10) = v26;
    *(&v35 - 9) = a9;
    uint64_t v33 = v36;
    *(&v35 - 8) = v37;
    *(&v35 - 7) = v33;
    uint64_t v34 = v35;
    *(&v35 - 6) = v29;
    *(&v35 - 5) = v34;
    *((double *)&v35 - 4) = a2;
    *((double *)&v35 - 3) = a3;
    *(&v35 - 2) = v31;
    *(&v35 - 1) = v32;
    return (*(uint64_t (**)(void))(v33 + 16))(partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:));
  }
LABEL_7:
  __break(1u);
  return result;
}

void closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(const double *a1, int a2, const double *__C, int a4, vDSP_Length __M, double **a6, vDSP_Length __N, double a8, double a9)
{
  if (!a1)
  {
    __break(1u);
    goto LABEL_6;
  }
  double v10 = a8;
  double v9 = a9;
  if (!__C)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*a6)
  {
    vDSP_vtabiD(a1, 1, &v10, &v9, __C, __M, *a6, 1, __N);
    return;
  }
LABEL_7:
  __break(1u);
}

uint64_t closure #1 in static vDSP.linearInterpolate<A, B>(lookupTable:withOffsets:scale:baseOffset:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, double a10)
{
  uint64_t v19 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v20 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(a3, a9, a10, a4, a1, a5, a6, v19, a7, a8, v20);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
  *a2 = result;
  return result;
}

uint64_t static vDSP.fill<A>(_:with:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a3 + 8) + 16))(a2);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(void))(a3 + 16))(partial apply for closure #1 in static vDSP.fill<A>(_:with:));
  }
  return result;
}

{
  uint64_t result;

  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a3 + 8) + 16))(a2);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(void))(a3 + 16))(partial apply for closure #1 in static vDSP.fill<A>(_:with:));
  }
  return result;
}

uint64_t static vDSP.clear<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.clear<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.clear<A>(_:));
}

{
  return static vDSP.clear<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.clear<A>(_:));
}

uint64_t static vDSP.clear<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a3 + 8) + 16))(a2);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(uint64_t))(a3 + 16))(a4);
  }
  return result;
}

BOOL static vDSP.WindowSequence.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void vDSP.WindowSequence.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int vDSP.WindowSequence.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t static vDSP.window<A>(ofType:usingSequence:count:isHalfWindow:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  if (a3 < 1)
  {
    __break(1u);
  }
  else
  {
    if (a5 == MEMORY[0x1E4FBB470])
    {
      MEMORY[0x1F4188790](a1);
      specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.window<A>(ofType:usingSequence:count:isHalfWindow:));
      goto LABEL_6;
    }
    if (a5 == MEMORY[0x1E4FBB3D0])
    {
      MEMORY[0x1F4188790](a1);
      specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, (uint64_t (*)(void *, uint64_t *))partial apply for closure #2 in static vDSP.window<A>(ofType:usingSequence:count:isHalfWindow:));
LABEL_6:
      uint64_t v7 = _arrayForceCast<A, B>(_:)();
      swift_bridgeObjectRelease();
      return v7;
    }
  }
  uint64_t result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:));
}

{
  return static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(a1, a2, a3, a4, a5, (uint64_t)partial apply for closure #1 in static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:));
}

uint64_t static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a5 + 8) + 16))(a4);
  if (result < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(uint64_t))(a5 + 16))(a6);
  }
  return result;
}

uint64_t closure #1 in static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(uint64_t *a1, char a2, uint64_t a3, char a4, uint64_t (*a5)(void), uint64_t (*a6)(uint64_t, uint64_t, void), uint64_t (*a7)(uint64_t, uint64_t, void))
{
  switch(a2)
  {
    case 1:
      if (*a1) {
        goto LABEL_9;
      }
      goto LABEL_13;
    case 2:
      uint64_t v7 = *a1;
      if (!v7)
      {
        __break(1u);
LABEL_11:
        __break(1u);
LABEL_12:
        __break(1u);
LABEL_13:
        __break(1u);
        JUMPOUT(0x1D212C1CCLL);
      }
      uint64_t result = a6(v7, a3, a4 & 1);
      break;
    case 3:
      uint64_t v9 = *a1;
      if (!v9) {
        goto LABEL_11;
      }
      return a7(v9, a3, a4 & 1);
    default:
      if (!*a1) {
        goto LABEL_12;
      }
LABEL_9:
      uint64_t result = a5();
      break;
  }
  return result;
}

uint64_t static vDSP.ramp(withInitialValue:increment:count:)(int64_t a1, float a2, float a3)
{
  uint64_t v10 = *MEMORY[0x1E4F143B8];
  if (a1 < 1) {
    __break(1u);
  }
  uint64_t v6 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(void *)(v6 + 16) = a1;
  float v8 = a3;
  float __A = a2;
  vDSP_vramp(&__A, &v8, (float *)(v6 + 32), 1, a1);
  *(void *)(v6 + 16) = a1;
  return v6;
}

uint64_t static vDSP.formRamp<A>(withInitialValue:increment:result:)(float a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v17 = *MEMORY[0x1E4F143B8];
  uint64_t v9 = (*(uint64_t (**)(uint64_t))(*(void *)(a5 + 8) + 16))(a4);
  if (v9 < 0) {
    __break(1u);
  }
  float v11 = a1;
  float v12 = a2;
  uint64_t v14 = &v11;
  uint64_t v15 = &v12;
  uint64_t v16 = v9;
  return (*(uint64_t (**)(void *(*)(void *), unsigned char *, uint64_t, uint64_t, uint64_t))(a5 + 16))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.formRamp<A>(withInitialValue:increment:result:), v13, MEMORY[0x1E4FBC848] + 8, a4, a5);
}

uint64_t static vDSP.ramp(withInitialValue:increment:count:)(int64_t a1, double a2, double a3)
{
  __A[1] = *(double *)MEMORY[0x1E4F143B8];
  if (a1 < 1) {
    __break(1u);
  }
  uint64_t v6 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(void *)(v6 + 16) = a1;
  double __B = a3;
  __A[0] = a2;
  vDSP_vrampD(__A, &__B, (double *)(v6 + 32), 1, a1);
  *(void *)(v6 + 16) = a1;
  return v6;
}

uint64_t static vDSP.formRamp<A>(withInitialValue:increment:result:)(double a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v17 = *MEMORY[0x1E4F143B8];
  uint64_t v9 = (*(uint64_t (**)(uint64_t))(*(void *)(a5 + 8) + 16))(a4);
  if (v9 < 0) {
    __break(1u);
  }
  double v11 = a1;
  double v12 = a2;
  uint64_t v14 = &v11;
  uint64_t v15 = &v12;
  uint64_t v16 = v9;
  return (*(uint64_t (**)(void *(*)(void *), unsigned char *, uint64_t, uint64_t, uint64_t))(a5 + 16))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.formRamp<A>(withInitialValue:increment:result:), v13, MEMORY[0x1E4FBC848] + 8, a4, a5);
}

uint64_t static vDSP.ramp(in:count:)(int64_t a1, float a2, float a3)
{
  return static vDSP.ramp(in:count:)(a1, a2, a3);
}

{
  uint64_t v6;
  float v8;
  float __B;
  uint64_t v10;

  uint64_t v10 = *MEMORY[0x1E4F143B8];
  if (a1 < 1) {
    __break(1u);
  }
  uint64_t v6 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(void *)(v6 + 16) = a1;
  float v8 = a2;
  double __B = (float)(a3 - a2) / (float)((float)(unint64_t)a1 + -1.0);
  vDSP_vramp(&v8, &__B, (float *)(v6 + 32), 1, a1);
  *(void *)(v6 + 16) = a1;
  return v6;
}

uint64_t static vDSP.formRamp<A>(in:result:)(uint64_t a1, uint64_t a2, uint64_t a3, float a4, float a5)
{
  return static vDSP.formRamp<A>(in:result:)(a4, a5, a1, a2, a3, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:));
}

uint64_t static vDSP.ramp(in:count:)(int64_t a1, double a2, double a3)
{
  return static vDSP.ramp(in:count:)(a1, a2, a3);
}

{
  uint64_t v6;
  double __A;
  double __B[2];

  __B[1] = *(double *)MEMORY[0x1E4F143B8];
  if (a1 < 1) {
    __break(1u);
  }
  uint64_t v6 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(void *)(v6 + 16) = a1;
  float __A = a2;
  __B[0] = (a3 - a2) / ((double)(unint64_t)a1 + -1.0);
  vDSP_vrampD(&__A, __B, (double *)(v6 + 32), 1, a1);
  *(void *)(v6 + 16) = a1;
  return v6;
}

uint64_t static vDSP.formRamp<A>(in:result:)(uint64_t a1, uint64_t a2, uint64_t a3, double a4, double a5)
{
  return static vDSP.formRamp<A>(in:result:)(a4, a5, a1, a2, a3, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:));
}

uint64_t static vDSP.ramp(from:through:count:)(int64_t a1, float a2, float a3)
{
  return static vDSP.ramp(in:count:)(a1, a2, a3);
}

uint64_t static vDSP.formRamp<A>(from:through:result:)(uint64_t a1, uint64_t a2, uint64_t a3, float a4, float a5)
{
  return static vDSP.formRamp<A>(in:result:)(a4, a5, a1, a2, a3, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:));
}

uint64_t static vDSP.formRamp<A>(in:result:)(float a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v19 = *MEMORY[0x1E4F143B8];
  unint64_t v11 = (*(uint64_t (**)(uint64_t))(*(void *)(a5 + 8) + 16))(a4);
  if ((v11 & 0x8000000000000000) != 0) {
    __break(1u);
  }
  float v13 = a1;
  float v14 = (float)(a2 - a1) / (float)((float)v11 + -1.0);
  uint64_t v16 = &v13;
  uint64_t v17 = &v14;
  unint64_t v18 = v11;
  return (*(uint64_t (**)(uint64_t, unsigned char *, uint64_t, uint64_t, uint64_t))(a5 + 16))(a6, v15, MEMORY[0x1E4FBC848] + 8, a4, a5);
}

uint64_t static vDSP.ramp(from:through:count:)(int64_t a1, double a2, double a3)
{
  return static vDSP.ramp(in:count:)(a1, a2, a3);
}

uint64_t static vDSP.formRamp<A>(from:through:result:)(uint64_t a1, uint64_t a2, uint64_t a3, double a4, double a5)
{
  return static vDSP.formRamp<A>(in:result:)(a4, a5, a1, a2, a3, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:));
}

uint64_t static vDSP.formRamp<A>(in:result:)(double a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v19 = *MEMORY[0x1E4F143B8];
  unint64_t v11 = (*(uint64_t (**)(uint64_t))(*(void *)(a5 + 8) + 16))(a4);
  if ((v11 & 0x8000000000000000) != 0) {
    __break(1u);
  }
  double v13 = a1;
  double v14 = (a2 - a1) / ((double)v11 + -1.0);
  uint64_t v16 = &v13;
  uint64_t v17 = &v14;
  unint64_t v18 = v11;
  return (*(uint64_t (**)(uint64_t, unsigned char *, uint64_t, uint64_t, uint64_t))(a5 + 16))(a6, v15, MEMORY[0x1E4FBC848] + 8, a4, a5);
}

uint64_t static vDSP.ramp<A>(withInitialValue:multiplyingBy:increment:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v4, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.ramp<A>(withInitialValue:multiplyingBy:increment:));
}

{
  uint64_t v4;

  uint64_t v4 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v4, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.ramp<A>(withInitialValue:multiplyingBy:increment:));
}

uint64_t closure #1 in static vDSP.ramp<A>(withInitialValue:multiplyingBy:increment:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  uint64_t v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(a3, a7, a4, a1, a5, v14, a6, v15);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(uint64_t a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v30 = a6;
  uint64_t v27 = a1;
  uint64_t v12 = *(void *)(a5 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v14 = (char *)&v26 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v15 = *(void (**)(char *))(v12 + 16);
  uint64_t v26 = v16;
  v15(v14);
  uint64_t v17 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  uint64_t v28 = a7;
  uint64_t v18 = v17(a5, a7);
  uint64_t v29 = a8;
  uint64_t v19 = *(void *)(a8 + 8);
  uint64_t v20 = *(uint64_t (**)(uint64_t, uint64_t))(v19 + 16);
  uint64_t v21 = v30;
  uint64_t v22 = v20(v30, v19);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v12 + 8))(v14, a5);
  if (v18 == v22)
  {
    uint64_t result = v20(v21, v19);
    if ((result & 0x8000000000000000) == 0)
    {
      uint64_t v24 = MEMORY[0x1F4188790](result);
      *(&v26 - 8) = a5;
      *(&v26 - 7) = v21;
      uint64_t v25 = v29;
      *(&v26 - 6) = v28;
      *(&v26 - 5) = v25;
      *(&v26 - 4) = v26;
      *((float *)&v26 - 6) = a2;
      *(&v26 - 2) = v27;
      *(&v26 - 1) = v24;
      return (*(uint64_t (**)(void))(v25 + 16))(partial apply for closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:));
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

void closure #1 in closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(const float *a1, int a2, float *a3, float **a4, vDSP_Length __N, float a6)
{
  uint64_t v7 = *MEMORY[0x1E4F143B8];
  float __Step = a6;
  if (!a1)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  if (!*a4) {
    goto LABEL_5;
  }
  vDSP_vrampmul(a1, 1, a3, &__Step, *a4, 1, __N);
}

uint64_t closure #1 in static vDSP.ramp<A>(withInitialValue:multiplyingBy:increment:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  uint64_t v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(a3, a7, a4, a1, a5, v14, a6, v15);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(uint64_t a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v30 = a6;
  uint64_t v27 = a1;
  uint64_t v12 = *(void *)(a5 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v14 = (char *)&v26 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v15 = *(void (**)(char *))(v12 + 16);
  uint64_t v26 = v16;
  v15(v14);
  uint64_t v17 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  uint64_t v28 = a7;
  uint64_t v18 = v17(a5, a7);
  uint64_t v29 = a8;
  uint64_t v19 = *(void *)(a8 + 8);
  uint64_t v20 = *(uint64_t (**)(uint64_t, uint64_t))(v19 + 16);
  uint64_t v21 = v30;
  uint64_t v22 = v20(v30, v19);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v12 + 8))(v14, a5);
  if (v18 == v22)
  {
    uint64_t result = v20(v21, v19);
    if ((result & 0x8000000000000000) == 0)
    {
      uint64_t v24 = MEMORY[0x1F4188790](result);
      *(&v26 - 8) = a5;
      *(&v26 - 7) = v21;
      uint64_t v25 = v29;
      *(&v26 - 6) = v28;
      *(&v26 - 5) = v25;
      *(&v26 - 4) = v26;
      *((double *)&v26 - 3) = a2;
      *(&v26 - 2) = v27;
      *(&v26 - 1) = v24;
      return (*(uint64_t (**)(void))(v25 + 16))(partial apply for closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:));
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

void closure #1 in closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(const double *a1, int a2, double *a3, double **a4, vDSP_Length __N, double a6)
{
  v6[1] = *(double *)MEMORY[0x1E4F143B8];
  v6[0] = a6;
  if (!a1)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  if (!*a4) {
    goto LABEL_5;
  }
  vDSP_vrampmulD(a1, 1, a3, v6, *a4, 1, __N);
}

uint64_t static vDSP.stereoRamp<A>(withInitialValue:multiplyingBy:_:increment:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.stereoRamp<A>(withInitialValue:multiplyingBy:_:increment:));
  uint64_t result = 0;
  __break(1u);
  return result;
}

{
  uint64_t v5;
  uint64_t result;

  uint64_t v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.stereoRamp<A>(withInitialValue:multiplyingBy:_:increment:));
  uint64_t result = 0;
  __break(1u);
  return result;
}

uint64_t static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(uint64_t a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v52 = a5;
  uint64_t v53 = a8;
  uint64_t v51 = a6;
  uint64_t v43 = a1;
  uint64_t v14 = *(void *)(a7 - 8);
  uint64_t v15 = MEMORY[0x1F4188790](a1);
  uint64_t v48 = (char *)&v41 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v17 = MEMORY[0x1F4188790](v15);
  uint64_t v19 = (char *)&v41 - v18;
  MEMORY[0x1F4188790](v17);
  uint64_t v21 = (char *)&v41 - v20;
  uint64_t v22 = *(void (**)(char *))(v14 + 16);
  uint64_t v49 = v23;
  v22((char *)&v41 - v20);
  uint64_t v44 = a4;
  uint64_t v47 = v22;
  ((void (*)(char *, uint64_t, uint64_t))v22)(v19, a4, a7);
  uint64_t v24 = *(uint64_t (**)(uint64_t, uint64_t))(a9 + 16);
  uint64_t v25 = v24(a7, a9);
  uint64_t v50 = a9;
  uint64_t v46 = v24;
  uint64_t v26 = v24(a7, a9);
  uint64_t v27 = *(void (**)(char *, uint64_t))(v14 + 8);
  v27(v19, a7);
  uint64_t result = ((uint64_t (*)(char *, uint64_t))v27)(v21, a7);
  if (v25 != v26)
  {
    __break(1u);
    goto LABEL_7;
  }
  int v45 = (uint64_t (*)(char *, uint64_t))v27;
  uint64_t v42 = a10;
  uint64_t v29 = *(void *)(a10 + 8);
  uint64_t v30 = *(uint64_t (**)(uint64_t, uint64_t))(v29 + 16);
  uint64_t v31 = v53;
  uint64_t v32 = v30(v53, v29);
  uint64_t result = v30(v31, v29);
  if (v32 != result)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  uint64_t v33 = v48;
  ((void (*)(char *, uint64_t, uint64_t))v47)(v48, v49, a7);
  uint64_t v34 = v46(a7, v50);
  uint64_t v35 = v53;
  uint64_t v36 = v30(v53, v29);
  uint64_t result = v45(v33, a7);
  if (v34 != v36)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v37 = v51;
  uint64_t result = v30(v35, v29);
  if ((result & 0x8000000000000000) == 0)
  {
    uint64_t v38 = MEMORY[0x1F4188790](result);
    *(&v41 - 10) = a7;
    *(&v41 - 9) = v35;
    uint64_t v39 = v42;
    *(&v41 - 8) = v50;
    *(&v41 - 7) = v39;
    uint64_t v40 = v49;
    *(&v41 - 6) = v37;
    *(&v41 - 5) = v40;
    *(&v41 - 4) = v44;
    *((float *)&v41 - 6) = a2;
    *(&v41 - 2) = v43;
    *(&v41 - 1) = v38;
    return (*(uint64_t (**)(void))(v39 + 16))(partial apply for closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:));
  }
LABEL_9:
  __break(1u);
  return result;
}

void closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(const float *__I1, int a2, const float *__I0, int a4, float *__Start, float **a6, float **a7, vDSP_Length __N, float a9)
{
  uint64_t v12 = *MEMORY[0x1E4F143B8];
  float __Step = a9;
  if (!__I0)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (!__I1)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  uint64_t v9 = *a6;
  if (!v9)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  uint64_t v10 = *a7;
  if (!v10) {
    goto LABEL_9;
  }
  vDSP_vrampmul2(__I0, __I1, 1, __Start, &__Step, v9, v10, 1, __N);
}

uint64_t static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(uint64_t a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v52 = a5;
  uint64_t v53 = a8;
  uint64_t v51 = a6;
  uint64_t v43 = a1;
  uint64_t v14 = *(void *)(a7 - 8);
  uint64_t v15 = MEMORY[0x1F4188790](a1);
  uint64_t v48 = (char *)&v41 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v17 = MEMORY[0x1F4188790](v15);
  uint64_t v19 = (char *)&v41 - v18;
  MEMORY[0x1F4188790](v17);
  uint64_t v21 = (char *)&v41 - v20;
  uint64_t v22 = *(void (**)(char *))(v14 + 16);
  uint64_t v49 = v23;
  v22((char *)&v41 - v20);
  uint64_t v44 = a4;
  uint64_t v47 = v22;
  ((void (*)(char *, uint64_t, uint64_t))v22)(v19, a4, a7);
  uint64_t v24 = *(uint64_t (**)(uint64_t, uint64_t))(a9 + 16);
  uint64_t v25 = v24(a7, a9);
  uint64_t v50 = a9;
  uint64_t v46 = v24;
  uint64_t v26 = v24(a7, a9);
  uint64_t v27 = *(void (**)(char *, uint64_t))(v14 + 8);
  v27(v19, a7);
  uint64_t result = ((uint64_t (*)(char *, uint64_t))v27)(v21, a7);
  if (v25 != v26)
  {
    __break(1u);
    goto LABEL_7;
  }
  int v45 = (uint64_t (*)(char *, uint64_t))v27;
  uint64_t v42 = a10;
  uint64_t v29 = *(void *)(a10 + 8);
  uint64_t v30 = *(uint64_t (**)(uint64_t, uint64_t))(v29 + 16);
  uint64_t v31 = v53;
  uint64_t v32 = v30(v53, v29);
  uint64_t result = v30(v31, v29);
  if (v32 != result)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  uint64_t v33 = v48;
  ((void (*)(char *, uint64_t, uint64_t))v47)(v48, v49, a7);
  uint64_t v34 = v46(a7, v50);
  uint64_t v35 = v53;
  uint64_t v36 = v30(v53, v29);
  uint64_t result = v45(v33, a7);
  if (v34 != v36)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v37 = v51;
  uint64_t result = v30(v35, v29);
  if ((result & 0x8000000000000000) == 0)
  {
    uint64_t v38 = MEMORY[0x1F4188790](result);
    *(&v41 - 10) = a7;
    *(&v41 - 9) = v35;
    uint64_t v39 = v42;
    *(&v41 - 8) = v50;
    *(&v41 - 7) = v39;
    uint64_t v40 = v49;
    *(&v41 - 6) = v37;
    *(&v41 - 5) = v40;
    *(&v41 - 4) = v44;
    *((double *)&v41 - 3) = a2;
    *(&v41 - 2) = v43;
    *(&v41 - 1) = v38;
    return (*(uint64_t (**)(void))(v39 + 16))(partial apply for closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:));
  }
LABEL_9:
  __break(1u);
  return result;
}

void closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(const double *__I1, int a2, const double *__I0, int a4, double *__Start, double **a6, double **a7, vDSP_Length __N, double a9)
{
  __Step[1] = *(double *)MEMORY[0x1E4F143B8];
  __Step[0] = a9;
  if (!__I0)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (!__I1)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  uint64_t v9 = *a6;
  if (!v9)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  uint64_t v10 = *a7;
  if (!v10) {
    goto LABEL_9;
  }
  vDSP_vrampmul2D(__I0, __I1, 1, __Start, __Step, v9, v10, 1, __N);
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(v2 + 32);
  uint64_t v4 = *(void *)(v2 + 56);
  uint64_t v5 = *(void *)(v2 + 72);
  long long v8 = *(_OWORD *)(v2 + 16);
  uint64_t v9 = v3;
  long long v10 = *(_OWORD *)(v2 + 40);
  uint64_t v11 = v4;
  uint64_t v12 = v5;
  uint64_t v13 = a1;
  long long v14 = *(_OWORD *)(v2 + 80);
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t, void))(v10 + 24))(a2, &v7, MEMORY[0x1E4FBC848] + 8, v8);
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(values:atIndices:)(uint64_t a1, void *a2, uint64_t *a3, unint64_t *a4, uint64_t (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v9 = v5[2];
  uint64_t v10 = v5[3];
  uint64_t v11 = v5[5];
  uint64_t v18 = v5[4];
  uint64_t v13 = v5[6];
  uint64_t v12 = v5[7];
  uint64_t v17 = v5[8];
  uint64_t v14 = __swift_instantiateConcreteTypeFromMangledName(a3);
  uint64_t v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a4, a3);
  uint64_t result = a5(v13, v12, a1, v9, v10, v14, v18, v11, v15);
  *a2 = v17;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(uint64_t a1)
{
  uint64_t v2 = *(void *)(v1 + 32);
  uint64_t v3 = *(void *)(v1 + 56);
  uint64_t v4 = *(void *)(v1 + 72);
  uint64_t v5 = *(void *)(v1 + 88);
  uint64_t v6 = *(void *)(v1 + 96);
  long long v9 = *(_OWORD *)(v1 + 16);
  uint64_t v10 = v2;
  long long v11 = *(_OWORD *)(v1 + 40);
  uint64_t v12 = v3;
  uint64_t v13 = v4;
  uint64_t v14 = *(void *)(v1 + 80);
  uint64_t v15 = v5;
  uint64_t v16 = a1;
  uint64_t v17 = v6;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t), uint64_t *, uint64_t, void))(v11 + 24))(partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:), &v8, MEMORY[0x1E4FBC848] + 8, v9);
}

{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v8;
  long long v9;
  uint64_t v10;
  long long v11;
  uint64_t v12;
  uint64_t v13;
  long long v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;

  uint64_t v2 = *(void *)(v1 + 32);
  uint64_t v3 = *(void *)(v1 + 56);
  uint64_t v4 = *(void *)(v1 + 72);
  uint64_t v5 = *(void *)(v1 + 96);
  uint64_t v6 = *(void *)(v1 + 104);
  long long v9 = *(_OWORD *)(v1 + 16);
  uint64_t v10 = v2;
  long long v11 = *(_OWORD *)(v1 + 40);
  uint64_t v12 = v3;
  uint64_t v13 = v4;
  uint64_t v14 = *(_OWORD *)(v1 + 80);
  uint64_t v15 = v5;
  uint64_t v16 = a1;
  uint64_t v17 = v6;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t), uint64_t *, uint64_t, void))(v11 + 24))(partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:), &v8, MEMORY[0x1E4FBC848] + 8, v9);
}

uint64_t partial apply for closure #1 in static vDSP.linearInterpolate<A, B>(lookupTable:withOffsets:scale:baseOffset:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.linearInterpolate<A, B>(lookupTable:withOffsets:scale:baseOffset:)(a1, a2, *(void *)(v2 + 48), *(void *)(v2 + 56), *(void *)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), *(float *)(v2 + 64), *(float *)(v2 + 68));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.linearInterpolate<A, B>(lookupTable:withOffsets:scale:baseOffset:)(a1, a2, *(void *)(v2 + 48), *(void *)(v2 + 56), *(void *)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), *(double *)(v2 + 64), *(double *)(v2 + 72));
}

void partial apply for closure #1 in static vDSP.fill<A>(_:with:)(float **a1)
{
  vDSP_Length v2 = *(void *)(v1 + 24);
  float __A = *(float *)(v1 + 16);
  if (*a1) {
    vDSP_vfill(&__A, *a1, 1, v2);
  }
  else {
    __break(1u);
  }
}

void partial apply for closure #1 in static vDSP.fill<A>(_:with:)(double **a1)
{
  vDSP_Length v2 = *(void *)(v1 + 24);
  double __A = *(double *)(v1 + 16);
  if (*a1) {
    vDSP_vfillD(&__A, *a1, 1, v2);
  }
  else {
    __break(1u);
  }
}

uint64_t partial apply for closure #1 in static vDSP.clear<A>(_:)(uint64_t *a1)
{
  return partial apply for closure #1 in static vDSP.clear<A>(_:)(a1, MEMORY[0x1E4F16B10]);
}

{
  return partial apply for closure #1 in static vDSP.clear<A>(_:)(a1, MEMORY[0x1E4F16B18]);
}

uint64_t partial apply for closure #1 in static vDSP.clear<A>(_:)(uint64_t *a1, uint64_t (*a2)(uint64_t, uint64_t, void))
{
  uint64_t result = *a1;
  if (result) {
    return a2(result, 1, *(void *)(v2 + 16));
  }
  __break(1u);
  return result;
}

uint64_t *partial apply for closure #2 in static vDSP.window<A>(ofType:usingSequence:count:isHalfWindow:)(uint64_t *a1, void *a2)
{
  return partial apply for closure #2 in static vDSP.window<A>(ofType:usingSequence:count:isHalfWindow:)(a1, a2, MEMORY[0x1E4F16930], MEMORY[0x1E4F16920], MEMORY[0x1E4F16890]);
}

uint64_t *partial apply for closure #1 in static vDSP.window<A>(ofType:usingSequence:count:isHalfWindow:)(uint64_t *a1, void *a2)
{
  return partial apply for closure #2 in static vDSP.window<A>(ofType:usingSequence:count:isHalfWindow:)(a1, a2, MEMORY[0x1E4F16928], MEMORY[0x1E4F16918], MEMORY[0x1E4F16888]);
}

uint64_t *partial apply for closure #2 in static vDSP.window<A>(ofType:usingSequence:count:isHalfWindow:)(uint64_t *result, void *a2, uint64_t (*a3)(void), uint64_t (*a4)(uint64_t, uint64_t, void), uint64_t (*a5)(uint64_t, uint64_t, void))
{
  uint64_t v8 = result[1];
  if (v8 < 0)
  {
    __break(1u);
  }
  else
  {
    long long v11 = *(char **)(v5 + 16);
    char v12 = *(unsigned char *)(v5 + 24);
    uint64_t v13 = *(void *)(v5 + 32);
    uint64_t result = (uint64_t *)closure #1 in static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(result, *v11, v8, v12, a3, a4, a5);
    *a2 = v13;
  }
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(uint64_t *a1)
{
  return closure #1 in static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(a1, *(unsigned char *)(v1 + 16), *(void *)(v1 + 24), *(unsigned char *)(v1 + 32), MEMORY[0x1E4F16928], MEMORY[0x1E4F16918], MEMORY[0x1E4F16888]);
}

{
  uint64_t v1;

  return closure #1 in static vDSP.formWindow<A>(usingSequence:result:isHalfWindow:)(a1, *(unsigned char *)(v1 + 16), *(void *)(v1 + 24), *(unsigned char *)(v1 + 32), MEMORY[0x1E4F16930], MEMORY[0x1E4F16920], MEMORY[0x1E4F16890]);
}

uint64_t partial apply for closure #1 in static vDSP.ramp<A>(withInitialValue:multiplyingBy:increment:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.ramp<A>(withInitialValue:multiplyingBy:increment:)(a1, a2, *(void *)(v2 + 32), *(void *)(v2 + 40), *(void *)(v2 + 16), *(void *)(v2 + 24), *(float *)(v2 + 48));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.ramp<A>(withInitialValue:multiplyingBy:increment:)(a1, a2, *(void *)(v2 + 32), *(void *)(v2 + 40), *(void *)(v2 + 16), *(void *)(v2 + 24), *(double *)(v2 + 48));
}

uint64_t partial apply for closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(uint64_t a1)
{
  uint64_t v2 = *(void *)(v1 + 16);
  uint64_t v3 = *(void *)(v1 + 32);
  uint64_t v4 = *(void *)(v1 + 64);
  void v6[4] = *(_DWORD *)(v1 + 56);
  uint64_t v7 = v4;
  uint64_t v8 = a1;
  return (*(uint64_t (**)(void (*)(const float *, int), _DWORD *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:), v6, MEMORY[0x1E4FBC848] + 8, v2);
}

{
  void *v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  void v6[5];

  uint64_t v2 = v1[2];
  uint64_t v3 = v1[4];
  uint64_t v4 = v1[8];
  double v6[2] = v1[7];
  void v6[3] = v4;
  void v6[4] = a1;
  return (*(uint64_t (**)(void (*)(const double *, int), void *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:), v6, MEMORY[0x1E4FBC848] + 8, v2);
}

uint64_t partial apply for closure #1 in static vDSP.stereoRamp<A>(withInitialValue:multiplyingBy:_:increment:)(uint64_t a1, uint64_t *a2)
{
  uint64_t v4 = *(uint64_t **)(v2 + 32);
  uint64_t v6 = *(void *)(v2 + 40);
  *uint64_t v4 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in closure #1 in static vDSP.stereoRamp<A>(withInitialValue:multiplyingBy:_:increment:));
  uint64_t result = swift_bridgeObjectRelease();
  *a2 = v6;
  return result;
}

{
  uint64_t v2;
  uint64_t *v4;
  uint64_t result;
  uint64_t v6;

  uint64_t v4 = *(uint64_t **)(v2 + 32);
  uint64_t v6 = *(void *)(v2 + 40);
  *uint64_t v4 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in closure #1 in static vDSP.stereoRamp<A>(withInitialValue:multiplyingBy:_:increment:));
  uint64_t result = swift_bridgeObjectRelease();
  *a2 = v6;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(uint64_t a1)
{
  uint64_t v2 = *(void *)(v1 + 40);
  int v3 = *(_DWORD *)(v1 + 72);
  uint64_t v4 = *(void *)(v1 + 80);
  double v6[2] = *(void *)(v1 + 16);
  long long v7 = *(_OWORD *)(v1 + 24);
  uint64_t v8 = v2;
  long long v9 = *(_OWORD *)(v1 + 56);
  int v10 = v3;
  uint64_t v11 = v4;
  uint64_t v12 = a1;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t), void *, uint64_t, void))(v2 + 16))(partial apply for closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:), v6, MEMORY[0x1E4FBC848] + 8, v7);
}

{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  void v6[3];
  long long v7;
  uint64_t v8;
  long long v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;

  uint64_t v2 = *(void *)(v1 + 40);
  int v3 = *(void *)(v1 + 72);
  uint64_t v4 = *(void *)(v1 + 80);
  double v6[2] = *(void *)(v1 + 16);
  long long v7 = *(_OWORD *)(v1 + 24);
  uint64_t v8 = v2;
  long long v9 = *(_OWORD *)(v1 + 56);
  int v10 = v3;
  uint64_t v11 = v4;
  uint64_t v12 = a1;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t), void *, uint64_t, void))(v2 + 16))(partial apply for closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:), v6, MEMORY[0x1E4FBC848] + 8, v7);
}

unint64_t lazy protocol witness table accessor for type vDSP.WindowSequence and conformance vDSP.WindowSequence()
{
  unint64_t result = lazy protocol witness table cache variable for type vDSP.WindowSequence and conformance vDSP.WindowSequence;
  if (!lazy protocol witness table cache variable for type vDSP.WindowSequence and conformance vDSP.WindowSequence)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.WindowSequence and conformance vDSP.WindowSequence);
  }
  return result;
}

unsigned char *storeEnumTagSinglePayload for vDSP.WindowSequence(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 3 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 3) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFD) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFC)
  {
    unsigned int v6 = ((a2 - 253) >> 8) + 1;
    *unint64_t result = a2 + 3;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D212E7E4);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 3;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vDSP.WindowSequence()
{
  return &type metadata for vDSP.WindowSequence;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(uint64_t a1)
{
  uint64_t v2 = *(void *)(v1 + 56);
  uint64_t v3 = *(void *)(v1 + 64);
  long long v4 = *(_OWORD *)(v1 + 32);
  long long v7 = *(_OWORD *)(v1 + 16);
  long long v8 = v4;
  uint64_t v9 = v2;
  uint64_t v10 = v3;
  long long v11 = *(_OWORD *)(v1 + 72);
  uint64_t v12 = a1;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t), uint64_t *, uint64_t, void))(v4 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:), &v6, MEMORY[0x1E4FBC848] + 8, v7);
}

{
  uint64_t v1;
  uint64_t v2;
  int v3;
  long long v4;
  uint64_t v6;
  long long v7;
  long long v8;
  uint64_t v9;
  int v10;
  long long v11;
  uint64_t v12;

  uint64_t v2 = *(void *)(v1 + 56);
  uint64_t v3 = *(_DWORD *)(v1 + 64);
  long long v4 = *(_OWORD *)(v1 + 32);
  long long v7 = *(_OWORD *)(v1 + 16);
  long long v8 = v4;
  uint64_t v9 = v2;
  uint64_t v10 = v3;
  long long v11 = *(_OWORD *)(v1 + 72);
  uint64_t v12 = a1;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t), uint64_t *, uint64_t, void))(v4 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:), &v6, MEMORY[0x1E4FBC848] + 8, v7);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(v2 + 16);
  uint64_t v4 = *(void *)(v2 + 32);
  uint64_t v5 = *(void *)(v2 + 80);
  uint64_t v6 = *(void *)(v2 + 88);
  void v8[2] = *(void *)(v2 + 56);
  void v8[3] = a1;
  v8[4] = a2;
  long long v9 = *(_OWORD *)(v2 + 64);
  uint64_t v10 = v5;
  uint64_t v11 = v6;
  return (*(uint64_t (**)(void (*)(const double *, int), void *, uint64_t, uint64_t))(v4 + 24))(partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:), v8, MEMORY[0x1E4FBC848] + 8, v3);
}

{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  _DWORD v8[6];
  uint64_t v9;
  uint64_t v10;
  long long v11;
  uint64_t v12;
  uint64_t v13;

  uint64_t v3 = *(void *)(v2 + 16);
  uint64_t v4 = *(void *)(v2 + 32);
  uint64_t v5 = *(void *)(v2 + 80);
  uint64_t v6 = *(void *)(v2 + 88);
  v8[4] = *(_DWORD *)(v2 + 56);
  long long v9 = a1;
  uint64_t v10 = a2;
  uint64_t v11 = *(_OWORD *)(v2 + 64);
  uint64_t v12 = v5;
  uint64_t v13 = v6;
  return (*(uint64_t (**)(void (*)(const float *, int), _DWORD *, uint64_t, uint64_t))(v4 + 24))(partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:), v8, MEMORY[0x1E4FBC848] + 8, v3);
}

void partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(const double *a1, int a2)
{
  closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(a1, a2, *(const double **)(v2 + 24), *(void *)(v2 + 32), *(double **)(v2 + 40), *(double ***)(v2 + 48), *(double ***)(v2 + 56), *(void *)(v2 + 64), *(double *)(v2 + 16));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.stereoRamp<A>(withInitialValue:multiplyingBy:_:increment:)(uint64_t a1, void *a2)
{
  uint64_t v4 = *((void *)v2 + 2);
  uint64_t v14 = *((void *)v2 + 3);
  uint64_t v5 = *((void *)v2 + 4);
  uint64_t v6 = *((void *)v2 + 5);
  uint64_t v7 = *((void *)v2 + 6);
  double v8 = v2[7];
  uint64_t v9 = *((void *)v2 + 8);
  uint64_t v10 = *((void *)v2 + 9);
  uint64_t v11 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v12 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t result = static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(v5, v8, v6, v7, a1, v9, v4, v11, v14, v12);
  *a2 = v10;
  return result;
}

{
  uint64_t v2;
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  float v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t result;
  uint64_t v14;

  uint64_t v4 = *(void *)(v2 + 16);
  uint64_t v14 = *(void *)(v2 + 24);
  uint64_t v5 = *(void *)(v2 + 32);
  uint64_t v6 = *(void *)(v2 + 40);
  uint64_t v7 = *(void *)(v2 + 48);
  double v8 = *(float *)(v2 + 56);
  uint64_t v9 = *(void *)(v2 + 64);
  uint64_t v10 = *(void *)(v2 + 72);
  uint64_t v11 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v12 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t result = static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(v5, v8, v6, v7, a1, v9, v4, v11, v14, v12);
  *a2 = v10;
  return result;
}

void partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(const float *a1, int a2)
{
  closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.formStereoRamp<A, B>(withInitialValue:multiplyingBy:_:increment:results:_:)(a1, a2, *(const float **)(v2 + 24), *(void *)(v2 + 32), *(float **)(v2 + 40), *(float ***)(v2 + 48), *(float ***)(v2 + 56), *(void *)(v2 + 64), *(float *)(v2 + 16));
}

void partial apply for closure #1 in closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(const double *a1, int a2)
{
  closure #1 in closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(a1, a2, *(double **)(v2 + 24), *(double ***)(v2 + 32), *(void *)(v2 + 40), *(double *)(v2 + 16));
}

void partial apply for closure #1 in closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(const float *a1, int a2)
{
  closure #1 in closure #1 in static vDSP.formRamp<A, B>(withInitialValue:multiplyingBy:increment:result:)(a1, a2, *(float **)(v2 + 24), *(float ***)(v2 + 32), *(void *)(v2 + 40), *(float *)(v2 + 16));
}

void *partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:)(void *a1)
{
  return partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:)(a1, MEMORY[0x1E4F16DA0]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:)(a1, MEMORY[0x1E4F16D98]);
}

void *partial apply for closure #1 in closure #1 in static vDSP.formRamp<A>(from:through:result:)(void *result, uint64_t (*a2)(void, void, void, uint64_t, void))
{
  if (*result) {
    return (void *)a2(v2[2], v2[3], *result, 1, v2[4]);
  }
  __break(1u);
  return result;
}

void *partial apply for closure #1 in closure #1 in closure #1 in static vDSP.formRamp<A>(withInitialValue:increment:result:)(void *a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.formRamp<A>(withInitialValue:increment:result:)(a1, MEMORY[0x1E4F16DA0]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.formRamp<A>(withInitialValue:increment:result:)(a1, MEMORY[0x1E4F16D98]);
}

void *partial apply for closure #1 in closure #1 in closure #1 in static vDSP.formRamp<A>(withInitialValue:increment:result:)(void *result, uint64_t (*a2)(void, void, void, uint64_t, void))
{
  if (*result) {
    return (void *)a2(v2[2], v2[3], *result, 1, v2[4]);
  }
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(v2 + 24);
  uint64_t v4 = *(void *)(v2 + 48);
  long long v5 = *(_OWORD *)(v2 + 88);
  uint64_t v6 = *(void *)(v2 + 104);
  v8[1] = *(_OWORD *)(v2 + 72);
  uint64_t v9 = a1;
  uint64_t v10 = a2;
  long long v11 = v5;
  uint64_t v12 = v6;
  return (*(uint64_t (**)(void (*)(const double *, int), _OWORD *, uint64_t, uint64_t))(v4 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:), v8, MEMORY[0x1E4FBC848] + 8, v3);
}

{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  long long v5;
  void v7[5];
  long long v8;

  uint64_t v3 = *(void *)(v2 + 24);
  uint64_t v4 = *(void *)(v2 + 48);
  long long v5 = *(_OWORD *)(v2 + 80);
  _OWORD v7[2] = *(void *)(v2 + 72);
  void v7[3] = a1;
  void v7[4] = a2;
  double v8 = v5;
  return (*(uint64_t (**)(void (*)(const float *, int), void *, uint64_t, uint64_t))(v4 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:), v7, MEMORY[0x1E4FBC848] + 8, v3);
}

void partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(const double *a1, int a2)
{
  closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(a1, a2, *(const double **)(v2 + 32), *(void *)(v2 + 40), *(void *)(v2 + 48), *(double ***)(v2 + 56), *(void *)(v2 + 64), *(double *)(v2 + 16), *(double *)(v2 + 24));
}

void partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(const float *a1, int a2)
{
  closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(lookupTable:withOffsets:scale:baseOffset:result:)(a1, a2, *(const float **)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), *(float ***)(v2 + 48), *(void *)(v2 + 56), *(float *)(v2 + 16), *(float *)(v2 + 20));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, a2, MEMORY[0x1E4F16CA0]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, a2, MEMORY[0x1E4F16C98]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 24);
  uint64_t v5 = *(void *)(v3 + 48);
  uint64_t v6 = *(void *)(v3 + 72);
  _OWORD v8[2] = a1;
  void v8[3] = a2;
  v8[4] = v6;
  long long v9 = *(_OWORD *)(v3 + 80);
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E4FBC848] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  return closure #1 in closure #1 in closure #1 in static vDSP.linearInterpolate<A, B, C>(values:atIndices:result:)(a1, a2, *(void *)(v3 + 16), *(void *)(v3 + 24), *(uint64_t **)(v3 + 32), *(void *)(v3 + 40), *(void *)(v3 + 48), a3);
}

uint64_t vImage.BufferType.init(bufferTypeCode:model:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X1>, char *a3@<X8>)
{
  char v3 = 0;
  switch(result)
  {
    case 1:
      BOOL v4 = (a2 & 0x1FFFFFFF8) == 0;
      char v5 = 17;
      char v6 = 8 * a2;
      unint64_t v7 = 0x141111110E031110;
      goto LABEL_5;
    case 2:
      BOOL v4 = (a2 & 0x1FFFFFFF8) == 0;
      char v5 = 18;
      char v6 = 8 * a2;
      unint64_t v7 = 0x151212120C041212;
      goto LABEL_5;
    case 3:
      BOOL v4 = (a2 & 0x1FFFFFFF8) == 0;
      char v5 = 19;
      char v6 = 8 * a2;
      unint64_t v7 = 0x161313130D051313;
LABEL_5:
      unint64_t v8 = v7 >> v6;
      if (v4) {
        char v5 = v8;
      }
      *a3 = v5;
      return result;
    case 4:
      *a3 = 2;
      return result;
    case 17:
      goto LABEL_10;
    case 18:
      *a3 = 11;
      return result;
    case 19:
      *a3 = 6;
      return result;
    case 20:
      *a3 = 15;
      return result;
    case 21:
      *a3 = 9;
      return result;
    case 22:
      *a3 = 7;
      return result;
    case 23:
      *a3 = 8;
      return result;
    case 24:
      *a3 = 1;
      return result;
    case 25:
      *a3 = 10;
      return result;
    default:
      char v3 = 23;
LABEL_10:
      *a3 = v3;
      return result;
  }
}

double static vImage_ARGBToYpCbCrMatrix.itu_R_601_4.getter@<D0>(_OWORD *a1@<X8>)
{
  double result = 0.0000849609638;
  *a1 = xmmword_1D213A160;
  a1[1] = xmmword_1D213A170;
  return result;
}

double static vImage_ARGBToYpCbCrMatrix.itu_R_709_2.getter@<D0>(_OWORD *a1@<X8>)
{
  double result = 0.000352343834;
  *a1 = xmmword_1D213A180;
  a1[1] = xmmword_1D213A190;
  return result;
}

uint64_t static vImageCVImageFormatRef.make(format:matrix:chromaSiting:colorSpace:alphaIsOpaqueHint:)(char *a1, _OWORD *a2, char *a3, uint64_t a4, char a5)
{
  uint64_t v10 = *MEMORY[0x1E4F143B8];
  uint64_t v5 = *a1;
  uint64_t v6 = *a3;
  long long v7 = a2[1];
  v9[0] = *a2;
  v9[1] = v7;
  return MEMORY[0x1D2600F60](dword_1D213A4BC[v5], v9, **((void **)&unk_1E690DB78 + v6), a4, a5 & 1);
}

uint64_t static vImageCVImageFormatRef.make(buffer:)()
{
  uint64_t result = MEMORY[0x1D2600F70]();
  if (!result) {
    __break(1u);
  }
  return result;
}

BOOL vImageCVImageFormatRef.alphaIsOpaqueHint.getter()
{
  return MEMORY[0x1D2600F80](v0) != 0;
}

uint64_t key path setter for vImageCVImageFormatRef.alphaIsOpaqueHint : vImageCVImageFormatRef(unsigned __int8 *a1, uint64_t *a2)
{
  int v2 = *a1;
  uint64_t v3 = *a2;
  if (v2) {
    uint64_t v4 = 2;
  }
  else {
    uint64_t v4 = 0;
  }
  uint64_t result = MEMORY[0x1D2600FF0](v3, v4);
  if (result)
  {
    uint64_t result = _assertionFailure(_:_:file:line:flags:)();
    __break(1u);
  }
  return result;
}

uint64_t vImageCVImageFormatRef.alphaIsOpaqueHint.setter(char a1)
{
  if (a1) {
    uint64_t v2 = 2;
  }
  else {
    uint64_t v2 = 0;
  }
  uint64_t result = MEMORY[0x1D2600FF0](v1, v2);
  if (result)
  {
    uint64_t result = _assertionFailure(_:_:file:line:flags:)();
    __break(1u);
  }
  return result;
}

uint64_t (*vImageCVImageFormatRef.alphaIsOpaqueHint.modify(uint64_t a1))(uint64_t a1)
{
  *(void *)a1 = v1;
  *(unsigned char *)(a1 + 8) = MEMORY[0x1D2600F80](v1) != 0;
  return vImageCVImageFormatRef.alphaIsOpaqueHint.modify;
}

uint64_t vImageCVImageFormatRef.alphaIsOpaqueHint.modify(uint64_t a1)
{
  if (*(unsigned char *)(a1 + 8)) {
    uint64_t v1 = 2;
  }
  else {
    uint64_t v1 = 0;
  }
  uint64_t result = MEMORY[0x1D2600FF0](*(void *)a1, v1);
  if (result)
  {
    uint64_t result = _assertionFailure(_:_:file:line:flags:)();
    __break(1u);
  }
  return result;
}

void vImageCVImageFormatRef.channelCount.getter()
{
}

char *vImageCVImageFormatRef.channels.getter()
{
  uint64_t v1 = (const void *)MEMORY[0x1D2600FB0](v0);
  unsigned int v2 = MEMORY[0x1D2600F90](v0);
  uint64_t v3 = specialized _copyCollectionToContiguousArray<A>(_:)(v1, v2);
  id v4 = v0;
  uint64_t v5 = specialized Sequence.compactMap<A>(_:)((uint64_t)v3, (uint64_t)v4);
  swift_release();

  return v5;
}

void vImageCVImageFormatRef.colorSpace.getter()
{
}

long long *vImageCVImageFormatRef.channelDescription(bufferType:)@<X0>(uint64_t a1@<X8>)
{
  uint64_t v3 = vImage.BufferType.bufferTypeCode.getter();
  uint64_t result = (long long *)MEMORY[0x1D2600FA0](v1, v3);
  if (result)
  {
    long long v5 = *result;
    long long v6 = result[1];
  }
  else
  {
    long long v5 = 0uLL;
    long long v6 = 0uLL;
  }
  *(_OWORD *)a1 = v5;
  *(_OWORD *)(a1 + 16) = v6;
  *(unsigned char *)(a1 + 32) = result == 0;
  return result;
}

uint64_t vImage.BufferType.bufferTypeCode.getter()
{
  return dword_1D213A5BC[*v0];
}

void vImageCVImageFormatRef.chromaSiting.getter(char *a1@<X8>)
{
  uint64_t v3 = (void *)MEMORY[0x1D2600FC0](v1);

  vImageCVImageFormatRef.ChromaSiting.init(location:)(v3, a1);
}

void vImageCVImageFormatRef.chromaSiting.setter(unsigned char *a1)
{
  unsigned int v2 = (id *)MEMORY[0x1E4F24A38];
  id v3 = 0;
  switch(*a1)
  {
    case 1:
      unsigned int v2 = (id *)MEMORY[0x1E4F24A28];
      goto LABEL_8;
    case 2:
      unsigned int v2 = (id *)MEMORY[0x1E4F24A48];
      goto LABEL_8;
    case 3:
      unsigned int v2 = (id *)MEMORY[0x1E4F24A40];
      goto LABEL_8;
    case 4:
      unsigned int v2 = (id *)MEMORY[0x1E4F24A20];
      goto LABEL_8;
    case 5:
      unsigned int v2 = (id *)MEMORY[0x1E4F24A18];
      goto LABEL_8;
    case 6:
      unsigned int v2 = (id *)MEMORY[0x1E4F24A30];
      goto LABEL_8;
    case 7:
      goto LABEL_9;
    default:
LABEL_8:
      id v3 = *v2;
LABEL_9:
      uint64_t v4 = MEMORY[0x1D2601000](v1, v3);

      if (v4)
      {
        _assertionFailure(_:_:file:line:flags:)();
        __break(1u);
        JUMPOUT(0x1D212F908);
      }
      return;
  }
}

void vImageCVImageFormatRef.ChromaSiting.init(location:)(void *a1@<X0>, char *a2@<X8>)
{
  if (!a1) {
    goto LABEL_16;
  }
  uint64_t v4 = (void *)*MEMORY[0x1E4F24A38];
  type metadata accessor for CFStringRef(0);
  lazy protocol witness table accessor for type CFStringRef and conformance CFStringRef();
  id v5 = v4;
  id v6 = a1;
  char v7 = static _CFObject.== infix(_:_:)();

  if ((v7 & 1) == 0)
  {
    long long v9 = (void *)*MEMORY[0x1E4F24A28];
    id v10 = v6;
    id v11 = v9;
    char v12 = static _CFObject.== infix(_:_:)();

    if (v12)
    {

      char v8 = 1;
      goto LABEL_17;
    }
    uint64_t v13 = (void *)*MEMORY[0x1E4F24A48];
    id v14 = v10;
    id v15 = v13;
    char v16 = static _CFObject.== infix(_:_:)();

    if (v16)
    {

      char v8 = 2;
      goto LABEL_17;
    }
    uint64_t v17 = (void *)*MEMORY[0x1E4F24A40];
    id v18 = v14;
    id v19 = v17;
    char v20 = static _CFObject.== infix(_:_:)();

    if (v20)
    {

      char v8 = 3;
      goto LABEL_17;
    }
    uint64_t v21 = (void *)*MEMORY[0x1E4F24A20];
    id v22 = v18;
    id v23 = v21;
    char v24 = static _CFObject.== infix(_:_:)();

    if (v24)
    {

      char v8 = 4;
      goto LABEL_17;
    }
    uint64_t v25 = (void *)*MEMORY[0x1E4F24A18];
    id v26 = v22;
    id v27 = v25;
    char v28 = static _CFObject.== infix(_:_:)();

    if (v28)
    {

      char v8 = 5;
      goto LABEL_17;
    }
    id v29 = (id)*MEMORY[0x1E4F24A30];
    id v30 = v26;
    id v31 = v29;
    char v32 = static _CFObject.== infix(_:_:)();

    if (v32)
    {
      char v8 = 6;
      goto LABEL_17;
    }
LABEL_16:
    char v8 = 7;
    goto LABEL_17;
  }

  char v8 = 0;
LABEL_17:
  *a2 = v8;
}

void (*vImageCVImageFormatRef.chromaSiting.modify(void *a1))(uint64_t a1, char a2)
{
  *a1 = v1;
  unsigned int v2 = (char *)(a1 + 1);
  id v3 = (void *)MEMORY[0x1D2600FC0](v1);
  vImageCVImageFormatRef.ChromaSiting.init(location:)(v3, v2);
  return vImageCVImageFormatRef.chromaSiting.modify;
}

void vImageCVImageFormatRef.chromaSiting.modify(uint64_t a1, char a2)
{
  unsigned int v2 = (char *)(a1 + 8);
  if (a2)
  {
    char v3 = *v2;
    unsigned int v2 = &v3;
  }
  vImageCVImageFormatRef.chromaSiting.setter(v2);
}

uint64_t key path setter for vImageCVImageFormatRef.colorSpace : vImageCVImageFormatRef(void *a1, void *a2)
{
  uint64_t result = MEMORY[0x1D2601010](*a2, *a1);
  if (result)
  {
    uint64_t result = _assertionFailure(_:_:file:line:flags:)();
    __break(1u);
  }
  return result;
}

void vImageCVImageFormatRef.colorSpace.setter(void *a1)
{
  if (MEMORY[0x1D2601010](v1))
  {
    _assertionFailure(_:_:file:line:flags:)();
    __break(1u);
  }
  else
  {
  }
}

void (*vImageCVImageFormatRef.colorSpace.modify(void *a1))(uint64_t a1)
{
  a1[1] = v1;
  *a1 = MEMORY[0x1D2600FD0](v1);
  return vImageCVImageFormatRef.colorSpace.modify;
}

void vImageCVImageFormatRef.colorSpace.modify(uint64_t a1)
{
  id v1 = *(id *)a1;
  if (MEMORY[0x1D2601010](*(void *)(a1 + 8)))
  {
    _assertionFailure(_:_:file:line:flags:)();
    __break(1u);
  }
  else
  {
  }
}

void vImageCVImageFormatRef.formatCode.getter()
{
}

void vImage.BufferType.init(rawValue:)()
{
}

uint64_t vImage.BufferType.rawValue.getter()
{
  return *v0;
}

void protocol witness for RawRepresentable.init(rawValue:) in conformance vImage.BufferType()
{
}

void protocol witness for RawRepresentable.rawValue.getter in conformance vImage.BufferType(void *a1@<X8>)
{
  *a1 = *v1;
}

BOOL static vImageCVImageFormatRef.Format.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void vImageCVImageFormatRef.Format.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

char *specialized Sequence.compactMap<A>(_:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2 = *(void *)(a1 + 16);
  if (!v2) {
    return (char *)MEMORY[0x1E4FBC860];
  }
  swift_bridgeObjectRetain();
  uint64_t v5 = 0;
  id v6 = (char *)MEMORY[0x1E4FBC860];
  do
  {
    uint64_t v7 = *(unsigned int *)(a1 + 4 * v5 + 32);
    char v8 = (CGColorSpace *)MEMORY[0x1D2600FD0](a2);
    long long v9 = v8;
    if (v8)
    {
      CGColorSpaceModel Model = CGColorSpaceGetModel(v8);
    }
    else
    {
      CGColorSpaceModel Model = kCGColorSpaceModelMonochrome;
    }
    vImage.BufferType.init(bufferTypeCode:model:)(v7, Model | ((unint64_t)(v9 == 0) << 32), &v15);
    char v11 = v15;
    if (v15 != 23)
    {
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
        id v6 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v6 + 2) + 1, 1, v6);
      }
      unint64_t v13 = *((void *)v6 + 2);
      unint64_t v12 = *((void *)v6 + 3);
      if (v13 >= v12 >> 1) {
        id v6 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v12 > 1), v13 + 1, 1, v6);
      }
      *((void *)v6 + 2) = v13 + 1;
      v6[v13 + 32] = v11;
    }
    ++v5;
  }
  while (v2 != v5);
  swift_bridgeObjectRelease();
  return v6;
}

unint64_t lazy protocol witness table accessor for type vImage.BufferType and conformance vImage.BufferType()
{
  unint64_t result = lazy protocol witness table cache variable for type vImage.BufferType and conformance vImage.BufferType;
  if (!lazy protocol witness table cache variable for type vImage.BufferType and conformance vImage.BufferType)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.BufferType and conformance vImage.BufferType);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type vImageCVImageFormatRef.Format and conformance vImageCVImageFormatRef.Format()
{
  unint64_t result = lazy protocol witness table cache variable for type vImageCVImageFormatRef.Format and conformance vImageCVImageFormatRef.Format;
  if (!lazy protocol witness table cache variable for type vImageCVImageFormatRef.Format and conformance vImageCVImageFormatRef.Format)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImageCVImageFormatRef.Format and conformance vImageCVImageFormatRef.Format);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type vImageCVImageFormatRef.ChromaSiting and conformance vImageCVImageFormatRef.ChromaSiting()
{
  unint64_t result = lazy protocol witness table cache variable for type vImageCVImageFormatRef.ChromaSiting and conformance vImageCVImageFormatRef.ChromaSiting;
  if (!lazy protocol witness table cache variable for type vImageCVImageFormatRef.ChromaSiting and conformance vImageCVImageFormatRef.ChromaSiting)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImageCVImageFormatRef.ChromaSiting and conformance vImageCVImageFormatRef.ChromaSiting);
  }
  return result;
}

uint64_t sub_1D2130188@<X0>(void *a1@<X0>, BOOL *a2@<X8>)
{
  uint64_t result = MEMORY[0x1D2600F80](*a1);
  *a2 = result != 0;
  return result;
}

void sub_1D21301C0(void *a1@<X0>, unsigned char *a2@<X8>)
{
  char v3 = (void *)MEMORY[0x1D2600FC0](*a1);
  vImageCVImageFormatRef.ChromaSiting.init(location:)(v3, &v4);
  *a2 = v4;
}

void sub_1D2130200(char *a1)
{
  char v1 = *a1;
  vImageCVImageFormatRef.chromaSiting.setter(&v1);
}

uint64_t sub_1D2130238@<X0>(void *a1@<X0>, uint64_t *a2@<X8>)
{
  uint64_t result = MEMORY[0x1D2600FD0](*a1);
  *a2 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for vImage.BufferType(unsigned __int8 *a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0xEA) {
    goto LABEL_17;
  }
  if (a2 + 22 >= 0xFFFF00) {
    int v2 = 4;
  }
  else {
    int v2 = 2;
  }
  if ((a2 + 22) >> 8 < 0xFF) {
    int v3 = 1;
  }
  else {
    int v3 = v2;
  }
  if (v3 == 4)
  {
    int v4 = *(_DWORD *)(a1 + 1);
    if (v4) {
      return (*a1 | (v4 << 8)) - 22;
    }
  }
  else
  {
    if (v3 == 2)
    {
      int v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1)) {
        goto LABEL_17;
      }
      return (*a1 | (v4 << 8)) - 22;
    }
    int v4 = a1[1];
    if (a1[1]) {
      return (*a1 | (v4 << 8)) - 22;
    }
  }
LABEL_17:
  unsigned int v6 = *a1;
  BOOL v7 = v6 >= 0x17;
  int v8 = v6 - 23;
  if (!v7) {
    int v8 = -1;
  }
  return (v8 + 1);
}

unsigned char *storeEnumTagSinglePayload for vImage.BufferType(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 22 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 22) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xEA) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xE9)
  {
    unsigned int v6 = ((a2 - 234) >> 8) + 1;
    *uint64_t result = a2 + 22;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D21303C4);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 22;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vImage.BufferType()
{
  return &type metadata for vImage.BufferType;
}

uint64_t getEnumTagSinglePayload for vImageCVImageFormatRef.Format(unsigned __int8 *a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0xC1) {
    goto LABEL_17;
  }
  if (a2 + 63 >= 0xFFFF00) {
    int v2 = 4;
  }
  else {
    int v2 = 2;
  }
  if ((a2 + 63) >> 8 < 0xFF) {
    int v3 = 1;
  }
  else {
    int v3 = v2;
  }
  if (v3 == 4)
  {
    int v4 = *(_DWORD *)(a1 + 1);
    if (v4) {
      return (*a1 | (v4 << 8)) - 63;
    }
  }
  else
  {
    if (v3 == 2)
    {
      int v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1)) {
        goto LABEL_17;
      }
      return (*a1 | (v4 << 8)) - 63;
    }
    int v4 = a1[1];
    if (a1[1]) {
      return (*a1 | (v4 << 8)) - 63;
    }
  }
LABEL_17:
  unsigned int v6 = *a1;
  BOOL v7 = v6 >= 0x40;
  int v8 = v6 - 64;
  if (!v7) {
    int v8 = -1;
  }
  return (v8 + 1);
}

unsigned char *storeEnumTagSinglePayload for vImageCVImageFormatRef.Format(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 63 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 63) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xC1) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xC0)
  {
    unsigned int v6 = ((a2 - 193) >> 8) + 1;
    *uint64_t result = a2 + 63;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D2130558);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 63;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vImageCVImageFormatRef.Format()
{
  return &type metadata for vImageCVImageFormatRef.Format;
}

uint64_t getEnumTagSinglePayload for vImageCVImageFormatRef.ChromaSiting(unsigned __int8 *a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0xFA) {
    goto LABEL_17;
  }
  if (a2 + 6 >= 0xFFFF00) {
    int v2 = 4;
  }
  else {
    int v2 = 2;
  }
  if ((a2 + 6) >> 8 < 0xFF) {
    int v3 = 1;
  }
  else {
    int v3 = v2;
  }
  if (v3 == 4)
  {
    int v4 = *(_DWORD *)(a1 + 1);
    if (v4) {
      return (*a1 | (v4 << 8)) - 6;
    }
  }
  else
  {
    if (v3 == 2)
    {
      int v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1)) {
        goto LABEL_17;
      }
      return (*a1 | (v4 << 8)) - 6;
    }
    int v4 = a1[1];
    if (a1[1]) {
      return (*a1 | (v4 << 8)) - 6;
    }
  }
LABEL_17:
  unsigned int v6 = *a1;
  BOOL v7 = v6 >= 7;
  int v8 = v6 - 7;
  if (!v7) {
    int v8 = -1;
  }
  return (v8 + 1);
}

unsigned char *storeEnumTagSinglePayload for vImageCVImageFormatRef.ChromaSiting(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 6 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 6) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFA) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xF9)
  {
    unsigned int v6 = ((a2 - 250) >> 8) + 1;
    *uint64_t result = a2 + 6;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D21306ECLL);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 6;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vImageCVImageFormatRef.ChromaSiting()
{
  return &type metadata for vImageCVImageFormatRef.ChromaSiting;
}

unint64_t lazy protocol witness table accessor for type CFStringRef and conformance CFStringRef()
{
  unint64_t result = lazy protocol witness table cache variable for type CFStringRef and conformance CFStringRef;
  if (!lazy protocol witness table cache variable for type CFStringRef and conformance CFStringRef)
  {
    type metadata accessor for CFStringRef(255);
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type CFStringRef and conformance CFStringRef);
  }
  return result;
}

uint64_t BNNS.LossLayer.__allocating_init(input:output:lossFunction:lossReduction:filterParameters:)(_OWORD *a1, long long *a2, long long *a3, unsigned __int8 *a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v32 = *MEMORY[0x1E4F143B8];
  outlined init with take of BNNS.LossFunction(a3, v31);
  long long v25 = a3[1];
  long long v26 = *a3;
  long long v23 = a3[3];
  long long v24 = a3[2];
  long long v22 = a3[4];
  unsigned __int8 v16 = *a4;
  if (a7 == 1)
  {
    uint64_t v17 = 0;
  }
  else
  {
    int v27 = a5;
    uint64_t v28 = a6;
    uint64_t v29 = a7;
    uint64_t v30 = a8;
    uint64_t v17 = &v27;
  }
  uint64_t v18 = closure #1 in BNNS.LossLayer.init(input:output:lossFunction:lossReduction:filterParameters:)((uint64_t)v17, v31, a1, a2, v16);
  type metadata accessor for BNNS.LossLayer();
  uint64_t v19 = swift_allocObject();
  uint64_t v20 = v19;
  *(void *)(v19 + 24) = 0xFFFFFFFF00000000;
  *(_OWORD *)(v19 + 32) = 0u;
  *(_OWORD *)(v19 + 48) = 0u;
  *(_OWORD *)(v19 + 64) = 0u;
  *(_OWORD *)(v19 + 80) = 0u;
  *(void *)(v19 + 96) = 0;
  if (v18)
  {
    *(void *)(v19 + 16) = v18;
    *(_OWORD *)(v19 + 24) = v26;
    *(_OWORD *)(v19 + 40) = v25;
    *(_OWORD *)(v19 + 56) = v24;
    *(_OWORD *)(v19 + 72) = v23;
    *(_OWORD *)(v19 + 88) = v22;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v20;
}

uint64_t closure #1 in BNNS.LossLayer.init(input:output:lossFunction:lossReduction:filterParameters:)(uint64_t a1, _OWORD *a2, _OWORD *a3, long long *a4, unsigned __int8 a5)
{
  uint64_t v133 = *MEMORY[0x1E4F143B8];
  outlined init with take of BNNS.LossFunction(a2, &v126);
  uint64_t v10 = v129;
  switch(v129 >> 29)
  {
    case 0u:
      long long v11 = a3[6];
      *(_OWORD *)&v116[116] = a3[7];
      long long v12 = a3[9];
      *(_OWORD *)&v116[132] = a3[8];
      *(_OWORD *)&v116[148] = v12;
      *(_OWORD *)&v116[164] = a3[10];
      long long v13 = a3[2];
      *(_OWORD *)&v116[52] = a3[3];
      long long v14 = a3[5];
      *(_OWORD *)&v116[68] = a3[4];
      *(_OWORD *)&v116[84] = v14;
      *(_OWORD *)&v116[100] = v11;
      long long v15 = a3[1];
      *(_OWORD *)&v116[4] = *a3;
      *(_OWORD *)&v116[20] = v15;
      *(_OWORD *)&v116[36] = v13;
      long long v16 = a4[8];
      long long v17 = a4[9];
      long long v18 = a4[6];
      long long v103 = a4[7];
      long long v104 = v16;
      long long v19 = a4[10];
      long long v105 = v17;
      long long v106 = v19;
      long long v20 = a4[4];
      long long v101 = a4[5];
      long long v102 = v18;
      long long v21 = a4[2];
      long long v99 = a4[3];
      long long v100 = v20;
      long long v22 = a4[1];
      long long v96 = *a4;
      v129 &= 0xFFFFFFFF1FFFFFFFLL;
      long long v97 = v22;
      long long v98 = v21;
      long long v92 = *(_OWORD *)&v116[128];
      long long v93 = *(_OWORD *)&v116[144];
      long long v94 = *(_OWORD *)&v116[160];
      long long v88 = *(_OWORD *)&v116[64];
      long long v89 = *(_OWORD *)&v116[80];
      long long v90 = *(_OWORD *)&v116[96];
      long long v91 = *(_OWORD *)&v116[112];
      long long v84 = *(_OWORD *)v116;
      long long v85 = *(_OWORD *)&v116[16];
      long long v23 = *(_OWORD *)&v116[48];
      long long v86 = *(_OWORD *)&v116[32];
      int v24 = v126;
      int v25 = a5;
      int v26 = 1;
      goto LABEL_7;
    case 1u:
      long long v28 = a3[6];
      *(_OWORD *)&v116[116] = a3[7];
      long long v29 = a3[9];
      *(_OWORD *)&v116[132] = a3[8];
      *(_OWORD *)&v116[148] = v29;
      *(_OWORD *)&v116[164] = a3[10];
      long long v30 = a3[2];
      *(_OWORD *)&v116[52] = a3[3];
      long long v31 = a3[5];
      *(_OWORD *)&v116[68] = a3[4];
      *(_OWORD *)&v116[84] = v31;
      *(_OWORD *)&v116[100] = v28;
      long long v32 = a3[1];
      *(_OWORD *)&v116[4] = *a3;
      *(_OWORD *)&v116[20] = v32;
      *(_OWORD *)&v116[36] = v30;
      long long v33 = a4[8];
      long long v34 = a4[9];
      long long v35 = a4[6];
      long long v103 = a4[7];
      long long v104 = v33;
      long long v36 = a4[10];
      long long v105 = v34;
      long long v106 = v36;
      long long v37 = a4[4];
      long long v101 = a4[5];
      long long v102 = v35;
      long long v38 = a4[2];
      long long v99 = a4[3];
      long long v100 = v37;
      long long v39 = a4[1];
      long long v96 = *a4;
      v129 &= 0xFFFFFFFF1FFFFFFFLL;
      long long v97 = v39;
      long long v98 = v38;
      long long v92 = *(_OWORD *)&v116[128];
      long long v93 = *(_OWORD *)&v116[144];
      long long v94 = *(_OWORD *)&v116[160];
      long long v88 = *(_OWORD *)&v116[64];
      long long v89 = *(_OWORD *)&v116[80];
      long long v90 = *(_OWORD *)&v116[96];
      long long v91 = *(_OWORD *)&v116[112];
      long long v84 = *(_OWORD *)v116;
      long long v85 = *(_OWORD *)&v116[16];
      long long v23 = *(_OWORD *)&v116[48];
      long long v86 = *(_OWORD *)&v116[32];
      int v24 = v126;
      int v25 = a5;
      int v26 = 2;
      goto LABEL_7;
    case 2u:
      long long v40 = a3[6];
      *(_OWORD *)&v116[116] = a3[7];
      long long v41 = a3[9];
      *(_OWORD *)&v116[132] = a3[8];
      *(_OWORD *)&v116[148] = v41;
      *(_OWORD *)&v116[164] = a3[10];
      long long v42 = a3[2];
      *(_OWORD *)&v116[52] = a3[3];
      long long v43 = a3[5];
      *(_OWORD *)&v116[68] = a3[4];
      *(_OWORD *)&v116[84] = v43;
      *(_OWORD *)&v116[100] = v40;
      long long v44 = a3[1];
      *(_OWORD *)&v116[4] = *a3;
      *(_OWORD *)&v116[20] = v44;
      *(_OWORD *)&v116[36] = v42;
      long long v45 = a4[8];
      long long v46 = a4[9];
      long long v47 = a4[6];
      long long v103 = a4[7];
      long long v104 = v45;
      long long v48 = a4[10];
      long long v105 = v46;
      long long v106 = v48;
      long long v49 = a4[4];
      long long v101 = a4[5];
      long long v102 = v47;
      long long v50 = a4[2];
      long long v99 = a4[3];
      long long v100 = v49;
      long long v51 = a4[1];
      long long v96 = *a4;
      v129 &= 0xFFFFFFFF1FFFFFFFLL;
      long long v97 = v51;
      long long v98 = v50;
      long long v92 = *(_OWORD *)&v116[128];
      long long v93 = *(_OWORD *)&v116[144];
      long long v94 = *(_OWORD *)&v116[160];
      long long v88 = *(_OWORD *)&v116[64];
      long long v89 = *(_OWORD *)&v116[80];
      long long v90 = *(_OWORD *)&v116[96];
      long long v91 = *(_OWORD *)&v116[112];
      long long v84 = *(_OWORD *)v116;
      long long v85 = *(_OWORD *)&v116[16];
      long long v23 = *(_OWORD *)&v116[48];
      long long v86 = *(_OWORD *)&v116[32];
      int v24 = v126;
      int v25 = a5;
      int v26 = 4;
LABEL_7:
      int v83 = v26;
      long long v87 = v23;
      int v95 = *(_DWORD *)&v116[176];
      int v107 = v25;
      int v108 = v24;
      break;
    case 3u:
      long long v52 = a3[6];
      *(_OWORD *)&v116[116] = a3[7];
      long long v53 = a3[9];
      *(_OWORD *)&v116[132] = a3[8];
      *(_OWORD *)&v116[148] = v53;
      *(_OWORD *)&v116[164] = a3[10];
      long long v54 = a3[2];
      *(_OWORD *)&v116[52] = a3[3];
      long long v55 = a3[5];
      *(_OWORD *)&v116[68] = a3[4];
      *(_OWORD *)&v116[84] = v55;
      *(_OWORD *)&v116[100] = v52;
      long long v56 = a3[1];
      *(_OWORD *)&v116[4] = *a3;
      *(_OWORD *)&v116[20] = v56;
      *(_OWORD *)&v116[36] = v54;
      long long v57 = a4[8];
      long long v58 = a4[9];
      long long v59 = a4[6];
      long long v103 = a4[7];
      long long v104 = v57;
      long long v60 = a4[10];
      long long v105 = v58;
      long long v106 = v60;
      long long v61 = a4[4];
      long long v101 = a4[5];
      long long v102 = v59;
      long long v62 = a4[2];
      long long v99 = a4[3];
      long long v100 = v61;
      long long v63 = a4[1];
      long long v96 = *a4;
      v129 &= 0xFFFFFFFF1FFFFFFFLL;
      long long v97 = v63;
      long long v98 = v62;
      long long v92 = *(_OWORD *)&v116[128];
      long long v93 = *(_OWORD *)&v116[144];
      long long v94 = *(_OWORD *)&v116[160];
      long long v88 = *(_OWORD *)&v116[64];
      long long v89 = *(_OWORD *)&v116[80];
      long long v90 = *(_OWORD *)&v116[96];
      long long v91 = *(_OWORD *)&v116[112];
      long long v84 = *(_OWORD *)v116;
      long long v85 = *(_OWORD *)&v116[16];
      long long v86 = *(_OWORD *)&v116[32];
      int v83 = 5;
      long long v87 = *(_OWORD *)&v116[48];
      int v95 = *(_DWORD *)&v116[176];
      int v107 = a5;
      int v108 = v126;
      long long v109 = v127;
      long long v110 = v128;
      char v111 = v10 & 1;
      int v112 = HIDWORD(v10);
      long long v113 = v130;
      uint64_t v114 = v131;
      uint64_t v115 = v132;
      break;
    default:
      outlined init with take of BNNS.LossFunction(a2, &v117);
      switch(v121 >> 29)
      {
        case 1u:
          v121 &= 0xFFFFFFFF1FFFFFFFLL;
          int v27 = 2;
          break;
        case 2u:
          v121 &= 0xFFFFFFFF1FFFFFFFLL;
          int v27 = 4;
          break;
        case 3u:
          v121 &= 0xFFFFFFFF1FFFFFFFLL;
          int v27 = 5;
          break;
        case 4u:
          uint64_t v64 = v123 | v122;
          uint64_t v65 = v125 | v120 | v119;
          uint64_t v66 = v118 | *((void *)&v117 + 1);
          if (v121 != 0x80000000 || v64 | (unint64_t)v117 | v124 | v65 | v66)
          {
            uint64_t v67 = v64 | v124 | v65 | v66;
            if (v121 == 0x80000000 && (void)v117 == 1 && !v67)
            {
              int v27 = 6;
            }
            else if (v121 == 0x80000000 && (void)v117 == 2 && !v67)
            {
              int v27 = 7;
            }
            else if (v121 == 0x80000000 && (void)v117 == 3 && !v67)
            {
              int v27 = 8;
            }
            else
            {
              if (v67) {
                BOOL v68 = 0;
              }
              else {
                BOOL v68 = (void)v117 == 4;
              }
              if (v68 && v121 == 0x80000000) {
                int v27 = 9;
              }
              else {
                int v27 = 10;
              }
            }
          }
          else
          {
            int v27 = 3;
          }
          break;
        default:
          v121 &= 0xFFFFFFFF1FFFFFFFLL;
          int v27 = 1;
          break;
      }
      long long v70 = a3[6];
      *(_OWORD *)&v116[116] = a3[7];
      long long v71 = a3[9];
      *(_OWORD *)&v116[132] = a3[8];
      *(_OWORD *)&v116[148] = v71;
      *(_OWORD *)&v116[164] = a3[10];
      long long v72 = a3[2];
      *(_OWORD *)&v116[52] = a3[3];
      long long v73 = a3[5];
      *(_OWORD *)&v116[68] = a3[4];
      *(_OWORD *)&v116[84] = v73;
      *(_OWORD *)&v116[100] = v70;
      long long v74 = a3[1];
      *(_OWORD *)&v116[4] = *a3;
      *(_OWORD *)&v116[20] = v74;
      *(_OWORD *)&v116[36] = v72;
      long long v75 = a4[8];
      long long v76 = a4[9];
      long long v77 = a4[6];
      long long v103 = a4[7];
      long long v104 = v75;
      long long v78 = a4[10];
      long long v105 = v76;
      long long v106 = v78;
      long long v79 = a4[4];
      long long v101 = a4[5];
      long long v102 = v77;
      long long v80 = a4[2];
      long long v99 = a4[3];
      long long v100 = v79;
      long long v81 = a4[1];
      long long v96 = *a4;
      long long v97 = v81;
      long long v98 = v80;
      long long v92 = *(_OWORD *)&v116[128];
      long long v93 = *(_OWORD *)&v116[144];
      long long v94 = *(_OWORD *)&v116[160];
      long long v88 = *(_OWORD *)&v116[64];
      long long v89 = *(_OWORD *)&v116[80];
      long long v90 = *(_OWORD *)&v116[96];
      long long v91 = *(_OWORD *)&v116[112];
      long long v84 = *(_OWORD *)v116;
      long long v85 = *(_OWORD *)&v116[16];
      long long v86 = *(_OWORD *)&v116[32];
      int v83 = v27;
      long long v87 = *(_OWORD *)&v116[48];
      int v95 = *(_DWORD *)&v116[176];
      int v107 = a5;
      break;
  }
  return MEMORY[0x1D2600020](&v83, a1);
}

uint64_t BNNS.LossFunction.bnnsLossFunction.getter()
{
  uint64_t v1 = *v0;
  uint64_t v2 = v0[5];
  uint64_t result = 1;
  switch(v2 >> 29)
  {
    case 1u:
      uint64_t result = 2;
      break;
    case 2u:
      uint64_t result = 4;
      break;
    case 3u:
      uint64_t result = 5;
      break;
    case 4u:
      uint64_t v5 = v0[1];
      uint64_t v4 = v0[2];
      uint64_t v6 = v0[8] | v0[9];
      if (v2 != 0x80000000
        || (v7.i64[0] = v0[4],
            v7.i64[1] = v0[3],
            v8.i64[0] = v0[7],
            v8.i64[1] = v0[6],
            int8x16_t v9 = vorrq_s8(v8, v7),
            v6 | *(void *)&vorr_s8(*(int8x8_t *)v9.i8, (int8x8_t)*(_OWORD *)&vextq_s8(v9, v9, 8uLL)) | v5 | v4 | v1))
      {
        uint64_t v10 = v6 | v0[7] | v0[6] | v0[4] | v0[3] | v4 | v5;
        if (v2 == 0x80000000 && v1 == 1 && !v10)
        {
          uint64_t result = 6;
        }
        else if (v2 == 0x80000000 && v1 == 2 && !v10)
        {
          uint64_t result = 7;
        }
        else if (v2 == 0x80000000 && v1 == 3 && !v10)
        {
          uint64_t result = 8;
        }
        else
        {
          if (v10) {
            BOOL v11 = 0;
          }
          else {
            BOOL v11 = v1 == 4;
          }
          if (v11 && v2 == 0x80000000) {
            uint64_t result = 9;
          }
          else {
            uint64_t result = 10;
          }
        }
      }
      else
      {
        uint64_t result = 3;
      }
      break;
    default:
      return result;
  }
  return result;
}

uint64_t BNNS.LossReduction.bnnsLossReductionFunction.getter()
{
  return *v0;
}

uint64_t BNNS.LossLayer.apply(batchSize:input:labels:output:generatingInputGradient:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _OWORD *a5)
{
  uint64_t v67 = *MEMORY[0x1E4F143B8];
  uint64_t v6 = *(void **)(a2 + 136);
  if (v6 && (int8x16_t v7 = *(void **)(a4 + 136)) != 0 && (v8 = *(const void **)(a3 + 136)) != 0)
  {
    long long v9 = a5[9];
    *(_OWORD *)&in_delta.stride[7] = a5[8];
    *(_OWORD *)&in_delta.BNNSDataType data_type = v9;
    *(_OWORD *)&in_delta.table_BNNSDataType data_type = a5[10];
    long long v10 = a5[5];
    *(_OWORD *)&in_delta.size[7] = a5[4];
    *(_OWORD *)&in_delta.stride[1] = v10;
    long long v11 = a5[7];
    *(_OWORD *)&in_delta.stride[3] = a5[6];
    *(_OWORD *)&in_delta.stride[5] = v11;
    long long v12 = a5[1];
    *(_OWORD *)&in_delta.flags = *a5;
    *(_OWORD *)&in_delta.size[1] = v12;
    long long v13 = a5[3];
    *(_OWORD *)&in_delta.size[3] = a5[2];
    *(_OWORD *)&in_delta.size[5] = v13;
    long long v41 = v8;
    long long v42 = *(void **)(v5 + 16);
    out = v7;
    in = v6;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v62);
    outlined init with take of BNNS.Shape((uint64_t)v62, (uint64_t)v64);
    outlined init with take of BNNS.Shape((uint64_t)v64, (uint64_t)v61);
    BNNS.Shape.size.getter((uint64_t)&v53);
    unint64_t v15 = v53;
    unint64_t v14 = v54;
    unint64_t v17 = v55;
    unint64_t v16 = v56;
    unint64_t v18 = v57;
    unint64_t v33 = v59;
    unint64_t v36 = v58;
    unint64_t v30 = v60;
    outlined init with take of BNNS.Shape((uint64_t)v64, (uint64_t)v61);
    BNNS.Shape.stride.getter((uint64_t)&v53);
    size_t v37 = specialized static BNNS.calculateBatchStride(size:stride:)(v15, v14, v17, v16, v18, v36, v33, v30, v53, v54, v55, v56, v57, v58, v59, v60);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v61);
    outlined init with take of BNNS.Shape((uint64_t)v61, (uint64_t)v65);
    outlined init with take of BNNS.Shape((uint64_t)v65, (uint64_t)&v53);
    BNNS.Shape.size.getter((uint64_t)&v48);
    long long v19 = v48;
    long long v20 = v49;
    long long v21 = v50;
    unint64_t v34 = v51;
    unint64_t v31 = v52;
    outlined init with take of BNNS.Shape((uint64_t)v65, (uint64_t)&v53);
    BNNS.Shape.stride.getter((uint64_t)&v48);
    size_t v35 = specialized static BNNS.calculateBatchStride(size:stride:)(v19, *((unint64_t *)&v19 + 1), v20, *((unint64_t *)&v20 + 1), v21, *((unint64_t *)&v21 + 1), v34, v31, v48, *((unint64_t *)&v48 + 1), v49, *((unint64_t *)&v49 + 1), v50, *((unint64_t *)&v50 + 1), v51, v52);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v53);
    outlined init with take of BNNS.Shape((uint64_t)&v53, (uint64_t)v66);
    outlined init with take of BNNS.Shape((uint64_t)v66, (uint64_t)&v48);
    BNNS.Shape.size.getter((uint64_t)&v43);
    long long v22 = v43;
    long long v23 = v44;
    long long v24 = v45;
    unint64_t v25 = v46;
    unint64_t v32 = v47;
    outlined init with take of BNNS.Shape((uint64_t)v66, (uint64_t)&v48);
    BNNS.Shape.stride.getter((uint64_t)&v43);
    size_t in_delta_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v22, *((unint64_t *)&v22 + 1), v23, *((unint64_t *)&v23 + 1), v24, *((unint64_t *)&v24 + 1), v25, v32, v43, *((unint64_t *)&v43 + 1), v44, *((unint64_t *)&v44 + 1), v45, *((unint64_t *)&v45 + 1), v46, v47);
    uint64_t result = BNNSLossFilterApplyBatch(v42, a1, in, v37, v41, v35, 0, 0, out, &in_delta, in_delta_stride);
    if (!result) {
      return result;
    }
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    unsigned char *v28 = 0;
  }
  else
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    unsigned char *v29 = 2;
  }
  return swift_willThrow();
}

uint64_t BNNS.LossLayer.__allocating_init(bnnsFilter:)(uint64_t a1)
{
  uint64_t v2 = swift_allocObject();
  uint64_t v3 = v2;
  *(void *)(v2 + 24) = 0xFFFFFFFF00000000;
  *(_OWORD *)(v2 + 32) = 0u;
  *(_OWORD *)(v2 + 48) = 0u;
  *(_OWORD *)(v2 + 64) = 0u;
  *(_OWORD *)(v2 + 80) = 0u;
  *(void *)(v2 + 96) = 0;
  if (a1)
  {
    *(void *)(v2 + 16) = a1;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v3;
}

_OWORD *outlined init with take of BNNS.LossFunction(_OWORD *a1, _OWORD *a2)
{
  *a2 = *a1;
  long long v2 = a1[1];
  long long v3 = a1[2];
  long long v4 = a1[4];
  a2[3] = a1[3];
  a2[4] = v4;
  a2[1] = v2;
  a2[2] = v3;
  return a2;
}

uint64_t type metadata accessor for BNNS.LossLayer()
{
  return self;
}

uint64_t BNNS.LossLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.LossLayer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

uint64_t BNNS.LossLayer.apply(batchSize:input:labels:output:weights:broadcastsWeights:generatingInputGradient:)(int64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, char a6, _OWORD *a7)
{
  uint64_t v8 = v7;
  v44[1] = *MEMORY[0x1E4F143B8];
  outlined init with take of BNNSNDArrayDescriptor?(a5, (uint64_t)v35, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v35) == 1) {
    return BNNS.LossLayer.apply(batchSize:input:labels:output:generatingInputGradient:)(a1, a2, a3, a4, a7);
  }
  v36[0] = *(_OWORD *)&v35[16];
  v36[1] = *(_OWORD *)&v35[18];
  v36[2] = *(_OWORD *)&v35[20];
  *(_OWORD *)&v35[31] = *(_OWORD *)&v35[8];
  *(_OWORD *)&v35[33] = *(_OWORD *)&v35[10];
  *(_OWORD *)&v35[35] = *(_OWORD *)&v35[12];
  *(_OWORD *)&v35[37] = *(_OWORD *)&v35[14];
  *(_OWORD *)&v35[23] = *(_OWORD *)v35;
  *(_OWORD *)&v35[25] = *(_OWORD *)&v35[2];
  *(_OWORD *)&v35[27] = *(_OWORD *)&v35[4];
  *(_OWORD *)&v35[29] = *(_OWORD *)&v35[6];
  outlined init with take of BNNSNDArrayDescriptor?(a2 + 136, (uint64_t)v40, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v40, (uint64_t)&v41, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  unint64_t v16 = v41;
  if (!v41) {
    goto LABEL_10;
  }
  outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v39, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v39, (uint64_t)&v42, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  unint64_t v31 = v42;
  if (!v42
    || (outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v36 + 8, (uint64_t)v37, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v37, (uint64_t)v44, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), (unint64_t v17 = (const void *)v44[0]) == 0)|| (outlined init with take of BNNSNDArrayDescriptor?(a3 + 136, (uint64_t)v38, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v38, (uint64_t)&v43, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), (v18 = v43) == 0))
  {
LABEL_10:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v20 = 2;
    return swift_willThrow();
  }
  if ((a6 & 1) == 0)
  {
    unint64_t v21 = *(void *)(v8 + 24);
    uint64_t v22 = *(void *)(v8 + 64);
    if (v21 < 0xFFFFFFFF00000000 || (v22 & 0xFFFFFFFE) != 0)
    {
      size_t v19 = a1;
      switch(v22 >> 29)
      {
        case 1u:
        case 2u:
        case 4u:
          unint64_t v30 = v43;
          BNNSNDArrayDescriptor.shape.getter((uint64_t)v34);
          outlined init with take of BNNS.Shape((uint64_t)v34, (uint64_t)&v32);
          outlined init with take of BNNS.Shape((uint64_t)&v32, (uint64_t)v33);
          uint64_t v23 = BNNS.Shape.denseTensorSize.getter();
          size_t v19 = a1 * v23;
          if ((unsigned __int128)(a1 * (__int128)v23) >> 64 != (a1 * v23) >> 63) {
            __break(1u);
          }
          unint64_t v18 = v30;
          goto LABEL_17;
        case 3u:
          goto LABEL_18;
        default:
          goto LABEL_17;
      }
    }
    goto LABEL_18;
  }
  size_t v19 = 1;
LABEL_17:
  long long v24 = a7[9];
  *(_OWORD *)&v32.stride[7] = a7[8];
  *(_OWORD *)&v32.BNNSDataType data_type = v24;
  *(_OWORD *)&v32.table_BNNSDataType data_type = a7[10];
  long long v25 = a7[5];
  *(_OWORD *)&v32.size[7] = a7[4];
  *(_OWORD *)&v32.stride[1] = v25;
  long long v26 = a7[7];
  *(_OWORD *)&v32.stride[3] = a7[6];
  *(_OWORD *)&v32.stride[5] = v26;
  long long v27 = a7[1];
  *(_OWORD *)&v32.flags = *a7;
  *(_OWORD *)&v32.size[1] = v27;
  long long v28 = a7[3];
  *(_OWORD *)&v32.size[3] = a7[2];
  *(_OWORD *)&v32.size[5] = v28;
  uint64_t result = closure #1 in BNNS.LossLayer.apply(batchSize:input:labels:output:weights:broadcastsWeights:generatingInputGradient:)(&v32, v8, a1, v16, v18, v17, v34, v19, v31);
  if (v34[0])
  {
LABEL_18:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    unsigned char *v29 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t closure #1 in BNNS.LossLayer.apply(batchSize:input:labels:output:weights:broadcastsWeights:generatingInputGradient:)@<X0>(BNNSNDArrayDescriptor *a1@<X0>, uint64_t a2@<X1>, size_t a3@<X2>, const void *a4@<X3>, const void *a5@<X5>, const void *a6@<X7>, _DWORD *a7@<X8>, size_t a8, void *out)
{
  unint64_t v34 = *(void **)(a2 + 16);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v56);
  outlined init with take of BNNS.Shape((uint64_t)v56, (uint64_t)v57);
  outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v66);
  BNNS.Shape.size.getter((uint64_t)&v58);
  unint64_t v9 = v58;
  unint64_t v10 = v59;
  unint64_t v11 = v60;
  unint64_t v12 = v61;
  unint64_t v13 = v62;
  unint64_t v14 = v63;
  unint64_t v15 = v64;
  unint64_t v16 = v65;
  outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v66);
  BNNS.Shape.stride.getter((uint64_t)&v58);
  unint64_t in_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v9, v10, v11, v12, v13, v14, v15, v16, v58, v59, v60, v61, v62, v63, v64, v65);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v55);
  outlined init with take of BNNS.Shape((uint64_t)v55, (uint64_t)&v58);
  outlined init with take of BNNS.Shape((uint64_t)&v58, (uint64_t)v66);
  BNNS.Shape.size.getter((uint64_t)&v47);
  unint64_t v17 = v47;
  unint64_t v18 = v48;
  unint64_t v19 = v49;
  unint64_t v20 = v50;
  unint64_t v21 = v51;
  unint64_t v22 = v52;
  unint64_t v24 = v53;
  unint64_t v23 = v54;
  outlined init with take of BNNS.Shape((uint64_t)&v58, (uint64_t)v66);
  BNNS.Shape.stride.getter((uint64_t)&v47);
  unint64_t labels_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v17, v18, v19, v20, v21, v22, v24, v23, v47, v48, v49, v50, v51, v52, v53, v54);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v47);
  outlined init with take of BNNS.Shape((uint64_t)&v47, (uint64_t)v66);
  outlined init with take of BNNS.Shape((uint64_t)v66, (uint64_t)v46);
  BNNS.Shape.size.getter((uint64_t)&v41);
  long long v25 = v41;
  long long v26 = v42;
  long long v27 = v43;
  unint64_t v29 = v44;
  unint64_t v28 = v45;
  outlined init with take of BNNS.Shape((uint64_t)v66, (uint64_t)v46);
  BNNS.Shape.stride.getter((uint64_t)&v41);
  size_t in_delta_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v25, *((unint64_t *)&v25 + 1), v26, *((unint64_t *)&v26 + 1), v27, *((unint64_t *)&v27 + 1), v29, v28, v41, *((unint64_t *)&v41 + 1), v42, *((unint64_t *)&v42 + 1), v43, *((unint64_t *)&v43 + 1), v44, v45);
  uint64_t result = BNNSLossFilterApplyBatch(v34, a3, a4, in_stride, a5, labels_stride, a6, a8, out, a1, in_delta_stride);
  *a7 = result;
  return result;
}

uint64_t BNNS.LossFunction.YoloParameters.init(huberDelta:gridColumnCount:gridRowsCount:anchorBoxCount:anchorBoxSize:rescore:xyScale:whScale:objectScale:noObjectScale:classificationScale:objectMinimumIoU:noObjectMaximumIoU:anchorsData:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, char a5@<W4>, uint64_t a6@<X5>, uint64_t a7@<X8>, float a8@<S0>, float a9@<S1>, float a10@<S2>, float a11@<S3>, float a12@<S4>, float a13@<S5>, float a14@<S6>, float a15@<S7>)
{
  *(float *)a7 = a8;
  *(void *)(a7 + 8) = result;
  *(void *)(a7 + 16) = a2;
  *(void *)(a7 + 24) = a3;
  *(void *)(a7 + 32) = a4;
  *(unsigned char *)(a7 + 40) = a5;
  *(float *)(a7 + 44) = a9;
  *(float *)(a7 + 48) = a10;
  *(float *)(a7 + 52) = a11;
  *(float *)(a7 + 56) = a12;
  *(float *)(a7 + 60) = a13;
  *(float *)(a7 + 64) = a14;
  *(float *)(a7 + 68) = a15;
  *(void *)(a7 + 72) = a6;
  return result;
}

float BNNS.LossFunction.YoloParameters.huberDelta.getter()
{
  return *(float *)v0;
}

uint64_t BNNS.LossFunction.YoloParameters.gridColumnCount.getter()
{
  return *(void *)(v0 + 8);
}

uint64_t BNNS.LossFunction.YoloParameters.gridRowsCount.getter()
{
  return *(void *)(v0 + 16);
}

uint64_t BNNS.LossFunction.YoloParameters.anchorBoxCount.getter()
{
  return *(void *)(v0 + 24);
}

uint64_t BNNS.LossFunction.YoloParameters.anchorBoxSize.getter()
{
  return *(void *)(v0 + 32);
}

uint64_t BNNS.LossFunction.YoloParameters.rescore.getter()
{
  return *(unsigned __int8 *)(v0 + 40);
}

float BNNS.LossFunction.YoloParameters.xyScale.getter()
{
  return *(float *)(v0 + 44);
}

float BNNS.LossFunction.YoloParameters.whScale.getter()
{
  return *(float *)(v0 + 48);
}

float BNNS.LossFunction.YoloParameters.objectScale.getter()
{
  return *(float *)(v0 + 52);
}

float BNNS.LossFunction.YoloParameters.noObjectScale.getter()
{
  return *(float *)(v0 + 56);
}

float BNNS.LossFunction.YoloParameters.classificationScale.getter()
{
  return *(float *)(v0 + 60);
}

float BNNS.LossFunction.YoloParameters.objectMinimumIoU.getter()
{
  return *(float *)(v0 + 64);
}

float BNNS.LossFunction.YoloParameters.noObjectMaximumIoU.getter()
{
  return *(float *)(v0 + 68);
}

uint64_t BNNS.LossFunction.YoloParameters.anchorsData.getter()
{
  return *(void *)(v0 + 72);
}

BOOL static BNNS.LossReduction.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void BNNS.LossReduction.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int BNNS.LossReduction.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

unint64_t lazy protocol witness table accessor for type BNNS.LossReduction and conformance BNNS.LossReduction()
{
  unint64_t result = lazy protocol witness table cache variable for type BNNS.LossReduction and conformance BNNS.LossReduction;
  if (!lazy protocol witness table cache variable for type BNNS.LossReduction and conformance BNNS.LossReduction)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.LossReduction and conformance BNNS.LossReduction);
  }
  return result;
}

uint64_t method lookup function for BNNS.LossLayer(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.LossLayer);
}

uint64_t dispatch thunk of BNNS.LossLayer.apply(batchSize:input:labels:output:generatingInputGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5)
{
  uint64_t v6 = a2[17];
  int v7 = *((_DWORD *)a2 + 36);
  uint64_t v8 = a2[19];
  int v9 = *((_DWORD *)a2 + 40);
  uint64_t v10 = a3[17];
  int v11 = *((_DWORD *)a3 + 36);
  uint64_t v12 = a3[19];
  int v13 = *((_DWORD *)a3 + 40);
  uint64_t v14 = a4[17];
  int v15 = *((_DWORD *)a4 + 36);
  uint64_t v16 = a4[19];
  int v17 = *((_DWORD *)a4 + 40);
  uint64_t v18 = a5[17];
  int v19 = *((_DWORD *)a5 + 36);
  uint64_t v20 = a5[19];
  int v21 = *((_DWORD *)a5 + 40);
  long long v98 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(void *)v5 + 128);
  uint64_t v84 = *a2;
  long long v85 = *(_OWORD *)(a2 + 1);
  long long v86 = *(_OWORD *)(a2 + 3);
  long long v87 = *(_OWORD *)(a2 + 5);
  long long v88 = *(_OWORD *)(a2 + 7);
  long long v89 = *(_OWORD *)(a2 + 9);
  long long v90 = *(_OWORD *)(a2 + 11);
  long long v91 = *(_OWORD *)(a2 + 13);
  long long v92 = *(_OWORD *)(a2 + 15);
  uint64_t v93 = v6;
  int v94 = v7;
  uint64_t v95 = v8;
  int v96 = v9;
  uint64_t v97 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v70 = *a3;
  long long v71 = *(_OWORD *)(a3 + 1);
  long long v72 = *(_OWORD *)(a3 + 3);
  long long v73 = *(_OWORD *)(a3 + 5);
  long long v74 = *(_OWORD *)(a3 + 7);
  long long v75 = *(_OWORD *)(a3 + 9);
  long long v76 = *(_OWORD *)(a3 + 11);
  long long v22 = *(_OWORD *)(a3 + 15);
  long long v77 = *(_OWORD *)(a3 + 13);
  long long v23 = *(_OWORD *)(a4 + 1);
  long long v24 = *(_OWORD *)(a4 + 3);
  long long v25 = *(_OWORD *)(a4 + 5);
  long long v26 = *(_OWORD *)(a4 + 7);
  long long v27 = *(_OWORD *)(a4 + 9);
  long long v28 = *(_OWORD *)(a4 + 11);
  long long v29 = *(_OWORD *)(a4 + 13);
  uint64_t v30 = *a4;
  long long v31 = *(_OWORD *)(a4 + 15);
  long long v32 = *(_OWORD *)(a5 + 1);
  long long v33 = *(_OWORD *)(a5 + 3);
  long long v34 = *(_OWORD *)(a5 + 5);
  long long v35 = *(_OWORD *)(a5 + 7);
  long long v36 = *(_OWORD *)(a5 + 9);
  long long v37 = *(_OWORD *)(a5 + 11);
  long long v38 = *(_OWORD *)(a5 + 13);
  uint64_t v39 = *a5;
  long long v40 = *(_OWORD *)(a5 + 15);
  long long v78 = v22;
  uint64_t v79 = v10;
  int v80 = v11;
  uint64_t v81 = v12;
  int v82 = v13;
  uint64_t v83 = *(uint64_t *)((char *)a3 + 164);
  *(void *)&long long v22 = *(uint64_t *)((char *)a5 + 164);
  uint64_t v56 = v30;
  long long v57 = v23;
  *(void *)&long long v23 = *(uint64_t *)((char *)a4 + 164);
  long long v58 = v24;
  long long v59 = v25;
  long long v60 = v26;
  long long v61 = v27;
  long long v62 = v28;
  long long v63 = v29;
  long long v64 = v31;
  uint64_t v65 = v14;
  int v66 = v15;
  uint64_t v67 = v16;
  int v68 = v17;
  uint64_t v69 = v23;
  uint64_t v42 = v39;
  long long v43 = v32;
  long long v44 = v33;
  long long v45 = v34;
  long long v46 = v35;
  long long v47 = v36;
  long long v48 = v37;
  long long v49 = v38;
  long long v50 = v40;
  uint64_t v51 = v18;
  int v52 = v19;
  uint64_t v53 = v20;
  int v54 = v21;
  uint64_t v55 = v22;
  return v98(a1, &v84, &v70, &v56, &v42);
}

__n128 __swift_memcpy80_8(uint64_t a1, uint64_t a2)
{
  *(_OWORD *)a1 = *(_OWORD *)a2;
  __n128 result = *(__n128 *)(a2 + 16);
  long long v3 = *(_OWORD *)(a2 + 32);
  long long v4 = *(_OWORD *)(a2 + 64);
  *(_OWORD *)(a1 + 48) = *(_OWORD *)(a2 + 48);
  *(_OWORD *)(a1 + 64) = v4;
  *(__n128 *)(a1 + 16) = result;
  *(_OWORD *)(a1 + 32) = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.LossFunction(uint64_t a1, int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0 && *(unsigned char *)(a1 + 80)) {
    return *(_DWORD *)a1 + 0x80000000;
  }
  unsigned int v2 = *(_DWORD *)(a1 + 4);
  if (v2 > 0x80000000) {
    int v3 = ~v2;
  }
  else {
    int v3 = -1;
  }
  return (v3 + 1);
}

double storeEnumTagSinglePayload for BNNS.LossFunction(uint64_t a1, int a2, int a3)
{
  if (a2 < 0)
  {
    *(void *)(a1 + 72) = 0;
    double result = 0.0;
    *(_OWORD *)(a1 + 56) = 0u;
    *(_OWORD *)(a1 + 40) = 0u;
    *(_OWORD *)(a1 + 24) = 0u;
    *(_OWORD *)(a1 + 8) = 0u;
    *(void *)a1 = a2 ^ 0x80000000;
    if (a3 < 0) {
      *(unsigned char *)(a1 + 80) = 1;
    }
  }
  else
  {
    if ((a3 & 0x80000000) == 0)
    {
      if (!a2) {
        return result;
      }
LABEL_8:
      *(void *)a1 = (unint64_t)-a2 << 32;
      double result = 0.0;
      *(_OWORD *)(a1 + 8) = 0u;
      *(_OWORD *)(a1 + 24) = 0u;
      *(_OWORD *)(a1 + 40) = 0u;
      *(_OWORD *)(a1 + 56) = 0u;
      *(void *)(a1 + 72) = 0;
      return result;
    }
    *(unsigned char *)(a1 + 80) = 0;
    if (a2) {
      goto LABEL_8;
    }
  }
  return result;
}

uint64_t getEnumTag for BNNS.LossFunction(_DWORD *a1)
{
  int v1 = a1[10];
  if (v1 >= 0) {
    return v1 >> 29;
  }
  else {
    return (*a1 + 4);
  }
}

uint64_t destructiveProjectEnumData for BNNS.LossFunction(uint64_t result)
{
  *(void *)(result + 40) &= 0xFFFFFFFF1FFFFFFFLL;
  return result;
}

unsigned int *destructiveInjectEnumTag for BNNS.LossFunction(unsigned int *result, unsigned int a2)
{
  if (a2 < 4)
  {
    unint64_t v2 = *((void *)result + 5) & 0xFFFFFFFF00000001 | (a2 << 29);
    *(void *)double result = *result;
    *((void *)result + 5) = v2;
  }
  else
  {
    *(void *)double result = a2 - 4;
    *(_OWORD *)(result + 2) = 0u;
    *(_OWORD *)(result + 6) = 0u;
    *((void *)result + 5) = 0x80000000;
    *((_OWORD *)result + 3) = 0u;
    *((_OWORD *)result + 4) = 0u;
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.LossFunction()
{
  return &type metadata for BNNS.LossFunction;
}

uint64_t getEnumTagSinglePayload for BNNS.LossFunction.YoloParameters(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFF && *(unsigned char *)(a1 + 80)) {
    return (*(_DWORD *)a1 + 255);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 40);
  BOOL v4 = v3 >= 2;
  int v5 = (v3 + 2147483646) & 0x7FFFFFFF;
  if (!v4) {
    int v5 = -1;
  }
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.LossFunction.YoloParameters(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(void *)(result + 72) = 0;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(void *)double result = a2 - 255;
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 80) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 80) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 40) = a2 + 1;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.LossFunction.YoloParameters()
{
  return &type metadata for BNNS.LossFunction.YoloParameters;
}

unsigned char *storeEnumTagSinglePayload for BNNS.LossReduction(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 4 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 4) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFC) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFB)
  {
    unsigned int v6 = ((a2 - 252) >> 8) + 1;
    *double result = a2 + 4;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D21322A4);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *double result = a2 + 4;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.LossReduction()
{
  return &type metadata for BNNS.LossReduction;
}

uint64_t static BNNS.clipByGlobalNorm(threshold:inputs:outputs:globalNorm:)(uint64_t a1, uint64_t a2, float a3, float a4)
{
  size_t v4 = *(void *)(a1 + 16);
  if (v4 != *(void *)(a2 + 16)) {
    __break(1u);
  }
  int v7 = (char *)MEMORY[0x1E4FBC860];
  if (v4)
  {
    long long v24 = (char *)MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v4, 0);
    int v9 = (long long *)(a1 + 32);
    uint64_t v10 = v24;
    int64_t v23 = v4;
    do
    {
      long long v42 = v9[8];
      long long v44 = v9[9];
      long long v46 = v9[10];
      long long v34 = v9[4];
      long long v36 = v9[5];
      long long v38 = v9[6];
      long long v40 = v9[7];
      long long v26 = *v9;
      long long v28 = v9[1];
      long long v30 = v9[2];
      long long v32 = v9[3];
      int v11 = (_OWORD *)swift_slowAlloc();
      unsigned char v11[8] = v42;
      v11[9] = v44;
      v11[10] = v46;
      v11[4] = v34;
      void v11[5] = v36;
      v11[6] = v38;
      v11[7] = v40;
      *int v11 = v26;
      v11[1] = v28;
      void v11[2] = v30;
      _OWORD v11[3] = v32;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v10 + 2) + 1, 1);
        uint64_t v10 = v24;
      }
      unint64_t v13 = *((void *)v10 + 2);
      unint64_t v12 = *((void *)v10 + 3);
      if (v13 >= v12 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v12 > 1), v13 + 1, 1);
        uint64_t v10 = v24;
      }
      *((void *)v10 + 2) = v13 + 1;
      *(void *)&v10[8 * v13 + 32] = v11;
      v9 += 11;
      --v4;
    }
    while (v4);
    long long v25 = v7;
    int64_t v14 = v23;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v23, 0);
    int v15 = (long long *)(a2 + 32);
    do
    {
      long long v43 = v15[8];
      long long v45 = v15[9];
      long long v47 = v15[10];
      long long v35 = v15[4];
      long long v37 = v15[5];
      long long v39 = v15[6];
      long long v41 = v15[7];
      long long v27 = *v15;
      long long v29 = v15[1];
      long long v31 = v15[2];
      long long v33 = v15[3];
      uint64_t v16 = (_OWORD *)swift_slowAlloc();
      v16[8] = v43;
      v16[9] = v45;
      v16[10] = v47;
      v16[4] = v35;
      void v16[5] = v37;
      v16[6] = v39;
      v16[7] = v41;
      *uint64_t v16 = v27;
      v16[1] = v29;
      _OWORD v16[2] = v31;
      v16[3] = v33;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v7 + 2) + 1, 1);
        int v7 = v25;
      }
      unint64_t v18 = *((void *)v7 + 2);
      unint64_t v17 = *((void *)v7 + 3);
      if (v18 >= v17 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v17 > 1), v18 + 1, 1);
        int v7 = v25;
      }
      *((void *)v7 + 2) = v18 + 1;
      *(void *)&v7[8 * v18 + 32] = v16;
      v15 += 11;
      --v14;
    }
    while (v14);
    size_t v4 = v23;
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
    int v7 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v7 + 2), 0, v7);
  }
  swift_bridgeObjectRetain();
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
    uint64_t v10 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v10 + 2), 0, v10);
  }
  swift_bridgeObjectRetain();
  int v19 = BNNSClipByGlobalNorm((BNNSNDArrayDescriptor **)v7 + 4, (const BNNSNDArrayDescriptor **)v10 + 4, v4, a3, a4);
  swift_bridgeObjectRelease_n();
  uint64_t result = swift_bridgeObjectRelease_n();
  if (v19)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *int v21 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.clip(to:input:output:)(_OWORD *a1, _OWORD *a2, float a3, float a4)
{
  uint64_t v18 = *MEMORY[0x1E4F143B8];
  long long v4 = a1[9];
  *(_OWORD *)&src.stride[7] = a1[8];
  *(_OWORD *)&src.BNNSDataType data_type = v4;
  *(_OWORD *)&src.table_BNNSDataType data_type = a1[10];
  long long v5 = a1[5];
  *(_OWORD *)&src.size[7] = a1[4];
  *(_OWORD *)&src.stride[1] = v5;
  long long v6 = a1[7];
  *(_OWORD *)&src.stride[3] = a1[6];
  *(_OWORD *)&src.stride[5] = v6;
  long long v7 = a1[1];
  *(_OWORD *)&src.flags = *a1;
  *(_OWORD *)&src.size[1] = v7;
  long long v8 = a1[3];
  *(_OWORD *)&src.size[3] = a1[2];
  *(_OWORD *)&src.size[5] = v8;
  long long v9 = a2[9];
  *(_OWORD *)&v16.stride[7] = a2[8];
  *(_OWORD *)&v16.BNNSDataType data_type = v9;
  *(_OWORD *)&v16.table_BNNSDataType data_type = a2[10];
  long long v10 = a2[5];
  *(_OWORD *)&v16.size[7] = a2[4];
  *(_OWORD *)&v16.stride[1] = v10;
  long long v11 = a2[7];
  *(_OWORD *)&v16.stride[3] = a2[6];
  *(_OWORD *)&v16.stride[5] = v11;
  long long v12 = a2[1];
  *(_OWORD *)&v16.flags = *a2;
  *(_OWORD *)&v16.size[1] = v12;
  long long v13 = a2[3];
  *(_OWORD *)&v16.size[3] = a2[2];
  *(_OWORD *)&v16.size[5] = v13;
  uint64_t result = BNNSClipByValue(&v16, &src, a3, a4);
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    unsigned char *v15 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.clipByNorm(threshold:input:output:axes:)(_OWORD *a1, _OWORD *a2, uint64_t a3, float a4)
{
  uint64_t v20 = *MEMORY[0x1E4F143B8];
  long long v5 = a1[9];
  *(_OWORD *)&src.stride[7] = a1[8];
  *(_OWORD *)&src.BNNSDataType data_type = v5;
  *(_OWORD *)&src.table_BNNSDataType data_type = a1[10];
  long long v6 = a1[5];
  *(_OWORD *)&src.size[7] = a1[4];
  *(_OWORD *)&src.stride[1] = v6;
  long long v7 = a1[7];
  *(_OWORD *)&src.stride[3] = a1[6];
  *(_OWORD *)&src.stride[5] = v7;
  long long v8 = a1[1];
  *(_OWORD *)&src.flags = *a1;
  *(_OWORD *)&src.size[1] = v8;
  long long v9 = a1[3];
  *(_OWORD *)&src.size[3] = a1[2];
  *(_OWORD *)&src.size[5] = v9;
  long long v10 = a2[9];
  *(_OWORD *)&v18.stride[7] = a2[8];
  *(_OWORD *)&v18.BNNSDataType data_type = v10;
  *(_OWORD *)&v18.table_BNNSDataType data_type = a2[10];
  long long v11 = a2[5];
  *(_OWORD *)&v18.size[7] = a2[4];
  *(_OWORD *)&v18.stride[1] = v11;
  long long v12 = a2[7];
  *(_OWORD *)&v18.stride[3] = a2[6];
  *(_OWORD *)&v18.stride[5] = v12;
  long long v13 = a2[1];
  *(_OWORD *)&v18.flags = *a2;
  *(_OWORD *)&v18.size[1] = v13;
  long long v14 = a2[3];
  *(_OWORD *)&v18.size[3] = a2[2];
  *(_OWORD *)&v18.size[5] = v14;
  uint32_t v15 = specialized static BNNS.computeAxisFlags(_:)(a3);
  uint64_t result = BNNSClipByNorm(&v18, &src, a4, v15);
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *unint64_t v17 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t specialized static BNNS.computeAxisFlags(_:)(uint64_t a1)
{
  if (a1) {
    uint64_t v1 = a1;
  }
  else {
    uint64_t v1 = MEMORY[0x1E4FBC860];
  }
  int64_t v2 = *(void *)(v1 + 16);
  if (!v2)
  {
    swift_bridgeObjectRetain();
    swift_bridgeObjectRelease();
    long long v4 = (int8x16_t *)MEMORY[0x1E4FBC860];
    unint64_t v8 = *(void *)(MEMORY[0x1E4FBC860] + 16);
    if (v8) {
      goto LABEL_10;
    }
LABEL_13:
    uint64_t v10 = 0;
    goto LABEL_19;
  }
  int v21 = (int8x16_t *)MEMORY[0x1E4FBC860];
  swift_bridgeObjectRetain();
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v2, 0);
  uint64_t v3 = 0;
  long long v4 = v21;
  unint64_t v5 = v21[1].u64[0];
  do
  {
    uint64_t v6 = *(void *)(v1 + 8 * v3 + 32);
    unint64_t v7 = v21[1].u64[1];
    if (v5 >= v7 >> 1) {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v7 > 1), v5 + 1, 1);
    }
    ++v3;
    v21[1].i64[0] = v5 + 1;
    v21[2].i32[v5++] = 1 << v6;
  }
  while (v2 != v3);
  swift_bridgeObjectRelease();
  unint64_t v8 = v21[1].u64[0];
  if (!v8) {
    goto LABEL_13;
  }
LABEL_10:
  if (v8 < 8)
  {
    uint64_t v9 = 0;
    LODWORD(v10) = 0;
LABEL_17:
    unint64_t v17 = v8 - v9;
    BNNSNDArrayDescriptor v18 = &v4[2].i32[v9];
    do
    {
      int v19 = *v18++;
      uint64_t v10 = v19 | v10;
      --v17;
    }
    while (v17);
    goto LABEL_19;
  }
  uint64_t v9 = v8 & 0x7FFFFFFFFFFFFFF8;
  long long v11 = v4 + 3;
  int8x16_t v12 = 0uLL;
  uint64_t v13 = v8 & 0x7FFFFFFFFFFFFFF8;
  int8x16_t v14 = 0uLL;
  do
  {
    int8x16_t v12 = vorrq_s8(v11[-1], v12);
    int8x16_t v14 = vorrq_s8(*v11, v14);
    v11 += 2;
    v13 -= 8;
  }
  while (v13);
  int8x16_t v15 = vorrq_s8(v14, v12);
  int8x8_t v16 = vorr_s8(*(int8x8_t *)v15.i8, (int8x8_t)*(_OWORD *)&vextq_s8(v15, v15, 8uLL));
  uint64_t v10 = (v16.i32[0] | v16.i32[1]);
  if (v8 != v9) {
    goto LABEL_17;
  }
LABEL_19:
  swift_bridgeObjectRelease();
  return v10;
}

uint64_t static _CFObject.== infix(_:_:)()
{
  return MEMORY[0x1F4186958]();
}

uint64_t _CFObject.hash(into:)()
{
  return MEMORY[0x1F4186968]();
}

uint64_t _CFObject.hashValue.getter()
{
  return MEMORY[0x1F4186978]();
}

uint64_t BinaryFloatingPoint.init<A>(_:)()
{
  return MEMORY[0x1F41832D0]();
}

uint64_t static BinaryFloatingPoint<>.random<A>(in:using:)()
{
  return MEMORY[0x1F41832D8]();
}

uint64_t static BinaryFloatingPoint<>.random(in:)()
{
  return MEMORY[0x1F41832E0]();
}

uint64_t dispatch thunk of static Comparable.< infix(_:_:)()
{
  return MEMORY[0x1F4183740]();
}

uint64_t dispatch thunk of MutableCollection.withContiguousMutableStorageIfAvailable<A>(_:)()
{
  return MEMORY[0x1F4183770]();
}

uint64_t type metadata accessor for ClosedRange()
{
  return MEMORY[0x1F4183808]();
}

uint64_t type metadata accessor for UnsafePointer()
{
  return MEMORY[0x1F4183830]();
}

uint64_t UnsafeBufferPointer.baseAddress.getter()
{
  return MEMORY[0x1F4183840]();
}

uint64_t UnsafeBufferPointer.init(start:count:)()
{
  return MEMORY[0x1F4183850]();
}

uint64_t type metadata accessor for UnsafeBufferPointer()
{
  return MEMORY[0x1F4183870]();
}

uint64_t String.utf8CString.getter()
{
  return MEMORY[0x1F41838B8]();
}

Swift::Void __swiftcall String.append(_:)(Swift::String a1)
{
}

uint64_t String.init(cString:)()
{
  return MEMORY[0x1F4183A60]();
}

uint64_t dispatch thunk of Sequence.withContiguousStorageIfAvailable<A>(_:)()
{
  return MEMORY[0x1F4183BD0]();
}

Swift::Void __swiftcall Array.reserveCapacity(_:)(Swift::Int a1)
{
}

Swift::Void __swiftcall Array._makeMutableAndUnique()()
{
}

uint64_t static Array._allocateUninitialized(_:)()
{
  return MEMORY[0x1F4183EF0]();
}

uint64_t Array.withUnsafeBufferPointer<A>(_:)()
{
  return MEMORY[0x1F4183F00]();
}

uint64_t static Array._allocateBufferUninitialized(minimumCapacity:)()
{
  return MEMORY[0x1F4183F08]();
}

uint64_t Array.init(_unsafeUninitializedCapacity:initializingWith:)()
{
  return MEMORY[0x1F4183F10]();
}

uint64_t Array.count.getter()
{
  return MEMORY[0x1F4183F40]();
}

uint64_t Array.endIndex.getter()
{
  return MEMORY[0x1F4183F78]();
}

uint64_t type metadata accessor for Array()
{
  return MEMORY[0x1F4183FA8]();
}

uint64_t Array.init<A>(_:)()
{
  return MEMORY[0x1F4183FD8]();
}

uint64_t Array.subscript.getter()
{
  return MEMORY[0x1F4183FF0]();
}

uint64_t Double.significandWidth.getter()
{
  return MEMORY[0x1F4184028]();
}

uint64_t Double.exponent.getter()
{
  return MEMORY[0x1F4184058]();
}

uint64_t Set.init(minimumCapacity:)()
{
  return MEMORY[0x1F4184138]();
}

uint64_t dispatch thunk of Collection.startIndex.getter()
{
  return MEMORY[0x1F41842D0]();
}

uint64_t dispatch thunk of Collection.count.getter()
{
  return MEMORY[0x1F41842F8]();
}

uint64_t dispatch thunk of Collection.formIndex(after:)()
{
  return MEMORY[0x1F4184338]();
}

uint64_t dispatch thunk of Collection.subscript.read()
{
  return MEMORY[0x1F4184440]();
}

uint64_t static UnsafeMutablePointer.allocate(capacity:)()
{
  return MEMORY[0x1F4184618]();
}

uint64_t type metadata accessor for UnsafeMutablePointer()
{
  return MEMORY[0x1F4184620]();
}

uint64_t type metadata accessor for Optional()
{
  return MEMORY[0x1F4184640]();
}

uint64_t UnsafeMutableBufferPointer.initialize<A>(from:)()
{
  return MEMORY[0x1F4184670]();
}

uint64_t UnsafeMutableBufferPointer.baseAddress.getter()
{
  return MEMORY[0x1F4184680]();
}

uint64_t UnsafeMutableBufferPointer.init(start:count:)()
{
  return MEMORY[0x1F4184688]();
}

uint64_t UnsafeMutableBufferPointer.endIndex.getter()
{
  return MEMORY[0x1F41846A8]();
}

uint64_t type metadata accessor for UnsafeMutableBufferPointer()
{
  return MEMORY[0x1F41846C0]();
}

uint64_t dispatch thunk of BinaryInteger.init<A>(truncatingIfNeeded:)()
{
  return MEMORY[0x1F41848C0]();
}

uint64_t dispatch thunk of BinaryInteger._lowWord.getter()
{
  return MEMORY[0x1F4184938]();
}

uint64_t dispatch thunk of BinaryInteger.bitWidth.getter()
{
  return MEMORY[0x1F4184940]();
}

uint64_t dispatch thunk of static BinaryInteger.isSigned.getter()
{
  return MEMORY[0x1F4184950]();
}

Swift::Void __swiftcall ArraySlice._makeMutableAndUnique()()
{
}

uint64_t ArraySlice.withUnsafeBufferPointer<A>(_:)()
{
  return MEMORY[0x1F41849F0]();
}

Swift::Int __swiftcall ArraySlice._getCount()()
{
  return MEMORY[0x1F4184A20]();
}

uint64_t static _SetStorage.copy(original:)()
{
  return MEMORY[0x1F4184C90]();
}

uint64_t static _SetStorage.resize(original:capacity:move:)()
{
  return MEMORY[0x1F4184C98]();
}

Swift::Void __swiftcall _StringGuts.grow(_:)(Swift::Int a1)
{
}

uint64_t type metadata accessor for _ArrayBuffer()
{
  return MEMORY[0x1F4184D98]();
}

Swift::Void __swiftcall ContiguousArray.reserveCapacity(_:)(Swift::Int a1)
{
}

Swift::Void __swiftcall ContiguousArray._makeMutableAndUnique()()
{
}

uint64_t ContiguousArray.withUnsafeBufferPointer<A>(_:)()
{
  return MEMORY[0x1F4184F18]();
}

uint64_t ContiguousArray.append(_:)()
{
  return MEMORY[0x1F4184F48]();
}

uint64_t ContiguousArray.init()()
{
  return MEMORY[0x1F4184F68]();
}

uint64_t type metadata accessor for ContiguousArray()
{
  return MEMORY[0x1F4184F70]();
}

uint64_t ContiguousArray.init<A>(_:)()
{
  return MEMORY[0x1F4184F78]();
}

uint64_t _arrayForceCast<A, B>(_:)()
{
  return MEMORY[0x1F4185048]();
}

uint64_t _print_unlocked<A, B>(_:_:)()
{
  return MEMORY[0x1F4185050]();
}

uint64_t static FixedWidthInteger.random<A>(in:using:)()
{
  return MEMORY[0x1F4185138]();
}

uint64_t static FixedWidthInteger.random(in:)()
{
  return MEMORY[0x1F4185148]();
}

uint64_t FixedWidthInteger.init<A>(exactly:)()
{
  return MEMORY[0x1F4185158]();
}

uint64_t _assertionFailure(_:_:file:line:flags:)()
{
  return MEMORY[0x1F41852A0]();
}

uint64_t _ContiguousArrayBuffer.firstElementAddress.getter()
{
  return MEMORY[0x1F4185728]();
}

uint64_t dispatch thunk of CustomStringConvertible.description.getter()
{
  return MEMORY[0x1F4185758]();
}

uint64_t dispatch thunk of ExpressibleByFloatLiteral.init(floatLiteral:)()
{
  return MEMORY[0x1F41859B8]();
}

uint64_t isKnownUniquelyReferenced<A>(_:)()
{
  return MEMORY[0x1F41859E0]();
}

uint64_t dispatch thunk of ExpressibleByIntegerLiteral.init(integerLiteral:)()
{
  return MEMORY[0x1F4185A20]();
}

uint64_t dispatch thunk of _ExpressibleByBuiltinFloatLiteral.init(_builtinFloatLiteral:)()
{
  return MEMORY[0x1F4185D48]();
}

uint64_t dispatch thunk of _ExpressibleByBuiltinIntegerLiteral.init(_builtinIntegerLiteral:)()
{
  return MEMORY[0x1F4185D58]();
}

uint64_t ELEMENT_TYPE_OF_SET_VIOLATES_HASHABLE_REQUIREMENTS(_:)()
{
  return MEMORY[0x1F4185DD8]();
}

uint64_t static Hasher._hash(seed:_:)()
{
  return MEMORY[0x1F4185EA8]();
}

uint64_t Hasher.init(_seed:)()
{
  return MEMORY[0x1F4185EB0]();
}

Swift::Void __swiftcall Hasher._combine(_:)(Swift::UInt a1)
{
}

Swift::Void __swiftcall Hasher._combine(_:)(Swift::UInt32 a1)
{
}

Swift::Int __swiftcall Hasher._finalize()()
{
  return MEMORY[0x1F4185EF8]();
}

uint64_t _typeName(_:qualified:)()
{
  return MEMORY[0x1F4186318]();
}

uint64_t BLASGetThreading()
{
  return MEMORY[0x1F40D0ED8]();
}

int BNNSArithmeticFilterApplyBackwardBatch(void *filter, size_t batch_size, size_t number_of_inputs, const void **in, const size_t *in_stride, BNNSNDArrayDescriptor **in_delta, const size_t *in_delta_stride, const void *out, const size_t out_stride, BNNSNDArrayDescriptor *out_delta, const size_t out_delta_stride)
{
  return MEMORY[0x1F40D0EF8](filter, batch_size, number_of_inputs, in, in_stride, in_delta, in_delta_stride, out);
}

int BNNSArithmeticFilterApplyBatch(void *filter, size_t batch_size, size_t number_of_inputs, const void **in, const size_t *in_stride, void *out, size_t out_stride)
{
  return MEMORY[0x1F40D0F00](filter, batch_size, number_of_inputs, in, in_stride, out, out_stride);
}

int BNNSBandPart(const int num_lower, const int num_upper, const BNNSNDArrayDescriptor *input, BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D0F10](*(void *)&num_lower, *(void *)&num_upper, input, output, filter_params);
}

int BNNSClipByGlobalNorm(BNNSNDArrayDescriptor **dest, const BNNSNDArrayDescriptor **src, size_t count, float max_norm, float use_norm)
{
  return MEMORY[0x1F40D0F18](dest, src, count, max_norm, use_norm);
}

int BNNSClipByNorm(BNNSNDArrayDescriptor *dest, const BNNSNDArrayDescriptor *src, float max_norm, uint32_t axis_flags)
{
  return MEMORY[0x1F40D0F20](dest, src, *(void *)&axis_flags, max_norm);
}

int BNNSClipByValue(BNNSNDArrayDescriptor *dest, const BNNSNDArrayDescriptor *src, float min_val, float max_val)
{
  return MEMORY[0x1F40D0F28](dest, src, min_val, max_val);
}

int BNNSCompareTensor(const BNNSNDArrayDescriptor *in0, const BNNSNDArrayDescriptor *in1, BNNSRelationalOperator op, BNNSNDArrayDescriptor *out)
{
  return MEMORY[0x1F40D0F30](in0, in1, *(void *)&op, out);
}

int BNNSComputeNorm(BNNSNDArrayDescriptor *dest, const BNNSNDArrayDescriptor *src, BNNSNormType norm_type, uint32_t axis_flags)
{
  return MEMORY[0x1F40D0F40](dest, src, *(void *)&norm_type, *(void *)&axis_flags);
}

int BNNSComputeNormBackward(const void *in, BNNSNDArrayDescriptor *in_delta, const void *out, const BNNSNDArrayDescriptor *out_delta, BNNSNormType norm_type, uint32_t axis_flags)
{
  return MEMORY[0x1F40D0F48](in, in_delta, out, out_delta, *(void *)&norm_type, *(void *)&axis_flags);
}

int BNNSCopy(BNNSNDArrayDescriptor *dest, const BNNSNDArrayDescriptor *src, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D0F58](dest, src, filter_params);
}

void *__cdecl BNNSCreateNearestNeighbors(const unsigned int max_n_samples, const unsigned int n_features, const unsigned int n_neighbors, const BNNSDataType data_type, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D0F60](*(void *)&max_n_samples, *(void *)&n_features, *(void *)&n_neighbors, *(void *)&data_type, filter_params);
}

void *__cdecl BNNSCreateRandomGenerator(BNNSRandomGeneratorMethod method, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D0F68](*(void *)&method, filter_params);
}

void *__cdecl BNNSCreateRandomGeneratorWithSeed(BNNSRandomGeneratorMethod method, uint64_t seed, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D0F70](*(void *)&method, seed, filter_params);
}

int BNNSCropResize(const BNNSLayerParametersCropResize *layer_params, const BNNSNDArrayDescriptor *input, const BNNSNDArrayDescriptor *roi, BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D0F78](layer_params, input, roi, output, filter_params);
}

int BNNSCropResizeBackward(const BNNSLayerParametersCropResize *layer_params, BNNSNDArrayDescriptor *in_delta, const BNNSNDArrayDescriptor *roi, const BNNSNDArrayDescriptor *out_delta, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D0F80](layer_params, in_delta, roi, out_delta, filter_params);
}

void BNNSDestroyNearestNeighbors(void *knn)
{
}

void BNNSDestroyRandomGenerator(void *generator)
{
}

int BNNSDirectApplyActivationBatch(const BNNSLayerParametersActivation *layer_params, const BNNSFilterParameters *filter_params, size_t batch_size, size_t in_stride, size_t out_stride)
{
  return MEMORY[0x1F40D0FA8](layer_params, filter_params, batch_size, in_stride, out_stride);
}

int BNNSDirectApplyQuantizer(const BNNSLayerParametersQuantization *layer_params, const BNNSFilterParameters *filter_params, size_t batch_size, size_t input_stride, size_t output_stride)
{
  return MEMORY[0x1F40D0FD8](layer_params, filter_params, batch_size, input_stride, output_stride);
}

int BNNSDirectApplyReduction(const BNNSLayerParametersReduction *layer_params, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D0FE8](layer_params, filter_params);
}

int BNNSFilterApplyBackwardBatch(void *filter, size_t batch_size, const void *in, size_t in_stride, BNNSNDArrayDescriptor *in_delta, size_t in_delta_stride, const void *out, size_t out_stride, const BNNSNDArrayDescriptor *out_delta, size_t out_delta_stride, BNNSNDArrayDescriptor *weights_delta, BNNSNDArrayDescriptor *bias_delta)
{
  return MEMORY[0x1F40D1008](filter, batch_size, in, in_stride, in_delta, in_delta_stride, out, out_stride);
}

int BNNSFilterApplyBackwardTwoInputBatch(void *filter, size_t batch_size, const void *inA, size_t inA_stride, BNNSNDArrayDescriptor *inA_delta, size_t inA_delta_stride, const void *inB, size_t inB_stride, BNNSNDArrayDescriptor *inB_delta, size_t inB_delta_stride, const void *out, size_t out_stride, const BNNSNDArrayDescriptor *out_delta, size_t out_delta_stride, BNNSNDArrayDescriptor *weights_delta, BNNSNDArrayDescriptor *bias_delta)
{
  return MEMORY[0x1F40D1010](filter, batch_size, inA, inA_stride, inA_delta, inA_delta_stride, inB, inB_stride);
}

int BNNSFilterApplyBatch(void *filter, size_t batch_size, const void *in, size_t in_stride, void *out, size_t out_stride)
{
  return MEMORY[0x1F40D1018](filter, batch_size, in, in_stride, out, out_stride);
}

int BNNSFilterApplyTwoInputBatch(void *filter, size_t batch_size, const void *inA, size_t inA_stride, const void *inB, size_t inB_stride, void *out, size_t out_stride)
{
  return MEMORY[0x1F40D1030](filter, batch_size, inA, inA_stride, inB, inB_stride, out, out_stride);
}

void *__cdecl BNNSFilterCreateFusedLayer(const size_t number_of_fused_filters, const BNNSFilterType *filter_type, const void **layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D1058](number_of_fused_filters, filter_type, layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerActivation(const BNNSLayerParametersActivation *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D1060](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerArithmetic(const BNNSLayerParametersArithmetic *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D1068](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerBroadcastMatMul(const BNNSLayerParametersBroadcastMatMul *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D1080](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerConvolution(const BNNSLayerParametersConvolution *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D1090](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerDropout(const BNNSLayerParametersDropout *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D1098](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerEmbedding(const BNNSLayerParametersEmbedding *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D10A0](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerFullyConnected(const BNNSLayerParametersFullyConnected *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D10A8](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerGram(const BNNSLayerParametersGram *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D10B8](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerLoss(const void *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D10C0](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerNormalization(BNNSFilterType normType, const BNNSLayerParametersNormalization *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D10D0](*(void *)&normType, layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerPadding(const BNNSLayerParametersPadding *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D10D8](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerPermute(const BNNSLayerParametersPermute *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D10E0](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerPooling(const BNNSLayerParametersPooling *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D10E8](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerReduction(const BNNSLayerParametersReduction *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D10F8](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerResize(const BNNSLayerParametersResize *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D1100](layer_params, filter_params);
}

void *__cdecl BNNSFilterCreateLayerTransposedConvolution(const BNNSLayerParametersConvolution *layer_params, const BNNSFilterParameters *filter_params)
{
  return (void *)MEMORY[0x1F40D1110](layer_params, filter_params);
}

void BNNSFilterDestroy(void *filter)
{
}

int BNNSFusedFilterApplyBackwardBatch(void *filter, size_t batch_size, const void *in, size_t in_stride, BNNSNDArrayDescriptor *in_delta, size_t in_delta_stride, const void *out, size_t out_stride, BNNSNDArrayDescriptor *out_delta, size_t out_delta_stride, BNNSNDArrayDescriptor **delta_parameters)
{
  return MEMORY[0x1F40D1148](filter, batch_size, in, in_stride, in_delta, in_delta_stride, out, out_stride);
}

int BNNSFusedFilterApplyBackwardMultiInputBatch(void *filter, size_t batch_size, size_t number_of_inputs, const void **in, const size_t *in_stride, BNNSNDArrayDescriptor **in_delta, const size_t *in_delta_stride, const void *out, const size_t out_stride, BNNSNDArrayDescriptor *out_delta, size_t out_delta_stride, BNNSNDArrayDescriptor **delta_parameters)
{
  return MEMORY[0x1F40D1150](filter, batch_size, number_of_inputs, in, in_stride, in_delta, in_delta_stride, out);
}

int BNNSFusedFilterApplyBatch(void *filter, size_t batch_size, const void *in, size_t in_stride, void *out, size_t out_stride, BOOL training)
{
  return MEMORY[0x1F40D1158](filter, batch_size, in, in_stride, out, out_stride, training);
}

int BNNSFusedFilterApplyMultiInputBatch(void *filter, size_t batch_size, size_t number_of_inputs, const void **in, const size_t *in_stride, void *out, size_t out_stride, BOOL training)
{
  return MEMORY[0x1F40D1160](filter, batch_size, number_of_inputs, in, in_stride, out, out_stride, training);
}

int BNNSGather(size_t axis, const BNNSNDArrayDescriptor *input, const BNNSNDArrayDescriptor *indices, BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D1168](axis, input, indices, output, filter_params);
}

int BNNSGatherND(const BNNSNDArrayDescriptor *input, const BNNSNDArrayDescriptor *indices, BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D1170](input, indices, output, filter_params);
}

uint64_t BNNSGraphCompileFromFile_v2()
{
  return MEMORY[0x1F40D1178]();
}

uint64_t BNNSGraphCompileOptionsDestroy()
{
  return MEMORY[0x1F40D1188]();
}

uint64_t BNNSGraphCompileOptionsGetGenerateDebugInfo()
{
  return MEMORY[0x1F40D1198]();
}

uint64_t BNNSGraphCompileOptionsGetOptimizationPreference()
{
  return MEMORY[0x1F40D11A0]();
}

uint64_t BNNSGraphCompileOptionsGetTargetSingleThread()
{
  return MEMORY[0x1F40D11A8]();
}

uint64_t BNNSGraphCompileOptionsMakeDefault()
{
  return MEMORY[0x1F40D11B8]();
}

uint64_t BNNSGraphCompileOptionsSetGenerateDebugInfo()
{
  return MEMORY[0x1F40D11D0]();
}

uint64_t BNNSGraphCompileOptionsSetTargetSingleThread()
{
  return MEMORY[0x1F40D1238]();
}

uint64_t BNNSGraphContextDestroy_v2()
{
  return MEMORY[0x1F40D1258]();
}

uint64_t BNNSGraphContextEnableNanAndInfChecks()
{
  return MEMORY[0x1F40D1268]();
}

uint64_t BNNSGraphContextExecute_v2()
{
  return MEMORY[0x1F40D1270]();
}

uint64_t BNNSGraphContextGetTensor()
{
  return MEMORY[0x1F40D1288]();
}

uint64_t BNNSGraphContextMake()
{
  return MEMORY[0x1F40D12A8]();
}

uint64_t BNNSGraphContextSetArgumentType()
{
  return MEMORY[0x1F40D12C8]();
}

uint64_t BNNSGraphContextSetBatchSize_v2()
{
  return MEMORY[0x1F40D12D8]();
}

uint64_t BNNSGraphContextSetDynamicShapes_v2()
{
  return MEMORY[0x1F40D12E8]();
}

uint64_t BNNSGraphGetArgumentCount()
{
  return MEMORY[0x1F40D1318]();
}

uint64_t BNNSGraphGetArgumentNames()
{
  return MEMORY[0x1F40D1328]();
}

uint64_t BNNSGraphGetArgumentPosition()
{
  return MEMORY[0x1F40D1330]();
}

uint64_t BNNSGraphGetFunctionCount()
{
  return MEMORY[0x1F40D1340]();
}

uint64_t BNNSGraphGetFunctionNames()
{
  return MEMORY[0x1F40D1350]();
}

int BNNSLossFilterApplyBatch(void *filter, size_t batch_size, const void *in, size_t in_stride, const void *labels, size_t labels_stride, const void *weights, size_t weights_size, void *out, BNNSNDArrayDescriptor *in_delta, size_t in_delta_stride)
{
  return MEMORY[0x1F40D13B8](filter, batch_size, in, in_stride, labels, labels_stride, weights, weights_size);
}

int BNNSMatMul(const BOOL transA, const BOOL transB, const float alpha, const BNNSNDArrayDescriptor *inputA, const BNNSNDArrayDescriptor *inputB, const BNNSNDArrayDescriptor *output, void *workspace, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D13C0](transA, transB, inputA, inputB, output, workspace, filter_params, alpha);
}

ssize_t BNNSMatMulWorkspaceSize(const BOOL transA, const BOOL transB, const float alpha, const BNNSNDArrayDescriptor *inputA, const BNNSNDArrayDescriptor *inputB, const BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D13C8](transA, transB, inputA, inputB, output, filter_params, alpha);
}

int BNNSNDArrayFullyConnectedSparsifySparseCOO(const BNNSNDArrayDescriptor *in_dense_shape, const BNNSNDArrayDescriptor *in_indices, const BNNSNDArrayDescriptor *in_values, BNNSNDArrayDescriptor *out, const BNNSSparsityParameters *sparse_params, const size_t batch_size, void *workspace, const size_t workspace_size, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D13E0](in_dense_shape, in_indices, in_values, out, sparse_params, batch_size, workspace, workspace_size);
}

int BNNSNDArrayFullyConnectedSparsifySparseCSR(const BNNSNDArrayDescriptor *in_dense_shape, const BNNSNDArrayDescriptor *in_column_indices, const BNNSNDArrayDescriptor *in_row_starts, const BNNSNDArrayDescriptor *in_values, BNNSNDArrayDescriptor *out, const BNNSSparsityParameters *sparse_params, const size_t batch_size, void *workspace, const size_t workspace_size, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D13E8](in_dense_shape, in_column_indices, in_row_starts, in_values, out, sparse_params, batch_size, workspace);
}

size_t BNNSNDArrayGetDataSize(const BNNSNDArrayDescriptor *array)
{
  return MEMORY[0x1F40D13F0](array);
}

int BNNSNearestNeighborsGetInfo(void *knn, const int sample_number, int *indices, void *distances)
{
  return MEMORY[0x1F40D13F8](knn, *(void *)&sample_number, indices, distances);
}

int BNNSNearestNeighborsLoad(void *knn, const unsigned int n_new_samples, const void *data_ptr)
{
  return MEMORY[0x1F40D1400](knn, *(void *)&n_new_samples, data_ptr);
}

int BNNSNormalizationFilterApplyBackwardBatch(void *filter, size_t batch_size, BNNSNDArrayDescriptor *in_delta, size_t in_delta_stride, const void *out, size_t out_stride, BNNSNDArrayDescriptor *out_delta, size_t out_delta_stride, BNNSNDArrayDescriptor *beta_delta, BNNSNDArrayDescriptor *gamma_delta)
{
  return MEMORY[0x1F40D1408](filter, batch_size, in_delta, in_delta_stride, out, out_stride, out_delta, out_delta_stride);
}

int BNNSNormalizationFilterApplyBatch(void *filter, size_t batch_size, const void *in, size_t in_stride, void *out, size_t out_stride, BOOL training)
{
  return MEMORY[0x1F40D1410](filter, batch_size, in, in_stride, out, out_stride, training);
}

int BNNSOptimizerStep(BNNSOptimizerFunction function, const void *OptimizerAlgFields, size_t number_of_parameters, BNNSNDArrayDescriptor **parameters, const BNNSNDArrayDescriptor **gradients, BNNSNDArrayDescriptor **accumulators, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D1418](*(void *)&function, OptimizerAlgFields, number_of_parameters, parameters, gradients, accumulators, filter_params);
}

int BNNSPoolingFilterApplyBackwardBatch(void *filter, size_t batch_size, const void *in, size_t in_stride, BNNSNDArrayDescriptor *in_delta, size_t in_delta_stride, const void *out, size_t out_stride, BNNSNDArrayDescriptor *out_delta, size_t out_delta_stride, BNNSNDArrayDescriptor *bias_delta, const size_t *indices, size_t idx_stride)
{
  return MEMORY[0x1F40D1420](filter, batch_size, in, in_stride, in_delta, in_delta_stride, out, out_stride);
}

int BNNSPoolingFilterApplyBackwardBatchEx(void *filter, size_t batch_size, const void *in, size_t in_stride, BNNSNDArrayDescriptor *in_delta, size_t in_delta_stride, const void *out, size_t out_stride, BNNSNDArrayDescriptor *out_delta, size_t out_delta_stride, BNNSNDArrayDescriptor *bias_delta, const BNNSDataType indices_data_type, const void *indices, size_t idx_stride)
{
  return MEMORY[0x1F40D1428](filter, batch_size, in, in_stride, in_delta, in_delta_stride, out, out_stride);
}

int BNNSPoolingFilterApplyBatch(void *filter, size_t batch_size, const void *in, size_t in_stride, void *out, size_t out_stride, size_t *indices, size_t idx_stride)
{
  return MEMORY[0x1F40D1430](filter, batch_size, in, in_stride, out, out_stride, indices, idx_stride);
}

int BNNSPoolingFilterApplyBatchEx(void *filter, size_t batch_size, const void *in, size_t in_stride, void *out, size_t out_stride, const BNNSDataType indices_data_type, void *indices, size_t idx_stride)
{
  return MEMORY[0x1F40D1438](filter, batch_size, in, in_stride, out, out_stride, *(void *)&indices_data_type, indices);
}

int BNNSRandomFillNormalFloat(void *generator, BNNSNDArrayDescriptor *desc, float mean, float stddev)
{
  return MEMORY[0x1F40D1440](generator, desc, mean, stddev);
}

int BNNSRandomFillUniformFloat(void *generator, BNNSNDArrayDescriptor *desc, float a, float b)
{
  return MEMORY[0x1F40D1448](generator, desc, a, b);
}

int BNNSRandomFillUniformInt(void *generator, BNNSNDArrayDescriptor *desc, int64_t a, int64_t b)
{
  return MEMORY[0x1F40D1450](generator, desc, a, b);
}

int BNNSRandomGeneratorGetState(void *generator, size_t state_size, void *state)
{
  return MEMORY[0x1F40D1458](generator, state_size, state);
}

int BNNSRandomGeneratorSetState(void *generator, size_t state_size, void *state)
{
  return MEMORY[0x1F40D1460](generator, state_size, state);
}

size_t BNNSRandomGeneratorStateSize(void *generator)
{
  return MEMORY[0x1F40D1468](generator);
}

int BNNSScatter(size_t axis, BNNSReduceFunction op, const BNNSNDArrayDescriptor *input, const BNNSNDArrayDescriptor *indices, BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D1470](axis, *(void *)&op, input, indices, output, filter_params);
}

int BNNSScatterND(BNNSReduceFunction op, const BNNSNDArrayDescriptor *input, const BNNSNDArrayDescriptor *indices, BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D1478](*(void *)&op, input, indices, output, filter_params);
}

int BNNSShuffle(const BNNSShuffleType type, const BNNSNDArrayDescriptor *input, BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D1480](*(void *)&type, input, output, filter_params);
}

int BNNSTile(const BNNSNDArrayDescriptor *input, BNNSNDArrayDescriptor *output, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D1488](input, output, filter_params);
}

int BNNSTileBackward(BNNSNDArrayDescriptor *in_delta, const BNNSNDArrayDescriptor *out_delta, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D1490](in_delta, out_delta, filter_params);
}

int BNNSTranspose(BNNSNDArrayDescriptor *dest, const BNNSNDArrayDescriptor *src, size_t axis0, size_t axis1, const BNNSFilterParameters *filter_params)
{
  return MEMORY[0x1F40D1498](dest, src, axis0, axis1, filter_params);
}

CGColorSpaceModel CGColorSpaceGetModel(CGColorSpaceRef space)
{
  return MEMORY[0x1F40D9BE0](space);
}

CGBitmapInfo CGImageGetBitmapInfo(CGImageRef image)
{
  return MEMORY[0x1F40DA8D8](image);
}

size_t CGImageGetBitsPerComponent(CGImageRef image)
{
  return MEMORY[0x1F40DA8E0](image);
}

size_t CGImageGetBitsPerPixel(CGImageRef image)
{
  return MEMORY[0x1F40DA8E8](image);
}

CGColorSpaceRef CGImageGetColorSpace(CGImageRef image)
{
  return (CGColorSpaceRef)MEMORY[0x1F40DA908](image);
}

CGColorRenderingIntent CGImageGetRenderingIntent(CGImageRef image)
{
  return MEMORY[0x1F40DA988](image);
}

CGFloat CGRectGetHeight(CGRect rect)
{
  MEMORY[0x1F40DB250]((__n128)rect.origin, *(__n128 *)&rect.origin.y, (__n128)rect.size, *(__n128 *)&rect.size.height);
  return result;
}

CGFloat CGRectGetWidth(CGRect rect)
{
  MEMORY[0x1F40DB2A8]((__n128)rect.origin, *(__n128 *)&rect.origin.y, (__n128)rect.size, *(__n128 *)&rect.size.height);
  return result;
}

CGRect CGRectIntegral(CGRect rect)
{
  MEMORY[0x1F40DB2C0]((__n128)rect.origin, *(__n128 *)&rect.origin.y, (__n128)rect.size, *(__n128 *)&rect.size.height);
  result.size.CGFloat height = v4;
  result.size.unint64_t width = v3;
  result.origin.double y = v2;
  result.origin.double x = v1;
  return result;
}

CGRect CGRectIntersection(CGRect r1, CGRect r2)
{
  MEMORY[0x1F40DB2C8]((__n128)r1.origin, *(__n128 *)&r1.origin.y, (__n128)r1.size, *(__n128 *)&r1.size.height, (__n128)r2.origin, *(__n128 *)&r2.origin.y, (__n128)r2.size, *(__n128 *)&r2.size.height);
  result.size.CGFloat height = v5;
  result.size.unint64_t width = v4;
  result.origin.double y = v3;
  result.origin.double x = v2;
  return result;
}

BOOL CGRectIsEmpty(CGRect rect)
{
  return MEMORY[0x1F40DB2D8]((__n128)rect.origin, *(__n128 *)&rect.origin.y, (__n128)rect.size, *(__n128 *)&rect.size.height);
}

CVReturn CVPixelBufferCreateWithBytes(CFAllocatorRef allocator, size_t width, size_t height, OSType pixelFormatType, void *baseAddress, size_t bytesPerRow, CVPixelBufferReleaseBytesCallback releaseCallback, void *releaseRefCon, CFDictionaryRef pixelBufferAttributes, CVPixelBufferRef *pixelBufferOut)
{
  return MEMORY[0x1F40DFD00](allocator, width, height, *(void *)&pixelFormatType, baseAddress, bytesPerRow, releaseCallback, releaseRefCon);
}

void *__cdecl CVPixelBufferGetBaseAddressOfPlane(CVPixelBufferRef pixelBuffer, size_t planeIndex)
{
  return (void *)MEMORY[0x1F40DFD30](pixelBuffer, planeIndex);
}

size_t CVPixelBufferGetBytesPerRowOfPlane(CVPixelBufferRef pixelBuffer, size_t planeIndex)
{
  return MEMORY[0x1F40DFD48](pixelBuffer, planeIndex);
}

size_t CVPixelBufferGetHeight(CVPixelBufferRef pixelBuffer)
{
  return MEMORY[0x1F40DFD68](pixelBuffer);
}

size_t CVPixelBufferGetHeightOfPlane(CVPixelBufferRef pixelBuffer, size_t planeIndex)
{
  return MEMORY[0x1F40DFD70](pixelBuffer, planeIndex);
}

size_t CVPixelBufferGetPlaneCount(CVPixelBufferRef pixelBuffer)
{
  return MEMORY[0x1F40DFD88](pixelBuffer);
}

size_t CVPixelBufferGetWidth(CVPixelBufferRef pixelBuffer)
{
  return MEMORY[0x1F40DFD98](pixelBuffer);
}

size_t CVPixelBufferGetWidthOfPlane(CVPixelBufferRef pixelBuffer, size_t planeIndex)
{
  return MEMORY[0x1F40DFDA0](pixelBuffer, planeIndex);
}

CVReturn CVPixelBufferLockBaseAddress(CVPixelBufferRef pixelBuffer, CVPixelBufferLockFlags lockFlags)
{
  return MEMORY[0x1F40DFDB8](pixelBuffer, lockFlags);
}

CVReturn CVPixelBufferUnlockBaseAddress(CVPixelBufferRef pixelBuffer, CVPixelBufferLockFlags unlockFlags)
{
  return MEMORY[0x1F40DFE48](pixelBuffer, unlockFlags);
}

uint64_t _swift_isClassOrObjCExistentialType()
{
  return MEMORY[0x1F4186340]();
}

uint64_t _swift_stdlib_reportUnimplementedInitializer()
{
  return MEMORY[0x1F4186388]();
}

void bzero(void *a1, size_t a2)
{
}

void free(void *a1)
{
}

float log2f(float a1)
{
  MEMORY[0x1F40CC8B0](a1);
  return result;
}

size_t malloc_size(const void *ptr)
{
  return MEMORY[0x1F40CCB80](ptr);
}

void *__cdecl memcpy(void *__dst, const void *__src, size_t __n)
{
  return (void *)MEMORY[0x1F40CCCF0](__dst, __src, __n);
}

void *__cdecl memmove(void *__dst, const void *__src, size_t __len)
{
  return (void *)MEMORY[0x1F40CCD10](__dst, __src, __len);
}

uint64_t self
{
  return MEMORY[0x1F41817D0]();
}

void objc_release(id a1)
{
}

id objc_retain(id a1)
{
  return (id)MEMORY[0x1F41818B0](a1);
}

id objc_retainAutoreleasedReturnValue(id a1)
{
  return (id)MEMORY[0x1F41818C8](a1);
}

double quadrature_integrate(const quadrature_integrate_function *__f, double __a, double __b, const quadrature_integrate_options *options, quadrature_status *status, double *abs_error, size_t workspace_size, void *workspace)
{
  MEMORY[0x1F40D17C0](__f, options, status, abs_error, workspace_size, workspace, __a, __b);
  return result;
}

uint64_t swift_allocBox()
{
  return MEMORY[0x1F41863D0]();
}

uint64_t swift_allocError()
{
  return MEMORY[0x1F41863E0]();
}

uint64_t swift_allocObject()
{
  return MEMORY[0x1F41863E8]();
}

uint64_t swift_arrayDestroy()
{
  return MEMORY[0x1F4186410]();
}

uint64_t swift_arrayInitWithCopy()
{
  return MEMORY[0x1F4186418]();
}

uint64_t swift_beginAccess()
{
  return MEMORY[0x1F4186430]();
}

uint64_t swift_bridgeObjectRelease()
{
  return MEMORY[0x1F4186440]();
}

uint64_t swift_bridgeObjectRelease_n()
{
  return MEMORY[0x1F4186448]();
}

uint64_t swift_bridgeObjectRetain()
{
  return MEMORY[0x1F4186450]();
}

uint64_t swift_bridgeObjectRetain_n()
{
  return MEMORY[0x1F4186458]();
}

uint64_t swift_checkMetadataState()
{
  return MEMORY[0x1F4186460]();
}

uint64_t swift_deallocClassInstance()
{
  return MEMORY[0x1F4186488]();
}

uint64_t swift_deallocPartialClassInstance()
{
  return MEMORY[0x1F41864A0]();
}

uint64_t swift_dynamicCast()
{
  return MEMORY[0x1F41864B8]();
}

uint64_t swift_dynamicCastMetatype()
{
  return MEMORY[0x1F41864D0]();
}

uint64_t swift_endAccess()
{
  return MEMORY[0x1F4186518]();
}

uint64_t swift_errorRelease()
{
  return MEMORY[0x1F4186528]();
}

uint64_t swift_getAssociatedConformanceWitness()
{
  return MEMORY[0x1F4186538]();
}

uint64_t swift_getAssociatedTypeWitness()
{
  return MEMORY[0x1F4186540]();
}

uint64_t swift_getForeignTypeMetadata()
{
  return MEMORY[0x1F41865A8]();
}

uint64_t swift_getGenericMetadata()
{
  return MEMORY[0x1F41865E0]();
}

uint64_t swift_getTupleTypeMetadata2()
{
  return MEMORY[0x1F4186650]();
}

uint64_t swift_getTypeByMangledNameInContext2()
{
  return MEMORY[0x1F4186668]();
}

uint64_t swift_getTypeByMangledNameInContextInMetadataState2()
{
  return MEMORY[0x1F4186670]();
}

uint64_t swift_getWitnessTable()
{
  return MEMORY[0x1F4186680]();
}

uint64_t swift_initClassMetadata2()
{
  return MEMORY[0x1F4186688]();
}

uint64_t swift_initEnumMetadataSinglePayload()
{
  return MEMORY[0x1F41866A0]();
}

uint64_t swift_initStackObject()
{
  return MEMORY[0x1F41866A8]();
}

uint64_t swift_isEscapingClosureAtFileLocation()
{
  return MEMORY[0x1F41866C8]();
}

uint64_t swift_isUniquelyReferenced_nonNull_native()
{
  return MEMORY[0x1F4186700]();
}

uint64_t swift_makeBoxUnique()
{
  return MEMORY[0x1F4186710]();
}

uint64_t swift_once()
{
  return MEMORY[0x1F4186728]();
}

uint64_t swift_release()
{
  return MEMORY[0x1F4186758]();
}

uint64_t swift_retain()
{
  return MEMORY[0x1F4186770]();
}

uint64_t swift_setDeallocating()
{
  return MEMORY[0x1F4186790]();
}

uint64_t swift_slowAlloc()
{
  return MEMORY[0x1F4186798]();
}

uint64_t swift_slowDealloc()
{
  return MEMORY[0x1F41867A0]();
}

uint64_t swift_task_alloc()
{
  return MEMORY[0x1F4188250]();
}

uint64_t swift_task_dealloc()
{
  return MEMORY[0x1F4188260]();
}

uint64_t swift_unexpectedError()
{
  return MEMORY[0x1F41867D8]();
}

uint64_t swift_unknownObjectRelease()
{
  return MEMORY[0x1F41867E0]();
}

uint64_t swift_unknownObjectRetain()
{
  return MEMORY[0x1F41867F0]();
}

uint64_t swift_willThrow()
{
  return MEMORY[0x1F41868E0]();
}

vDSP_DFT_Setup vDSP_DCT_CreateSetup(vDSP_DFT_Setup __Previous, vDSP_Length __Length, vDSP_DCT_Type __Type)
{
  return (vDSP_DFT_Setup)MEMORY[0x1F40D1940](__Previous, __Length, *(void *)&__Type);
}

void vDSP_DCT_Execute(const vDSP_DFT_SetupStruct *__Setup, const float *__Input, float *__Output)
{
}

void vDSP_DFT_DestroySetup(vDSP_DFT_Setup __Setup)
{
}

void vDSP_DFT_DestroySetupD(vDSP_DFT_SetupD __Setup)
{
}

void vDSP_DFT_Interleaved_DestroySetup(vDSP_DFT_Interleaved_Setup Setup)
{
}

void vDSP_DFT_Interleaved_DestroySetupD(vDSP_DFT_Interleaved_SetupD Setup)
{
}

void vDSP_biquadm(vDSP_biquadm_Setup __Setup, const float **__X, vDSP_Stride __IX, float **__Y, vDSP_Stride __IY, vDSP_Length __N)
{
}

void vDSP_biquadmD(vDSP_biquadm_SetupD __Setup, const double **__X, vDSP_Stride __IX, double **__Y, vDSP_Stride __IY, vDSP_Length __N)
{
}

void vDSP_conv(const float *__A, vDSP_Stride __IA, const float *__F, vDSP_Stride __IF, float *__C, vDSP_Stride __IC, vDSP_Length __N, vDSP_Length __P)
{
}

void vDSP_convD(const double *__A, vDSP_Stride __IA, const double *__F, vDSP_Stride __IF, double *__C, vDSP_Stride __IC, vDSP_Length __N, vDSP_Length __P)
{
}

FFTSetup vDSP_create_fftsetup(vDSP_Length __Log2n, FFTRadix __Radix)
{
  return (FFTSetup)MEMORY[0x1F40D19F0](__Log2n, *(void *)&__Radix);
}

FFTSetupD vDSP_create_fftsetupD(vDSP_Length __Log2n, FFTRadix __Radix)
{
  return (FFTSetupD)MEMORY[0x1F40D19F8](__Log2n, *(void *)&__Radix);
}

void vDSP_ctoz(const DSPComplex *__C, vDSP_Stride __IC, const DSPSplitComplex *__Z, vDSP_Stride __IZ, vDSP_Length __N)
{
}

void vDSP_ctozD(const DSPDoubleComplex *__C, vDSP_Stride __IC, const DSPDoubleSplitComplex *__Z, vDSP_Stride __IZ, vDSP_Length __N)
{
}

void vDSP_deq22(const float *__A, vDSP_Stride __IA, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_deq22D(const double *__A, vDSP_Stride __IA, const double *__B, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_destroy_fftsetup(FFTSetup __setup)
{
}

void vDSP_destroy_fftsetupD(FFTSetupD __setup)
{
}

void vDSP_fft_zrop(FFTSetup __Setup, const DSPSplitComplex *__A, vDSP_Stride __IA, const DSPSplitComplex *__C, vDSP_Stride __IC, vDSP_Length __Log2N, FFTDirection __Direction)
{
}

void vDSP_fft_zropD(FFTSetupD __Setup, const DSPDoubleSplitComplex *__A, vDSP_Stride __IA, const DSPDoubleSplitComplex *__C, vDSP_Stride __IC, vDSP_Length __Log2N, FFTDirection __Direction)
{
}

void vDSP_normalize(const float *__A, vDSP_Stride __IA, float *__C, vDSP_Stride __IC, float *__Mean, float *__StandardDeviation, vDSP_Length __N)
{
}

void vDSP_normalizeD(const double *__A, vDSP_Stride __IA, double *__C, vDSP_Stride __IC, double *__Mean, double *__StandardDeviation, vDSP_Length __N)
{
}

void vDSP_svdiv(const float *__A, const float *__B, vDSP_Stride __IB, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_svdivD(const double *__A, const double *__B, vDSP_Stride __IB, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vclip(const float *__A, vDSP_Stride __IA, const float *__B, const float *__C, float *__D, vDSP_Stride __ID, vDSP_Length __N)
{
}

void vDSP_vclipc(const float *__A, vDSP_Stride __IA, const float *__B, const float *__C, float *__D, vDSP_Stride __ID, vDSP_Length __N, vDSP_Length *__NLow, vDSP_Length *__NHigh)
{
}

void vDSP_vclipcD(const double *__A, vDSP_Stride __IA, const double *__B, const double *__C, double *__D, vDSP_Stride __ID, vDSP_Length __N, vDSP_Length *__NLow, vDSP_Length *__NHigh)
{
}

void vDSP_vdbcon(const float *__A, vDSP_Stride __IA, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N, unsigned int __F)
{
}

void vDSP_vdbconD(const double *__A, vDSP_Stride __IA, const double *__B, double *__C, vDSP_Stride __IC, vDSP_Length __N, unsigned int __F)
{
}

void vDSP_vfill(const float *__A, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vfillD(const double *__A, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vintb(const float *__A, vDSP_Stride __IA, const float *__B, vDSP_Stride __IB, const float *__C, float *__D, vDSP_Stride __ID, vDSP_Length __N)
{
}

void vDSP_vintbD(const double *__A, vDSP_Stride __IA, const double *__B, vDSP_Stride __IB, const double *__C, double *__D, vDSP_Stride __ID, vDSP_Length __N)
{
}

void vDSP_vlim(const float *__A, vDSP_Stride __IA, const float *__B, const float *__C, float *__D, vDSP_Stride __ID, vDSP_Length __N)
{
}

void vDSP_vramp(const float *__A, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vrampD(const double *__A, const double *__B, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vrampmul(const float *__I, vDSP_Stride __IS, float *__Start, const float *__Step, float *__O, vDSP_Stride __OS, vDSP_Length __N)
{
}

void vDSP_vrampmul2(const float *__I0, const float *__I1, vDSP_Stride __IS, float *__Start, const float *__Step, float *__O0, float *__O1, vDSP_Stride __OS, vDSP_Length __N)
{
}

void vDSP_vrampmul2D(const double *__I0, const double *__I1, vDSP_Stride __IS, double *__Start, const double *__Step, double *__O0, double *__O1, vDSP_Stride __OS, vDSP_Length __N)
{
}

void vDSP_vrampmulD(const double *__I, vDSP_Stride __IS, double *__Start, const double *__Step, double *__O, vDSP_Stride __OS, vDSP_Length __N)
{
}

void vDSP_vrsum(const float *__A, vDSP_Stride __IA, const float *__S, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vrsumD(const double *__A, vDSP_Stride __IA, const double *__S, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vsdiv(const float *__A, vDSP_Stride __IA, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vsimps(const float *__A, vDSP_Stride __IA, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vsimpsD(const double *__A, vDSP_Stride __IA, const double *__B, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vsma(const float *__A, vDSP_Stride __IA, const float *__B, const float *__C, vDSP_Stride __IC, float *__D, vDSP_Stride __ID, vDSP_Length __N)
{
}

void vDSP_vsmaD(const double *__A, vDSP_Stride __IA, const double *__B, const double *__C, vDSP_Stride __IC, double *__D, vDSP_Stride __ID, vDSP_Length __N)
{
}

void vDSP_vsmsa(const float *__A, vDSP_Stride __IA, const float *__B, const float *__C, float *__D, vDSP_Stride __ID, vDSP_Length __N)
{
}

void vDSP_vsmsaD(const double *__A, vDSP_Stride __IA, const double *__B, const double *__C, double *__D, vDSP_Stride __ID, vDSP_Length __N)
{
}

void vDSP_vsmsma(const float *__A, vDSP_Stride __IA, const float *__B, const float *__C, vDSP_Stride __IC, const float *__D, float *__E, vDSP_Stride __IE, vDSP_Length __N)
{
}

void vDSP_vsmsmaD(const double *__A, vDSP_Stride __IA, const double *__B, const double *__C, vDSP_Stride __IC, const double *__D, double *__E, vDSP_Stride __IE, vDSP_Length __N)
{
}

void vDSP_vsmul(const float *__A, vDSP_Stride __IA, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vsmulD(const double *__A, vDSP_Stride __IA, const double *__B, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vtabi(const float *__A, vDSP_Stride __IA, const float *__S1, const float *__S2, const float *__C, vDSP_Length __M, float *__D, vDSP_Stride __ID, vDSP_Length __N)
{
}

void vDSP_vtabiD(const double *__A, vDSP_Stride __IA, const double *__S1, const double *__S2, const double *__C, vDSP_Length __M, double *__D, vDSP_Stride __ID, vDSP_Length __N)
{
}

void vDSP_vthr(const float *__A, vDSP_Stride __IA, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vthrD(const double *__A, vDSP_Stride __IA, const double *__B, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vthres(const float *__A, vDSP_Stride __IA, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vthresD(const double *__A, vDSP_Stride __IA, const double *__B, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vthrsc(const float *__A, vDSP_Stride __IA, const float *__B, const float *__C, float *__D, vDSP_Stride __ID, vDSP_Length __N)
{
}

void vDSP_vthrscD(const double *__A, vDSP_Stride __IA, const double *__B, const double *__C, double *__D, vDSP_Stride __ID, vDSP_Length __N)
{
}

void vDSP_vtrapz(const float *__A, vDSP_Stride __IA, const float *__B, float *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_vtrapzD(const double *__A, vDSP_Stride __IA, const double *__B, double *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_ztoc(const DSPSplitComplex *__Z, vDSP_Stride __IZ, DSPComplex *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

void vDSP_ztocD(const DSPDoubleSplitComplex *__Z, vDSP_Stride __IZ, DSPDoubleComplex *__C, vDSP_Stride __IC, vDSP_Length __N)
{
}

vImage_Error vImageAffineWarpCG_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_CGAffineTransform *transform, Pixel_8 backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D21F0](src, dest, tempBuffer, transform, backColor, *(void *)&flags);
}

vImage_Error vImageAffineWarpCG_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_CGAffineTransform *transform, Pixel_F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D21F8](src, dest, tempBuffer, transform, *(void *)&flags, backColor);
}

vImage_Error vImageAffineWarpD_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_AffineTransform_Double *transform, const Pixel_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2200](src, dest, tempBuffer, transform, backColor, *(void *)&flags);
}

vImage_Error vImageAlphaBlend_ARGB8888(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2238](srcTop, srcBottom, dest, *(void *)&flags);
}

vImage_Error vImageAlphaBlend_ARGBFFFF(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2240](srcTop, srcBottom, dest, *(void *)&flags);
}

vImage_Error vImageAlphaBlend_NonpremultipliedToPremultiplied_ARGB8888(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2248](srcTop, srcBottom, dest, *(void *)&flags);
}

vImage_Error vImageAlphaBlend_NonpremultipliedToPremultiplied_ARGBFFFF(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2250](srcTop, srcBottom, dest, *(void *)&flags);
}

vImage_Error vImageBoxConvolve_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, uint32_t kernel_height, uint32_t kernel_width, const Pixel_8888 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2258](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, *(void *)&kernel_height, *(void *)&kernel_width, backgroundColor);
}

vImage_Error vImageBoxConvolve_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, uint32_t kernel_height, uint32_t kernel_width, Pixel_8 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2268](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, *(void *)&kernel_height, *(void *)&kernel_width, backgroundColor);
}

vImage_Error vImageBuffer_CopyToCVPixelBuffer(const vImage_Buffer *buffer, const vImage_CGImageFormat *bufferFormat, CVPixelBufferRef cvPixelBuffer, vImageCVImageFormatRef cvImageFormat, const CGFloat *backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2290](buffer, bufferFormat, cvPixelBuffer, cvImageFormat, backgroundColor, *(void *)&flags);
}

CGSize vImageBuffer_GetSize(const vImage_Buffer *buf)
{
  MEMORY[0x1F40D2298](buf);
  result.CGFloat height = v2;
  result.unint64_t width = v1;
  return result;
}

vImage_Error vImageBuffer_Init(vImage_Buffer *buf, vImagePixelCount height, vImagePixelCount width, uint32_t pixelBits, vImage_Flags flags)
{
  return MEMORY[0x1F40D22A0](buf, height, width, *(void *)&pixelBits, *(void *)&flags);
}

vImage_Error vImageBuffer_InitForCopyFromCVPixelBuffer(vImage_Buffer *buffers, const vImageConverterRef converter, const CVPixelBufferRef pixelBuffer, vImage_Flags flags)
{
  return MEMORY[0x1F40D22B0](buffers, converter, pixelBuffer, *(void *)&flags);
}

vImage_Error vImageBuffer_InitWithCGImage(vImage_Buffer *buf, vImage_CGImageFormat *format, const CGFloat *backgroundColor, CGImageRef image, vImage_Flags flags)
{
  return MEMORY[0x1F40D22B8](buf, format, backgroundColor, image, *(void *)&flags);
}

vImage_Error vImageBuffer_InitWithCVPixelBuffer(vImage_Buffer *buffer, vImage_CGImageFormat *desiredFormat, CVPixelBufferRef cvPixelBuffer, vImageCVImageFormatRef cvImageFormat, const CGFloat *backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D22C8](buffer, desiredFormat, cvPixelBuffer, cvImageFormat, backgroundColor, *(void *)&flags);
}

uint32_t vImageCGImageFormat_GetComponentCount(const vImage_CGImageFormat *format)
{
  return MEMORY[0x1F40D2310](format);
}

Boolean vImageCGImageFormat_IsEqual(const vImage_CGImageFormat *f1, const vImage_CGImageFormat *f2)
{
  return MEMORY[0x1F40D2318](f1, f2);
}

vImageCVImageFormatRef vImageCVImageFormat_Create(uint32_t imageFormatType, const vImage_ARGBToYpCbCrMatrix *matrix, CFStringRef cvImageBufferChromaLocation, CGColorSpaceRef baseColorspace, int alphaIsOneHint)
{
  return (vImageCVImageFormatRef)MEMORY[0x1F40D2320](*(void *)&imageFormatType, matrix, cvImageBufferChromaLocation, baseColorspace, *(void *)&alphaIsOneHint);
}

vImageCVImageFormatRef vImageCVImageFormat_CreateWithCVPixelBuffer(CVPixelBufferRef buffer)
{
  return (vImageCVImageFormatRef)MEMORY[0x1F40D2328](buffer);
}

int vImageCVImageFormat_GetAlphaHint(vImageConstCVImageFormatRef format)
{
  return MEMORY[0x1F40D2330](format);
}

uint32_t vImageCVImageFormat_GetChannelCount(vImageConstCVImageFormatRef format)
{
  return MEMORY[0x1F40D2338](format);
}

const vImageChannelDescription *__cdecl vImageCVImageFormat_GetChannelDescription(vImageConstCVImageFormatRef format, vImageBufferTypeCode type)
{
  return (const vImageChannelDescription *)MEMORY[0x1F40D2340](format, *(void *)&type);
}

const vImageBufferTypeCode *__cdecl vImageCVImageFormat_GetChannelNames(vImageConstCVImageFormatRef format)
{
  return (const vImageBufferTypeCode *)MEMORY[0x1F40D2348](format);
}

CFStringRef vImageCVImageFormat_GetChromaSiting(vImageConstCVImageFormatRef format)
{
  return (CFStringRef)MEMORY[0x1F40D2350](format);
}

CGColorSpaceRef vImageCVImageFormat_GetColorSpace(vImageConstCVImageFormatRef format)
{
  return (CGColorSpaceRef)MEMORY[0x1F40D2358](format);
}

uint32_t vImageCVImageFormat_GetFormatCode(vImageConstCVImageFormatRef format)
{
  return MEMORY[0x1F40D2360](format);
}

vImage_Error vImageCVImageFormat_SetAlphaHint(vImageCVImageFormatRef format, int alphaIsOne)
{
  return MEMORY[0x1F40D2370](format, *(void *)&alphaIsOne);
}

vImage_Error vImageCVImageFormat_SetChromaSiting(vImageCVImageFormatRef format, CFStringRef siting)
{
  return MEMORY[0x1F40D2378](format, siting);
}

vImage_Error vImageCVImageFormat_SetColorSpace(vImageCVImageFormatRef format, CGColorSpaceRef colorspace)
{
  return MEMORY[0x1F40D2380](format, colorspace);
}

vImage_Error vImageConvert_16UToF(const vImage_Buffer *src, const vImage_Buffer *dest, float offset, float scale, vImage_Flags flags)
{
  return MEMORY[0x1F40D23B0](src, dest, *(void *)&flags, offset, scale);
}

vImage_Error vImageConvert_16Uto16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D23C0](src, dest, *(void *)&flags);
}

vImage_Error vImageConvert_420Yp8_CbCr8ToARGB8888(const vImage_Buffer *srcYp, const vImage_Buffer *srcCbCr, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], const uint8_t alpha, vImage_Flags flags)
{
  return MEMORY[0x1F40D23E8](srcYp, srcCbCr, dest, info, permuteMap, alpha, *(void *)&flags);
}

vImage_Error vImageConvert_ARGB16UToARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const uint8_t permuteMap[4], uint8_t copyMask, const Pixel_8888 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2480](src, dest, permuteMap, copyMask, backgroundColor, *(void *)&flags);
}

vImage_Error vImageConvert_ARGB16UtoPlanar16U(const vImage_Buffer *argbSrc, const vImage_Buffer *aDest, const vImage_Buffer *rDest, const vImage_Buffer *gDest, const vImage_Buffer *bDest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2498](argbSrc, aDest, rDest, gDest, bDest, *(void *)&flags);
}

vImage_Error vImageConvert_ARGB8888ToARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, const uint8_t permuteMap[4], uint8_t copyMask, const Pixel_ARGB_16U backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2508](src, dest, permuteMap, copyMask, backgroundColor, *(void *)&flags);
}

vImage_Error vImageConvert_ARGB8888toPlanar8(const vImage_Buffer *srcARGB, const vImage_Buffer *destA, const vImage_Buffer *destR, const vImage_Buffer *destG, const vImage_Buffer *destB, vImage_Flags flags)
{
  return MEMORY[0x1F40D2528](srcARGB, destA, destR, destG, destB, *(void *)&flags);
}

vImage_Error vImageConvert_ARGB8888toPlanarF(const vImage_Buffer *src, const vImage_Buffer *alpha, const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, const Pixel_FFFF maxFloat, const Pixel_FFFF minFloat, vImage_Flags flags)
{
  return MEMORY[0x1F40D2530](src, alpha, red, green, blue, maxFloat, minFloat, *(void *)&flags);
}

vImage_Error vImageConvert_ARGB8888toRGB888(const vImage_Buffer *a1, const vImage_Buffer *a2, vImage_Flags a3)
{
  return MEMORY[0x1F40D2538](a1, a2, *(void *)&a3);
}

vImage_Error vImageConvert_ARGBFFFFtoARGB8888_dithered(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_FFFF maxFloat, const Pixel_FFFF minFloat, int dither, const uint8_t permuteMap[4], vImage_Flags flags)
{
  return MEMORY[0x1F40D2548](src, dest, maxFloat, minFloat, *(void *)&dither, permuteMap, *(void *)&flags);
}

vImage_Error vImageConvert_ARGBFFFFtoPlanar8(const vImage_Buffer *src, const vImage_Buffer *alpha, const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, const Pixel_FFFF maxFloat, const Pixel_FFFF minFloat, vImage_Flags flags)
{
  return MEMORY[0x1F40D2550](src, alpha, red, green, blue, maxFloat, minFloat, *(void *)&flags);
}

vImage_Error vImageConvert_ARGBFFFFtoPlanarF(const vImage_Buffer *srcARGB, const vImage_Buffer *destA, const vImage_Buffer *destR, const vImage_Buffer *destG, const vImage_Buffer *destB, vImage_Flags flags)
{
  return MEMORY[0x1F40D2560](srcARGB, destA, destR, destG, destB, *(void *)&flags);
}

vImage_Error vImageConvert_ARGBFFFFtoRGBFFF(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2568](src, dest, *(void *)&flags);
}

vImage_Error vImageConvert_AnyToAny(const vImageConverterRef converter, const vImage_Buffer *srcs, const vImage_Buffer *dests, void *tempBuffer, vImage_Flags flags)
{
  return MEMORY[0x1F40D2578](converter, srcs, dests, tempBuffer, *(void *)&flags);
}

vImage_Error vImageConvert_ChunkyToPlanar8(const void *srcChannels[], const vImage_Buffer *destPlanarBuffers[], unsigned int channelCount, size_t srcStrideBytes, vImagePixelCount srcWidth, vImagePixelCount srcHeight, size_t srcRowBytes, vImage_Flags flags)
{
  return MEMORY[0x1F40D2590](srcChannels, destPlanarBuffers, *(void *)&channelCount, srcStrideBytes, srcWidth, srcHeight, srcRowBytes, *(void *)&flags);
}

vImage_Error vImageConvert_ChunkyToPlanarF(const void *srcChannels[], const vImage_Buffer *destPlanarBuffers[], unsigned int channelCount, size_t srcStrideBytes, vImagePixelCount srcWidth, vImagePixelCount srcHeight, size_t srcRowBytes, vImage_Flags flags)
{
  return MEMORY[0x1F40D2598](srcChannels, destPlanarBuffers, *(void *)&channelCount, srcStrideBytes, srcWidth, srcHeight, srcRowBytes, *(void *)&flags);
}

vImage_Error vImageConvert_FTo16U(const vImage_Buffer *src, const vImage_Buffer *dest, float offset, float scale, vImage_Flags flags)
{
  return MEMORY[0x1F40D25A0](src, dest, *(void *)&flags, offset, scale);
}

vImage_Error vImageConvert_Planar16FtoPlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D25B0](src, dest, *(void *)&flags);
}

vImage_Error vImageConvert_Planar16UtoARGB16U(const vImage_Buffer *aSrc, const vImage_Buffer *rSrc, const vImage_Buffer *gSrc, const vImage_Buffer *bSrc, const vImage_Buffer *argbDest, vImage_Flags flags)
{
  return MEMORY[0x1F40D25C0](aSrc, rSrc, gSrc, bSrc, argbDest, *(void *)&flags);
}

vImage_Error vImageConvert_Planar8toARGB8888(const vImage_Buffer *srcA, const vImage_Buffer *srcR, const vImage_Buffer *srcG, const vImage_Buffer *srcB, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2600](srcA, srcR, srcG, srcB, dest, *(void *)&flags);
}

vImage_Error vImageConvert_Planar8toPlanar16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2610](src, dest, *(void *)&flags);
}

vImage_Error vImageConvert_Planar8toPlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, Pixel_F maxFloat, Pixel_F minFloat, vImage_Flags flags)
{
  return MEMORY[0x1F40D2618](src, dest, *(void *)&flags, maxFloat, minFloat);
}

vImage_Error vImageConvert_Planar8toRGB888(const vImage_Buffer *planarRed, const vImage_Buffer *planarGreen, const vImage_Buffer *planarBlue, const vImage_Buffer *rgbDest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2620](planarRed, planarGreen, planarBlue, rgbDest, *(void *)&flags);
}

vImage_Error vImageConvert_PlanarFToARGB8888(const vImage_Buffer *alpha, const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, const vImage_Buffer *dest, const Pixel_FFFF maxFloat, const Pixel_FFFF minFloat, vImage_Flags flags)
{
  return MEMORY[0x1F40D2628](alpha, red, green, blue, dest, maxFloat, minFloat, *(void *)&flags);
}

vImage_Error vImageConvert_PlanarFtoARGBFFFF(const vImage_Buffer *srcA, const vImage_Buffer *srcR, const vImage_Buffer *srcG, const vImage_Buffer *srcB, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2638](srcA, srcR, srcG, srcB, dest, *(void *)&flags);
}

vImage_Error vImageConvert_PlanarFtoPlanar16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2648](src, dest, *(void *)&flags);
}

vImage_Error vImageConvert_PlanarFtoPlanar8(const vImage_Buffer *src, const vImage_Buffer *dest, Pixel_F maxFloat, Pixel_F minFloat, vImage_Flags flags)
{
  return MEMORY[0x1F40D2650](src, dest, *(void *)&flags, maxFloat, minFloat);
}

vImage_Error vImageConvert_PlanarFtoRGBFFF(const vImage_Buffer *planarRed, const vImage_Buffer *planarGreen, const vImage_Buffer *planarBlue, const vImage_Buffer *rgbDest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2658](planarRed, planarGreen, planarBlue, rgbDest, *(void *)&flags);
}

vImage_Error vImageConvert_PlanarToChunky8(const vImage_Buffer *srcPlanarBuffers[], void *destChannels[], unsigned int channelCount, size_t destStrideBytes, vImagePixelCount destWidth, vImagePixelCount destHeight, size_t destRowBytes, vImage_Flags flags)
{
  return MEMORY[0x1F40D2660](srcPlanarBuffers, destChannels, *(void *)&channelCount, destStrideBytes, destWidth, destHeight, destRowBytes, *(void *)&flags);
}

vImage_Error vImageConvert_PlanarToChunkyF(const vImage_Buffer *srcPlanarBuffers[], void *destChannels[], unsigned int channelCount, size_t destStrideBytes, vImagePixelCount destWidth, vImagePixelCount destHeight, size_t destRowBytes, vImage_Flags flags)
{
  return MEMORY[0x1F40D2668](srcPlanarBuffers, destChannels, *(void *)&channelCount, destStrideBytes, destWidth, destHeight, destRowBytes, *(void *)&flags);
}

vImage_Error vImageConvert_RGB888toPlanar8(const vImage_Buffer *rgbSrc, const vImage_Buffer *redDest, const vImage_Buffer *greenDest, const vImage_Buffer *blueDest, vImage_Flags flags)
{
  return MEMORY[0x1F40D26C8](rgbSrc, redDest, greenDest, blueDest, *(void *)&flags);
}

vImage_Error vImageConvert_RGBA8888toRGB888(const vImage_Buffer *a1, const vImage_Buffer *a2, vImage_Flags a3)
{
  return MEMORY[0x1F40D26F8](a1, a2, *(void *)&a3);
}

vImage_Error vImageConvert_RGBAFFFFtoRGBFFF(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2700](src, dest, *(void *)&flags);
}

vImage_Error vImageConvert_RGBFFFtoPlanarF(const vImage_Buffer *rgbSrc, const vImage_Buffer *redDest, const vImage_Buffer *greenDest, const vImage_Buffer *blueDest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2708](rgbSrc, redDest, greenDest, blueDest, *(void *)&flags);
}

vImage_Error vImageConvert_RGBFFFtoRGB888_dithered(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_F maxFloat[3], const Pixel_F minFloat[3], int dither, vImage_Flags flags)
{
  return MEMORY[0x1F40D2710](src, dest, maxFloat, minFloat, *(void *)&dither, *(void *)&flags);
}

vImageConverterRef vImageConverter_CreateForCGToCVImageFormat(const vImage_CGImageFormat *srcFormat, vImageCVImageFormatRef destFormat, const CGFloat *backgroundColor, vImage_Flags flags, vImage_Error *error)
{
  return (vImageConverterRef)MEMORY[0x1F40D2790](srcFormat, destFormat, backgroundColor, *(void *)&flags, error);
}

vImageConverterRef vImageConverter_CreateForCVToCGImageFormat(vImageCVImageFormatRef srcFormat, const vImage_CGImageFormat *destFormat, const CGFloat *backgroundColor, vImage_Flags flags, vImage_Error *error)
{
  return (vImageConverterRef)MEMORY[0x1F40D2798](srcFormat, destFormat, backgroundColor, *(void *)&flags, error);
}

vImageConverterRef vImageConverter_CreateWithCGColorConversionInfo(CGColorConversionInfoRef colorConversionInfoRef, const vImage_CGImageFormat *sFormat, const vImage_CGImageFormat *dFormat, const CGFloat *bg, vImage_Flags flags, vImage_Error *error)
{
  return (vImageConverterRef)MEMORY[0x1F40D27A0](colorConversionInfoRef, sFormat, dFormat, bg, *(void *)&flags, error);
}

vImageConverterRef vImageConverter_CreateWithCGImageFormat(const vImage_CGImageFormat *srcFormat, const vImage_CGImageFormat *destFormat, const CGFloat *backgroundColor, vImage_Flags flags, vImage_Error *error)
{
  return (vImageConverterRef)MEMORY[0x1F40D27A8](srcFormat, destFormat, backgroundColor, *(void *)&flags, error);
}

vImage_Error vImageConverter_MustOperateOutOfPlace(const vImageConverterRef converter, const vImage_Buffer *srcs, const vImage_Buffer *dests, vImage_Flags flags)
{
  return MEMORY[0x1F40D27C8](converter, srcs, dests, *(void *)&flags);
}

vImage_Error vImageConvolveFloatKernel_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernelHeight, uint32_t kernelWidth, float bias, const Pixel_8888 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D27E0](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(void *)&kernelHeight, *(void *)&kernelWidth, bias);
}

vImage_Error vImageConvolveMultiKernel_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const int16_t *kernels[4], uint32_t kernel_height, uint32_t kernel_width, const int32_t divisors[4], const int32_t biases[4], const Pixel_8888 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D27E8](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernels, *(void *)&kernel_height, *(void *)&kernel_width);
}

vImage_Error vImageConvolveWithBias_ARGB16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, float bias, const Pixel_ARGB_16F backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D27F0](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(void *)&kernel_height, *(void *)&kernel_width, bias);
}

vImage_Error vImageConvolveWithBias_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const int16_t *kernel, uint32_t kernel_height, uint32_t kernel_width, int32_t divisor, int32_t bias, const Pixel_8888 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D27F8](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(void *)&kernel_height, *(void *)&kernel_width);
}

vImage_Error vImageConvolveWithBias_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, float bias, const Pixel_FFFF backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2800](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(void *)&kernel_height, *(void *)&kernel_width, bias);
}

vImage_Error vImageConvolveWithBias_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, float bias, Pixel_16F backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2808](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(void *)&kernel_height, *(void *)&kernel_width, bias);
}

vImage_Error vImageConvolveWithBias_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const int16_t *kernel, uint32_t kernel_height, uint32_t kernel_width, int32_t divisor, int32_t bias, Pixel_8 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2810](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(void *)&kernel_height, *(void *)&kernel_width);
}

vImage_Error vImageConvolveWithBias_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, float bias, Pixel_F backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2818](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(void *)&kernel_height, *(void *)&kernel_width, bias, backgroundColor);
}

vImage_Error vImageConvolve_ARGB16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, const Pixel_ARGB_16F backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2820](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(void *)&kernel_height, *(void *)&kernel_width);
}

vImage_Error vImageConvolve_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const int16_t *kernel, uint32_t kernel_height, uint32_t kernel_width, int32_t divisor, const Pixel_8888 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2828](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(void *)&kernel_height, *(void *)&kernel_width);
}

vImage_Error vImageConvolve_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, const Pixel_FFFF backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2830](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(void *)&kernel_height, *(void *)&kernel_width);
}

vImage_Error vImageConvolve_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, Pixel_16F backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2838](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(void *)&kernel_height, *(void *)&kernel_width);
}

vImage_Error vImageConvolve_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const int16_t *kernel, uint32_t kernel_height, uint32_t kernel_width, int32_t divisor, Pixel_8 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2848](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(void *)&kernel_height, *(void *)&kernel_width);
}

vImage_Error vImageConvolve_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, Pixel_F backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2850](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, *(void *)&kernel_height, *(void *)&kernel_width, backgroundColor);
}

vImage_Error vImageCopyBuffer(const vImage_Buffer *src, const vImage_Buffer *dest, size_t pixelSize, vImage_Flags flags)
{
  return MEMORY[0x1F40D2858](src, dest, pixelSize, *(void *)&flags);
}

CGImageRef vImageCreateCGImageFromBuffer(const vImage_Buffer *buf, const vImage_CGImageFormat *format, void (__cdecl *callback)(void *, void *), void *userData, vImage_Flags flags, vImage_Error *error)
{
  return (CGImageRef)MEMORY[0x1F40D2860](buf, format, callback, userData, *(void *)&flags, error);
}

GammaFunction vImageCreateGammaFunction(float gamma, int gamma_type, vImage_Flags flags)
{
  return (GammaFunction)MEMORY[0x1F40D2868](*(void *)&gamma_type, *(void *)&flags, gamma);
}

vImage_Error vImageDilate_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const unsigned __int8 *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1F40D2878](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, *(void *)&flags);
}

vImage_Error vImageDilate_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1F40D2880](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, *(void *)&flags);
}

vImage_Error vImageDilate_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const unsigned __int8 *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1F40D2888](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, *(void *)&flags);
}

vImage_Error vImageDilate_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1F40D2890](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, *(void *)&flags);
}

vImage_Error vImageErode_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const unsigned __int8 *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1F40D28B8](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, *(void *)&flags);
}

vImage_Error vImageErode_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1F40D28C0](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, *(void *)&flags);
}

vImage_Error vImageErode_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const unsigned __int8 *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1F40D28C8](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, *(void *)&flags);
}

vImage_Error vImageErode_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1F40D28D0](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, *(void *)&flags);
}

vImage_Error vImageFlatten_ARGB8888ToRGB888(const vImage_Buffer *a1, const vImage_Buffer *a2, const Pixel_8888 a3, BOOL a4, vImage_Flags a5)
{
  return MEMORY[0x1F40D28F0](a1, a2, a3, a4, *(void *)&a5);
}

vImage_Error vImageFlatten_ARGBFFFFToRGBFFF(const vImage_Buffer *a1, const vImage_Buffer *a2, const Pixel_FFFF a3, BOOL a4, vImage_Flags a5)
{
  return MEMORY[0x1F40D28F8](a1, a2, a3, a4, *(void *)&a5);
}

vImage_Error vImageFlatten_RGBA8888ToRGB888(const vImage_Buffer *a1, const vImage_Buffer *a2, const Pixel_8888 a3, BOOL a4, vImage_Flags a5)
{
  return MEMORY[0x1F40D2908](a1, a2, a3, a4, *(void *)&a5);
}

vImage_Error vImageFlatten_RGBAFFFFToRGBFFF(const vImage_Buffer *a1, const vImage_Buffer *a2, const Pixel_FFFF a3, BOOL a4, vImage_Flags a5)
{
  return MEMORY[0x1F40D2910](a1, a2, a3, a4, *(void *)&a5);
}

vImage_Error vImageFloodFill_ARGB16U(const vImage_Buffer *srcDest, void *tempBuffer, vImagePixelCount seedX, vImagePixelCount seedY, Pixel_ARGB_16U newValue, int connectivity, vImage_Flags flags)
{
  return MEMORY[0x1F40D2918](srcDest, tempBuffer, seedX, seedY, newValue, *(void *)&connectivity, *(void *)&flags);
}

vImage_Error vImageFloodFill_ARGB8888(const vImage_Buffer *srcDest, void *tempBuffer, vImagePixelCount seedX, vImagePixelCount seedY, Pixel_8888 newValue, int connectivity, vImage_Flags flags)
{
  return MEMORY[0x1F40D2920](srcDest, tempBuffer, seedX, seedY, newValue, *(void *)&connectivity, *(void *)&flags);
}

vImage_Error vImageFloodFill_Planar16U(const vImage_Buffer *srcDest, void *tempBuffer, vImagePixelCount seedX, vImagePixelCount seedY, Pixel_16U newValue, int connectivity, vImage_Flags flags)
{
  return MEMORY[0x1F40D2928](srcDest, tempBuffer, seedX, seedY, newValue, *(void *)&connectivity, *(void *)&flags);
}

vImage_Error vImageFloodFill_Planar8(const vImage_Buffer *srcDest, void *tempBuffer, vImagePixelCount seedX, vImagePixelCount seedY, Pixel_8 newValue, int connectivity, vImage_Flags flags)
{
  return MEMORY[0x1F40D2930](srcDest, tempBuffer, seedX, seedY, newValue, *(void *)&connectivity, *(void *)&flags);
}

vImage_Error vImageGamma_Planar8toPlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, const GammaFunction gamma, vImage_Flags flags)
{
  return MEMORY[0x1F40D2938](src, dest, gamma, *(void *)&flags);
}

vImage_Error vImageGamma_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, const GammaFunction gamma, vImage_Flags flags)
{
  return MEMORY[0x1F40D2940](src, dest, gamma, *(void *)&flags);
}

vImage_Error vImageGetPerspectiveWarp(const float srcPoints[4][2], const float destPoints[4][2], vImage_PerpsectiveTransform *transform, vImage_Flags flags)
{
  return MEMORY[0x1F40D2948](srcPoints, destPoints, transform, *(void *)&flags);
}

vImage_Error vImageHistogramCalculation_ARGB8888(const vImage_Buffer *src, vImagePixelCount *histogram[4], vImage_Flags flags)
{
  return MEMORY[0x1F40D2950](src, histogram, *(void *)&flags);
}

vImage_Error vImageHistogramCalculation_ARGBFFFF(const vImage_Buffer *src, vImagePixelCount *histogram[4], unsigned int histogram_entries, Pixel_F minVal, Pixel_F maxVal, vImage_Flags flags)
{
  return MEMORY[0x1F40D2958](src, histogram, *(void *)&histogram_entries, *(void *)&flags, minVal, maxVal);
}

vImage_Error vImageHistogramCalculation_Planar8(const vImage_Buffer *src, vImagePixelCount *histogram, vImage_Flags flags)
{
  return MEMORY[0x1F40D2960](src, histogram, *(void *)&flags);
}

vImage_Error vImageHistogramCalculation_PlanarF(const vImage_Buffer *src, vImagePixelCount *histogram, unsigned int histogram_entries, Pixel_F minVal, Pixel_F maxVal, vImage_Flags flags)
{
  return MEMORY[0x1F40D2968](src, histogram, *(void *)&histogram_entries, *(void *)&flags, minVal, maxVal);
}

vImage_Error vImageHistogramSpecification_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const vImagePixelCount *desired_histogram[4], vImage_Flags flags)
{
  return MEMORY[0x1F40D2970](src, dest, desired_histogram, *(void *)&flags);
}

vImage_Error vImageHistogramSpecification_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImagePixelCount *desired_histogram[4], unsigned int histogram_entries, Pixel_F minVal, Pixel_F maxVal, vImage_Flags flags)
{
  return MEMORY[0x1F40D2978](src, dest, tempBuffer, desired_histogram, *(void *)&histogram_entries, *(void *)&flags, minVal, maxVal);
}

vImage_Error vImageHistogramSpecification_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, const vImagePixelCount *desired_histogram, vImage_Flags flags)
{
  return MEMORY[0x1F40D2980](src, dest, desired_histogram, *(void *)&flags);
}

vImage_Error vImageHistogramSpecification_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImagePixelCount *desired_histogram, unsigned int histogram_entries, Pixel_F minVal, Pixel_F maxVal, vImage_Flags flags)
{
  return MEMORY[0x1F40D2988](src, dest, tempBuffer, desired_histogram, *(void *)&histogram_entries, *(void *)&flags, minVal, maxVal);
}

vImage_Error vImageHorizontalReflect_ARGB16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2990](src, dest, *(void *)&flags);
}

vImage_Error vImageHorizontalReflect_ARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2998](src, dest, *(void *)&flags);
}

vImage_Error vImageHorizontalReflect_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D29A0](src, dest, *(void *)&flags);
}

vImage_Error vImageHorizontalReflect_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D29A8](src, dest, *(void *)&flags);
}

vImage_Error vImageHorizontalReflect_CbCr16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D29B0](src, dest, *(void *)&flags);
}

vImage_Error vImageHorizontalReflect_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D29B8](src, dest, *(void *)&flags);
}

vImage_Error vImageHorizontalReflect_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D29C0](src, dest, *(void *)&flags);
}

vImage_Error vImageHorizontalReflect_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D29C8](src, dest, *(void *)&flags);
}

vImage_Error vImageHorizontalShearD_ARGB16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_ARGB_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D29D0](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShearD_ARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_ARGB_16U backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D29D8](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShearD_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_8888 backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D29E0](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShearD_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_FFFF backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D29E8](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShearD_CbCr16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_16F16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D29F0](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShearD_CbCr16U(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_16U16U backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D29F8](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShearD_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2A00](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShearD_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, Pixel_8 backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2A08](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShearD_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, Pixel_F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2A10](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, *(void *)&flags, xTranslate, shearSlope, backColor);
}

vImage_Error vImageHorizontalShear_CbCr8(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float xTranslate, float shearSlope, ResamplingFilter filter, const Pixel_88 backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2A18](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageHorizontalShear_Planar16U(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float xTranslate, float shearSlope, ResamplingFilter filter, Pixel_16U backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2A20](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, xTranslate, shearSlope);
}

vImage_Error vImageInterpolatedLookupTable_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_F *table, vImagePixelCount tableEntries, float maxFloat, float minFloat, vImage_Flags flags)
{
  return MEMORY[0x1F40D2A28](src, dest, table, tableEntries, *(void *)&flags, maxFloat, minFloat);
}

vImage_Error vImageLookupTable_Planar16(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_16U table[65536], vImage_Flags flags)
{
  return MEMORY[0x1F40D2A30](src, dest, table, *(void *)&flags);
}

vImage_Error vImageLookupTable_Planar8toPlanar24(const vImage_Buffer *src, const vImage_Buffer *dest, const uint32_t table[256], vImage_Flags flags)
{
  return MEMORY[0x1F40D2A38](src, dest, table, *(void *)&flags);
}

vImage_Error vImageLookupTable_PlanarFtoPlanar8(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_8 table[4096], vImage_Flags flags)
{
  return MEMORY[0x1F40D2A40](src, dest, table, *(void *)&flags);
}

vImage_Error vImageMatrixMultiply_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const int16_t matrix[16], int32_t divisor, const int16_t *pre_bias, const int32_t *post_bias, vImage_Flags flags)
{
  return MEMORY[0x1F40D2A58](src, dest, matrix, *(void *)&divisor, pre_bias, post_bias, *(void *)&flags);
}

vImage_Error vImageMatrixMultiply_ARGB8888ToPlanar8(const vImage_Buffer *src, const vImage_Buffer *dest, const int16_t matrix[4], int32_t divisor, const int16_t pre_bias[4], int32_t post_bias, vImage_Flags flags)
{
  return MEMORY[0x1F40D2A60](src, dest, matrix, *(void *)&divisor, pre_bias, *(void *)&post_bias, *(void *)&flags);
}

vImage_Error vImageMatrixMultiply_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, const float matrix[16], const float *pre_bias, const float *post_bias, vImage_Flags flags)
{
  return MEMORY[0x1F40D2A68](src, dest, matrix, pre_bias, post_bias, *(void *)&flags);
}

vImage_Error vImageMatrixMultiply_ARGBFFFFToPlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, const float matrix[4], const float pre_bias[4], float post_bias, vImage_Flags flags)
{
  return MEMORY[0x1F40D2A70](src, dest, matrix, pre_bias, *(void *)&flags, post_bias);
}

vImage_Error vImageMatrixMultiply_Planar8(const vImage_Buffer *srcs[], const vImage_Buffer *dests[], uint32_t src_planes, uint32_t dest_planes, const int16_t matrix[], int32_t divisor, const int16_t *pre_bias, const int32_t *post_bias, vImage_Flags flags)
{
  return MEMORY[0x1F40D2A78](srcs, dests, *(void *)&src_planes, *(void *)&dest_planes, matrix, *(void *)&divisor, pre_bias, post_bias);
}

vImage_Error vImageMatrixMultiply_PlanarF(const vImage_Buffer *srcs[], const vImage_Buffer *dests[], uint32_t src_planes, uint32_t dest_planes, const float matrix[], const float *pre_bias, const float *post_bias, vImage_Flags flags)
{
  return MEMORY[0x1F40D2A80](srcs, dests, *(void *)&src_planes, *(void *)&dest_planes, matrix, pre_bias, post_bias, *(void *)&flags);
}

vImage_Error vImageMax_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1F40D2A88](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel_height, kernel_width, *(void *)&flags);
}

vImage_Error vImageMax_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1F40D2A90](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel_height, kernel_width, *(void *)&flags);
}

vImage_Error vImageMax_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1F40D2A98](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel_height, kernel_width, *(void *)&flags);
}

vImage_Error vImageMax_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1F40D2AA0](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel_height, kernel_width, *(void *)&flags);
}

vImage_Error vImageMin_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1F40D2AA8](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel_height, kernel_width, *(void *)&flags);
}

vImage_Error vImageMin_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1F40D2AB0](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel_height, kernel_width, *(void *)&flags);
}

vImage_Error vImageMin_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1F40D2AB8](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel_height, kernel_width, *(void *)&flags);
}

vImage_Error vImageMin_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags)
{
  return MEMORY[0x1F40D2AC0](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel_height, kernel_width, *(void *)&flags);
}

vImage_Error vImageMultiDimensionalInterpolatedLookupTable_PlanarF(const vImage_Buffer srcs[], const vImage_Buffer dests[], void *tempBuffer, vImage_MultidimensionalTable table, vImage_InterpolationMethod method, vImage_Flags flags)
{
  return MEMORY[0x1F40D2AC8](srcs, dests, tempBuffer, table, *(void *)&method, *(void *)&flags);
}

vImage_MultidimensionalTable vImageMultidimensionalTable_Create(const uint16_t *tableData, uint32_t numSrcChannels, uint32_t numDestChannels, const uint8_t table_entries_per_dimension[], vImageMDTableUsageHint hint, vImage_Flags flags, vImage_Error *err)
{
  return (vImage_MultidimensionalTable)MEMORY[0x1F40D2AD0](tableData, *(void *)&numSrcChannels, *(void *)&numDestChannels, table_entries_per_dimension, *(void *)&hint, *(void *)&flags, err);
}

vImage_Error vImageMultidimensionalTable_Release(vImage_MultidimensionalTable table)
{
  return MEMORY[0x1F40D2AD8](table);
}

vImage_Error vImageOverwriteChannelsWithPixel_ARGB16U(const Pixel_ARGB_16U the_pixel, const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t copyMask, vImage_Flags flags)
{
  return MEMORY[0x1F40D2AE0](the_pixel, src, dest, copyMask, *(void *)&flags);
}

vImage_Error vImageOverwriteChannelsWithPixel_ARGB8888(const Pixel_8888 the_pixel, const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t copyMask, vImage_Flags flags)
{
  return MEMORY[0x1F40D2AE8](the_pixel, src, dest, copyMask, *(void *)&flags);
}

vImage_Error vImageOverwriteChannelsWithPixel_ARGBFFFF(const Pixel_FFFF the_pixel, const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t copyMask, vImage_Flags flags)
{
  return MEMORY[0x1F40D2AF0](the_pixel, src, dest, copyMask, *(void *)&flags);
}

vImage_Error vImageOverwriteChannelsWithScalar_ARGB8888(Pixel_8 scalar, const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t copyMask, vImage_Flags flags)
{
  return MEMORY[0x1F40D2AF8](scalar, src, dest, copyMask, *(void *)&flags);
}

vImage_Error vImageOverwriteChannelsWithScalar_ARGBFFFF(Pixel_F scalar, const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t copyMask, vImage_Flags flags)
{
  return MEMORY[0x1F40D2B00](src, dest, copyMask, *(void *)&flags, scalar);
}

vImage_Error vImageOverwriteChannelsWithScalar_Planar16F(Pixel_16F scalar, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2B08](scalar, dest, *(void *)&flags);
}

vImage_Error vImageOverwriteChannelsWithScalar_Planar8(Pixel_8 scalar, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2B18](scalar, dest, *(void *)&flags);
}

vImage_Error vImageOverwriteChannelsWithScalar_PlanarF(Pixel_F scalar, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2B20](dest, *(void *)&flags, scalar);
}

vImage_Error vImagePermuteChannels_RGB888(const vImage_Buffer *src, const vImage_Buffer *dest, const uint8_t permuteMap[3], vImage_Flags flags)
{
  return MEMORY[0x1F40D2B78](src, dest, permuteMap, *(void *)&flags);
}

vImage_Error vImagePerspectiveWarp_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_PerpsectiveTransform *transform, vImage_WarpInterpolation interpolation, Pixel_8888 backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2B80](src, dest, tempBuffer, transform, *(void *)&interpolation, backColor, *(void *)&flags);
}

vImage_Error vImagePerspectiveWarp_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_PerpsectiveTransform *transform, vImage_WarpInterpolation interpolation, Pixel_8 backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2B88](src, dest, tempBuffer, transform, *(void *)&interpolation, backColor, *(void *)&flags);
}

vImage_Error vImagePiecewiseGamma_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, const float exponentialCoeffs[3], const float gamma, const float linearCoeffs[2], const Pixel_8 boundary, vImage_Flags flags)
{
  return MEMORY[0x1F40D2B90](src, dest, exponentialCoeffs, linearCoeffs, boundary, *(void *)&flags, gamma);
}

vImage_Error vImagePiecewiseGamma_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, const float exponentialCoeffs[3], const float gamma, const float linearCoeffs[2], const float boundary, vImage_Flags flags)
{
  return MEMORY[0x1F40D2B98](src, dest, exponentialCoeffs, linearCoeffs, *(void *)&flags, gamma, boundary);
}

vImage_Error vImagePremultipliedAlphaBlendDarken_RGBA8888(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2BA0](srcTop, srcBottom, dest, *(void *)&flags);
}

vImage_Error vImagePremultipliedAlphaBlendLighten_RGBA8888(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2BA8](srcTop, srcBottom, dest, *(void *)&flags);
}

vImage_Error vImagePremultipliedAlphaBlendMultiply_RGBA8888(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2BB0](srcTop, srcBottom, dest, *(void *)&flags);
}

vImage_Error vImagePremultipliedAlphaBlendScreen_RGBA8888(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2BB8](srcTop, srcBottom, dest, *(void *)&flags);
}

vImage_Error vImagePremultipliedAlphaBlend_ARGB8888(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2BD0](srcTop, srcBottom, dest, *(void *)&flags);
}

vImage_Error vImagePremultipliedAlphaBlend_ARGBFFFF(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2BD8](srcTop, srcBottom, dest, *(void *)&flags);
}

vImage_Error vImagePremultipliedConstAlphaBlend_ARGB8888(const vImage_Buffer *srcTop, Pixel_8 constAlpha, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2BF0](srcTop, constAlpha, srcBottom, dest, *(void *)&flags);
}

vImage_Error vImagePremultipliedConstAlphaBlend_ARGBFFFF(const vImage_Buffer *srcTop, Pixel_F constAlpha, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2BF8](srcTop, srcBottom, dest, *(void *)&flags, constAlpha);
}

vImage_Error vImageRotate90_ARGB16F(const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, const Pixel_ARGB_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2C38](src, dest, rotationConstant, backColor, *(void *)&flags);
}

vImage_Error vImageRotate90_ARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, const Pixel_ARGB_16U backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2C40](src, dest, rotationConstant, backColor, *(void *)&flags);
}

vImage_Error vImageRotate90_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, const Pixel_8888 backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2C48](src, dest, rotationConstant, backColor, *(void *)&flags);
}

vImage_Error vImageRotate90_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, const Pixel_FFFF backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2C58](src, dest, rotationConstant, backColor, *(void *)&flags);
}

vImage_Error vImageRotate90_CbCr16F(const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, const Pixel_16F16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2C60](src, dest, rotationConstant, backColor, *(void *)&flags);
}

vImage_Error vImageRotate90_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, const Pixel_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2C68](src, dest, rotationConstant, backColor, *(void *)&flags);
}

vImage_Error vImageRotate90_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, Pixel_8 backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2C78](src, dest, rotationConstant, backColor, *(void *)&flags);
}

vImage_Error vImageRotate90_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, Pixel_F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2C80](src, dest, rotationConstant, *(void *)&flags, backColor);
}

vImage_Error vImageRotate_ARGB16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, const Pixel_ARGB_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2C88](src, dest, tempBuffer, backColor, *(void *)&flags, angleInRadians);
}

vImage_Error vImageRotate_ARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, const Pixel_ARGB_16U backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2C90](src, dest, tempBuffer, backColor, *(void *)&flags, angleInRadians);
}

vImage_Error vImageRotate_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, const Pixel_8888 backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2C98](src, dest, tempBuffer, backColor, *(void *)&flags, angleInRadians);
}

vImage_Error vImageRotate_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, const Pixel_FFFF backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2CA0](src, dest, tempBuffer, backColor, *(void *)&flags, angleInRadians);
}

vImage_Error vImageRotate_CbCr16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, const Pixel_16F16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2CA8](src, dest, tempBuffer, backColor, *(void *)&flags, angleInRadians);
}

vImage_Error vImageRotate_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, const Pixel_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2CB0](src, dest, tempBuffer, backColor, *(void *)&flags, angleInRadians);
}

vImage_Error vImageRotate_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, Pixel_8 backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2CB8](src, dest, tempBuffer, backColor, *(void *)&flags, angleInRadians);
}

vImage_Error vImageRotate_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, Pixel_F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2CC0](src, dest, tempBuffer, *(void *)&flags, angleInRadians, backColor);
}

vImage_Error vImageSepConvolve_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernelX, uint32_t kernelX_width, const float *kernelY, uint32_t kernelY_width, float bias, const Pixel_8888 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2D20](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernelX, *(void *)&kernelX_width, kernelY, bias);
}

vImage_Error vImageSepConvolve_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernelX, uint32_t kernelX_width, const float *kernelY, uint32_t kernelY_width, float bias, Pixel_16F backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2D28](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernelX, *(void *)&kernelX_width, kernelY, bias);
}

vImage_Error vImageSepConvolve_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernelX, uint32_t kernelX_width, const float *kernelY, uint32_t kernelY_width, float bias, Pixel_16U backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2D30](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernelX, *(void *)&kernelX_width, kernelY, bias);
}

vImage_Error vImageSepConvolve_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernelX, uint32_t kernelX_width, const float *kernelY, uint32_t kernelY_width, float bias, Pixel_F backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2D38](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernelX, *(void *)&kernelX_width, kernelY, bias, backgroundColor);
}

vImage_Error vImageTableLookUp_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_8 alphaTable[256], const Pixel_8 redTable[256], const Pixel_8 greenTable[256], const Pixel_8 blueTable[256], vImage_Flags flags)
{
  return MEMORY[0x1F40D2D40](src, dest, alphaTable, redTable, greenTable, blueTable, *(void *)&flags);
}

vImage_Error vImageTentConvolve_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, uint32_t kernel_height, uint32_t kernel_width, const Pixel_8888 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2D48](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, *(void *)&kernel_height, *(void *)&kernel_width, backgroundColor);
}

vImage_Error vImageTentConvolve_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, uint32_t kernel_height, uint32_t kernel_width, Pixel_8 backgroundColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2D50](src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, *(void *)&kernel_height, *(void *)&kernel_width, backgroundColor);
}

vImage_Error vImageVerticalReflect_ARGB16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2D88](src, dest, *(void *)&flags);
}

vImage_Error vImageVerticalReflect_ARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2D90](src, dest, *(void *)&flags);
}

vImage_Error vImageVerticalReflect_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2D98](src, dest, *(void *)&flags);
}

vImage_Error vImageVerticalReflect_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2DA0](src, dest, *(void *)&flags);
}

vImage_Error vImageVerticalReflect_CbCr16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2DA8](src, dest, *(void *)&flags);
}

vImage_Error vImageVerticalReflect_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2DB0](src, dest, *(void *)&flags);
}

vImage_Error vImageVerticalReflect_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2DB8](src, dest, *(void *)&flags);
}

vImage_Error vImageVerticalReflect_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags)
{
  return MEMORY[0x1F40D2DC0](src, dest, *(void *)&flags);
}

vImage_Error vImageVerticalShearD_ARGB16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, const Pixel_ARGB_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2DC8](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, yTranslate, shearSlope);
}

vImage_Error vImageVerticalShearD_ARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, const Pixel_ARGB_16U backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2DD0](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, yTranslate, shearSlope);
}

vImage_Error vImageVerticalShearD_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, const Pixel_8888 backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2DD8](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, yTranslate, shearSlope);
}

vImage_Error vImageVerticalShearD_ARGBFFFF(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, const Pixel_FFFF backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2DE0](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, yTranslate, shearSlope);
}

vImage_Error vImageVerticalShearD_CbCr16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, const Pixel_16F16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2DE8](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, yTranslate, shearSlope);
}

vImage_Error vImageVerticalShearD_CbCr16U(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, const Pixel_16U16U backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2DF0](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, yTranslate, shearSlope);
}

vImage_Error vImageVerticalShearD_Planar16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, const Pixel_16F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2DF8](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, yTranslate, shearSlope);
}

vImage_Error vImageVerticalShearD_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, Pixel_8 backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2E00](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, yTranslate, shearSlope);
}

vImage_Error vImageVerticalShearD_PlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, Pixel_F backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2E08](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, *(void *)&flags, yTranslate, shearSlope, backColor);
}

vImage_Error vImageVerticalShear_CbCr8(const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float yTranslate, float shearSlope, ResamplingFilter filter, const Pixel_88 backColor, vImage_Flags flags)
{
  return MEMORY[0x1F40D2E10](src, dest, srcOffsetToROI_X, srcOffsetToROI_Y, filter, backColor, *(void *)&flags, yTranslate, shearSlope);
}