uint64_t vImage.PixelBuffer<>.init(planarBuffers:)@<X0>(void *a1@<X0>, uint64_t *a2@<X8>)
{
  void *v4;
  uint64_t v5;
  uint64_t v6;
  void *v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  void *v18;
  uint64_t v19;
  long long v20;
  uint64_t v21;
  long long v22;
  uint64_t result;
  uint64_t v24;
  _OWORD v25[2];
  _OWORD v26[2];
  uint64_t v27;

  v27 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 2)
  {
    __break(1u);
    goto LABEL_18;
  }
  v4 = (void *)a1[4];
  if (!v4[2])
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = v4[6];
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = v4[5];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v7 = (void *)a1[5];
  if (!v7[2])
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v9)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (v6 != v9)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v10 = swift_allocObject();
  *(_OWORD *)(v10 + 16) = xmmword_1D2135280;
  v11 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v6);
  v13 = v12;
  v15 = v14;
  v17 = v16;
  type metadata accessor for vImage.BufferReference();
  v18 = (void *)swift_allocObject();
  v18[2] = v11;
  v18[3] = v13;
  v18[4] = v15;
  v18[5] = v17;
  *(void *)(v10 + 32) = v11;
  *(void *)(v10 + 40) = v13;
  *(void *)(v10 + 48) = v15;
  *(void *)(v10 + 56) = v17;
  *(void *)(v10 + 64) = v18;
  v24 = v10;
  v19 = a1[4];
  if (!*(void *)(v19 + 16))
  {
LABEL_30:
    __break(1u);
LABEL_31:
    __break(1u);
  }
  v20 = *(_OWORD *)(v19 + 48);
  v26[0] = *(_OWORD *)(v19 + 32);
  v26[1] = v20;
  v21 = a1[5];
  if (!*(void *)(v21 + 16)) {
    goto LABEL_31;
  }
  v22 = *(_OWORD *)(v21 + 48);
  v25[0] = *(_OWORD *)(v21 + 32);
  v25[1] = v22;
  closure #1 in closure #1 in vImage.PixelBuffer<>.init(planarBuffers:)((uint64_t)v25, (uint64_t)v26, (uint64_t)&v24);
  result = swift_bridgeObjectRelease();
  *a2 = v10;
  return result;
}

{
  void *v4;
  uint64_t v5;
  uint64_t v6;
  void *v7;
  uint64_t v8;
  uint64_t v9;
  void *v10;
  uint64_t v11;
  uint64_t v12;
  void *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  vImagePixelCount v18;
  vImagePixelCount v19;
  vImagePixelCount v20;
  vImagePixelCount v21;
  size_t v22;
  size_t v23;
  void *v24;
  uint64_t v25;
  long long v26;
  uint64_t v27;
  long long v28;
  uint64_t v29;
  long long v30;
  uint64_t v31;
  long long v32;
  uint64_t result;
  vImage_Buffer srcB;
  vImage_Buffer srcG;
  vImage_Buffer srcR;
  vImage_Buffer srcA;
  vImage_Buffer dest;
  uint64_t v39;

  v39 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_34;
  }
  v4 = (void *)a1[4];
  if (!v4[2])
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v5 = v4[6];
  if (v5 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v6 = v4[5];
  if (v6 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v5)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v7 = (void *)a1[5];
  if (!v7[2])
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v8)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v9)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (v5 != v8)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (v6 != v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v10 = (void *)a1[6];
  if (!v10[2])
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (!v11)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v12)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (v5 != v11)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v6 != v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  v13 = (void *)a1[7];
  if (!v13[2])
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  v14 = v13[6];
  if (v14 < 0)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  v15 = v13[5];
  if (v15 < 0)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (!v14)
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  if (!v15)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  if (v5 != v14)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (v6 != v15)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v16 = swift_allocObject();
  *(_OWORD *)(v16 + 16) = xmmword_1D2135280;
  v17 = (void *)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v6);
  v19 = v18;
  v21 = v20;
  v23 = v22;
  type metadata accessor for vImage.BufferReference();
  v24 = (void *)swift_allocObject();
  v24[2] = v17;
  v24[3] = v19;
  v24[4] = v21;
  v24[5] = v23;
  *(void *)(v16 + 32) = v17;
  *(void *)(v16 + 40) = v19;
  *(void *)(v16 + 48) = v21;
  *(void *)(v16 + 56) = v23;
  *(void *)(v16 + 64) = v24;
  dest.data = v17;
  dest.height = v19;
  dest.width = v21;
  dest.rowBytes = v23;
  v25 = a1[4];
  if (!*(void *)(v25 + 16))
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  v26 = *(_OWORD *)(v25 + 48);
  *(_OWORD *)&srcA.data = *(_OWORD *)(v25 + 32);
  *(_OWORD *)&srcA.width = v26;
  v27 = a1[5];
  if (!*(void *)(v27 + 16))
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  v28 = *(_OWORD *)(v27 + 48);
  *(_OWORD *)&srcR.data = *(_OWORD *)(v27 + 32);
  *(_OWORD *)&srcR.width = v28;
  v29 = a1[6];
  if (!*(void *)(v29 + 16))
  {
LABEL_62:
    __break(1u);
LABEL_63:
    __break(1u);
  }
  v30 = *(_OWORD *)(v29 + 48);
  *(_OWORD *)&srcG.data = *(_OWORD *)(v29 + 32);
  *(_OWORD *)&srcG.width = v30;
  v31 = a1[7];
  if (!*(void *)(v31 + 16)) {
    goto LABEL_63;
  }
  v32 = *(_OWORD *)(v31 + 48);
  *(_OWORD *)&srcB.data = *(_OWORD *)(v31 + 32);
  *(_OWORD *)&srcB.width = v32;
  vImageConvert_PlanarFtoARGBFFFF(&srcA, &srcR, &srcG, &srcB, &dest, 0);
  result = swift_bridgeObjectRelease();
  *a2 = v16;
  return result;
}

{
  void *v4;
  uint64_t v5;
  uint64_t v6;
  void *v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  void *v18;
  uint64_t v19;
  long long v20;
  uint64_t v21;
  long long v22;
  uint64_t result;
  uint64_t v24;
  _OWORD v25[2];
  _OWORD v26[2];
  uint64_t v27;

  v27 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 2)
  {
    __break(1u);
    goto LABEL_18;
  }
  v4 = (void *)a1[4];
  if (!v4[2])
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  v5 = v4[6];
  if (v5 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  v6 = v4[5];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  v7 = (void *)a1[5];
  if (!v7[2])
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v9)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v5 != v8)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (v6 != v9)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v10 = swift_allocObject();
  *(_OWORD *)(v10 + 16) = xmmword_1D2135280;
  v11 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v6);
  v13 = v12;
  v15 = v14;
  v17 = v16;
  type metadata accessor for vImage.BufferReference();
  v18 = (void *)swift_allocObject();
  v18[2] = v11;
  v18[3] = v13;
  v18[4] = v15;
  v18[5] = v17;
  *(void *)(v10 + 32) = v11;
  *(void *)(v10 + 40) = v13;
  *(void *)(v10 + 48) = v15;
  *(void *)(v10 + 56) = v17;
  *(void *)(v10 + 64) = v18;
  v24 = v10;
  v19 = a1[4];
  if (!*(void *)(v19 + 16))
  {
LABEL_30:
    __break(1u);
LABEL_31:
    __break(1u);
  }
  v20 = *(_OWORD *)(v19 + 48);
  v26[0] = *(_OWORD *)(v19 + 32);
  v26[1] = v20;
  v21 = a1[5];
  if (!*(void *)(v21 + 16)) {
    goto LABEL_31;
  }
  v22 = *(_OWORD *)(v21 + 48);
  v25[0] = *(_OWORD *)(v21 + 32);
  v25[1] = v22;
  closure #1 in closure #1 in vImage.PixelBuffer<>.init(planarBuffers:)((uint64_t)v25, (uint64_t)v26, (uint64_t)&v24);
  result = swift_bridgeObjectRelease();
  *a2 = v10;
  return result;
}

{
  void *v4;
  uint64_t v5;
  uint64_t v6;
  void *v7;
  uint64_t v8;
  uint64_t v9;
  void *v10;
  uint64_t v11;
  uint64_t v12;
  void *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  vImagePixelCount v18;
  vImagePixelCount v19;
  vImagePixelCount v20;
  vImagePixelCount v21;
  size_t v22;
  size_t v23;
  void *v24;
  uint64_t v25;
  long long v26;
  uint64_t v27;
  long long v28;
  uint64_t v29;
  long long v30;
  uint64_t v31;
  long long v32;
  uint64_t result;
  vImage_Buffer srcB;
  vImage_Buffer srcG;
  vImage_Buffer srcR;
  vImage_Buffer srcA;
  vImage_Buffer dest;
  uint64_t v39;

  v39 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_34;
  }
  v4 = (void *)a1[4];
  if (!v4[2])
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v5 = v4[6];
  if (v5 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v6 = v4[5];
  if (v6 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v5)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v7 = (void *)a1[5];
  if (!v7[2])
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v8)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v9)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (v5 != v8)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (v6 != v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v10 = (void *)a1[6];
  if (!v10[2])
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (!v11)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v12)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (v5 != v11)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v6 != v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  v13 = (void *)a1[7];
  if (!v13[2])
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  v14 = v13[6];
  if (v14 < 0)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  v15 = v13[5];
  if (v15 < 0)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (!v14)
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  if (!v15)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  if (v5 != v14)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (v6 != v15)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v16 = swift_allocObject();
  *(_OWORD *)(v16 + 16) = xmmword_1D2135280;
  v17 = (void *)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v6);
  v19 = v18;
  v21 = v20;
  v23 = v22;
  type metadata accessor for vImage.BufferReference();
  v24 = (void *)swift_allocObject();
  v24[2] = v17;
  v24[3] = v19;
  v24[4] = v21;
  v24[5] = v23;
  *(void *)(v16 + 32) = v17;
  *(void *)(v16 + 40) = v19;
  *(void *)(v16 + 48) = v21;
  *(void *)(v16 + 56) = v23;
  *(void *)(v16 + 64) = v24;
  dest.data = v17;
  dest.height = v19;
  dest.width = v21;
  dest.rowBytes = v23;
  v25 = a1[4];
  if (!*(void *)(v25 + 16))
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  v26 = *(_OWORD *)(v25 + 48);
  *(_OWORD *)&srcA.data = *(_OWORD *)(v25 + 32);
  *(_OWORD *)&srcA.width = v26;
  v27 = a1[5];
  if (!*(void *)(v27 + 16))
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  v28 = *(_OWORD *)(v27 + 48);
  *(_OWORD *)&srcR.data = *(_OWORD *)(v27 + 32);
  *(_OWORD *)&srcR.width = v28;
  v29 = a1[6];
  if (!*(void *)(v29 + 16))
  {
LABEL_62:
    __break(1u);
LABEL_63:
    __break(1u);
  }
  v30 = *(_OWORD *)(v29 + 48);
  *(_OWORD *)&srcG.data = *(_OWORD *)(v29 + 32);
  *(_OWORD *)&srcG.width = v30;
  v31 = a1[7];
  if (!*(void *)(v31 + 16)) {
    goto LABEL_63;
  }
  v32 = *(_OWORD *)(v31 + 48);
  *(_OWORD *)&srcB.data = *(_OWORD *)(v31 + 32);
  *(_OWORD *)&srcB.width = v32;
  vImageConvert_Planar8toARGB8888(&srcA, &srcR, &srcG, &srcB, &dest, 0);
  result = swift_bridgeObjectRelease();
  *a2 = v16;
  return result;
}

{
  void *v4;
  uint64_t v5;
  uint64_t v6;
  void *v7;
  uint64_t v8;
  uint64_t v9;
  void *v10;
  uint64_t v11;
  uint64_t v12;
  void *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  vImagePixelCount v18;
  vImagePixelCount v19;
  vImagePixelCount v20;
  vImagePixelCount v21;
  size_t v22;
  size_t v23;
  void *v24;
  uint64_t v25;
  long long v26;
  uint64_t v27;
  long long v28;
  uint64_t v29;
  long long v30;
  uint64_t v31;
  long long v32;
  uint64_t result;
  vImage_Buffer blue;
  vImage_Buffer green;
  vImage_Buffer red;
  vImage_Buffer alpha;
  vImage_Buffer dest;
  uint64_t v39;

  v39 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_34;
  }
  v4 = (void *)a1[4];
  if (!v4[2])
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v5 = v4[6];
  if (v5 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v6 = v4[5];
  if (v6 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v5)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v7 = (void *)a1[5];
  if (!v7[2])
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v8)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v9)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (v5 != v8)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (v6 != v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v10 = (void *)a1[6];
  if (!v10[2])
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (!v11)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v12)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (v5 != v11)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v6 != v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  v13 = (void *)a1[7];
  if (!v13[2])
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  v14 = v13[6];
  if (v14 < 0)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  v15 = v13[5];
  if (v15 < 0)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (!v14)
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  if (!v15)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  if (v5 != v14)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (v6 != v15)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v16 = swift_allocObject();
  *(_OWORD *)(v16 + 16) = xmmword_1D2135280;
  v17 = (void *)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v6);
  v19 = v18;
  v21 = v20;
  v23 = v22;
  type metadata accessor for vImage.BufferReference();
  v24 = (void *)swift_allocObject();
  v24[2] = v17;
  v24[3] = v19;
  v24[4] = v21;
  v24[5] = v23;
  *(void *)(v16 + 32) = v17;
  *(void *)(v16 + 40) = v19;
  *(void *)(v16 + 48) = v21;
  *(void *)(v16 + 56) = v23;
  *(void *)(v16 + 64) = v24;
  dest.data = v17;
  dest.height = v19;
  dest.width = v21;
  dest.rowBytes = v23;
  v25 = a1[4];
  if (!*(void *)(v25 + 16))
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  v26 = *(_OWORD *)(v25 + 48);
  *(_OWORD *)&alpha.data = *(_OWORD *)(v25 + 32);
  *(_OWORD *)&alpha.width = v26;
  v27 = a1[5];
  if (!*(void *)(v27 + 16))
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  v28 = *(_OWORD *)(v27 + 48);
  *(_OWORD *)&red.data = *(_OWORD *)(v27 + 32);
  *(_OWORD *)&red.width = v28;
  v29 = a1[6];
  if (!*(void *)(v29 + 16))
  {
LABEL_62:
    __break(1u);
LABEL_63:
    __break(1u);
  }
  v30 = *(_OWORD *)(v29 + 48);
  *(_OWORD *)&green.data = *(_OWORD *)(v29 + 32);
  *(_OWORD *)&green.width = v30;
  v31 = a1[7];
  if (!*(void *)(v31 + 16)) {
    goto LABEL_63;
  }
  v32 = *(_OWORD *)(v31 + 48);
  *(_OWORD *)&blue.data = *(_OWORD *)(v31 + 32);
  *(_OWORD *)&blue.width = v32;
  vImageConvert_PlanarFToARGB8888(&alpha, &red, &green, &blue, &dest, flt_1F28D77A0, flt_1F28D7770, 0);
  result = swift_bridgeObjectRelease();
  *a2 = v16;
  return result;
}

{
  void *v4;
  uint64_t v5;
  uint64_t v6;
  void *v7;
  uint64_t v8;
  uint64_t v9;
  void *v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  void *v14;
  vImagePixelCount v15;
  vImagePixelCount v16;
  vImagePixelCount v17;
  vImagePixelCount v18;
  size_t v19;
  size_t v20;
  void *v21;
  uint64_t v22;
  long long v23;
  uint64_t v24;
  long long v25;
  uint64_t v26;
  long long v27;
  uint64_t result;
  vImage_Buffer planarBlue;
  vImage_Buffer planarGreen;
  vImage_Buffer planarRed;
  vImage_Buffer rgbDest;
  uint64_t v33;

  v33 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 3)
  {
    __break(1u);
    goto LABEL_26;
  }
  v4 = (void *)a1[4];
  if (!v4[2])
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  v5 = v4[6];
  if (v5 < 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v6 = v4[5];
  if (v6 < 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (!v5)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v6)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v7 = (void *)a1[5];
  if (!v7[2])
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v8)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (!v9)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (v5 != v8)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (v6 != v9)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v10 = (void *)a1[6];
  if (!v10[2])
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v11)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v12)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (v5 != v11)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (v6 != v12)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v13 = swift_allocObject();
  *(_OWORD *)(v13 + 16) = xmmword_1D2135280;
  v14 = (void *)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v6);
  v16 = v15;
  v18 = v17;
  v20 = v19;
  type metadata accessor for vImage.BufferReference();
  v21 = (void *)swift_allocObject();
  v21[2] = v14;
  v21[3] = v16;
  v21[4] = v18;
  v21[5] = v20;
  *(void *)(v13 + 32) = v14;
  *(void *)(v13 + 40) = v16;
  *(void *)(v13 + 48) = v18;
  *(void *)(v13 + 56) = v20;
  *(void *)(v13 + 64) = v21;
  rgbDest.data = v14;
  rgbDest.height = v16;
  rgbDest.width = v18;
  rgbDest.rowBytes = v20;
  v22 = a1[4];
  if (!*(void *)(v22 + 16))
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v23 = *(_OWORD *)(v22 + 48);
  *(_OWORD *)&planarRed.data = *(_OWORD *)(v22 + 32);
  *(_OWORD *)&planarRed.width = v23;
  v24 = a1[5];
  if (!*(void *)(v24 + 16))
  {
LABEL_46:
    __break(1u);
LABEL_47:
    __break(1u);
  }
  v25 = *(_OWORD *)(v24 + 48);
  *(_OWORD *)&planarGreen.data = *(_OWORD *)(v24 + 32);
  *(_OWORD *)&planarGreen.width = v25;
  v26 = a1[6];
  if (!*(void *)(v26 + 16)) {
    goto LABEL_47;
  }
  v27 = *(_OWORD *)(v26 + 48);
  *(_OWORD *)&planarBlue.data = *(_OWORD *)(v26 + 32);
  *(_OWORD *)&planarBlue.width = v27;
  vImageConvert_Planar8toRGB888(&planarRed, &planarGreen, &planarBlue, &rgbDest, 0);
  result = swift_bridgeObjectRelease();
  *a2 = v13;
  return result;
}

{
  void *v4;
  uint64_t v5;
  uint64_t v6;
  void *v7;
  uint64_t v8;
  uint64_t v9;
  void *v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  void *v14;
  vImagePixelCount v15;
  vImagePixelCount v16;
  vImagePixelCount v17;
  vImagePixelCount v18;
  size_t v19;
  size_t v20;
  void *v21;
  uint64_t v22;
  long long v23;
  uint64_t v24;
  long long v25;
  uint64_t v26;
  long long v27;
  uint64_t result;
  vImage_Buffer planarBlue;
  vImage_Buffer planarGreen;
  vImage_Buffer planarRed;
  vImage_Buffer rgbDest;
  uint64_t v33;

  v33 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 3)
  {
    __break(1u);
    goto LABEL_26;
  }
  v4 = (void *)a1[4];
  if (!v4[2])
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  v5 = v4[6];
  if (v5 < 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v6 = v4[5];
  if (v6 < 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (!v5)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v6)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  v7 = (void *)a1[5];
  if (!v7[2])
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v8)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (!v9)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (v5 != v8)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (v6 != v9)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  v10 = (void *)a1[6];
  if (!v10[2])
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v11)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v12)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (v5 != v11)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (v6 != v12)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v13 = swift_allocObject();
  *(_OWORD *)(v13 + 16) = xmmword_1D2135280;
  v14 = (void *)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v6);
  v16 = v15;
  v18 = v17;
  v20 = v19;
  type metadata accessor for vImage.BufferReference();
  v21 = (void *)swift_allocObject();
  v21[2] = v14;
  v21[3] = v16;
  v21[4] = v18;
  v21[5] = v20;
  *(void *)(v13 + 32) = v14;
  *(void *)(v13 + 40) = v16;
  *(void *)(v13 + 48) = v18;
  *(void *)(v13 + 56) = v20;
  *(void *)(v13 + 64) = v21;
  rgbDest.data = v14;
  rgbDest.height = v16;
  rgbDest.width = v18;
  rgbDest.rowBytes = v20;
  v22 = a1[4];
  if (!*(void *)(v22 + 16))
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v23 = *(_OWORD *)(v22 + 48);
  *(_OWORD *)&planarRed.data = *(_OWORD *)(v22 + 32);
  *(_OWORD *)&planarRed.width = v23;
  v24 = a1[5];
  if (!*(void *)(v24 + 16))
  {
LABEL_46:
    __break(1u);
LABEL_47:
    __break(1u);
  }
  v25 = *(_OWORD *)(v24 + 48);
  *(_OWORD *)&planarGreen.data = *(_OWORD *)(v24 + 32);
  *(_OWORD *)&planarGreen.width = v25;
  v26 = a1[6];
  if (!*(void *)(v26 + 16)) {
    goto LABEL_47;
  }
  v27 = *(_OWORD *)(v26 + 48);
  *(_OWORD *)&planarBlue.data = *(_OWORD *)(v26 + 32);
  *(_OWORD *)&planarBlue.width = v27;
  vImageConvert_PlanarFtoRGBFFF(&planarRed, &planarGreen, &planarBlue, &rgbDest, 0);
  result = swift_bridgeObjectRelease();
  *a2 = v13;
  return result;
}

{
  void *v4;
  uint64_t v5;
  uint64_t v6;
  void *v7;
  uint64_t v8;
  uint64_t v9;
  void *v10;
  uint64_t v11;
  uint64_t v12;
  void *v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  vImagePixelCount v18;
  vImagePixelCount v19;
  vImagePixelCount v20;
  vImagePixelCount v21;
  size_t v22;
  size_t v23;
  void *v24;
  uint64_t v25;
  long long v26;
  uint64_t v27;
  long long v28;
  uint64_t v29;
  long long v30;
  uint64_t v31;
  long long v32;
  uint64_t result;
  vImage_Buffer bSrc;
  vImage_Buffer gSrc;
  vImage_Buffer rSrc;
  vImage_Buffer aSrc;
  vImage_Buffer argbDest;
  uint64_t v39;

  v39 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_34;
  }
  v4 = (void *)a1[4];
  if (!v4[2])
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  v5 = v4[6];
  if (v5 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  v6 = v4[5];
  if (v6 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v5)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  v7 = (void *)a1[5];
  if (!v7[2])
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  v8 = v7[6];
  if (v8 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  v9 = v7[5];
  if (v9 < 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v8)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v9)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (v5 != v8)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (v6 != v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  v10 = (void *)a1[6];
  if (!v10[2])
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  v11 = v10[6];
  if (v11 < 0)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  v12 = v10[5];
  if (v12 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (!v11)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v12)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (v5 != v11)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v6 != v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  v13 = (void *)a1[7];
  if (!v13[2])
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  v14 = v13[6];
  if (v14 < 0)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  v15 = v13[5];
  if (v15 < 0)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (!v14)
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  if (!v15)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  if (v5 != v14)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (v6 != v15)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  v16 = swift_allocObject();
  *(_OWORD *)(v16 + 16) = xmmword_1D2135280;
  v17 = (void *)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v6);
  v19 = v18;
  v21 = v20;
  v23 = v22;
  type metadata accessor for vImage.BufferReference();
  v24 = (void *)swift_allocObject();
  v24[2] = v17;
  v24[3] = v19;
  v24[4] = v21;
  v24[5] = v23;
  *(void *)(v16 + 32) = v17;
  *(void *)(v16 + 40) = v19;
  *(void *)(v16 + 48) = v21;
  *(void *)(v16 + 56) = v23;
  *(void *)(v16 + 64) = v24;
  argbDest.data = v17;
  argbDest.height = v19;
  argbDest.width = v21;
  argbDest.rowBytes = v23;
  v25 = a1[4];
  if (!*(void *)(v25 + 16))
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  v26 = *(_OWORD *)(v25 + 48);
  *(_OWORD *)&aSrc.data = *(_OWORD *)(v25 + 32);
  *(_OWORD *)&aSrc.width = v26;
  v27 = a1[5];
  if (!*(void *)(v27 + 16))
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  v28 = *(_OWORD *)(v27 + 48);
  *(_OWORD *)&rSrc.data = *(_OWORD *)(v27 + 32);
  *(_OWORD *)&rSrc.width = v28;
  v29 = a1[6];
  if (!*(void *)(v29 + 16))
  {
LABEL_62:
    __break(1u);
LABEL_63:
    __break(1u);
  }
  v30 = *(_OWORD *)(v29 + 48);
  *(_OWORD *)&gSrc.data = *(_OWORD *)(v29 + 32);
  *(_OWORD *)&gSrc.width = v30;
  v31 = a1[7];
  if (!*(void *)(v31 + 16)) {
    goto LABEL_63;
  }
  v32 = *(_OWORD *)(v31 + 48);
  *(_OWORD *)&bSrc.data = *(_OWORD *)(v31 + 32);
  *(_OWORD *)&bSrc.width = v32;
  vImageConvert_Planar16UtoARGB16U(&aSrc, &rSrc, &gSrc, &bSrc, &argbDest, 0);
  result = swift_bridgeObjectRelease();
  *a2 = v16;
  return result;
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>.init(planarBuffers:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<vImage_Buffer>?>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135290;
  *(void *)(inited + 32) = a2;
  v7 = (const vImage_Buffer **)(inited + 32);
  *(void *)(inited + 40) = a1;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutableRawPointer?>);
  uint64_t result = swift_initStackObject();
  *(_OWORD *)(result + 16) = xmmword_1D2135290;
  v9 = *(void **)a3;
  if (!*(void *)(*(void *)a3 + 16))
  {
    __break(1u);
    goto LABEL_7;
  }
  *(void *)(result + 32) = v9[4];
  uint64_t v10 = v9[4];
  if (v10)
  {
    *(void *)(result + 40) = v10 + 4;
    vImagePixelCount v11 = v9[6];
    if ((v11 & 0x8000000000000000) == 0)
    {
      vImagePixelCount v12 = v9[5];
      if ((v12 & 0x8000000000000000) == 0)
      {
        vImageConvert_PlanarToChunkyF(v7, (void **)(result + 32), 2u, 8uLL, v11, v12, v9[7], 0);
        swift_bridgeObjectRelease();
        return swift_bridgeObjectRelease();
      }
      goto LABEL_8;
    }
LABEL_7:
    __break(1u);
LABEL_8:
    __break(1u);
  }
  __break(1u);
  return result;
}

{
  uint64_t inited;
  const vImage_Buffer **v7;
  uint64_t result;
  void *v9;
  uint64_t v10;
  vImagePixelCount v11;
  vImagePixelCount v12;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<vImage_Buffer>?>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135290;
  *(void *)(inited + 32) = a2;
  v7 = (const vImage_Buffer **)(inited + 32);
  *(void *)(inited + 40) = a1;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutableRawPointer?>);
  uint64_t result = swift_initStackObject();
  *(_OWORD *)(result + 16) = xmmword_1D2135290;
  v9 = *(void **)a3;
  if (!*(void *)(*(void *)a3 + 16))
  {
    __break(1u);
    goto LABEL_7;
  }
  *(void *)(result + 32) = v9[4];
  uint64_t v10 = v9[4];
  if (v10)
  {
    *(void *)(result + 40) = v10 + 1;
    vImagePixelCount v11 = v9[6];
    if ((v11 & 0x8000000000000000) == 0)
    {
      vImagePixelCount v12 = v9[5];
      if ((v12 & 0x8000000000000000) == 0)
      {
        vImageConvert_PlanarToChunky8(v7, (void **)(result + 32), 2u, 2uLL, v11, v12, v9[7], 0);
        swift_bridgeObjectRelease();
        return swift_bridgeObjectRelease();
      }
      goto LABEL_8;
    }
LABEL_7:
    __break(1u);
LABEL_8:
    __break(1u);
  }
  __break(1u);
  return result;
}

void *vImage.PixelBuffer<>.planarBuffers()()
{
  uint64_t v13 = *MEMORY[0x1E4F143B8];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5Tm(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v10, 0, 2, (uint64_t)&v9, (uint64_t (*)(BOOL, unint64_t, uint64_t))specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:));
  if (!*(void *)(*(void *)v0 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  v2 = v1;
  unint64_t v3 = v1[2];
  if (!v3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v4 = v1[4];
  if (!*(void *)(v4 + 16))
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  long long v5 = *(_OWORD *)(v4 + 48);
  v12[0] = *(_OWORD *)(v4 + 32);
  v12[1] = v5;
  if (v3 < 2)
  {
LABEL_10:
    __break(1u);
LABEL_11:
    __break(1u);
  }
  uint64_t v6 = v1[5];
  if (!*(void *)(v6 + 16)) {
    goto LABEL_11;
  }
  long long v7 = *(_OWORD *)(v6 + 48);
  v11[0] = *(_OWORD *)(v6 + 32);
  v11[1] = v7;
  closure #1 in closure #1 in closure #2 in vImage.PixelBuffer<>.planarBuffers()((uint64_t)v11, v0, (uint64_t)v12);
  return v2;
}

{
  uint64_t *v0;
  void *v1;
  uint64_t v2;
  void *v3;
  long long v4;
  unint64_t v5;
  uint64_t v6;
  long long v7;
  uint64_t v8;
  long long v9;
  uint64_t v10;
  long long v11;
  uint64_t v12;
  long long v13;
  uint64_t v15;
  char v16[16];
  vImage_Buffer blue;
  vImage_Buffer green;
  vImage_Buffer red;
  vImage_Buffer alpha;
  vImage_Buffer src;
  uint64_t v22;

  v22 = *MEMORY[0x1E4F143B8];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7Planar8VGs5NeverOTgq5(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v16, 0, 4, (uint64_t)&v15);
  v2 = *v0;
  if (!*(void *)(*v0 + 16))
  {
    __break(1u);
    goto LABEL_12;
  }
  unint64_t v3 = v1;
  uint64_t v4 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&src.width = v4;
  long long v5 = v1[2];
  if (!v5)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  uint64_t v6 = v1[4];
  if (!*(void *)(v6 + 16))
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  long long v7 = *(_OWORD *)(v6 + 48);
  *(_OWORD *)&alpha.data = *(_OWORD *)(v6 + 32);
  *(_OWORD *)&alpha.width = v7;
  if (v5 < 2)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  v8 = v1[5];
  if (!*(void *)(v8 + 16))
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v9 = *(_OWORD *)(v8 + 48);
  *(_OWORD *)&red.data = *(_OWORD *)(v8 + 32);
  *(_OWORD *)&red.width = v9;
  if (v5 < 3)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v10 = v1[6];
  if (!*(void *)(v10 + 16))
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  vImagePixelCount v11 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&green.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&green.width = v11;
  if (v5 < 4)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
  }
  vImagePixelCount v12 = v1[7];
  if (!*(void *)(v12 + 16)) {
    goto LABEL_19;
  }
  uint64_t v13 = *(_OWORD *)(v12 + 48);
  *(_OWORD *)&blue.data = *(_OWORD *)(v12 + 32);
  *(_OWORD *)&blue.width = v13;
  vImageConvert_ARGBFFFFtoPlanar8(&src, &alpha, &red, &green, &blue, flt_1F28D7988, flt_1F28D7958, 0);
  return v3;
}

{
  uint64_t *v0;
  void *v1;
  uint64_t v2;
  void *v3;
  long long v4;
  unint64_t v5;
  uint64_t v6;
  long long v7;
  uint64_t v8;
  long long v9;
  uint64_t v10;
  long long v11;
  uint64_t v12;
  long long v13;
  uint64_t v15;
  char v16[16];
  vImage_Buffer destB;
  vImage_Buffer destG;
  vImage_Buffer destR;
  vImage_Buffer destA;
  vImage_Buffer srcARGB;
  uint64_t v22;

  v22 = *MEMORY[0x1E4F143B8];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v16, 0, 4, (uint64_t)&v15);
  v2 = *v0;
  if (!*(void *)(*v0 + 16))
  {
    __break(1u);
    goto LABEL_12;
  }
  unint64_t v3 = v1;
  uint64_t v4 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&srcARGB.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&srcARGB.width = v4;
  long long v5 = v1[2];
  if (!v5)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  uint64_t v6 = v1[4];
  if (!*(void *)(v6 + 16))
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  long long v7 = *(_OWORD *)(v6 + 48);
  *(_OWORD *)&destA.data = *(_OWORD *)(v6 + 32);
  *(_OWORD *)&destA.width = v7;
  if (v5 < 2)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  v8 = v1[5];
  if (!*(void *)(v8 + 16))
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v9 = *(_OWORD *)(v8 + 48);
  *(_OWORD *)&destR.data = *(_OWORD *)(v8 + 32);
  *(_OWORD *)&destR.width = v9;
  if (v5 < 3)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v10 = v1[6];
  if (!*(void *)(v10 + 16))
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  vImagePixelCount v11 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&destG.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&destG.width = v11;
  if (v5 < 4)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
  }
  vImagePixelCount v12 = v1[7];
  if (!*(void *)(v12 + 16)) {
    goto LABEL_19;
  }
  uint64_t v13 = *(_OWORD *)(v12 + 48);
  *(_OWORD *)&destB.data = *(_OWORD *)(v12 + 32);
  *(_OWORD *)&destB.width = v13;
  vImageConvert_ARGBFFFFtoPlanarF(&srcARGB, &destA, &destR, &destG, &destB, 0);
  return v3;
}

{
  uint64_t v0;
  void *v1;
  void *v2;
  unint64_t v3;
  uint64_t v4;
  long long v5;
  uint64_t v6;
  long long v7;
  uint64_t v9;
  char v10[16];
  _OWORD v11[2];
  _OWORD v12[2];
  uint64_t v13;

  uint64_t v13 = *MEMORY[0x1E4F143B8];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7Planar8VGs5NeverOTgq5(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v10, 0, 2, (uint64_t)&v9);
  if (!*(void *)(*(void *)v0 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  v2 = v1;
  unint64_t v3 = v1[2];
  if (!v3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v4 = v1[4];
  if (!*(void *)(v4 + 16))
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  long long v5 = *(_OWORD *)(v4 + 48);
  v12[0] = *(_OWORD *)(v4 + 32);
  v12[1] = v5;
  if (v3 < 2)
  {
LABEL_10:
    __break(1u);
LABEL_11:
    __break(1u);
  }
  uint64_t v6 = v1[5];
  if (!*(void *)(v6 + 16)) {
    goto LABEL_11;
  }
  long long v7 = *(_OWORD *)(v6 + 48);
  v11[0] = *(_OWORD *)(v6 + 32);
  v11[1] = v7;
  closure #1 in closure #1 in closure #2 in vImage.PixelBuffer<>.planarBuffers()((uint64_t)v11, v0, (uint64_t)v12);
  return v2;
}

{
  uint64_t *v0;
  void *v1;
  uint64_t v2;
  void *v3;
  long long v4;
  unint64_t v5;
  uint64_t v6;
  long long v7;
  uint64_t v8;
  long long v9;
  uint64_t v10;
  long long v11;
  uint64_t v12;
  long long v13;
  uint64_t v15;
  char v16[16];
  vImage_Buffer destB;
  vImage_Buffer destG;
  vImage_Buffer destR;
  vImage_Buffer destA;
  vImage_Buffer srcARGB;
  uint64_t v22;

  v22 = *MEMORY[0x1E4F143B8];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7Planar8VGs5NeverOTgq5(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v16, 0, 4, (uint64_t)&v15);
  v2 = *v0;
  if (!*(void *)(*v0 + 16))
  {
    __break(1u);
    goto LABEL_12;
  }
  unint64_t v3 = v1;
  uint64_t v4 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&srcARGB.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&srcARGB.width = v4;
  long long v5 = v1[2];
  if (!v5)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  uint64_t v6 = v1[4];
  if (!*(void *)(v6 + 16))
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  long long v7 = *(_OWORD *)(v6 + 48);
  *(_OWORD *)&destA.data = *(_OWORD *)(v6 + 32);
  *(_OWORD *)&destA.width = v7;
  if (v5 < 2)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  v8 = v1[5];
  if (!*(void *)(v8 + 16))
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v9 = *(_OWORD *)(v8 + 48);
  *(_OWORD *)&destR.data = *(_OWORD *)(v8 + 32);
  *(_OWORD *)&destR.width = v9;
  if (v5 < 3)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v10 = v1[6];
  if (!*(void *)(v10 + 16))
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  vImagePixelCount v11 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&destG.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&destG.width = v11;
  if (v5 < 4)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
  }
  vImagePixelCount v12 = v1[7];
  if (!*(void *)(v12 + 16)) {
    goto LABEL_19;
  }
  uint64_t v13 = *(_OWORD *)(v12 + 48);
  *(_OWORD *)&destB.data = *(_OWORD *)(v12 + 32);
  *(_OWORD *)&destB.width = v13;
  vImageConvert_ARGB8888toPlanar8(&srcARGB, &destA, &destR, &destG, &destB, 0);
  return v3;
}

{
  uint64_t *v0;
  void *v1;
  uint64_t v2;
  void *v3;
  long long v4;
  unint64_t v5;
  uint64_t v6;
  long long v7;
  uint64_t v8;
  long long v9;
  uint64_t v10;
  long long v11;
  uint64_t v12;
  long long v13;
  uint64_t v15;
  char v16[16];
  vImage_Buffer blue;
  vImage_Buffer green;
  vImage_Buffer red;
  vImage_Buffer alpha;
  vImage_Buffer src;
  uint64_t v22;

  v22 = *MEMORY[0x1E4F143B8];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v16, 0, 4, (uint64_t)&v15);
  v2 = *v0;
  if (!*(void *)(*v0 + 16))
  {
    __break(1u);
    goto LABEL_12;
  }
  unint64_t v3 = v1;
  uint64_t v4 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&src.width = v4;
  long long v5 = v1[2];
  if (!v5)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  uint64_t v6 = v1[4];
  if (!*(void *)(v6 + 16))
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  long long v7 = *(_OWORD *)(v6 + 48);
  *(_OWORD *)&alpha.data = *(_OWORD *)(v6 + 32);
  *(_OWORD *)&alpha.width = v7;
  if (v5 < 2)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  v8 = v1[5];
  if (!*(void *)(v8 + 16))
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v9 = *(_OWORD *)(v8 + 48);
  *(_OWORD *)&red.data = *(_OWORD *)(v8 + 32);
  *(_OWORD *)&red.width = v9;
  if (v5 < 3)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v10 = v1[6];
  if (!*(void *)(v10 + 16))
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  vImagePixelCount v11 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&green.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&green.width = v11;
  if (v5 < 4)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
  }
  vImagePixelCount v12 = v1[7];
  if (!*(void *)(v12 + 16)) {
    goto LABEL_19;
  }
  uint64_t v13 = *(_OWORD *)(v12 + 48);
  *(_OWORD *)&blue.data = *(_OWORD *)(v12 + 32);
  *(_OWORD *)&blue.width = v13;
  vImageConvert_ARGB8888toPlanarF(&src, &alpha, &red, &green, &blue, flt_1F28D7800, flt_1F28D77D0, 0);
  return v3;
}

{
  uint64_t *v0;
  void *v1;
  uint64_t v2;
  void *v3;
  long long v4;
  unint64_t v5;
  uint64_t v6;
  long long v7;
  uint64_t v8;
  long long v9;
  uint64_t v10;
  long long v11;
  uint64_t v13;
  char v14[16];
  vImage_Buffer blueDest;
  vImage_Buffer greenDest;
  vImage_Buffer redDest;
  vImage_Buffer rgbSrc;
  uint64_t v19;

  v19 = *MEMORY[0x1E4F143B8];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7Planar8VGs5NeverOTgq5(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v14, 0, 3, (uint64_t)&v13);
  v2 = *v0;
  if (!*(void *)(*v0 + 16))
  {
    __break(1u);
    goto LABEL_10;
  }
  unint64_t v3 = v1;
  uint64_t v4 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&rgbSrc.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&rgbSrc.width = v4;
  long long v5 = v1[2];
  if (!v5)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  uint64_t v6 = v1[4];
  if (!*(void *)(v6 + 16))
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  long long v7 = *(_OWORD *)(v6 + 48);
  *(_OWORD *)&redDest.data = *(_OWORD *)(v6 + 32);
  *(_OWORD *)&redDest.width = v7;
  if (v5 < 2)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v8 = v1[5];
  if (!*(void *)(v8 + 16))
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  uint64_t v9 = *(_OWORD *)(v8 + 48);
  *(_OWORD *)&greenDest.data = *(_OWORD *)(v8 + 32);
  *(_OWORD *)&greenDest.width = v9;
  if (v5 < 3)
  {
LABEL_14:
    __break(1u);
LABEL_15:
    __break(1u);
  }
  uint64_t v10 = v1[6];
  if (!*(void *)(v10 + 16)) {
    goto LABEL_15;
  }
  vImagePixelCount v11 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&blueDest.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&blueDest.width = v11;
  vImageConvert_RGB888toPlanar8(&rgbSrc, &redDest, &greenDest, &blueDest, 0);
  return v3;
}

{
  uint64_t *v0;
  void *v1;
  uint64_t v2;
  void *v3;
  long long v4;
  unint64_t v5;
  uint64_t v6;
  long long v7;
  uint64_t v8;
  long long v9;
  uint64_t v10;
  long long v11;
  uint64_t v13;
  char v14[16];
  vImage_Buffer blueDest;
  vImage_Buffer greenDest;
  vImage_Buffer redDest;
  vImage_Buffer rgbSrc;
  uint64_t v19;

  v19 = *MEMORY[0x1E4F143B8];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v14, 0, 3, (uint64_t)&v13);
  v2 = *v0;
  if (!*(void *)(*v0 + 16))
  {
    __break(1u);
    goto LABEL_10;
  }
  unint64_t v3 = v1;
  uint64_t v4 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&rgbSrc.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&rgbSrc.width = v4;
  long long v5 = v1[2];
  if (!v5)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  uint64_t v6 = v1[4];
  if (!*(void *)(v6 + 16))
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  long long v7 = *(_OWORD *)(v6 + 48);
  *(_OWORD *)&redDest.data = *(_OWORD *)(v6 + 32);
  *(_OWORD *)&redDest.width = v7;
  if (v5 < 2)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  v8 = v1[5];
  if (!*(void *)(v8 + 16))
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  uint64_t v9 = *(_OWORD *)(v8 + 48);
  *(_OWORD *)&greenDest.data = *(_OWORD *)(v8 + 32);
  *(_OWORD *)&greenDest.width = v9;
  if (v5 < 3)
  {
LABEL_14:
    __break(1u);
LABEL_15:
    __break(1u);
  }
  uint64_t v10 = v1[6];
  if (!*(void *)(v10 + 16)) {
    goto LABEL_15;
  }
  vImagePixelCount v11 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&blueDest.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&blueDest.width = v11;
  vImageConvert_RGBFFFtoPlanarF(&rgbSrc, &redDest, &greenDest, &blueDest, 0);
  return v3;
}

{
  uint64_t *v0;
  void *v1;
  uint64_t v2;
  void *v3;
  long long v4;
  unint64_t v5;
  uint64_t v6;
  long long v7;
  uint64_t v8;
  long long v9;
  uint64_t v10;
  long long v11;
  uint64_t v12;
  long long v13;
  uint64_t v15;
  char v16[16];
  vImage_Buffer bDest;
  vImage_Buffer gDest;
  vImage_Buffer rDest;
  vImage_Buffer aDest;
  vImage_Buffer argbSrc;
  uint64_t v22;

  v22 = *MEMORY[0x1E4F143B8];
  v1 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI9Planar16UVGs5NeverOTgq5(partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers(), (uint64_t)v16, 0, 4, (uint64_t)&v15);
  v2 = *v0;
  if (!*(void *)(*v0 + 16))
  {
    __break(1u);
    goto LABEL_12;
  }
  unint64_t v3 = v1;
  uint64_t v4 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&argbSrc.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&argbSrc.width = v4;
  long long v5 = v1[2];
  if (!v5)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  uint64_t v6 = v1[4];
  if (!*(void *)(v6 + 16))
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  long long v7 = *(_OWORD *)(v6 + 48);
  *(_OWORD *)&aDest.data = *(_OWORD *)(v6 + 32);
  *(_OWORD *)&aDest.width = v7;
  if (v5 < 2)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  v8 = v1[5];
  if (!*(void *)(v8 + 16))
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v9 = *(_OWORD *)(v8 + 48);
  *(_OWORD *)&rDest.data = *(_OWORD *)(v8 + 32);
  *(_OWORD *)&rDest.width = v9;
  if (v5 < 3)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v10 = v1[6];
  if (!*(void *)(v10 + 16))
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  vImagePixelCount v11 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&gDest.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&gDest.width = v11;
  if (v5 < 4)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
  }
  vImagePixelCount v12 = v1[7];
  if (!*(void *)(v12 + 16)) {
    goto LABEL_19;
  }
  uint64_t v13 = *(_OWORD *)(v12 + 48);
  *(_OWORD *)&bDest.data = *(_OWORD *)(v12 + 32);
  *(_OWORD *)&bDest.width = v13;
  vImageConvert_ARGB16UtoPlanar16U(&argbSrc, &aDest, &rDest, &gDest, &bDest, 0);
  return v3;
}

void *closure #1 in vImage.PixelBuffer<>.planarBuffers()@<X0>(uint64_t *a1@<X1>, uint64_t *a2@<X8>)
{
  uint64_t v5 = *a1;
  if (!*(void *)(*a1 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v6 = *(void *)(v5 + 48);
  if (v6 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  uint64_t v7 = *(void *)(v5 + 40);
  if (v7 < 0)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (!v6)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  if (!v7)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  unint64_t v3 = v2;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v8 = swift_allocObject();
  *(_OWORD *)(v8 + 16) = xmmword_1D2135280;
  uint64_t v9 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v6, v7);
  if (!v2)
  {
    uint64_t v13 = v9;
    uint64_t v14 = v10;
    uint64_t v15 = v11;
    uint64_t v16 = v12;
    type metadata accessor for vImage.BufferReference();
    uint64_t result = (void *)swift_allocObject();
    result[2] = v13;
    result[3] = v14;
    result[4] = v15;
    result[5] = v16;
    *(void *)(v8 + 32) = v13;
    *(void *)(v8 + 40) = v14;
    *(void *)(v8 + 48) = v15;
    *(void *)(v8 + 56) = v16;
    *(void *)(v8 + 64) = result;
    *a2 = v8;
    return result;
  }
LABEL_13:

  uint64_t result = (void *)_assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

{
  void *v2;
  void *v3;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *result;

  uint64_t v5 = *a1;
  if (!*(void *)(*a1 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v6 = *(void *)(v5 + 48);
  if (v6 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  uint64_t v7 = *(void *)(v5 + 40);
  if (v7 < 0)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (!v6)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  if (!v7)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  unint64_t v3 = v2;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v8 = swift_allocObject();
  *(_OWORD *)(v8 + 16) = xmmword_1D2135280;
  uint64_t v9 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v6, v7);
  if (!v2)
  {
    uint64_t v13 = v9;
    uint64_t v14 = v10;
    uint64_t v15 = v11;
    uint64_t v16 = v12;
    type metadata accessor for vImage.BufferReference();
    uint64_t result = (void *)swift_allocObject();
    result[2] = v13;
    result[3] = v14;
    result[4] = v15;
    result[5] = v16;
    *(void *)(v8 + 32) = v13;
    *(void *)(v8 + 40) = v14;
    *(void *)(v8 + 48) = v15;
    *(void *)(v8 + 56) = v16;
    *(void *)(v8 + 64) = result;
    *a2 = v8;
    return result;
  }
LABEL_13:

  uint64_t result = (void *)_assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

{
  void *v2;
  void *v3;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *result;

  uint64_t v5 = *a1;
  if (!*(void *)(*a1 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v6 = *(void *)(v5 + 48);
  if (v6 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  uint64_t v7 = *(void *)(v5 + 40);
  if (v7 < 0)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (!v6)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  if (!v7)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  unint64_t v3 = v2;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v8 = swift_allocObject();
  *(_OWORD *)(v8 + 16) = xmmword_1D2135280;
  uint64_t v9 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v6, v7);
  if (!v2)
  {
    uint64_t v13 = v9;
    uint64_t v14 = v10;
    uint64_t v15 = v11;
    uint64_t v16 = v12;
    type metadata accessor for vImage.BufferReference();
    uint64_t result = (void *)swift_allocObject();
    result[2] = v13;
    result[3] = v14;
    result[4] = v15;
    result[5] = v16;
    *(void *)(v8 + 32) = v13;
    *(void *)(v8 + 40) = v14;
    *(void *)(v8 + 48) = v15;
    *(void *)(v8 + 56) = v16;
    *(void *)(v8 + 64) = result;
    *a2 = v8;
    return result;
  }
LABEL_13:

  uint64_t result = (void *)_assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

void *_sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5(void *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5Tm(a1, a2, a3, a4, a5, (uint64_t (*)(BOOL, unint64_t, uint64_t))specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:));
}

void *_sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7Planar8VGs5NeverOTgq5(void *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5Tm(a1, a2, a3, a4, a5, (uint64_t (*)(BOOL, unint64_t, uint64_t))specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:));
}

char *_sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO13BufferWrapperVs5NeverOTg5(char *result, uint64_t a2, uint64_t a3, uint64_t a4)
{
  v21 = result;
  uint64_t v5 = a4 - a3;
  if (__OFSUB__(a4, a3))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v6 = v4;
  uint64_t v7 = MEMORY[0x1E4FBC860];
  if (!v5) {
    return (char *)v7;
  }
  uint64_t v26 = MEMORY[0x1E4FBC860];
  uint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v5 & ~(v5 >> 63), 0);
  if ((v5 & 0x8000000000000000) == 0)
  {
    uint64_t v7 = v26;
    if (a4 <= a3) {
      uint64_t v10 = a3;
    }
    else {
      uint64_t v10 = a4;
    }
    uint64_t v20 = v10;
    uint64_t v11 = a3;
    while (a4 != v11)
    {
      uint64_t v22 = v11;
      uint64_t result = (char *)((char *(*)(long long *__return_ptr, uint64_t *))v21)(&v23, &v22);
      if (v6)
      {
        swift_release();
        return (char *)v7;
      }
      uint64_t v6 = 0;
      long long v12 = v23;
      long long v13 = v24;
      uint64_t v14 = v25;
      uint64_t v26 = v7;
      unint64_t v16 = *(void *)(v7 + 16);
      unint64_t v15 = *(void *)(v7 + 24);
      if (v16 >= v15 >> 1)
      {
        long long v18 = v24;
        long long v19 = v23;
        uint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v15 > 1), v16 + 1, 1);
        long long v13 = v18;
        long long v12 = v19;
        uint64_t v7 = v26;
      }
      *(void *)(v7 + 16) = v16 + 1;
      uint64_t v17 = v7 + 40 * v16;
      *(_OWORD *)(v17 + 32) = v12;
      *(_OWORD *)(v17 + 48) = v13;
      *(void *)(v17 + 64) = v14;
      if (a4 < a3) {
        goto LABEL_19;
      }
      if (v20 == v11) {
        goto LABEL_20;
      }
      if (a4 == ++v11) {
        return (char *)v7;
      }
    }
    __break(1u);
LABEL_19:
    __break(1u);
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
LABEL_22:
  __break(1u);
  return result;
}

void *_sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI9Planar16UVGs5NeverOTgq5(void *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5Tm(a1, a2, a3, a4, a5, (uint64_t (*)(BOOL, unint64_t, uint64_t))specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:));
}

void *_sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_10Accelerate6vImageO11PixelBufferVy_AI7PlanarFVGs5NeverOTgq5Tm(void *result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(BOOL, unint64_t, uint64_t))
{
  void *(*v18)(uint64_t *__return_ptr, uint64_t *);
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;

  long long v18 = (void *(*)(uint64_t *__return_ptr, uint64_t *))result;
  uint64_t v7 = a4 - a3;
  if (__OFSUB__(a4, a3))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v8 = v6;
  uint64_t v9 = MEMORY[0x1E4FBC860];
  if (!v7) {
    return (void *)v9;
  }
  v21 = MEMORY[0x1E4FBC860];
  uint64_t result = (void *)a6(0, v7 & ~(v7 >> 63), 0);
  if ((v7 & 0x8000000000000000) == 0)
  {
    uint64_t v17 = a6;
    uint64_t v9 = v21;
    if (a4 <= a3) {
      uint64_t v13 = a3;
    }
    else {
      uint64_t v13 = a4;
    }
    uint64_t v14 = a3;
    while (a4 != v14)
    {
      long long v19 = v14;
      uint64_t result = v18(&v20, &v19);
      if (v8)
      {
        swift_release();
        return (void *)v9;
      }
      uint64_t v8 = 0;
      v21 = v9;
      unint64_t v16 = *(void *)(v9 + 16);
      unint64_t v15 = *(void *)(v9 + 24);
      if (v16 >= v15 >> 1)
      {
        uint64_t result = (void *)v17(v15 > 1, v16 + 1, 1);
        uint64_t v9 = v21;
      }
      *(void *)(v9 + 16) = v16 + 1;
      *(void *)(v9 + 8 * v16 + 32) = v20;
      if (a4 < a3) {
        goto LABEL_19;
      }
      if (v13 == v14) {
        goto LABEL_20;
      }
      if (a4 == ++v14) {
        return (void *)v9;
      }
    }
    __break(1u);
LABEL_19:
    __break(1u);
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
LABEL_22:
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #2 in vImage.PixelBuffer<>.planarBuffers()(uint64_t a1, uint64_t a2, uint64_t a3)
{
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer?>);
  uint64_t result = swift_initStackObject();
  *(_OWORD *)(result + 16) = xmmword_1D2135290;
  uint64_t v7 = *(void **)a2;
  if (!*(void *)(*(void *)a2 + 16))
  {
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v8 = (const void **)(result + 32);
  uint64_t v9 = v7[4];
  *(void *)(result + 32) = v9;
  if (v9)
  {
    *(void *)(result + 40) = v9 + 4;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<vImage_Buffer>?>);
    uint64_t result = swift_initStackObject();
    *(_OWORD *)(result + 16) = xmmword_1D2135290;
    *(void *)(result + 32) = a3;
    *(void *)(result + 40) = a1;
    vImagePixelCount v10 = v7[6];
    if ((v10 & 0x8000000000000000) == 0)
    {
      vImagePixelCount v11 = v7[5];
      if ((v11 & 0x8000000000000000) == 0)
      {
        vImageConvert_ChunkyToPlanarF(v8, (const vImage_Buffer **)(result + 32), 2u, 8uLL, v10, v11, v7[7], 0);
        swift_bridgeObjectRelease();
        return swift_bridgeObjectRelease();
      }
      goto LABEL_8;
    }
LABEL_7:
    __break(1u);
LABEL_8:
    __break(1u);
  }
  __break(1u);
  return result;
}

{
  uint64_t result;
  void *v7;
  const void **v8;
  uint64_t v9;
  vImagePixelCount v10;
  vImagePixelCount v11;

  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer?>);
  uint64_t result = swift_initStackObject();
  *(_OWORD *)(result + 16) = xmmword_1D2135290;
  uint64_t v7 = *(void **)a2;
  if (!*(void *)(*(void *)a2 + 16))
  {
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v8 = (const void **)(result + 32);
  uint64_t v9 = v7[4];
  *(void *)(result + 32) = v9;
  if (v9)
  {
    *(void *)(result + 40) = v9 + 1;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<vImage_Buffer>?>);
    uint64_t result = swift_initStackObject();
    *(_OWORD *)(result + 16) = xmmword_1D2135290;
    *(void *)(result + 32) = a3;
    *(void *)(result + 40) = a1;
    vImagePixelCount v10 = v7[6];
    if ((v10 & 0x8000000000000000) == 0)
    {
      vImagePixelCount v11 = v7[5];
      if ((v11 & 0x8000000000000000) == 0)
      {
        vImageConvert_ChunkyToPlanar8(v8, (const vImage_Buffer **)(result + 32), 2u, 2uLL, v10, v11, v7[7], 0);
        swift_bridgeObjectRelease();
        return swift_bridgeObjectRelease();
      }
      goto LABEL_8;
    }
LABEL_7:
    __break(1u);
LABEL_8:
    __break(1u);
  }
  __break(1u);
  return result;
}

vImage_Error vImage.PixelBuffer<>.convert(to:)(uint64_t a1)
{
  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v3 = v2[6];
  if (v3 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7) {
    goto LABEL_25;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = (void *)v5[4];
  size_t v10 = v2[7];
  size_t v11 = v5[7];
  src.data = v8;
  src.height = v4;
  src.width = 2 * v3;
  src.rowBytes = v10;
  dest.data = v9;
  dest.height = v4;
  dest.width = 2 * v3;
  dest.rowBytes = v11;
  return vImageConvert_PlanarFtoPlanar16F(&src, &dest, 0);
}

{
  uint64_t *v1;
  uint64_t v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  uint64_t v10;
  vImagePixelCount v11;
  size_t v12;
  uint64_t v13;
  vImagePixelCount v14;
  size_t v15;
  uint64_t v16;
  vImagePixelCount v17;
  size_t v18;
  long long v20;
  long long v21;
  long long v22;
  vImage_Buffer planarBlue;
  vImage_Buffer planarGreen;
  vImage_Buffer planarRed;
  vImage_Buffer rgbDest;
  uint64_t v27;

  v27 = *MEMORY[0x1E4F143B8];
  v2 = *v1;
  if (!*(void *)(*v1 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v3 = *(void *)(v2 + 48);
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  vImagePixelCount v4 = *(void *)(v2 + 40);
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v3)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v4)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v6)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v7)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v3 != v6)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v4 != v7)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  uint64_t v8 = (void *)v5[4];
  uint64_t v9 = v5[7];
  rgbDest.data = v8;
  rgbDest.height = v4;
  rgbDest.width = v3;
  rgbDest.rowBytes = v9;
  size_t v10 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (!*(void *)(v10 + 16))
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  uint64_t v20 = *(_OWORD *)(v10 + 32);
  size_t v11 = *(void *)(v10 + 48);
  long long v12 = *(void *)(v10 + 56);
  swift_bridgeObjectRelease();
  *(_OWORD *)&planarRed.data = v20;
  planarRed.width = v11;
  planarRed.rowBytes = v12;
  uint64_t v13 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(void *)(v13 + 16) < 2uLL)
  {
LABEL_30:
    __break(1u);
LABEL_31:
    __break(1u);
  }
  v21 = *(_OWORD *)(v13 + 64);
  uint64_t v14 = *(void *)(v13 + 80);
  uint64_t v15 = *(void *)(v13 + 88);
  swift_bridgeObjectRelease();
  *(_OWORD *)&planarGreen.data = v21;
  planarGreen.width = v14;
  planarGreen.rowBytes = v15;
  unint64_t v16 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(void *)(v16 + 16) < 3uLL) {
    goto LABEL_31;
  }
  uint64_t v22 = *(_OWORD *)(v16 + 96);
  uint64_t v17 = *(void *)(v16 + 112);
  long long v18 = *(void *)(v16 + 120);
  swift_bridgeObjectRelease();
  *(_OWORD *)&planarBlue.data = v22;
  planarBlue.width = v17;
  planarBlue.rowBytes = v18;
  return vImageConvert_PlanarFtoRGBFFF(&planarRed, &planarGreen, &planarBlue, &rgbDest, 0);
}

{
  uint64_t *v1;
  uint64_t v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  uint64_t v10;
  vImagePixelCount v11;
  size_t v12;
  uint64_t v13;
  vImagePixelCount v14;
  size_t v15;
  uint64_t v16;
  vImagePixelCount v17;
  size_t v18;
  uint64_t v19;
  vImagePixelCount v20;
  size_t v21;
  long long v23;
  long long v24;
  long long v25;
  long long v26;
  vImage_Buffer srcB;
  vImage_Buffer srcG;
  vImage_Buffer srcR;
  vImage_Buffer srcA;
  vImage_Buffer dest;
  uint64_t v32;

  v32 = *MEMORY[0x1E4F143B8];
  v2 = *v1;
  if (!*(void *)(*v1 + 16))
  {
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v3 = *(void *)(v2 + 48);
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  vImagePixelCount v4 = *(void *)(v2 + 40);
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v3)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v4)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v6)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v3 != v6)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (v4 != v7)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  uint64_t v8 = (void *)v5[4];
  uint64_t v9 = v5[7];
  dest.data = v8;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v9;
  size_t v10 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (!*(void *)(v10 + 16))
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  long long v23 = *(_OWORD *)(v10 + 32);
  size_t v11 = *(void *)(v10 + 48);
  long long v12 = *(void *)(v10 + 56);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcA.data = v23;
  srcA.width = v11;
  srcA.rowBytes = v12;
  uint64_t v13 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(void *)(v13 + 16) < 2uLL)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  long long v24 = *(_OWORD *)(v13 + 64);
  uint64_t v14 = *(void *)(v13 + 80);
  uint64_t v15 = *(void *)(v13 + 88);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcR.data = v24;
  srcR.width = v14;
  srcR.rowBytes = v15;
  unint64_t v16 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(void *)(v16 + 16) < 3uLL)
  {
LABEL_32:
    __break(1u);
LABEL_33:
    __break(1u);
  }
  uint64_t v25 = *(_OWORD *)(v16 + 96);
  uint64_t v17 = *(void *)(v16 + 112);
  long long v18 = *(void *)(v16 + 120);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcG.data = v25;
  srcG.width = v17;
  srcG.rowBytes = v18;
  long long v19 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(void *)(v19 + 16) < 4uLL) {
    goto LABEL_33;
  }
  uint64_t v26 = *(_OWORD *)(v19 + 128);
  uint64_t v20 = *(void *)(v19 + 144);
  v21 = *(void *)(v19 + 152);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcB.data = v26;
  srcB.width = v20;
  srcB.rowBytes = v21;
  return vImageConvert_Planar8toARGB8888(&srcA, &srcR, &srcG, &srcB, &dest, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7) {
    goto LABEL_25;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  size_t v10 = (void *)v5[4];
  size_t v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_16Uto16F(&src, &dest, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7) {
    goto LABEL_25;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  size_t v10 = (void *)v5[4];
  size_t v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_ARGBFFFFtoARGB8888_dithered(&src, &dest, flt_1F28D7A10, flt_1F28D79B8, 0, byte_1F28D79E8, 0);
}

{
  uint64_t v1;
  void *v2;
  unint64_t v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v3)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v4)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v6)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (v3 != v6)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v4 != v7)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v3 >> 62) {
    goto LABEL_27;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = 4 * v3;
  src.rowBytes = v9;
  size_t v10 = (void *)v5[4];
  size_t v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = 4 * v3;
  dest.rowBytes = v11;
  return vImageConvert_FTo16U(&src, &dest, 0.0, 0.000015259, 0);
}

{
  uint64_t v1;
  void *v2;
  unint64_t v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  void *v9;
  size_t v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v3)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v4)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v6)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (v3 != v6)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v4 != v7)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v3 >> 62) {
    goto LABEL_27;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = (void *)v5[4];
  size_t v10 = v2[7];
  size_t v11 = v5[7];
  src.data = v8;
  src.height = v4;
  src.width = 4 * v3;
  src.rowBytes = v10;
  dest.data = v9;
  dest.height = v4;
  dest.width = 4 * v3;
  dest.rowBytes = v11;
  return vImageConvert_PlanarFtoPlanar16F(&src, &dest, 0);
}

{
  uint64_t v1;
  void *v2;
  uint64_t v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  void *v9;
  size_t v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v3 = v2[6];
  if (v3 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7) {
    goto LABEL_25;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = (void *)v5[4];
  size_t v10 = v2[7];
  size_t v11 = v5[7];
  src.data = v8;
  src.height = v4;
  src.width = 2 * v3;
  src.rowBytes = v10;
  dest.data = v9;
  dest.height = v4;
  dest.width = 2 * v3;
  dest.rowBytes = v11;
  return vImageConvert_Planar8toPlanar16F(&src, &dest, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7) {
    goto LABEL_25;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  size_t v10 = (void *)v5[4];
  size_t v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_Planar8toPlanarF(&src, &dest, 1.0, 0.0, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7) {
    goto LABEL_25;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  size_t v10 = (void *)v5[4];
  size_t v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_Planar8toPlanar16F(&src, &dest, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7) {
    goto LABEL_25;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  size_t v10 = (void *)v5[4];
  size_t v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_ARGB8888ToARGB16U(&src, &dest, byte_1F28D78A8, 0, word_1F28D7880, 0);
}

{
  uint64_t v1;
  void *v2;
  unint64_t v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  void *v9;
  size_t v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v3)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v4)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v6)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (v3 != v6)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v4 != v7)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v3 >> 62) {
    goto LABEL_27;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = (void *)v5[4];
  size_t v10 = v2[7];
  size_t v11 = v5[7];
  src.data = v8;
  src.height = v4;
  src.width = 4 * v3;
  src.rowBytes = v10;
  dest.data = v9;
  dest.height = v4;
  dest.width = 4 * v3;
  dest.rowBytes = v11;
  return vImageConvert_Planar8toPlanar16F(&src, &dest, 0);
}

{
  uint64_t v1;
  void *v2;
  uint64_t v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  void *v9;
  size_t v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v3 = v2[6];
  if (v3 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7) {
    goto LABEL_25;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = (void *)v5[4];
  size_t v10 = v2[7];
  size_t v11 = v5[7];
  src.data = v8;
  src.height = v4;
  src.width = 2 * v3;
  src.rowBytes = v10;
  dest.data = v9;
  dest.height = v4;
  dest.width = 2 * v3;
  dest.rowBytes = v11;
  return vImageConvert_16Uto16F(&src, &dest, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7) {
    goto LABEL_25;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  size_t v10 = (void *)v5[4];
  size_t v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_RGBFFFtoRGB888_dithered(&src, &dest, flt_1F28D7928, flt_1F28D78F8, 0, 0);
}

{
  uint64_t *v1;
  uint64_t v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  uint64_t v10;
  vImagePixelCount v11;
  size_t v12;
  uint64_t v13;
  vImagePixelCount v14;
  size_t v15;
  uint64_t v16;
  vImagePixelCount v17;
  size_t v18;
  uint64_t v19;
  vImagePixelCount v20;
  size_t v21;
  long long v23;
  long long v24;
  long long v25;
  long long v26;
  vImage_Buffer srcB;
  vImage_Buffer srcG;
  vImage_Buffer srcR;
  vImage_Buffer srcA;
  vImage_Buffer dest;
  uint64_t v32;

  v32 = *MEMORY[0x1E4F143B8];
  v2 = *v1;
  if (!*(void *)(*v1 + 16))
  {
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v3 = *(void *)(v2 + 48);
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  vImagePixelCount v4 = *(void *)(v2 + 40);
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v3)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v4)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v6)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v3 != v6)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (v4 != v7)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  uint64_t v8 = (void *)v5[4];
  uint64_t v9 = v5[7];
  dest.data = v8;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v9;
  size_t v10 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (!*(void *)(v10 + 16))
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  long long v23 = *(_OWORD *)(v10 + 32);
  size_t v11 = *(void *)(v10 + 48);
  long long v12 = *(void *)(v10 + 56);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcA.data = v23;
  srcA.width = v11;
  srcA.rowBytes = v12;
  uint64_t v13 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(void *)(v13 + 16) < 2uLL)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  long long v24 = *(_OWORD *)(v13 + 64);
  uint64_t v14 = *(void *)(v13 + 80);
  uint64_t v15 = *(void *)(v13 + 88);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcR.data = v24;
  srcR.width = v14;
  srcR.rowBytes = v15;
  unint64_t v16 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(void *)(v16 + 16) < 3uLL)
  {
LABEL_32:
    __break(1u);
LABEL_33:
    __break(1u);
  }
  uint64_t v25 = *(_OWORD *)(v16 + 96);
  uint64_t v17 = *(void *)(v16 + 112);
  long long v18 = *(void *)(v16 + 120);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcG.data = v25;
  srcG.width = v17;
  srcG.rowBytes = v18;
  long long v19 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(void *)(v19 + 16) < 4uLL) {
    goto LABEL_33;
  }
  uint64_t v26 = *(_OWORD *)(v19 + 128);
  uint64_t v20 = *(void *)(v19 + 144);
  v21 = *(void *)(v19 + 152);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcB.data = v26;
  srcB.width = v20;
  srcB.rowBytes = v21;
  return vImageConvert_PlanarFtoARGBFFFF(&srcA, &srcR, &srcG, &srcB, &dest, 0);
}

{
  uint64_t v1;
  void *v2;
  unint64_t v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v3)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v4)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v6)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (v3 != v6)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v4 != v7)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v3 >> 62) {
    goto LABEL_27;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = 4 * v3;
  src.rowBytes = v9;
  size_t v10 = (void *)v5[4];
  size_t v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = 4 * v3;
  dest.rowBytes = v11;
  return vImageConvert_16UToF(&src, &dest, 0.0, 0.000015259, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7) {
    goto LABEL_25;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  size_t v10 = (void *)v5[4];
  size_t v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_ARGB16UToARGB8888(&src, &dest, byte_1F28D7BD0, 0, byte_1F28D7BA8, 0);
}

{
  uint64_t v1;
  void *v2;
  unint64_t v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  void *v9;
  size_t v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v3)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v4)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v6)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (v3 != v6)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v4 != v7)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v3 >> 62) {
    goto LABEL_27;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = (void *)v5[4];
  size_t v10 = v2[7];
  size_t v11 = v5[7];
  src.data = v8;
  src.height = v4;
  src.width = 4 * v3;
  src.rowBytes = v10;
  dest.data = v9;
  dest.height = v4;
  dest.width = 4 * v3;
  dest.rowBytes = v11;
  return vImageConvert_16Uto16F(&src, &dest, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7) {
    goto LABEL_25;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  size_t v10 = (void *)v5[4];
  size_t v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_PlanarFtoPlanar16F(&src, &dest, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  void *v10;
  size_t v11;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v15;

  uint64_t v15 = *MEMORY[0x1E4F143B8];
  v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v3)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v3 != v6)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v4 != v7) {
    goto LABEL_25;
  }
  uint64_t v8 = (void *)v2[4];
  uint64_t v9 = v2[7];
  src.data = v8;
  src.height = v4;
  src.width = v3;
  src.rowBytes = v9;
  size_t v10 = (void *)v5[4];
  size_t v11 = v5[7];
  dest.data = v10;
  dest.height = v4;
  dest.width = v3;
  dest.rowBytes = v11;
  return vImageConvert_PlanarFtoPlanar8(&src, &dest, 1.0, 0.0, 0);
}

{
  uint64_t *v1;
  uint64_t v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  size_t v9;
  uint64_t v10;
  vImagePixelCount v11;
  size_t v12;
  uint64_t v13;
  vImagePixelCount v14;
  size_t v15;
  uint64_t v16;
  vImagePixelCount v17;
  size_t v18;
  long long v20;
  long long v21;
  long long v22;
  vImage_Buffer planarBlue;
  vImage_Buffer planarGreen;
  vImage_Buffer planarRed;
  vImage_Buffer rgbDest;
  uint64_t v27;

  v27 = *MEMORY[0x1E4F143B8];
  v2 = *v1;
  if (!*(void *)(*v1 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v3 = *(void *)(v2 + 48);
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  vImagePixelCount v4 = *(void *)(v2 + 40);
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v3)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v4)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v6)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v7)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v3 != v6)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (v4 != v7)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  uint64_t v8 = (void *)v5[4];
  uint64_t v9 = v5[7];
  rgbDest.data = v8;
  rgbDest.height = v4;
  rgbDest.width = v3;
  rgbDest.rowBytes = v9;
  size_t v10 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (!*(void *)(v10 + 16))
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  uint64_t v20 = *(_OWORD *)(v10 + 32);
  size_t v11 = *(void *)(v10 + 48);
  long long v12 = *(void *)(v10 + 56);
  swift_bridgeObjectRelease();
  *(_OWORD *)&planarRed.data = v20;
  planarRed.width = v11;
  planarRed.rowBytes = v12;
  uint64_t v13 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(void *)(v13 + 16) < 2uLL)
  {
LABEL_30:
    __break(1u);
LABEL_31:
    __break(1u);
  }
  v21 = *(_OWORD *)(v13 + 64);
  uint64_t v14 = *(void *)(v13 + 80);
  uint64_t v15 = *(void *)(v13 + 88);
  swift_bridgeObjectRelease();
  *(_OWORD *)&planarGreen.data = v21;
  planarGreen.width = v14;
  planarGreen.rowBytes = v15;
  unint64_t v16 = specialized vImage.PixelBuffer<>.vImageBuffers.getter(v2);
  if (*(void *)(v16 + 16) < 3uLL) {
    goto LABEL_31;
  }
  uint64_t v22 = *(_OWORD *)(v16 + 96);
  uint64_t v17 = *(void *)(v16 + 112);
  long long v18 = *(void *)(v16 + 120);
  swift_bridgeObjectRelease();
  *(_OWORD *)&planarBlue.data = v22;
  planarBlue.width = v17;
  planarBlue.rowBytes = v18;
  return vImageConvert_Planar8toRGB888(&planarRed, &planarGreen, &planarBlue, &rgbDest, 0);
}

uint64_t __swift_instantiateConcreteTypeFromMangledName(uint64_t *a1)
{
  uint64_t result = *a1;
  if (result < 0)
  {
    uint64_t result = swift_getTypeByMangledNameInContext2();
    *a1 = result;
  }
  return result;
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance BNNSDataType(_DWORD *a1, _DWORD *a2)
{
  return *a1 == *a2;
}

_DWORD *protocol witness for RawRepresentable.init(rawValue:) in conformance BNNSDataType@<X0>(_DWORD *result@<X0>, uint64_t a2@<X8>)
{
  *(_DWORD *)a2 = *result;
  *(unsigned char *)(a2 + 4) = 0;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance BNNSDataType(_DWORD *a1@<X8>)
{
  *a1 = *v1;
}

uint64_t protocol witness for static Equatable.== infix(_:_:) in conformance CFStringRef()
{
  swift_getWitnessTable();

  return static _CFObject.== infix(_:_:)();
}

uint64_t protocol witness for Hashable.hashValue.getter in conformance CFStringRef()
{
  return _CFObject.hashValue.getter();
}

uint64_t protocol witness for Hashable.hash(into:) in conformance CFStringRef()
{
  return _CFObject.hash(into:)();
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance CFStringRef()
{
  return Hasher._finalize()();
}

void *partial apply for closure #1 in vImage.PixelBuffer<>.planarBuffers()@<X0>(uint64_t *a1@<X8>)
{
  return closure #1 in vImage.PixelBuffer<>.planarBuffers()(*(uint64_t **)(v1 + 16), a1);
}

{
  uint64_t v1;

  return closure #1 in vImage.PixelBuffer<>.planarBuffers()(*(uint64_t **)(v1 + 16), a1);
}

{
  uint64_t v1;

  return closure #1 in vImage.PixelBuffer<>.planarBuffers()(*(uint64_t **)(v1 + 16), a1);
}

void type metadata accessor for vImageCVImageFormatRef(uint64_t a1)
{
}

void type metadata accessor for BNNSDataType(uint64_t a1)
{
}

void type metadata accessor for BNNSTargetSystem(uint64_t a1)
{
}

void type metadata accessor for BNNSRelationalOperator(uint64_t a1)
{
}

__n128 __swift_memcpy40_8(uint64_t a1, uint64_t a2)
{
  __n128 result = *(__n128 *)a2;
  long long v3 = *(_OWORD *)(a2 + 16);
  *(void *)(a1 + 32) = *(void *)(a2 + 32);
  *(__n128 *)a1 = result;
  *(_OWORD *)(a1 + 16) = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for quadrature_integrate_options(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 40)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for quadrature_integrate_options(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(void *)__n128 result = (a2 - 1);
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 40) = v3;
  return result;
}

void type metadata accessor for quadrature_integrate_options(uint64_t a1)
{
}

__n128 __swift_memcpy52_4(uint64_t a1, uint64_t a2)
{
  __n128 result = *(__n128 *)a2;
  long long v3 = *(_OWORD *)(a2 + 16);
  long long v4 = *(_OWORD *)(a2 + 32);
  *(_DWORD *)(a1 + 48) = *(_DWORD *)(a2 + 48);
  *(_OWORD *)(a1 + 16) = v3;
  *(_OWORD *)(a1 + 32) = v4;
  *(__n128 *)a1 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSOptimizerAdamWithClippingFields(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 52)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for BNNSOptimizerAdamWithClippingFields(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(void *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_DWORD *)(result + 48) = 0;
    *(void *)__n128 result = (a2 - 1);
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 52) = v3;
  return result;
}

void type metadata accessor for BNNSOptimizerAdamWithClippingFields(uint64_t a1)
{
}

__n128 __swift_memcpy16_8(__n128 *a1, __n128 *a2)
{
  __n128 result = *a2;
  *a1 = *a2;
  return result;
}

void type metadata accessor for bnns_graph_shape_t(uint64_t a1)
{
}

void type metadata accessor for bnns_graph_context_t(uint64_t a1)
{
}

uint64_t getEnumTagSinglePayload for bnns_graph_t(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 16)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for bnns_graph_t(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(void *)__n128 result = (a2 - 1);
    *(void *)(result + 8) = 0;
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 16) = v3;
  return result;
}

void type metadata accessor for bnns_graph_t(uint64_t a1)
{
}

void type metadata accessor for BNNSGraphOptimizationPreference(uint64_t a1)
{
}

void type metadata accessor for bnns_graph_compile_options_t(uint64_t a1)
{
}

__n128 __swift_memcpy24_4(__n128 *a1, __n128 *a2)
{
  __n128 result = *a2;
  a1[1].n128_u64[0] = a2[1].n128_u64[0];
  *a1 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSLayerParametersCropResize(unsigned __int8 *a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFF && a1[24]) {
    return (*(_DWORD *)a1 + 255);
  }
  unsigned int v3 = *a1;
  BOOL v4 = v3 >= 2;
  int v5 = (v3 + 2147483646) & 0x7FFFFFFF;
  if (!v4) {
    int v5 = -1;
  }
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNSLayerParametersCropResize(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(void *)(result + 8) = 0;
    *(void *)(result + 16) = 0;
    *(void *)__n128 result = a2 - 255;
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 24) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 24) = 0;
    }
    if (a2) {
      *(unsigned char *)__n128 result = a2 + 1;
    }
  }
  return result;
}

void type metadata accessor for BNNSLayerParametersCropResize(uint64_t a1)
{
}

void *__swift_memcpy720_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x2D0uLL);
}

uint64_t getEnumTagSinglePayload for BNNSLayerParametersQuantization(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 720)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for BNNSLayerParametersQuantization(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(void *)(result + 712) = 0;
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 696) = 0u;
    *(_OWORD *)(result + 680) = 0u;
    *(_OWORD *)(result + 664) = 0u;
    *(_OWORD *)(result + 648) = 0u;
    *(_OWORD *)(result + 632) = 0u;
    *(_OWORD *)(result + 616) = 0u;
    *(_OWORD *)(result + 600) = 0u;
    *(_OWORD *)(result + 584) = 0u;
    *(_OWORD *)(result + 568) = 0u;
    *(_OWORD *)(result + 552) = 0u;
    *(_OWORD *)(result + 536) = 0u;
    *(_OWORD *)(result + 520) = 0u;
    *(_OWORD *)(result + 504) = 0u;
    *(_OWORD *)(result + 488) = 0u;
    *(_OWORD *)(result + 472) = 0u;
    *(_OWORD *)(result + 456) = 0u;
    *(_OWORD *)(result + 440) = 0u;
    *(_OWORD *)(result + 424) = 0u;
    *(_OWORD *)(result + 408) = 0u;
    *(_OWORD *)(result + 392) = 0u;
    *(_OWORD *)(result + 376) = 0u;
    *(_OWORD *)(result + 360) = 0u;
    *(_OWORD *)(result + 344) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(void *)__n128 result = (a2 - 1);
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 720) = v3;
  return result;
}

void type metadata accessor for BNNSLayerParametersQuantization(uint64_t a1)
{
}

void *__swift_memcpy1128_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x468uLL);
}

uint64_t getEnumTagSinglePayload for BNNSLayerParametersNormalization(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 1128)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for BNNSLayerParametersNormalization(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 1096) = 0u;
    *(_OWORD *)(result + 1112) = 0u;
    *(_OWORD *)(result + 1080) = 0u;
    *(_OWORD *)(result + 1064) = 0u;
    *(_OWORD *)(result + 1048) = 0u;
    *(_OWORD *)(result + 1032) = 0u;
    *(_OWORD *)(result + 1016) = 0u;
    *(_OWORD *)(result + 1000) = 0u;
    *(_OWORD *)(result + 984) = 0u;
    *(_OWORD *)(result + 968) = 0u;
    *(_OWORD *)(result + 952) = 0u;
    *(_OWORD *)(result + 936) = 0u;
    *(_OWORD *)(result + 920) = 0u;
    *(_OWORD *)(result + 904) = 0u;
    *(_OWORD *)(result + 888) = 0u;
    *(_OWORD *)(result + 872) = 0u;
    *(_OWORD *)(result + 856) = 0u;
    *(_OWORD *)(result + 840) = 0u;
    *(_OWORD *)(result + 824) = 0u;
    *(_OWORD *)(result + 808) = 0u;
    *(_OWORD *)(result + 792) = 0u;
    *(_OWORD *)(result + 776) = 0u;
    *(_OWORD *)(result + 760) = 0u;
    *(_OWORD *)(result + 744) = 0u;
    *(_OWORD *)(result + 728) = 0u;
    *(_OWORD *)(result + 712) = 0u;
    *(_OWORD *)(result + 696) = 0u;
    *(_OWORD *)(result + 680) = 0u;
    *(_OWORD *)(result + 664) = 0u;
    *(_OWORD *)(result + 648) = 0u;
    *(_OWORD *)(result + 632) = 0u;
    *(_OWORD *)(result + 616) = 0u;
    *(_OWORD *)(result + 600) = 0u;
    *(_OWORD *)(result + 584) = 0u;
    *(_OWORD *)(result + 568) = 0u;
    *(_OWORD *)(result + 552) = 0u;
    *(_OWORD *)(result + 536) = 0u;
    *(_OWORD *)(result + 520) = 0u;
    *(_OWORD *)(result + 504) = 0u;
    *(_OWORD *)(result + 488) = 0u;
    *(_OWORD *)(result + 472) = 0u;
    *(_OWORD *)(result + 456) = 0u;
    *(_OWORD *)(result + 440) = 0u;
    *(_OWORD *)(result + 424) = 0u;
    *(_OWORD *)(result + 408) = 0u;
    *(_OWORD *)(result + 392) = 0u;
    *(_OWORD *)(result + 376) = 0u;
    *(_OWORD *)(result + 360) = 0u;
    *(_OWORD *)(result + 344) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(void *)__n128 result = (a2 - 1);
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 1128) = v3;
  return result;
}

void type metadata accessor for BNNSLayerParametersNormalization(uint64_t a1)
{
}

void type metadata accessor for DSPDoubleSplitComplex(uint64_t a1)
{
}

__n128 __swift_memcpy36_4(uint64_t a1, uint64_t a2)
{
  __n128 result = *(__n128 *)a2;
  long long v3 = *(_OWORD *)(a2 + 16);
  *(_DWORD *)(a1 + 32) = *(_DWORD *)(a2 + 32);
  *(__n128 *)a1 = result;
  *(_OWORD *)(a1 + 16) = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for vImage_PerpsectiveTransform(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 36)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for vImage_PerpsectiveTransform(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(void *)(result + 16) = 0;
    *(void *)(result + 24) = 0;
    *(_DWORD *)(result + 32) = 0;
    *(void *)__n128 result = (a2 - 1);
    *(void *)(result + 8) = 0;
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 36) = v3;
  return result;
}

void type metadata accessor for vImage_PerpsectiveTransform(uint64_t a1)
{
}

void type metadata accessor for BNNSFlags(uint64_t a1)
{
}

void *__swift_memcpy840_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x348uLL);
}

uint64_t getEnumTagSinglePayload for BNNSLayerParametersConvolution(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 840)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for BNNSLayerParametersConvolution(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 808) = 0u;
    *(_OWORD *)(result + 792) = 0u;
    *(_OWORD *)(result + 776) = 0u;
    *(_OWORD *)(result + 824) = 0u;
    *(_OWORD *)(result + 760) = 0u;
    *(_OWORD *)(result + 744) = 0u;
    *(_OWORD *)(result + 728) = 0u;
    *(_OWORD *)(result + 712) = 0u;
    *(_OWORD *)(result + 696) = 0u;
    *(_OWORD *)(result + 680) = 0u;
    *(_OWORD *)(result + 664) = 0u;
    *(_OWORD *)(result + 648) = 0u;
    *(_OWORD *)(result + 632) = 0u;
    *(_OWORD *)(result + 616) = 0u;
    *(_OWORD *)(result + 600) = 0u;
    *(_OWORD *)(result + 584) = 0u;
    *(_OWORD *)(result + 568) = 0u;
    *(_OWORD *)(result + 552) = 0u;
    *(_OWORD *)(result + 536) = 0u;
    *(_OWORD *)(result + 520) = 0u;
    *(_OWORD *)(result + 504) = 0u;
    *(_OWORD *)(result + 488) = 0u;
    *(_OWORD *)(result + 472) = 0u;
    *(_OWORD *)(result + 456) = 0u;
    *(_OWORD *)(result + 440) = 0u;
    *(_OWORD *)(result + 424) = 0u;
    *(_OWORD *)(result + 408) = 0u;
    *(_OWORD *)(result + 392) = 0u;
    *(_OWORD *)(result + 376) = 0u;
    *(_OWORD *)(result + 360) = 0u;
    *(_OWORD *)(result + 344) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(void *)__n128 result = (a2 - 1);
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 840) = v3;
  return result;
}

void type metadata accessor for BNNSLayerParametersConvolution(uint64_t a1)
{
}

void *__swift_memcpy752_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x2F0uLL);
}

uint64_t getEnumTagSinglePayload for BNNSLayerParametersFullyConnected(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 752)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for BNNSLayerParametersFullyConnected(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(void *)(result + 744) = 0;
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 728) = 0u;
    *(_OWORD *)(result + 712) = 0u;
    *(_OWORD *)(result + 696) = 0u;
    *(_OWORD *)(result + 680) = 0u;
    *(_OWORD *)(result + 664) = 0u;
    *(_OWORD *)(result + 648) = 0u;
    *(_OWORD *)(result + 632) = 0u;
    *(_OWORD *)(result + 616) = 0u;
    *(_OWORD *)(result + 600) = 0u;
    *(_OWORD *)(result + 584) = 0u;
    *(_OWORD *)(result + 568) = 0u;
    *(_OWORD *)(result + 552) = 0u;
    *(_OWORD *)(result + 536) = 0u;
    *(_OWORD *)(result + 520) = 0u;
    *(_OWORD *)(result + 504) = 0u;
    *(_OWORD *)(result + 488) = 0u;
    *(_OWORD *)(result + 472) = 0u;
    *(_OWORD *)(result + 456) = 0u;
    *(_OWORD *)(result + 440) = 0u;
    *(_OWORD *)(result + 424) = 0u;
    *(_OWORD *)(result + 408) = 0u;
    *(_OWORD *)(result + 392) = 0u;
    *(_OWORD *)(result + 376) = 0u;
    *(_OWORD *)(result + 360) = 0u;
    *(_OWORD *)(result + 344) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(void *)__n128 result = (a2 - 1);
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 752) = v3;
  return result;
}

void type metadata accessor for BNNSLayerParametersFullyConnected(uint64_t a1)
{
}

void type metadata accessor for DSPDoubleComplex(uint64_t a1)
{
}

void *__swift_memcpy8_4(void *result, void *a2)
{
  *__n128 result = *a2;
  return result;
}

uint64_t getEnumTagSinglePayload for DSPComplex(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 8)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for DSPComplex(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(void *)__n128 result = (a2 - 1);
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 8) = v3;
  return result;
}

void type metadata accessor for DSPComplex(uint64_t a1)
{
}

uint64_t getEnumTagSinglePayload for DSPDoubleSplitComplex(uint64_t a1, int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 != 1 && *(unsigned char *)(a1 + 16)) {
    return (*(_DWORD *)a1 + 2);
  }
  if (*(void *)a1) {
    int v3 = -1;
  }
  else {
    int v3 = 0;
  }
  return (v3 + 1);
}

uint64_t storeEnumTagSinglePayload for DSPDoubleSplitComplex(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 1)
  {
    *(void *)__n128 result = a2 - 2;
    *(void *)(result + 8) = 0;
    if (a3 >= 2) {
      *(unsigned char *)(result + 16) = 1;
    }
  }
  else
  {
    if (a3 >= 2) {
      *(unsigned char *)(result + 16) = 0;
    }
    if (a2) {
      *(void *)__n128 result = 0;
    }
  }
  return result;
}

void type metadata accessor for DSPSplitComplex(uint64_t a1)
{
}

uint64_t getEnumTagSinglePayload for vImage_CGImageFormat(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0x7FFFFFFF && *(unsigned char *)(a1 + 40)) {
    return (*(_DWORD *)a1 + 0x7FFFFFFF);
  }
  unint64_t v3 = *(void *)(a1 + 8);
  if (v3 >= 0xFFFFFFFF) {
    LODWORD(v3) = -1;
  }
  int v4 = v3 - 1;
  if (v4 < 0) {
    int v4 = -1;
  }
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for vImage_CGImageFormat(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0x7FFFFFFE)
  {
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(void *)__n128 result = a2 - 0x7FFFFFFF;
    if (a3 >= 0x7FFFFFFF) {
      *(unsigned char *)(result + 40) = 1;
    }
  }
  else
  {
    if (a3 >= 0x7FFFFFFF) {
      *(unsigned char *)(result + 40) = 0;
    }
    if (a2) {
      *(void *)(result + 8) = a2;
    }
  }
  return result;
}

void type metadata accessor for vImage_CGImageFormat(uint64_t a1)
{
}

__n128 __swift_memcpy64_8(uint64_t a1, uint64_t a2)
{
  __n128 result = *(__n128 *)a2;
  long long v3 = *(_OWORD *)(a2 + 16);
  long long v4 = *(_OWORD *)(a2 + 48);
  *(_OWORD *)(a1 + 32) = *(_OWORD *)(a2 + 32);
  *(_OWORD *)(a1 + 48) = v4;
  *(__n128 *)a1 = result;
  *(_OWORD *)(a1 + 16) = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSLayerParametersArithmetic(uint64_t a1, int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 != 1 && *(unsigned char *)(a1 + 64)) {
    return (*(_DWORD *)a1 + 2);
  }
  if (*(void *)(a1 + 8)) {
    int v3 = -1;
  }
  else {
    int v3 = 0;
  }
  return (v3 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNSLayerParametersArithmetic(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 1)
  {
    *(void *)(result + 56) = 0;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(void *)__n128 result = a2 - 2;
    if (a3 >= 2) {
      *(unsigned char *)(result + 64) = 1;
    }
  }
  else
  {
    if (a3 >= 2) {
      *(unsigned char *)(result + 64) = 0;
    }
    if (a2) {
      *(void *)(result + 8) = 0;
    }
  }
  return result;
}

void type metadata accessor for BNNSLayerParametersArithmetic(uint64_t a1)
{
}

uint64_t initializeBufferWithCopyOfBuffer for BNNSTensor(uint64_t *a1, uint64_t *a2)
{
  uint64_t v2 = *a2;
  *a1 = *a2;
  uint64_t v3 = v2 + 16;
  swift_retain();
  return v3;
}

__n128 __swift_memcpy160_8(uint64_t a1, uint64_t a2)
{
  long long v2 = *(_OWORD *)(a2 + 16);
  *(_OWORD *)a1 = *(_OWORD *)a2;
  *(_OWORD *)(a1 + 16) = v2;
  long long v3 = *(_OWORD *)(a2 + 32);
  long long v4 = *(_OWORD *)(a2 + 48);
  long long v5 = *(_OWORD *)(a2 + 80);
  *(_OWORD *)(a1 + 64) = *(_OWORD *)(a2 + 64);
  *(_OWORD *)(a1 + 80) = v5;
  *(_OWORD *)(a1 + 32) = v3;
  *(_OWORD *)(a1 + 48) = v4;
  __n128 result = *(__n128 *)(a2 + 96);
  long long v7 = *(_OWORD *)(a2 + 112);
  long long v8 = *(_OWORD *)(a2 + 144);
  *(_OWORD *)(a1 + 128) = *(_OWORD *)(a2 + 128);
  *(_OWORD *)(a1 + 144) = v8;
  *(__n128 *)(a1 + 96) = result;
  *(_OWORD *)(a1 + 112) = v7;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSTensor(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 160)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for BNNSTensor(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(void *)(result + 152) = 0;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(void *)__n128 result = (a2 - 1);
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 160) = v3;
  return result;
}

void type metadata accessor for BNNSTensor(uint64_t a1)
{
}

void type metadata accessor for bnns_graph_argument_t(uint64_t a1)
{
}

__n128 __swift_memcpy32_8(_OWORD *a1, uint64_t a2)
{
  __n128 result = *(__n128 *)a2;
  long long v3 = *(_OWORD *)(a2 + 16);
  *a1 = *(_OWORD *)a2;
  a1[1] = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for vImage_Buffer(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 32)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for vImage_Buffer(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(void *)(result + 16) = 0;
    *(void *)(result + 24) = 0;
    *(void *)__n128 result = (a2 - 1);
    *(void *)(result + 8) = 0;
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 32) = v3;
  return result;
}

void type metadata accessor for vImage_Buffer(uint64_t a1)
{
}

__n128 __swift_memcpy176_8(uint64_t a1, long long *a2)
{
  long long v2 = *a2;
  long long v3 = a2[2];
  *(_OWORD *)(a1 + 16) = a2[1];
  *(_OWORD *)(a1 + 32) = v3;
  *(_OWORD *)a1 = v2;
  long long v4 = a2[3];
  long long v5 = a2[4];
  long long v6 = a2[6];
  *(_OWORD *)(a1 + 80) = a2[5];
  *(_OWORD *)(a1 + 96) = v6;
  *(_OWORD *)(a1 + 48) = v4;
  *(_OWORD *)(a1 + 64) = v5;
  __n128 result = (__n128)a2[7];
  long long v8 = a2[8];
  long long v9 = a2[10];
  *(_OWORD *)(a1 + 144) = a2[9];
  *(_OWORD *)(a1 + 160) = v9;
  *(__n128 *)(a1 + 112) = result;
  *(_OWORD *)(a1 + 128) = v8;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSNDArrayDescriptor(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 176)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for BNNSNDArrayDescriptor(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(void *)(result + 168) = 0;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(void *)__n128 result = (a2 - 1);
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 176) = v3;
  return result;
}

void type metadata accessor for BNNSNDArrayDescriptor(uint64_t a1)
{
}

void type metadata accessor for BNNSDataLayout(uint64_t a1)
{
}

void type metadata accessor for BNNSFilterType(uint64_t a1)
{
}

void type metadata accessor for CFStringRef(uint64_t a1)
{
}

void type metadata accessor for quadrature_integrator(uint64_t a1)
{
}

void type metadata accessor for BNNSOptimizerClippingFunction(uint64_t a1)
{
}

void type metadata accessor for BNNSOptimizerRegularizationFunction(uint64_t a1)
{
}

__n128 __swift_memcpy48_4(uint64_t a1, uint64_t a2)
{
  __n128 result = *(__n128 *)a2;
  long long v3 = *(_OWORD *)(a2 + 32);
  *(_OWORD *)(a1 + 16) = *(_OWORD *)(a2 + 16);
  *(_OWORD *)(a1 + 32) = v3;
  *(__n128 *)a1 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSOptimizerSGDMomentumWithClippingFields(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFF && *(unsigned char *)(a1 + 48)) {
    return (*(_DWORD *)a1 + 255);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 16);
  BOOL v4 = v3 >= 2;
  int v5 = (v3 + 2147483646) & 0x7FFFFFFF;
  if (!v4) {
    int v5 = -1;
  }
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNSOptimizerSGDMomentumWithClippingFields(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(void *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(void *)__n128 result = a2 - 255;
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 48) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 48) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 16) = a2 + 1;
    }
  }
  return result;
}

void type metadata accessor for BNNSOptimizerSGDMomentumWithClippingFields(uint64_t a1)
{
}

__n128 __swift_memcpy40_4(uint64_t a1, uint64_t a2)
{
  __n128 result = *(__n128 *)a2;
  long long v3 = *(_OWORD *)(a2 + 16);
  *(void *)(a1 + 32) = *(void *)(a2 + 32);
  *(__n128 *)a1 = result;
  *(_OWORD *)(a1 + 16) = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSOptimizerSGDMomentumFields(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFF && *(unsigned char *)(a1 + 40)) {
    return (*(_DWORD *)a1 + 255);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 16);
  BOOL v4 = v3 >= 2;
  int v5 = (v3 + 2147483646) & 0x7FFFFFFF;
  if (!v4) {
    int v5 = -1;
  }
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNSOptimizerSGDMomentumFields(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(void *)__n128 result = a2 - 255;
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 40) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 40) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 16) = a2 + 1;
    }
  }
  return result;
}

void type metadata accessor for BNNSOptimizerSGDMomentumFields(uint64_t a1)
{
}

uint64_t getEnumTagSinglePayload for BNNSOptimizerRMSPropWithClippingFields(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFF && *(unsigned char *)(a1 + 52)) {
    return (*(_DWORD *)a1 + 255);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 12);
  BOOL v4 = v3 >= 2;
  int v5 = (v3 + 2147483646) & 0x7FFFFFFF;
  if (!v4) {
    int v5 = -1;
  }
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNSOptimizerRMSPropWithClippingFields(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(void *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_DWORD *)(result + 48) = 0;
    *(void *)__n128 result = a2 - 255;
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 52) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 52) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 12) = a2 + 1;
    }
  }
  return result;
}

void type metadata accessor for BNNSOptimizerRMSPropWithClippingFields(uint64_t a1)
{
}

__n128 __swift_memcpy44_4(uint64_t a1, uint64_t a2)
{
  __n128 result = *(__n128 *)a2;
  long long v3 = *(_OWORD *)(a2 + 16);
  *(_OWORD *)(a1 + 28) = *(_OWORD *)(a2 + 28);
  *(__n128 *)a1 = result;
  *(_OWORD *)(a1 + 16) = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSOptimizerRMSPropFields(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFF && *(unsigned char *)(a1 + 44)) {
    return (*(_DWORD *)a1 + 255);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 12);
  BOOL v4 = v3 >= 2;
  int v5 = (v3 + 2147483646) & 0x7FFFFFFF;
  if (!v4) {
    int v5 = -1;
  }
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNSOptimizerRMSPropFields(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_DWORD *)(result + 40) = 0;
    *(void *)__n128 result = a2 - 255;
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 44) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 44) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 12) = a2 + 1;
    }
  }
  return result;
}

void type metadata accessor for BNNSOptimizerRMSPropFields(uint64_t a1)
{
}

uint64_t getEnumTagSinglePayload for BNNSOptimizerAdamFields(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFF && *(unsigned char *)(a1 + 44)) {
    return (*(_DWORD *)a1 + 255);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 28);
  BOOL v4 = v3 >= 2;
  int v5 = (v3 + 2147483646) & 0x7FFFFFFF;
  if (!v4) {
    int v5 = -1;
  }
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNSOptimizerAdamFields(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_DWORD *)(result + 40) = 0;
    *(void *)__n128 result = a2 - 255;
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 44) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 44) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 28) = a2 + 1;
    }
  }
  return result;
}

void type metadata accessor for BNNSOptimizerAdamFields(uint64_t a1)
{
}

void type metadata accessor for BNNSInterpolationMethod(uint64_t a1)
{
}

void type metadata accessor for BNNSBoxCoordinateMode(uint64_t a1)
{
}

void type metadata accessor for BNNSLinearSamplingMode(uint64_t a1)
{
}

void type metadata accessor for BNNSQuantizerFunction(uint64_t a1)
{
}

void type metadata accessor for CGColorRenderingIntent(uint64_t a1)
{
}

void type metadata accessor for CGBitmapInfo(uint64_t a1)
{
}

void type metadata accessor for CGColorSpaceRef(uint64_t a1)
{
}

__n128 __swift_memcpy48_8(uint64_t a1, uint64_t a2)
{
  __n128 result = *(__n128 *)a2;
  long long v3 = *(_OWORD *)(a2 + 32);
  *(_OWORD *)(a1 + 16) = *(_OWORD *)(a2 + 16);
  *(_OWORD *)(a1 + 32) = v3;
  *(__n128 *)a1 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNSActivation(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 48)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for BNNSActivation(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(void *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(void *)__n128 result = (a2 - 1);
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 48) = v3;
  return result;
}

void type metadata accessor for BNNSActivation(uint64_t a1)
{
}

void type metadata accessor for BNNSArithmeticFunction(uint64_t a1)
{
}

void type metadata accessor for bnns_graph_argument_t.__Unnamed_union___Anonymous_field0(uint64_t a1)
{
}

void type metadata accessor for BNNSNDArrayFlags(uint64_t a1)
{
}

uint64_t base witness table accessor for Equatable in CFStringRef()
{
  return lazy protocol witness table accessor for type CFStringRef and conformance CFStringRef(&lazy protocol witness table cache variable for type CFStringRef and conformance CFStringRef);
}

uint64_t base witness table accessor for Hashable in CFStringRef()
{
  return lazy protocol witness table accessor for type CFStringRef and conformance CFStringRef(&lazy protocol witness table cache variable for type CFStringRef and conformance CFStringRef);
}

uint64_t lazy protocol witness table accessor for type CFStringRef and conformance CFStringRef(unint64_t *a1)
{
  uint64_t result = *a1;
  if (!result)
  {
    type metadata accessor for CFStringRef(255);
    uint64_t result = swift_getWitnessTable();
    atomic_store(result, a1);
  }
  return result;
}

void type metadata accessor for BNNSOptimizerSGDMomentumVariant(uint64_t a1)
{
}

void type metadata accessor for BNNSActivationFunction(uint64_t a1)
{
}

void type metadata accessor for vImageCVImageFormatRef(uint64_t a1, unint64_t *a2)
{
  if (!*a2)
  {
    unint64_t ForeignTypeMetadata = swift_getForeignTypeMetadata();
    if (!v4) {
      atomic_store(ForeignTypeMetadata, a2);
    }
  }
}

ValueMetadata *type metadata accessor for vImage()
{
  return &type metadata for vImage;
}

ValueMetadata *type metadata accessor for vDSP()
{
  return &type metadata for vDSP;
}

ValueMetadata *type metadata accessor for vForce()
{
  return &type metadata for vForce;
}

ValueMetadata *type metadata accessor for vDSP.VectorizableFloat()
{
  return &type metadata for vDSP.VectorizableFloat;
}

ValueMetadata *type metadata accessor for vDSP.VectorizableDouble()
{
  return &type metadata for vDSP.VectorizableDouble;
}

vImage_Error vImage.PixelBuffer<>.init(interleavedBuffer:)@<X0>(uint64_t *a1@<X0>, void *a2@<X8>)
{
  uint64_t v51 = *MEMORY[0x1E4F143B8];
  uint64_t v3 = *a1;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v4 = swift_allocObject();
  *(_OWORD *)(v4 + 16) = xmmword_1D2135DC0;
  if (!*(void *)(v3 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  int v5 = (void *)v4;
  uint64_t v6 = *(void *)(v3 + 48);
  if (v6 < 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v7 = *(void *)(v3 + 40);
  if (v7 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v6)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v7)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v8 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v6, v7);
  vImagePixelCount v10 = v9;
  vImagePixelCount v12 = v11;
  size_t v14 = v13;
  type metadata accessor for vImage.BufferReference();
  uint64_t v15 = (void *)swift_allocObject();
  v15[2] = v8;
  v15[3] = v10;
  v15[4] = v12;
  v15[5] = v14;
  v5[4] = v8;
  v5[5] = v10;
  v5[6] = v12;
  v5[7] = v14;
  v5[8] = v15;
  uint64_t v16 = *(void *)(v3 + 48);
  if (v16 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v17 = *(void *)(v3 + 40);
  if (v17 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v16)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v17)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v44 = (void *)v8;
  v45 = a2;
  long long v18 = (void *)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v16, v17);
  vImagePixelCount v20 = v19;
  vImagePixelCount v22 = v21;
  size_t v24 = v23;
  uint64_t v25 = (void *)swift_allocObject();
  v25[2] = v18;
  v25[3] = v20;
  v25[4] = v22;
  v25[5] = v24;
  v5[9] = v18;
  v5[10] = v20;
  v5[11] = v22;
  v5[12] = v24;
  v5[13] = v25;
  uint64_t v26 = *(void *)(v3 + 48);
  if (v26 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  uint64_t v27 = *(void *)(v3 + 40);
  if (v27 < 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v26)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (!v27) {
    goto LABEL_27;
  }
  vImagePixelCount v39 = v22;
  vImagePixelCount v40 = v20;
  size_t v41 = v14;
  vImagePixelCount v42 = v12;
  vImagePixelCount v43 = v10;
  v28 = (void *)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v26, v27);
  vImagePixelCount v30 = v29;
  vImagePixelCount v32 = v31;
  size_t v34 = v33;
  v35 = (void *)swift_allocObject();
  v35[2] = v28;
  v35[3] = v30;
  v35[4] = v32;
  v35[5] = v34;
  v5[14] = v28;
  v5[15] = v30;
  v5[16] = v32;
  v5[17] = v34;
  v5[18] = v35;
  long long v46 = *(_OWORD *)(v3 + 32);
  vImagePixelCount v36 = *(void *)(v3 + 48);
  size_t v37 = *(void *)(v3 + 56);
  swift_bridgeObjectRelease();
  *(_OWORD *)&rgbSrc.data = v46;
  rgbSrc.width = v36;
  rgbSrc.rowBytes = v37;
  redDest.data = v44;
  redDest.height = v43;
  redDest.width = v42;
  redDest.rowBytes = v41;
  greenDest.data = v18;
  greenDest.height = v40;
  greenDest.width = v39;
  greenDest.rowBytes = v24;
  blueDest.data = v28;
  blueDest.height = v30;
  blueDest.width = v32;
  blueDest.rowBytes = v34;
  vImage_Error result = vImageConvert_RGBFFFtoPlanarF(&rgbSrc, &redDest, &greenDest, &blueDest, 0);
  void *v45 = v5;
  return result;
}

{
  uint64_t v3;
  uint64_t v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImagePixelCount v9;
  vImagePixelCount v10;
  vImagePixelCount v11;
  vImagePixelCount v12;
  size_t v13;
  size_t v14;
  void *v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  vImagePixelCount v19;
  vImagePixelCount v20;
  vImagePixelCount v21;
  vImagePixelCount v22;
  size_t v23;
  size_t v24;
  void *v25;
  uint64_t v26;
  uint64_t v27;
  void *v28;
  void *v29;
  vImagePixelCount v30;
  vImagePixelCount v31;
  vImagePixelCount v32;
  vImagePixelCount v33;
  size_t v34;
  size_t v35;
  void *v36;
  uint64_t v37;
  uint64_t v38;
  void *v39;
  vImagePixelCount v40;
  vImagePixelCount v41;
  vImagePixelCount v42;
  vImagePixelCount v43;
  size_t v44;
  size_t v45;
  void *v46;
  vImagePixelCount v47;
  size_t v48;
  vImage_Error result;
  long long v50;
  size_t v51;
  vImagePixelCount v52;
  size_t v53;
  void *v54;
  size_t v55;
  vImagePixelCount v56;
  vImagePixelCount v57;
  void *v58;
  vImagePixelCount v59;
  vImagePixelCount v60;
  void *v61;
  vImage_Buffer destB;
  vImage_Buffer destG;
  vImage_Buffer destR;
  vImage_Buffer destA;
  vImage_Buffer srcARGB;
  uint64_t v67;

  v67 = *MEMORY[0x1E4F143B8];
  uint64_t v3 = *a1;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v4 = swift_allocObject();
  *(_OWORD *)(v4 + 16) = xmmword_1D2135FC0;
  if (!*(void *)(v3 + 16))
  {
    __break(1u);
    goto LABEL_20;
  }
  int v5 = (void *)v4;
  uint64_t v6 = *(void *)(v3 + 48);
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = *(void *)(v3 + 40);
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v8 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v6, v7);
  vImagePixelCount v10 = v9;
  vImagePixelCount v12 = v11;
  size_t v14 = v13;
  type metadata accessor for vImage.BufferReference();
  uint64_t v15 = (void *)swift_allocObject();
  v15[2] = v8;
  v15[3] = v10;
  v15[4] = v12;
  v15[5] = v14;
  v5[4] = v8;
  v5[5] = v10;
  v5[6] = v12;
  v5[7] = v14;
  v5[8] = v15;
  uint64_t v16 = *(void *)(v3 + 48);
  if (v16 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  uint64_t v17 = *(void *)(v3 + 40);
  if (v17 < 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v16)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v17)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v59 = v12;
  v60 = v10;
  v61 = (void *)v8;
  long long v18 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v16, v17);
  vImagePixelCount v20 = v19;
  vImagePixelCount v22 = v21;
  size_t v24 = v23;
  uint64_t v25 = (void *)swift_allocObject();
  v25[2] = v18;
  v25[3] = v20;
  v25[4] = v22;
  v25[5] = v24;
  v5[9] = v18;
  v5[10] = v20;
  v5[11] = v22;
  v5[12] = v24;
  v5[13] = v25;
  uint64_t v26 = *(void *)(v3 + 48);
  if (v26 < 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  uint64_t v27 = *(void *)(v3 + 40);
  if (v27 < 0)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v26)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (!v27)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v55 = v24;
  v56 = v22;
  v57 = v20;
  v58 = (void *)v18;
  v28 = a2;
  vImagePixelCount v29 = (void *)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v26, v27);
  vImagePixelCount v31 = v30;
  size_t v33 = v32;
  v35 = v34;
  vImagePixelCount v36 = (void *)swift_allocObject();
  v36[2] = v29;
  v36[3] = v31;
  v36[4] = v33;
  v36[5] = v35;
  v5[14] = v29;
  v5[15] = v31;
  v5[16] = v33;
  v5[17] = v35;
  v5[18] = v36;
  size_t v37 = *(void *)(v3 + 48);
  if (v37 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v38 = *(void *)(v3 + 40);
  if (v38 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v37)
  {
LABEL_34:
    __break(1u);
LABEL_35:
    __break(1u);
  }
  if (!v38) {
    goto LABEL_35;
  }
  uint64_t v51 = v35;
  v52 = v33;
  v53 = v14;
  v54 = v28;
  vImagePixelCount v39 = (void *)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v37, v38);
  size_t v41 = v40;
  vImagePixelCount v43 = v42;
  v45 = v44;
  long long v46 = (void *)swift_allocObject();
  v46[2] = v39;
  v46[3] = v41;
  v46[4] = v43;
  v46[5] = v45;
  v5[19] = v39;
  v5[20] = v41;
  v5[21] = v43;
  v5[22] = v45;
  v5[23] = v46;
  v50 = *(_OWORD *)(v3 + 32);
  v47 = *(void *)(v3 + 48);
  v48 = *(void *)(v3 + 56);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcARGB.data = v50;
  srcARGB.width = v47;
  srcARGB.rowBytes = v48;
  destA.data = v61;
  destA.height = v60;
  destA.width = v59;
  destA.rowBytes = v53;
  destR.data = v58;
  destR.height = v57;
  destR.width = v56;
  destR.rowBytes = v55;
  destG.data = v29;
  destG.height = v31;
  destG.width = v52;
  destG.rowBytes = v51;
  destB.data = v39;
  destB.height = v41;
  destB.width = v43;
  destB.rowBytes = v45;
  vImage_Error result = vImageConvert_ARGB8888toPlanar8(&srcARGB, &destA, &destR, &destG, &destB, 0);
  *v54 = v5;
  return result;
}

{
  uint64_t v3;
  uint64_t v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImagePixelCount v9;
  vImagePixelCount v10;
  vImagePixelCount v11;
  vImagePixelCount v12;
  size_t v13;
  size_t v14;
  void *v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  vImagePixelCount v19;
  vImagePixelCount v20;
  vImagePixelCount v21;
  vImagePixelCount v22;
  size_t v23;
  size_t v24;
  void *v25;
  uint64_t v26;
  uint64_t v27;
  void *v28;
  void *v29;
  vImagePixelCount v30;
  vImagePixelCount v31;
  vImagePixelCount v32;
  vImagePixelCount v33;
  size_t v34;
  size_t v35;
  void *v36;
  uint64_t v37;
  uint64_t v38;
  void *v39;
  vImagePixelCount v40;
  vImagePixelCount v41;
  vImagePixelCount v42;
  vImagePixelCount v43;
  size_t v44;
  size_t v45;
  void *v46;
  vImagePixelCount v47;
  size_t v48;
  vImage_Error result;
  long long v50;
  size_t v51;
  vImagePixelCount v52;
  size_t v53;
  void *v54;
  size_t v55;
  vImagePixelCount v56;
  vImagePixelCount v57;
  void *v58;
  vImagePixelCount v59;
  vImagePixelCount v60;
  void *v61;
  vImage_Buffer destB;
  vImage_Buffer destG;
  vImage_Buffer destR;
  vImage_Buffer destA;
  vImage_Buffer srcARGB;
  uint64_t v67;

  v67 = *MEMORY[0x1E4F143B8];
  uint64_t v3 = *a1;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v4 = swift_allocObject();
  *(_OWORD *)(v4 + 16) = xmmword_1D2135FC0;
  if (!*(void *)(v3 + 16))
  {
    __break(1u);
    goto LABEL_20;
  }
  int v5 = (void *)v4;
  uint64_t v6 = *(void *)(v3 + 48);
  if (v6 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = *(void *)(v3 + 40);
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v6)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v8 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v6, v7);
  vImagePixelCount v10 = v9;
  vImagePixelCount v12 = v11;
  size_t v14 = v13;
  type metadata accessor for vImage.BufferReference();
  uint64_t v15 = (void *)swift_allocObject();
  v15[2] = v8;
  v15[3] = v10;
  v15[4] = v12;
  v15[5] = v14;
  v5[4] = v8;
  v5[5] = v10;
  v5[6] = v12;
  v5[7] = v14;
  v5[8] = v15;
  uint64_t v16 = *(void *)(v3 + 48);
  if (v16 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  uint64_t v17 = *(void *)(v3 + 40);
  if (v17 < 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v16)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v17)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  v59 = v12;
  v60 = v10;
  v61 = (void *)v8;
  long long v18 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v16, v17);
  vImagePixelCount v20 = v19;
  vImagePixelCount v22 = v21;
  size_t v24 = v23;
  uint64_t v25 = (void *)swift_allocObject();
  v25[2] = v18;
  v25[3] = v20;
  v25[4] = v22;
  v25[5] = v24;
  v5[9] = v18;
  v5[10] = v20;
  v5[11] = v22;
  v5[12] = v24;
  v5[13] = v25;
  uint64_t v26 = *(void *)(v3 + 48);
  if (v26 < 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  uint64_t v27 = *(void *)(v3 + 40);
  if (v27 < 0)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v26)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (!v27)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  v55 = v24;
  v56 = v22;
  v57 = v20;
  v58 = (void *)v18;
  v28 = a2;
  vImagePixelCount v29 = (void *)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v26, v27);
  vImagePixelCount v31 = v30;
  size_t v33 = v32;
  v35 = v34;
  vImagePixelCount v36 = (void *)swift_allocObject();
  v36[2] = v29;
  v36[3] = v31;
  v36[4] = v33;
  v36[5] = v35;
  v5[14] = v29;
  v5[15] = v31;
  v5[16] = v33;
  v5[17] = v35;
  v5[18] = v36;
  size_t v37 = *(void *)(v3 + 48);
  if (v37 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  v38 = *(void *)(v3 + 40);
  if (v38 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v37)
  {
LABEL_34:
    __break(1u);
LABEL_35:
    __break(1u);
  }
  if (!v38) {
    goto LABEL_35;
  }
  uint64_t v51 = v35;
  v52 = v33;
  v53 = v14;
  v54 = v28;
  vImagePixelCount v39 = (void *)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v37, v38);
  size_t v41 = v40;
  vImagePixelCount v43 = v42;
  v45 = v44;
  long long v46 = (void *)swift_allocObject();
  v46[2] = v39;
  v46[3] = v41;
  v46[4] = v43;
  v46[5] = v45;
  v5[19] = v39;
  v5[20] = v41;
  v5[21] = v43;
  v5[22] = v45;
  v5[23] = v46;
  v50 = *(_OWORD *)(v3 + 32);
  v47 = *(void *)(v3 + 48);
  v48 = *(void *)(v3 + 56);
  swift_bridgeObjectRelease();
  *(_OWORD *)&srcARGB.data = v50;
  srcARGB.width = v47;
  srcARGB.rowBytes = v48;
  destA.data = v61;
  destA.height = v60;
  destA.width = v59;
  destA.rowBytes = v53;
  destR.data = v58;
  destR.height = v57;
  destR.width = v56;
  destR.rowBytes = v55;
  destG.data = v29;
  destG.height = v31;
  destG.width = v52;
  destG.rowBytes = v51;
  destB.data = v39;
  destB.height = v41;
  destB.width = v43;
  destB.rowBytes = v45;
  vImage_Error result = vImageConvert_ARGBFFFFtoPlanarF(&srcARGB, &destA, &destR, &destG, &destB, 0);
  *v54 = v5;
  return result;
}

{
  uint64_t v3;
  uint64_t v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  vImagePixelCount v9;
  vImagePixelCount v10;
  vImagePixelCount v11;
  vImagePixelCount v12;
  size_t v13;
  size_t v14;
  void *v15;
  uint64_t v16;
  uint64_t v17;
  void *v18;
  vImagePixelCount v19;
  vImagePixelCount v20;
  vImagePixelCount v21;
  vImagePixelCount v22;
  size_t v23;
  size_t v24;
  void *v25;
  uint64_t v26;
  uint64_t v27;
  void *v28;
  vImagePixelCount v29;
  vImagePixelCount v30;
  vImagePixelCount v31;
  vImagePixelCount v32;
  size_t v33;
  size_t v34;
  void *v35;
  vImagePixelCount v36;
  size_t v37;
  vImage_Error result;
  vImagePixelCount v39;
  vImagePixelCount v40;
  size_t v41;
  vImagePixelCount v42;
  vImagePixelCount v43;
  void *v44;
  void *v45;
  long long v46;
  vImage_Buffer blueDest;
  vImage_Buffer greenDest;
  vImage_Buffer redDest;
  vImage_Buffer rgbSrc;
  uint64_t v51;

  uint64_t v51 = *MEMORY[0x1E4F143B8];
  uint64_t v3 = *a1;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v4 = swift_allocObject();
  *(_OWORD *)(v4 + 16) = xmmword_1D2135DC0;
  if (!*(void *)(v3 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  int v5 = (void *)v4;
  uint64_t v6 = *(void *)(v3 + 48);
  if (v6 < 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v7 = *(void *)(v3 + 40);
  if (v7 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v6)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v7)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v8 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v6, v7);
  vImagePixelCount v10 = v9;
  vImagePixelCount v12 = v11;
  size_t v14 = v13;
  type metadata accessor for vImage.BufferReference();
  uint64_t v15 = (void *)swift_allocObject();
  v15[2] = v8;
  v15[3] = v10;
  v15[4] = v12;
  v15[5] = v14;
  v5[4] = v8;
  v5[5] = v10;
  v5[6] = v12;
  v5[7] = v14;
  v5[8] = v15;
  uint64_t v16 = *(void *)(v3 + 48);
  if (v16 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v17 = *(void *)(v3 + 40);
  if (v17 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v16)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v17)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  v44 = (void *)v8;
  v45 = a2;
  long long v18 = (void *)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v16, v17);
  vImagePixelCount v20 = v19;
  vImagePixelCount v22 = v21;
  size_t v24 = v23;
  uint64_t v25 = (void *)swift_allocObject();
  v25[2] = v18;
  v25[3] = v20;
  v25[4] = v22;
  v25[5] = v24;
  v5[9] = v18;
  v5[10] = v20;
  v5[11] = v22;
  v5[12] = v24;
  v5[13] = v25;
  uint64_t v26 = *(void *)(v3 + 48);
  if (v26 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  uint64_t v27 = *(void *)(v3 + 40);
  if (v27 < 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v26)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (!v27) {
    goto LABEL_27;
  }
  vImagePixelCount v39 = v22;
  vImagePixelCount v40 = v20;
  size_t v41 = v14;
  vImagePixelCount v42 = v12;
  vImagePixelCount v43 = v10;
  v28 = (void *)specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v26, v27);
  vImagePixelCount v30 = v29;
  vImagePixelCount v32 = v31;
  size_t v34 = v33;
  v35 = (void *)swift_allocObject();
  v35[2] = v28;
  v35[3] = v30;
  v35[4] = v32;
  v35[5] = v34;
  v5[14] = v28;
  v5[15] = v30;
  v5[16] = v32;
  v5[17] = v34;
  v5[18] = v35;
  long long v46 = *(_OWORD *)(v3 + 32);
  vImagePixelCount v36 = *(void *)(v3 + 48);
  size_t v37 = *(void *)(v3 + 56);
  swift_bridgeObjectRelease();
  *(_OWORD *)&rgbSrc.data = v46;
  rgbSrc.width = v36;
  rgbSrc.rowBytes = v37;
  redDest.data = v44;
  redDest.height = v43;
  redDest.width = v42;
  redDest.rowBytes = v41;
  greenDest.data = v18;
  greenDest.height = v40;
  greenDest.width = v39;
  greenDest.rowBytes = v24;
  blueDest.data = v28;
  blueDest.height = v30;
  blueDest.width = v32;
  blueDest.rowBytes = v34;
  vImage_Error result = vImageConvert_RGB888toPlanar8(&rgbSrc, &redDest, &greenDest, &blueDest, 0);
  void *v45 = v5;
  return result;
}

uint64_t specialized vImage.PixelBuffer<>.vImageBuffers.getter(uint64_t a1)
{
  int64_t v1 = *(void *)(a1 + 16);
  uint64_t v2 = MEMORY[0x1E4FBC860];
  if (v1)
  {
    uint64_t v15 = MEMORY[0x1E4FBC860];
    swift_bridgeObjectRetain();
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v1, 0);
    uint64_t v2 = v15;
    unint64_t v4 = *(void *)(v15 + 16);
    uint64_t v5 = 32 * v4;
    uint64_t v6 = (long long *)(a1 + 48);
    do
    {
      long long v7 = *(v6 - 1);
      long long v8 = *v6;
      unint64_t v9 = *(void *)(v15 + 24);
      unint64_t v10 = v4 + 1;
      if (v4 >= v9 >> 1)
      {
        long long v13 = *v6;
        long long v14 = *(v6 - 1);
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v9 > 1), v4 + 1, 1);
        long long v8 = v13;
        long long v7 = v14;
      }
      *(void *)(v15 + 16) = v10;
      uint64_t v11 = v15 + v5;
      *(_OWORD *)(v11 + 32) = v7;
      *(_OWORD *)(v11 + 48) = v8;
      v5 += 32;
      uint64_t v6 = (long long *)((char *)v6 + 40);
      unint64_t v4 = v10;
      --v1;
    }
    while (v1);
    swift_bridgeObjectRelease();
  }
  return v2;
}

uint64_t vImage.PixelBuffer<>.interleave(destination:)()
{
  int64_t v1 = (void *)specialized vImage.PixelBuffer<>.pixelBuffers.getter(*v0, (void (*)(BOOL, unint64_t, uint64_t))specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:));
  vImage.PixelBuffer<>.interleave(planarSourceBuffers:)(v1);

  return swift_bridgeObjectRelease();
}

{
  uint64_t *v0;
  void *v1;
  uint64_t vars8;

  int64_t v1 = (void *)specialized vImage.PixelBuffer<>.pixelBuffers.getter(*v0);
  vImage.PixelBuffer<>.interleave(planarSourceBuffers:)(v1);

  return swift_bridgeObjectRelease();
}

{
  uint64_t *v0;
  void *v1;
  uint64_t vars8;

  int64_t v1 = (void *)specialized vImage.PixelBuffer<>.pixelBuffers.getter(*v0);
  vImage.PixelBuffer<>.interleave(planarSourceBuffers:)(v1);

  return swift_bridgeObjectRelease();
}

{
  uint64_t *v0;
  void *v1;
  uint64_t vars8;

  int64_t v1 = (void *)specialized vImage.PixelBuffer<>.pixelBuffers.getter(*v0);
  vImage.PixelBuffer<>.interleave(planarSourceBuffers:)(v1);

  return swift_bridgeObjectRelease();
}

uint64_t specialized vImage.PixelBuffer<>.pixelBuffers.getter(uint64_t a1)
{
  return specialized vImage.PixelBuffer<>.pixelBuffers.getter(a1, (void (*)(BOOL, unint64_t, uint64_t))specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:));
}

{
  return specialized vImage.PixelBuffer<>.pixelBuffers.getter(a1, (void (*)(BOOL, unint64_t, uint64_t))specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:));
}

uint64_t specialized vImage.PixelBuffer<>.pixelBuffers.getter(uint64_t a1, void (*a2)(BOOL, unint64_t, uint64_t))
{
  unint64_t v2 = *(void *)(a1 + 16);
  uint64_t v3 = MEMORY[0x1E4FBC860];
  if (v2)
  {
    uint64_t v16 = MEMORY[0x1E4FBC860];
    swift_bridgeObjectRetain();
    long long v14 = a2;
    a2(0, v2, 0);
    uint64_t v3 = v16;
    uint64_t v6 = (void *)(a1 + 64);
    do
    {
      long long v15 = *((_OWORD *)v6 - 2);
      uint64_t v7 = *(v6 - 2);
      uint64_t v8 = *(v6 - 1);
      uint64_t v9 = *v6;
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
      uint64_t v10 = swift_allocObject();
      *(_OWORD *)(v10 + 16) = xmmword_1D2135280;
      *(_OWORD *)(v10 + 32) = v15;
      *(void *)(v10 + 48) = v7;
      *(void *)(v10 + 56) = v8;
      *(void *)(v10 + 64) = v9;
      unint64_t v12 = *(void *)(v16 + 16);
      unint64_t v11 = *(void *)(v16 + 24);
      swift_retain();
      if (v12 >= v11 >> 1) {
        v14(v11 > 1, v12 + 1, 1);
      }
      v6 += 5;
      *(void *)(v16 + 16) = v12 + 1;
      *(void *)(v16 + 8 * v12 + 32) = v10;
      --v2;
    }
    while (v2);
    swift_bridgeObjectRelease();
  }
  return v3;
}

uint64_t vImage.PixelBuffer<>.applyPolynomial(coefficientSegments:boundaries:destination:)(uint64_t a1, uint64_t a2, void **a3)
{
  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 1, *v3, MEMORY[0x1E4F17088]);
}

{
  void **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 2, *v3, MEMORY[0x1E4F17088]);
}

{
  void **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 3, *v3, MEMORY[0x1E4F17088]);
}

{
  void **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 4, *v3, MEMORY[0x1E4F17088]);
}

{
  void **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 1, *v3, MEMORY[0x1E4F17090]);
}

{
  void **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 1, *v3, MEMORY[0x1E4F17098]);
}

{
  void **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 2, *v3, MEMORY[0x1E4F17090]);
}

{
  void **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 2, *v3, MEMORY[0x1E4F17098]);
}

{
  void **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 3, *v3, MEMORY[0x1E4F17090]);
}

{
  void **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 3, *v3, MEMORY[0x1E4F17098]);
}

{
  void **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 4, *v3, MEMORY[0x1E4F17090]);
}

{
  void **v3;

  return specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(a1, a2, *a3, 4, *v3, MEMORY[0x1E4F17098]);
}

uint64_t specialized vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(uint64_t a1, uint64_t a2, void *a3, uint64_t a4, void *a5, void (*a6)(void *, void *, uint64_t, uint64_t, void, uint64_t, void))
{
  v63[22] = *MEMORY[0x1E4F143B8];
  unint64_t v6 = *(void *)(a1 + 16);
  if (!v6)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (!a5[2])
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  uint64_t v8 = a5[6];
  if (v8 < 0)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  uint64_t v9 = a5[5];
  if (v9 < 0)
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  if (!v8)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  if (!v9)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!a3[2])
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  uint64_t v11 = a3[6];
  if (v11 < 0)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  uint64_t v12 = a3[5];
  if (v12 < 0)
  {
LABEL_68:
    __break(1u);
    goto LABEL_69;
  }
  if (!v11)
  {
LABEL_69:
    __break(1u);
    goto LABEL_70;
  }
  if (!v12)
  {
LABEL_70:
    __break(1u);
    goto LABEL_71;
  }
  if (v8 != v11)
  {
LABEL_71:
    __break(1u);
    goto LABEL_72;
  }
  if (v9 != v12)
  {
LABEL_72:
    __break(1u);
LABEL_73:
    __break(1u);
    goto LABEL_74;
  }
  uint64_t v60 = a5[5];
  v63[0] = MEMORY[0x1E4FBC860];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v6, 0);
  uint64_t v14 = 0;
  uint64_t v15 = v63[0];
  unint64_t v16 = *(void *)(v63[0] + 16);
  do
  {
    uint64_t v17 = *(void *)(*(void *)(a1 + 8 * v14 + 32) + 16);
    v63[0] = v15;
    unint64_t v18 = *(void *)(v15 + 24);
    if (v16 >= v18 >> 1)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v18 > 1), v16 + 1, 1);
      uint64_t v15 = v63[0];
    }
    ++v14;
    *(void *)(v15 + 16) = v16 + 1;
    *(void *)(v15 + 8 * v16++ + 32) = v17;
  }
  while (v6 != v14);
  uint64_t v19 = specialized Set.init<A>(_:)(v15, MEMORY[0x1E4FBB550], MEMORY[0x1E4FBB560], &demangling cache variable for type metadata for _SetStorage<Int>);
  swift_bridgeObjectRelease();
  uint64_t v20 = *(void *)(v19 + 16);
  swift_bridgeObjectRelease();
  if (v20 != 1) {
    goto LABEL_73;
  }
  uint64_t v21 = *(void *)(a2 + 16);
  if (v21 != v6 + 1)
  {
LABEL_74:
    __break(1u);
    goto LABEL_75;
  }
  if (v21 != 1)
  {
    if (!v21)
    {
LABEL_75:
      __break(1u);
      goto LABEL_76;
    }
    vImagePixelCount v22 = (float *)(a2 + 36);
    uint64_t v23 = *(void *)(a2 + 16);
    while (v23)
    {
      if (*v22 < *(v22 - 1)) {
        goto LABEL_59;
      }
      --v23;
      ++v22;
      if (v23 == 1) {
        goto LABEL_26;
      }
    }
    __break(1u);
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
LABEL_26:
  unint64_t v24 = *(void *)(*(void *)(a1 + 32) + 16);
  if (!v24)
  {
LABEL_76:
    __break(1u);
    goto LABEL_77;
  }
  if (v24 > 0x100000000)
  {
LABEL_77:
    __break(1u);
LABEL_78:
    __break(1u);
    goto LABEL_79;
  }
  uint64_t v59 = a5[4];
  if (!v59)
  {
LABEL_82:
    __break(1u);
LABEL_83:
    swift_setDeallocating();
    swift_arrayDestroy();
    __break(1u);
  }
  uint64_t v25 = v8 * a4;
  if ((unsigned __int128)(v8 * (__int128)a4) >> 64 != (v8 * a4) >> 63) {
    goto LABEL_78;
  }
  uint64_t v26 = a5[7];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135280;
  if (v25 < 0)
  {
LABEL_79:
    __break(1u);
LABEL_80:
    __break(1u);
    goto LABEL_81;
  }
  *(void *)(inited + 32) = v59;
  *(void *)(inited + 40) = v60;
  *(void *)(inited + 48) = v25;
  *(void *)(inited + 56) = v26;
  int v55 = v24;
  *(void *)(inited + 64) = 0;
  uint64_t v28 = a3[4];
  if (!v28) {
    goto LABEL_83;
  }
  int v29 = __clz(v6);
  uint64_t v30 = a3[7];
  uint64_t v31 = swift_initStackObject();
  *(_OWORD *)(v31 + 16) = xmmword_1D2135280;
  *(void *)(v31 + 32) = v28;
  *(void *)(v31 + 40) = v60;
  *(void *)(v31 + 48) = v25;
  *(void *)(v31 + 56) = v30;
  *(void *)(v31 + 64) = 0;
  if (v29 == 1) {
    goto LABEL_80;
  }
  uint64_t v58 = v28;
  uint64_t v53 = (64 - v29);
  uint64_t v54 = v30;
  swift_bridgeObjectRetain();
  uint64_t v32 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_SPySfGSgs5NeverOTg50161_s10Accelerate6vImageO11PixelBufferVA2A06StaticC6FormatRzrlE16_applyPolynomial19coefficientSegments10boundaries11destination14polynomialFunc15widthMultiplierySayw41SfGG_AmEy_qd__GSiSPySo01vb1_D0VG_ARSpySPyx14GSgGASs6UInt32K42WtXESitAA011SinglePlanecF0Rd__lFATSiXEfU0_SaySaySfGGTf1cn_nTf4ng_n(0, 1 << (64 - v29), a1);
  swift_bridgeObjectRelease();
  uint64_t v34 = *(void *)(v32 + 16);
  if (v21 == v34 + 1)
  {
    uint64_t v35 = a2;
    swift_bridgeObjectRetain();
    swift_release();
    int v36 = v55;
    goto LABEL_49;
  }
  unint64_t v37 = v34 - v21;
  if (v34 - v21 < -1)
  {
LABEL_81:
    __break(1u);
    goto LABEL_82;
  }
  if (v37 == -1)
  {
    uint64_t v39 = MEMORY[0x1E4FBC860];
  }
  else
  {
    v33.i32[0] = *(_DWORD *)(a2 + 4 * v21 + 28);
    int32x2_t v52 = v33;
    uint64_t v38 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    uint64_t v39 = v38;
    *(void *)(v38 + 16) = v37 + 1;
    *(_DWORD *)(v38 + 32) = v52.i32[0];
    uint64_t v40 = v38 + 32;
    if (v34 != v21)
    {
      if (v37 < 8)
      {
        unint64_t v41 = 0;
LABEL_45:
        uint64_t v45 = v41 + v21 - v34;
        long long v46 = (_DWORD *)(v40 + 4);
        int v36 = v55;
        do
          *v46++ = v52.i32[0];
        while (!__CFADD__(v45++, 1));
        goto LABEL_48;
      }
      unint64_t v41 = v37 & 0xFFFFFFFFFFFFFFF8;
      v40 += 4 * (v37 & 0xFFFFFFFFFFFFFFF8);
      int32x4_t v42 = vdupq_lane_s32(v52, 0);
      vImagePixelCount v43 = (int32x4_t *)(v38 + 52);
      unint64_t v44 = v37 & 0xFFFFFFFFFFFFFFF8;
      do
      {
        v43[-1] = v42;
        *vImagePixelCount v43 = v42;
        v43 += 2;
        v44 -= 8;
      }
      while (v44);
      if (v37 != v41) {
        goto LABEL_45;
      }
    }
  }
  int v36 = v55;
LABEL_48:
  swift_bridgeObjectRetain();
  specialized Array.append<A>(contentsOf:)(v39);
  swift_release();
  uint64_t v35 = a2;
LABEL_49:
  v63[0] = v59;
  v63[1] = v60;
  v63[2] = v25;
  v63[3] = v26;
  v62[0] = v58;
  v62[1] = v60;
  v62[2] = v25;
  v62[3] = v54;
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
    uint64_t v32 = (uint64_t)specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(void *)(v32 + 16), 0, (char *)v32, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<Float>?>);
  }
  a6(v63, v62, v32 + 32, v35 + 32, (v36 - 1), v53, 0);
  swift_bridgeObjectRelease();
  swift_release();
  uint64_t v48 = *(void *)(v32 + 16);
  if (v48)
  {
    swift_bridgeObjectRetain();
    for (uint64_t i = 0; i != v48; ++i)
    {
      uint64_t v50 = *(void *)(v32 + 8 * i + 32);
      if (v50) {
        MEMORY[0x1D26009C0](v50, -1, -1);
      }
    }
    swift_bridgeObjectRelease();
  }
  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>._applyPolynomial<A>(coefficientSegments:boundaries:destination:polynomialFunc:widthMultiplier:)(uint64_t a1, uint64_t a2, uint64_t *a3, void (*a4)(uint64_t *, uint64_t *, uint64_t, uint64_t, void, uint64_t, void), uint64_t a5, uint64_t a6)
{
  uint64_t v79 = *MEMORY[0x1E4F143B8];
  unint64_t v7 = *(void *)(a1 + 16);
  if (!v7)
  {
LABEL_53:
    __break(1u);
LABEL_54:
    __break(1u);
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  uint64_t v10 = *a3;
  uint64_t v67 = *v6;
  uint64_t v71 = *v6;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&v75);
  uint64_t v12 = v75;
  uint64_t v11 = v76;
  type metadata accessor for vImage.PixelBuffer();
  vImage.PixelBuffer.size.getter(&v71);
  swift_bridgeObjectRelease();
  if (v12 != v71 || v11 != v72) {
    goto LABEL_54;
  }
  v65 = a4;
  uint64_t v75 = MEMORY[0x1E4FBC860];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v7, 0);
  uint64_t v13 = 0;
  uint64_t v14 = v75;
  unint64_t v15 = *(void *)(v75 + 16);
  do
  {
    uint64_t v16 = *(void *)(*(void *)(a1 + 8 * v13 + 32) + 16);
    uint64_t v75 = v14;
    unint64_t v17 = *(void *)(v14 + 24);
    if (v15 >= v17 >> 1)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v17 > 1), v15 + 1, 1);
      uint64_t v14 = v75;
    }
    ++v13;
    *(void *)(v14 + 16) = v15 + 1;
    *(void *)(v14 + 8 * v15++ + 32) = v16;
  }
  while (v7 != v13);
  uint64_t v18 = specialized Set.init<A>(_:)(v14, MEMORY[0x1E4FBB550], MEMORY[0x1E4FBB560], &demangling cache variable for type metadata for _SetStorage<Int>);
  swift_bridgeObjectRelease();
  uint64_t v19 = *(void *)(v18 + 16);
  swift_bridgeObjectRelease();
  if (v19 != 1) {
    goto LABEL_55;
  }
  uint64_t v20 = *(void *)(a2 + 16);
  if (v20 != v7 + 1)
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  if (v20 != 1)
  {
    if (!v20)
    {
LABEL_57:
      __break(1u);
      goto LABEL_58;
    }
    uint64_t v21 = (float *)(a2 + 36);
    uint64_t v22 = *(void *)(a2 + 16);
    while (v22)
    {
      if (*v21 < *(v21 - 1)) {
        goto LABEL_52;
      }
      --v22;
      ++v21;
      if (v22 == 1) {
        goto LABEL_16;
      }
    }
    __break(1u);
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
LABEL_16:
  unint64_t v23 = *(void *)(*(void *)(a1 + 32) + 16);
  if (!v23)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (v23 > 0x100000000)
  {
LABEL_59:
    __break(1u);
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  uint64_t v64 = v10;
  uint64_t v75 = v67;
  uint64_t v24 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v24)
  {
LABEL_66:
    __break(1u);
LABEL_67:
    swift_setDeallocating();
    swift_arrayDestroy();
    __break(1u);
  }
  uint64_t v25 = v24;
  uint64_t v75 = v67;
  uint64_t v26 = vImage.PixelBuffer.width.getter();
  uint64_t v27 = v26 * a6;
  if ((unsigned __int128)(v26 * (__int128)a6) >> 64 != (v26 * a6) >> 63) {
    goto LABEL_60;
  }
  uint64_t v75 = v67;
  uint64_t v28 = vImage.PixelBuffer.height.getter();
  uint64_t v75 = v67;
  vImage.PixelBuffer<>.vImageBuffer.getter();
  uint64_t v30 = v29;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135280;
  if ((v28 | v27) < 0)
  {
LABEL_61:
    __break(1u);
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  *(void *)(inited + 32) = v25;
  *(void *)(inited + 40) = v28;
  uint64_t v62 = v28;
  uint64_t v63 = v27;
  *(void *)(inited + 48) = v27;
  *(void *)(inited + 56) = v30;
  uint64_t v61 = v30;
  *(void *)(inited + 64) = 0;
  uint64_t v75 = v64;
  uint64_t v68 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v68) {
    goto LABEL_67;
  }
  uint64_t v75 = v64;
  uint64_t v32 = vImage.PixelBuffer.width.getter();
  uint64_t v33 = v32 * a6;
  if ((unsigned __int128)(v32 * (__int128)a6) >> 64 != (v32 * a6) >> 63) {
    goto LABEL_62;
  }
  uint64_t v60 = v25;
  uint64_t v75 = v64;
  uint64_t v34 = vImage.PixelBuffer.height.getter();
  uint64_t v75 = v64;
  vImage.PixelBuffer<>.vImageBuffer.getter();
  uint64_t v36 = v35;
  uint64_t v37 = swift_initStackObject();
  *(_OWORD *)(v37 + 16) = xmmword_1D2135280;
  if ((v34 | v33) < 0)
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  int v38 = __clz(v7);
  uint64_t v39 = v68;
  *(void *)(v37 + 32) = v68;
  *(void *)(v37 + 40) = v34;
  *(void *)(v37 + 48) = v33;
  *(void *)(v37 + 56) = v36;
  *(void *)(v37 + 64) = 0;
  if (v38 == 1)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  uint64_t v40 = (64 - v38);
  swift_bridgeObjectRetain();
  uint64_t v41 = _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_SPySfGSgs5NeverOTg50161_s10Accelerate6vImageO11PixelBufferVA2A06StaticC6FormatRzrlE16_applyPolynomial19coefficientSegments10boundaries11destination14polynomialFunc15widthMultiplierySayw41SfGG_AmEy_qd__GSiSPySo01vb1_D0VG_ARSpySPyx14GSgGASs6UInt32K42WtXESitAA011SinglePlanecF0Rd__lFATSiXEfU0_SaySaySfGGTf1cn_nTf4ng_n(0, 1 << v40, a1);
  swift_bridgeObjectRelease();
  uint64_t v43 = *(void *)(v41 + 16);
  if (v20 == v43 + 1)
  {
    swift_bridgeObjectRetain();
    swift_release();
    unint64_t v44 = v65;
    goto LABEL_42;
  }
  unint64_t v45 = v43 - v20;
  if (v43 - v20 < -1)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (v45 == -1)
  {
    uint64_t v47 = MEMORY[0x1E4FBC860];
  }
  else
  {
    v42.i32[0] = *(_DWORD *)(a2 + 4 * v20 + 28);
    int32x2_t v66 = v42;
    uint64_t v46 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    uint64_t v47 = v46;
    *(void *)(v46 + 16) = v45 + 1;
    *(_DWORD *)(v46 + 32) = v66.i32[0];
    uint64_t v48 = v46 + 32;
    if (v43 != v20)
    {
      if (v45 < 8)
      {
        unint64_t v49 = 0;
LABEL_37:
        uint64_t v53 = v49 + v20 - v43;
        uint64_t v54 = (_DWORD *)(v48 + 4);
        uint64_t v39 = v68;
        do
          *v54++ = v66.i32[0];
        while (!__CFADD__(v53++, 1));
        unint64_t v44 = v65;
        goto LABEL_41;
      }
      unint64_t v49 = v45 & 0xFFFFFFFFFFFFFFF8;
      v48 += 4 * (v45 & 0xFFFFFFFFFFFFFFF8);
      int32x4_t v50 = vdupq_lane_s32(v66, 0);
      uint64_t v51 = (int32x4_t *)(v46 + 52);
      unint64_t v52 = v45 & 0xFFFFFFFFFFFFFFF8;
      do
      {
        v51[-1] = v50;
        *uint64_t v51 = v50;
        v51 += 2;
        v52 -= 8;
      }
      while (v52);
      if (v45 != v49) {
        goto LABEL_37;
      }
    }
  }
  unint64_t v44 = v65;
  uint64_t v39 = v68;
LABEL_41:
  uint64_t v75 = a2;
  swift_bridgeObjectRetain();
  specialized Array.append<A>(contentsOf:)(v47);
  swift_release();
  a2 = v75;
LABEL_42:
  uint64_t v75 = v60;
  uint64_t v76 = v62;
  uint64_t v77 = v63;
  uint64_t v78 = v61;
  uint64_t v71 = v39;
  uint64_t v72 = v34;
  uint64_t v73 = v33;
  uint64_t v74 = v36;
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
    uint64_t v41 = (uint64_t)specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(void *)(v41 + 16), 0, (char *)v41, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<Float>?>);
  }
  swift_bridgeObjectRetain();
  v44(&v75, &v71, v41 + 32, a2 + 32, (v23 - 1), v40, 0);
  swift_bridgeObjectRelease();
  swift_release();
  swift_bridgeObjectRelease();
  uint64_t v56 = *(void *)(v41 + 16);
  if (v56)
  {
    swift_bridgeObjectRetain();
    for (uint64_t i = 0; i != v56; ++i)
    {
      uint64_t v58 = *(void *)(v41 + 8 * i + 32);
      if (v58) {
        MEMORY[0x1D26009C0](v58, -1, -1);
      }
    }
    swift_bridgeObjectRelease();
  }
  return swift_bridgeObjectRelease();
}

char *specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(char *a1, int64_t a2, char a3, char *a4)
{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<vImage_Buffer>?>);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>?>);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<Int8>?>);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>>);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer?>);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafePointer<BNNSNDArrayDescriptor>>);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(a1, a2, a3, a4, &demangling cache variable for type metadata for _ContiguousArrayStorage<UInt>);
}

char *specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(char *result, int64_t a2, char a3, char *a4)
{
  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    uint64_t v12 = v11 - 32;
    if (v11 < 32) {
      uint64_t v12 = v11 - 29;
    }
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * (v12 >> 2);
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v13 = v10 + 32;
  uint64_t v14 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v13 >= &v14[4 * v8]) {
      memmove(v13, v14, 4 * v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 4 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<DSPComplex>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    uint64_t v12 = v11 - 32;
    if (v11 < 32) {
      uint64_t v12 = v11 - 25;
    }
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * (v12 >> 3);
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v13 = v10 + 32;
  uint64_t v14 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v13 >= &v14[8 * v8]) {
      memmove(v13, v14, 8 * v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 8 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<DSPDoubleComplex>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    uint64_t v12 = v11 - 32;
    if (v11 < 32) {
      uint64_t v12 = v11 - 17;
    }
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * (v12 >> 4);
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v13 = v10 + 32;
  uint64_t v14 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v13 >= &v14[16 * v8]) {
      memmove(v13, v14, 16 * v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 16 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<bnns_graph_shape_t>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    uint64_t v12 = v11 - 32;
    if (v11 < 32) {
      uint64_t v12 = v11 - 17;
    }
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * (v12 >> 4);
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v13 = v10 + 32;
  uint64_t v14 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v13 >= &v14[16 * v8]) {
      memmove(v13, v14, 16 * v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 16 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  size_t v11;
  char *v12;
  char *v13;

  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<BNNSTensor>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * ((uint64_t)(v11 - 32) / 160);
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v12 = v10 + 32;
  uint64_t v13 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v12 >= &v13[160 * v8]) {
      memmove(v12, v13, 160 * v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v12, v13, 160 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<bnns_graph_argument_t>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    uint64_t v12 = v11 - 32;
    if (v11 < 32) {
      uint64_t v12 = v11 - 17;
    }
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * (v12 >> 4);
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v13 = v10 + 32;
  uint64_t v14 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v13 >= &v14[16 * v8]) {
      memmove(v13, v14, 16 * v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 16 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Double>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    uint64_t v12 = v11 - 32;
    if (v11 < 32) {
      uint64_t v12 = v11 - 25;
    }
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * (v12 >> 3);
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v13 = v10 + 32;
  uint64_t v14 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v13 >= &v14[8 * v8]) {
      memmove(v13, v14, 8 * v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 8 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage_Buffer>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    uint64_t v12 = v11 - 32;
    if (v11 < 32) {
      uint64_t v12 = v11 - 1;
    }
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * (v12 >> 5);
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v13 = v10 + 32;
  uint64_t v14 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v13 >= &v14[32 * v8]) {
      memmove(v13, v14, 32 * v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 32 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  uint64_t v8;
  uint64_t v9;
  char *v10;
  int64_t v11;
  uint64_t v12;
  char *v13;
  char *v14;

  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<(x: Int, y: Int)>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    uint64_t v12 = v11 - 32;
    if (v11 < 32) {
      uint64_t v12 = v11 - 17;
    }
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * (v12 >> 4);
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v13 = v10 + 32;
  uint64_t v14 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v13 >= &v14[16 * v8]) {
      memmove(v13, v14, 16 * v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v13, v14, 16 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

{
  char v5;
  unint64_t v6;
  int64_t v7;
  int64_t v8;
  int64_t v9;
  char *v10;
  size_t v11;
  char *v12;
  char *v13;

  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferType>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    *((void *)v10 + 2) = v8;
    *((void *)v10 + 3) = 2 * v11 - 64;
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v12 = v10 + 32;
  uint64_t v13 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v12 >= &v13[v8]) {
      memmove(v12, v13, v8);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v12, v13, v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

char *specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(char *result, int64_t a2, char a3, char *a4, uint64_t *a5)
{
  char v6 = (char)result;
  if (a3)
  {
    unint64_t v7 = *((void *)a4 + 3);
    int64_t v8 = v7 >> 1;
    if ((uint64_t)(v7 >> 1) < a2)
    {
      if (v8 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v8 = v7 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v7 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v8 = a2;
      }
    }
  }
  else
  {
    int64_t v8 = a2;
  }
  uint64_t v9 = *((void *)a4 + 2);
  if (v8 <= v9) {
    uint64_t v10 = *((void *)a4 + 2);
  }
  else {
    uint64_t v10 = v8;
  }
  if (v10)
  {
    __swift_instantiateConcreteTypeFromMangledName(a5);
    int64_t v11 = (char *)swift_allocObject();
    int64_t v12 = _swift_stdlib_malloc_size(v11);
    uint64_t v13 = v12 - 32;
    if (v12 < 32) {
      uint64_t v13 = v12 - 25;
    }
    *((void *)v11 + 2) = v9;
    *((void *)v11 + 3) = 2 * (v13 >> 3);
  }
  else
  {
    int64_t v11 = (char *)MEMORY[0x1E4FBC860];
  }
  uint64_t v14 = v11 + 32;
  unint64_t v15 = a4 + 32;
  if (v6)
  {
    if (v11 != a4 || v14 >= &v15[8 * v9]) {
      memmove(v14, v15, 8 * v9);
    }
    *((void *)a4 + 2) = 0;
  }
  else
  {
    memcpy(v14, v15, 8 * v9);
  }
  swift_bridgeObjectRelease();
  return v11;
}

void *specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(void *result, int64_t a2, char a3, void *a4)
{
  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = a4[3];
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = a4[2];
  if (v7 <= v8) {
    uint64_t v9 = a4[2];
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<[UInt]>);
    uint64_t v10 = (void *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    uint64_t v12 = v11 - 32;
    if (v11 < 32) {
      uint64_t v12 = v11 - 25;
    }
    v10[2] = v8;
    v10[3] = 2 * (v12 >> 3);
  }
  else
  {
    uint64_t v10 = (void *)MEMORY[0x1E4FBC860];
  }
  if (v5)
  {
    if (v10 != a4 || v10 + 4 >= &a4[v8 + 4]) {
      memmove(v10 + 4, a4 + 4, 8 * v8);
    }
    a4[2] = 0;
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for [UInt]);
    swift_arrayInitWithCopy();
  }
  swift_bridgeObjectRelease();
  return v10;
}

uint64_t specialized Set._Variant.insert(_:)(uint64_t *a1, uint64_t a2, uint64_t *a3)
{
  char v5 = v3;
  uint64_t v8 = *v3;
  uint64_t v9 = static Hasher._hash(seed:_:)();
  uint64_t v10 = -1 << *(unsigned char *)(v8 + 32);
  unint64_t v11 = v9 & ~v10;
  if (((*(void *)(v8 + 56 + ((v11 >> 3) & 0xFFFFFFFFFFFFFF8)) >> v11) & 1) == 0)
  {
LABEL_7:
    char isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
    uint64_t v16 = *v5;
    *char v5 = 0x8000000000000000;
    specialized _NativeSet.insertNew(_:at:isUnique:)(a2, v11, isUniquelyReferenced_nonNull_native, a3);
    *char v5 = v16;
    swift_bridgeObjectRelease();
    uint64_t result = 1;
    goto LABEL_8;
  }
  uint64_t v12 = *(void *)(v8 + 48);
  if (*(void *)(v12 + 8 * v11) != a2)
  {
    uint64_t v13 = ~v10;
    do
    {
      unint64_t v11 = (v11 + 1) & v13;
      if (((*(void *)(v8 + 56 + ((v11 >> 3) & 0xFFFFFFFFFFFFFF8)) >> v11) & 1) == 0) {
        goto LABEL_7;
      }
    }
    while (*(void *)(v12 + 8 * v11) != a2);
  }
  uint64_t result = 0;
LABEL_8:
  *a1 = a2;
  return result;
}

uint64_t specialized Array.append<A>(contentsOf:)(uint64_t result)
{
  uint64_t v2 = *(void *)(result + 16);
  uint64_t v3 = *v1;
  int64_t v4 = *(void *)(*v1 + 16);
  int64_t v5 = v4 + v2;
  if (__OFADD__(v4, v2))
  {
    __break(1u);
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v6 = result;
  uint64_t result = swift_isUniquelyReferenced_nonNull_native();
  if (result && v5 <= *(void *)(v3 + 24) >> 1)
  {
    if (*(void *)(v6 + 16)) {
      goto LABEL_5;
    }
    goto LABEL_13;
  }
  if (v4 <= v5) {
    int64_t v11 = v4 + v2;
  }
  else {
    int64_t v11 = v4;
  }
  uint64_t result = (uint64_t)specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)result, v11, 1, (char *)v3);
  uint64_t v3 = result;
  if (!*(void *)(v6 + 16))
  {
LABEL_13:
    if (!v2) {
      goto LABEL_14;
    }
    goto LABEL_16;
  }
LABEL_5:
  uint64_t v7 = *(void *)(v3 + 16);
  if ((*(void *)(v3 + 24) >> 1) - v7 < v2)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  uint64_t result = (uint64_t)memcpy((void *)(v3 + 4 * v7 + 32), (const void *)(v6 + 32), 4 * v2);
  if (!v2)
  {
LABEL_14:
    uint64_t result = swift_bridgeObjectRelease();
    uint64_t *v1 = v3;
    return result;
  }
  uint64_t v8 = *(void *)(v3 + 16);
  BOOL v9 = __OFADD__(v8, v2);
  uint64_t v10 = v8 + v2;
  if (!v9)
  {
    *(void *)(v3 + 16) = v10;
    goto LABEL_14;
  }
LABEL_18:
  __break(1u);
  return result;
}

uint64_t specialized _NativeSet.resize(capacity:)(uint64_t a1, uint64_t *a2)
{
  uint64_t v3 = v2;
  uint64_t v4 = *v2;
  __swift_instantiateConcreteTypeFromMangledName(a2);
  uint64_t result = static _SetStorage.resize(original:capacity:move:)();
  uint64_t v6 = result;
  if (*(void *)(v4 + 16))
  {
    int64_t v7 = 0;
    uint64_t v8 = (uint64_t *)(v4 + 56);
    uint64_t v9 = 1 << *(unsigned char *)(v4 + 32);
    uint64_t v28 = -1 << v9;
    uint64_t v29 = v3;
    if (v9 < 64) {
      uint64_t v10 = ~(-1 << v9);
    }
    else {
      uint64_t v10 = -1;
    }
    unint64_t v11 = v10 & *(void *)(v4 + 56);
    uint64_t v30 = 1 << *(unsigned char *)(v4 + 32);
    int64_t v12 = (unint64_t)(v9 + 63) >> 6;
    uint64_t v13 = result + 56;
    while (1)
    {
      if (v11)
      {
        unint64_t v15 = __clz(__rbit64(v11));
        v11 &= v11 - 1;
        unint64_t v16 = v15 | (v7 << 6);
      }
      else
      {
        int64_t v17 = v7 + 1;
        if (__OFADD__(v7, 1))
        {
LABEL_38:
          __break(1u);
LABEL_39:
          __break(1u);
          return result;
        }
        if (v17 >= v12) {
          goto LABEL_33;
        }
        unint64_t v18 = v8[v17];
        ++v7;
        if (!v18)
        {
          int64_t v7 = v17 + 1;
          if (v17 + 1 >= v12) {
            goto LABEL_33;
          }
          unint64_t v18 = v8[v7];
          if (!v18)
          {
            int64_t v7 = v17 + 2;
            if (v17 + 2 >= v12) {
              goto LABEL_33;
            }
            unint64_t v18 = v8[v7];
            if (!v18)
            {
              int64_t v19 = v17 + 3;
              if (v19 >= v12)
              {
LABEL_33:
                if (v30 >= 64) {
                  bzero((void *)(v4 + 56), 8 * v12);
                }
                else {
                  *uint64_t v8 = v28;
                }
                uint64_t v3 = v29;
                *(void *)(v4 + 16) = 0;
                break;
              }
              unint64_t v18 = v8[v19];
              if (!v18)
              {
                while (1)
                {
                  int64_t v7 = v19 + 1;
                  if (__OFADD__(v19, 1)) {
                    goto LABEL_39;
                  }
                  if (v7 >= v12) {
                    goto LABEL_33;
                  }
                  unint64_t v18 = v8[v7];
                  ++v19;
                  if (v18) {
                    goto LABEL_23;
                  }
                }
              }
              int64_t v7 = v19;
            }
          }
        }
LABEL_23:
        unint64_t v11 = (v18 - 1) & v18;
        unint64_t v16 = __clz(__rbit64(v18)) + (v7 << 6);
      }
      uint64_t v20 = *(void *)(*(void *)(v4 + 48) + 8 * v16);
      uint64_t result = static Hasher._hash(seed:_:)();
      uint64_t v21 = -1 << *(unsigned char *)(v6 + 32);
      unint64_t v22 = result & ~v21;
      unint64_t v23 = v22 >> 6;
      if (((-1 << v22) & ~*(void *)(v13 + 8 * (v22 >> 6))) != 0)
      {
        unint64_t v14 = __clz(__rbit64((-1 << v22) & ~*(void *)(v13 + 8 * (v22 >> 6)))) | v22 & 0x7FFFFFFFFFFFFFC0;
      }
      else
      {
        char v24 = 0;
        unint64_t v25 = (unint64_t)(63 - v21) >> 6;
        do
        {
          if (++v23 == v25 && (v24 & 1) != 0)
          {
            __break(1u);
            goto LABEL_38;
          }
          BOOL v26 = v23 == v25;
          if (v23 == v25) {
            unint64_t v23 = 0;
          }
          v24 |= v26;
          uint64_t v27 = *(void *)(v13 + 8 * v23);
        }
        while (v27 == -1);
        unint64_t v14 = __clz(__rbit64(~v27)) + (v23 << 6);
      }
      *(void *)(v13 + ((v14 >> 3) & 0x1FFFFFFFFFFFFFF8)) |= 1 << v14;
      *(void *)(*(void *)(v6 + 48) + 8 * v14) = v20;
      ++*(void *)(v6 + 16);
    }
  }
  uint64_t result = swift_release();
  *uint64_t v3 = v6;
  return result;
}

uint64_t specialized _NativeSet.insertNew(_:at:isUnique:)(uint64_t result, unint64_t a2, char a3, uint64_t *a4)
{
  uint64_t v6 = result;
  unint64_t v7 = *(void *)(*v4 + 16);
  unint64_t v8 = *(void *)(*v4 + 24);
  if (v8 > v7 && (a3 & 1) != 0) {
    goto LABEL_14;
  }
  uint64_t v9 = v7 + 1;
  if (a3)
  {
    specialized _NativeSet.resize(capacity:)(v9, a4);
  }
  else
  {
    if (v8 > v7)
    {
      uint64_t result = (uint64_t)specialized _NativeSet.copy()(a4);
      goto LABEL_14;
    }
    specialized _NativeSet.copyAndResize(capacity:)(v9, a4);
  }
  uint64_t v10 = *v4;
  uint64_t result = static Hasher._hash(seed:_:)();
  uint64_t v11 = -1 << *(unsigned char *)(v10 + 32);
  a2 = result & ~v11;
  if ((*(void *)(v10 + 56 + ((a2 >> 3) & 0xFFFFFFFFFFFFFF8)) >> a2))
  {
    uint64_t v12 = *(void *)(v10 + 48);
    if (*(void *)(v12 + 8 * a2) == v6)
    {
LABEL_13:
      uint64_t result = ELEMENT_TYPE_OF_SET_VIOLATES_HASHABLE_REQUIREMENTS(_:)();
      __break(1u);
    }
    else
    {
      uint64_t v13 = ~v11;
      while (1)
      {
        a2 = (a2 + 1) & v13;
        if (((*(void *)(v10 + 56 + ((a2 >> 3) & 0xFFFFFFFFFFFFFF8)) >> a2) & 1) == 0) {
          break;
        }
        if (*(void *)(v12 + 8 * a2) == v6) {
          goto LABEL_13;
        }
      }
    }
  }
LABEL_14:
  uint64_t v14 = *v4;
  *(void *)(*v4 + 8 * (a2 >> 6) + 56) |= 1 << a2;
  *(void *)(*(void *)(v14 + 48) + 8 * a2) = v6;
  uint64_t v15 = *(void *)(v14 + 16);
  BOOL v16 = __OFADD__(v15, 1);
  uint64_t v17 = v15 + 1;
  if (v16) {
    __break(1u);
  }
  else {
    *(void *)(v14 + 16) = v17;
  }
  return result;
}

void *specialized _NativeSet.copy()(uint64_t *a1)
{
  uint64_t v2 = v1;
  __swift_instantiateConcreteTypeFromMangledName(a1);
  uint64_t v3 = *v1;
  uint64_t v4 = static _SetStorage.copy(original:)();
  uint64_t v5 = v4;
  if (!*(void *)(v3 + 16))
  {
LABEL_28:
    uint64_t result = (void *)swift_release();
    uint64_t *v2 = v5;
    return result;
  }
  uint64_t result = (void *)(v4 + 56);
  uint64_t v7 = v3 + 56;
  unint64_t v8 = (unint64_t)((1 << *(unsigned char *)(v5 + 32)) + 63) >> 6;
  if (v5 != v3 || (unint64_t)result >= v3 + 56 + 8 * v8) {
    uint64_t result = memmove(result, (const void *)(v3 + 56), 8 * v8);
  }
  int64_t v10 = 0;
  *(void *)(v5 + 16) = *(void *)(v3 + 16);
  uint64_t v11 = 1 << *(unsigned char *)(v3 + 32);
  uint64_t v12 = -1;
  if (v11 < 64) {
    uint64_t v12 = ~(-1 << v11);
  }
  unint64_t v13 = v12 & *(void *)(v3 + 56);
  int64_t v14 = (unint64_t)(v11 + 63) >> 6;
  while (1)
  {
    if (v13)
    {
      unint64_t v15 = __clz(__rbit64(v13));
      v13 &= v13 - 1;
      unint64_t v16 = v15 | (v10 << 6);
      goto LABEL_12;
    }
    int64_t v17 = v10 + 1;
    if (__OFADD__(v10, 1))
    {
      __break(1u);
      goto LABEL_30;
    }
    if (v17 >= v14) {
      goto LABEL_28;
    }
    unint64_t v18 = *(void *)(v7 + 8 * v17);
    ++v10;
    if (!v18)
    {
      int64_t v10 = v17 + 1;
      if (v17 + 1 >= v14) {
        goto LABEL_28;
      }
      unint64_t v18 = *(void *)(v7 + 8 * v10);
      if (!v18)
      {
        int64_t v10 = v17 + 2;
        if (v17 + 2 >= v14) {
          goto LABEL_28;
        }
        unint64_t v18 = *(void *)(v7 + 8 * v10);
        if (!v18) {
          break;
        }
      }
    }
LABEL_27:
    unint64_t v13 = (v18 - 1) & v18;
    unint64_t v16 = __clz(__rbit64(v18)) + (v10 << 6);
LABEL_12:
    *(void *)(*(void *)(v5 + 48) + 8 * v16) = *(void *)(*(void *)(v3 + 48) + 8 * v16);
  }
  int64_t v19 = v17 + 3;
  if (v19 >= v14) {
    goto LABEL_28;
  }
  unint64_t v18 = *(void *)(v7 + 8 * v19);
  if (v18)
  {
    int64_t v10 = v19;
    goto LABEL_27;
  }
  while (1)
  {
    int64_t v10 = v19 + 1;
    if (__OFADD__(v19, 1)) {
      break;
    }
    if (v10 >= v14) {
      goto LABEL_28;
    }
    unint64_t v18 = *(void *)(v7 + 8 * v10);
    ++v19;
    if (v18) {
      goto LABEL_27;
    }
  }
LABEL_30:
  __break(1u);
  return result;
}

uint64_t specialized _NativeSet.copyAndResize(capacity:)(uint64_t a1, uint64_t *a2)
{
  uint64_t v3 = v2;
  uint64_t v4 = *v2;
  __swift_instantiateConcreteTypeFromMangledName(a2);
  uint64_t result = static _SetStorage.resize(original:capacity:move:)();
  uint64_t v6 = result;
  if (!*(void *)(v4 + 16))
  {
    uint64_t result = swift_release();
LABEL_35:
    *uint64_t v3 = v6;
    return result;
  }
  uint64_t v28 = v3;
  int64_t v7 = 0;
  uint64_t v8 = v4 + 56;
  uint64_t v9 = 1 << *(unsigned char *)(v4 + 32);
  if (v9 < 64) {
    uint64_t v10 = ~(-1 << v9);
  }
  else {
    uint64_t v10 = -1;
  }
  unint64_t v11 = v10 & *(void *)(v4 + 56);
  int64_t v12 = (unint64_t)(v9 + 63) >> 6;
  uint64_t v13 = result + 56;
  while (1)
  {
    if (v11)
    {
      unint64_t v15 = __clz(__rbit64(v11));
      v11 &= v11 - 1;
      unint64_t v16 = v15 | (v7 << 6);
      goto LABEL_24;
    }
    int64_t v17 = v7 + 1;
    if (__OFADD__(v7, 1))
    {
LABEL_36:
      __break(1u);
      goto LABEL_37;
    }
    if (v17 >= v12) {
      goto LABEL_33;
    }
    unint64_t v18 = *(void *)(v8 + 8 * v17);
    ++v7;
    if (!v18)
    {
      int64_t v7 = v17 + 1;
      if (v17 + 1 >= v12) {
        goto LABEL_33;
      }
      unint64_t v18 = *(void *)(v8 + 8 * v7);
      if (!v18)
      {
        int64_t v7 = v17 + 2;
        if (v17 + 2 >= v12) {
          goto LABEL_33;
        }
        unint64_t v18 = *(void *)(v8 + 8 * v7);
        if (!v18) {
          break;
        }
      }
    }
LABEL_23:
    unint64_t v11 = (v18 - 1) & v18;
    unint64_t v16 = __clz(__rbit64(v18)) + (v7 << 6);
LABEL_24:
    uint64_t v20 = *(void *)(*(void *)(v4 + 48) + 8 * v16);
    uint64_t result = static Hasher._hash(seed:_:)();
    uint64_t v21 = -1 << *(unsigned char *)(v6 + 32);
    unint64_t v22 = result & ~v21;
    unint64_t v23 = v22 >> 6;
    if (((-1 << v22) & ~*(void *)(v13 + 8 * (v22 >> 6))) != 0)
    {
      unint64_t v14 = __clz(__rbit64((-1 << v22) & ~*(void *)(v13 + 8 * (v22 >> 6)))) | v22 & 0x7FFFFFFFFFFFFFC0;
    }
    else
    {
      char v24 = 0;
      unint64_t v25 = (unint64_t)(63 - v21) >> 6;
      do
      {
        if (++v23 == v25 && (v24 & 1) != 0)
        {
          __break(1u);
          goto LABEL_36;
        }
        BOOL v26 = v23 == v25;
        if (v23 == v25) {
          unint64_t v23 = 0;
        }
        v24 |= v26;
        uint64_t v27 = *(void *)(v13 + 8 * v23);
      }
      while (v27 == -1);
      unint64_t v14 = __clz(__rbit64(~v27)) + (v23 << 6);
    }
    *(void *)(v13 + ((v14 >> 3) & 0x1FFFFFFFFFFFFFF8)) |= 1 << v14;
    *(void *)(*(void *)(v6 + 48) + 8 * v14) = v20;
    ++*(void *)(v6 + 16);
  }
  int64_t v19 = v17 + 3;
  if (v19 >= v12)
  {
LABEL_33:
    uint64_t result = swift_release();
    uint64_t v3 = v28;
    goto LABEL_35;
  }
  unint64_t v18 = *(void *)(v8 + 8 * v19);
  if (v18)
  {
    int64_t v7 = v19;
    goto LABEL_23;
  }
  while (1)
  {
    int64_t v7 = v19 + 1;
    if (__OFADD__(v19, 1)) {
      break;
    }
    if (v7 >= v12) {
      goto LABEL_33;
    }
    unint64_t v18 = *(void *)(v8 + 8 * v7);
    ++v19;
    if (v18) {
      goto LABEL_23;
    }
  }
LABEL_37:
  __break(1u);
  return result;
}

uint64_t _sSlsE3mapySayqd__Gqd__7ElementQzqd_0_YKXEqd_0_YKs5ErrorRd_0_r0_lFSnySiG_SPySfGSgs5NeverOTg50161_s10Accelerate6vImageO11PixelBufferVA2A06StaticC6FormatRzrlE16_applyPolynomial19coefficientSegments10boundaries11destination14polynomialFunc15widthMultiplierySayw41SfGG_AmEy_qd__GSiSPySo01vb1_D0VG_ARSpySPyx14GSgGASs6UInt32K42WtXESitAA011SinglePlanecF0Rd__lFATSiXEfU0_SaySaySfGGTf1cn_nTf4ng_n(uint64_t result, uint64_t a2, uint64_t a3)
{
  uint64_t v3 = a2 - result;
  if (__OFSUB__(a2, result))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v4 = MEMORY[0x1E4FBC860];
  if (!v3) {
    return v4;
  }
  uint64_t v7 = result;
  uint64_t v18 = MEMORY[0x1E4FBC860];
  uint64_t result = (uint64_t)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v3 & ~(v3 >> 63), 0);
  if (a2 >= v7 && (v3 & 0x8000000000000000) == 0)
  {
    uint64_t v8 = a3 + 32;
    uint64_t v4 = v18;
    unint64_t v9 = *(void *)(a3 + 16);
    while (a2 != v7)
    {
      if ((uint64_t)(v9 - 1) >= v7) {
        unint64_t v10 = v7;
      }
      else {
        unint64_t v10 = v9 - 1;
      }
      if (v10 >= v9) {
        goto LABEL_19;
      }
      uint64_t v11 = *(void *)(v8 + 8 * v10);
      unint64_t v12 = *(void *)(v11 + 16);
      if (v12 >> 61) {
        goto LABEL_20;
      }
      size_t v13 = 4 * v12;
      swift_bridgeObjectRetain();
      unint64_t v14 = (void *)swift_slowAlloc();
      unint64_t v15 = v14;
      if (*(void *)(v11 + 16)) {
        memcpy(v14, (const void *)(v11 + 32), v13);
      }
      uint64_t result = swift_bridgeObjectRelease();
      unint64_t v17 = *(void *)(v18 + 16);
      unint64_t v16 = *(void *)(v18 + 24);
      if (v17 >= v16 >> 1) {
        uint64_t result = (uint64_t)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v16 > 1), v17 + 1, 1);
      }
      *(void *)(v18 + 16) = v17 + 1;
      *(void *)(v18 + 8 * v17 + 32) = v15;
      if (a2 == ++v7) {
        return v4;
      }
    }
    __break(1u);
LABEL_19:
    __break(1u);
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
LABEL_22:
  __break(1u);
  return result;
}

uint64_t specialized Set.init<A>(_:)(uint64_t a1)
{
  return specialized Set.init<A>(_:)(a1, MEMORY[0x1E4FBB550], MEMORY[0x1E4FBB560], &demangling cache variable for type metadata for _SetStorage<Int>);
}

{
  return specialized Set.init<A>(_:)(a1, MEMORY[0x1E4FBB808], MEMORY[0x1E4FBB818], &demangling cache variable for type metadata for _SetStorage<UInt>);
}

uint64_t specialized Set.init<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t *a4)
{
  uint64_t v6 = *(void *)(a1 + 16);
  uint64_t result = Set.init(minimumCapacity:)();
  uint64_t v11 = result;
  if (v6)
  {
    uint64_t v8 = (uint64_t *)(a1 + 32);
    do
    {
      uint64_t v9 = *v8++;
      specialized Set._Variant.insert(_:)(&v10, v9, a4);
      --v6;
    }
    while (v6);
    return v11;
  }
  return result;
}

uint64_t static vDSP.convolve<A, B>(_:withKernel:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:));
}

{
  return static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(uint64_t a1, void *a2)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

uint64_t static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

{
  return static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

void closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, const float *a3, uint64_t a4, uint64_t a5, float **a6, vDSP_Length a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  if (!a3) {
    goto LABEL_11;
  }
  if (!a1)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  unint64_t v15 = *(uint64_t (**)(uint64_t, uint64_t))(a12 + 16);
  uint64_t v16 = v15(a9, a12);
  uint64_t v17 = v16 - 1;
  if (__OFSUB__(v16, 1))
  {
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v18 = *a6;
  if (v18)
  {
    if ((a7 & 0x8000000000000000) == 0)
    {
      vDSP_Length v19 = v15(a9, a12);
      if ((v19 & 0x8000000000000000) == 0)
      {
        vDSP_conv(a3, 1, (const float *)(a1 + 4 * v17), -1, v18, 1, a7, v19);
        return;
      }
      goto LABEL_10;
    }
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
LABEL_13:
  __break(1u);
}

uint64_t static vDSP.convolve<A, B>(_:withKernel:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(void *, uint64_t *))
{
  uint64_t v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  uint64_t v11 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  BOOL v12 = __OFSUB__(v10, v11);
  uint64_t result = v10 - v11;
  if (v12)
  {
    __break(1u);
  }
  else
  {
    uint64_t v14 = MEMORY[0x1F4188790](result);
    return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v14, a7);
  }
  return result;
}

void closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, const double *a3, uint64_t a4, uint64_t a5, double **a6, vDSP_Length a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  if (!a3) {
    goto LABEL_11;
  }
  if (!a1)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  unint64_t v15 = *(uint64_t (**)(uint64_t, uint64_t))(a12 + 16);
  uint64_t v16 = v15(a9, a12);
  uint64_t v17 = v16 - 1;
  if (__OFSUB__(v16, 1))
  {
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v18 = *a6;
  if (v18)
  {
    if ((a7 & 0x8000000000000000) == 0)
    {
      vDSP_Length v19 = v15(a9, a12);
      if ((v19 & 0x8000000000000000) == 0)
      {
        vDSP_convD(a3, 1, (const double *)(a1 + 8 * v17), -1, v18, 1, a7, v19);
        return;
      }
      goto LABEL_10;
    }
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
LABEL_13:
  __break(1u);
}

uint64_t static vDSP.correlate<A, B>(_:withKernel:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.correlate<A, B>(_:withKernel:));
}

{
  return static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.correlate<A, B>(_:withKernel:));
}

uint64_t static vDSP.correlate<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:));
}

{
  return static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:));
}

uint64_t static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v49 = a2;
  uint64_t v15 = *(void *)(a5 - 8);
  uint64_t v16 = ((uint64_t (*)(void))MEMORY[0x1F4188790])();
  uint64_t v18 = (char *)&v41 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v20 = *(void *)(v19 - 8);
  MEMORY[0x1F4188790](v16);
  unint64_t v22 = (char *)&v41 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v43 = v23;
  char v24 = *(uint64_t (**)(uint64_t))(*(void *)(v23 + 8) + 16);
  uint64_t v46 = v25;
  uint64_t v47 = v26;
  uint64_t v48 = v24(v26);
  uint64_t v27 = *(void (**)(char *, uint64_t, uint64_t))(v20 + 16);
  uint64_t v42 = a1;
  v27(v22, a1, a4);
  (*(void (**)(char *, uint64_t, uint64_t))(v15 + 16))(v18, v49, a5);
  uint64_t v28 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  uint64_t v44 = a7;
  uint64_t v29 = v28(a4, a7);
  uint64_t v30 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  uint64_t v45 = a8;
  uint64_t v31 = v30(a5, a8);
  (*(void (**)(char *, uint64_t))(v15 + 8))(v18, a5);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v20 + 8))(v22, a4);
  uint64_t v33 = v48 + v31;
  if (__OFADD__(v48, v31))
  {
    __break(1u);
    goto LABEL_6;
  }
  BOOL v34 = __OFSUB__(v33, 1);
  uint64_t v35 = v33 - 1;
  if (v34)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (v29 >= v35)
  {
    uint64_t v36 = MEMORY[0x1F4188790](a10);
    *(&v41 - 10) = a4;
    *(&v41 - 9) = a5;
    uint64_t v37 = v43;
    uint64_t v38 = v44;
    *(&v41 - 8) = v47;
    *(&v41 - 7) = v38;
    *(&v41 - 6) = v45;
    *(&v41 - 5) = v37;
    uint64_t v39 = v49;
    *(&v41 - 4) = v42;
    *(&v41 - 3) = v39;
    *(&v41 - 2) = v40;
    return (*(uint64_t (**)(uint64_t))(v37 + 16))(v36);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t *a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t (*a14)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  if (!a3)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  uint64_t v14 = result;
  if (!result)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  uint64_t v15 = *a5;
  if (*a5)
  {
    if (a6 < 0)
    {
      __break(1u);
    }
    else
    {
      uint64_t result = (*(uint64_t (**)(uint64_t))(a12 + 16))(a9);
      if ((result & 0x8000000000000000) == 0) {
        return a14(a3, 1, v14, 1, v15, 1, a6, result);
      }
    }
    __break(1u);
    goto LABEL_9;
  }
LABEL_11:
  __break(1u);
  return result;
}

uint64_t static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  return static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, a3, a4, a5, a6, a7, a8, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:));
}

{
  return static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, a3, a4, a5, a6, a7, a8, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:));
}

uint64_t static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  return static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, (uint64_t)partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:));
}

{
  return static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, (uint64_t)partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:));
}

uint64_t static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t (*a9)(void *, uint64_t *))
{
  uint64_t v9 = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v9, a9);
}

uint64_t closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t *a11, unint64_t *a12, void (*a13)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v17 = __swift_instantiateConcreteTypeFromMangledName(a11);
  uint64_t v18 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a12, a11);
  a13(a3, a4, a5, a6, a1, a7, a8, v17, a9, a10, v18);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a9 + 16))(a7, a9);
  *a2 = result;
  return result;
}

uint64_t static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  uint64_t v43 = a4;
  uint64_t v18 = *(void *)(a7 - 8);
  uint64_t v19 = MEMORY[0x1F4188790](a1);
  uint64_t v21 = (char *)&v39 - ((v20 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v23 = *(void *)(v22 - 8);
  uint64_t result = MEMORY[0x1F4188790](v19);
  uint64_t v27 = (char *)&v39 - ((v26 + 15) & 0xFFFFFFFFFFFFFFF0);
  if (v28 < 3 || a3 < 4)
  {
    __break(1u);
    goto LABEL_8;
  }
  uint64_t v29 = *(uint64_t (**)(char *, uint64_t, uint64_t))(v23 + 16);
  uint64_t v41 = v25;
  uint64_t result = v29(v27, v25, a6);
  uint64_t v30 = a3;
  uint64_t v31 = a2 * a3;
  uint64_t v42 = v30;
  if ((unsigned __int128)(a2 * (__int128)v30) >> 64 != v31 >> 63)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v39 = a5;
  uint64_t v40 = a8;
  uint64_t v32 = (*(uint64_t (**)(uint64_t, uint64_t))(a9 + 16))(a6, a9);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v23 + 8))(v27, a6);
  if (v31 != v32)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  uint64_t v33 = v43;
  (*(void (**)(char *, uint64_t, uint64_t))(v18 + 16))(v21, v43, a7);
  uint64_t v34 = (*(uint64_t (**)(uint64_t, uint64_t))(a10 + 16))(a7, a10);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v18 + 8))(v21, a7);
  if (v34 == 9)
  {
    uint64_t v35 = MEMORY[0x1F4188790](a12);
    *(&v39 - 10) = a6;
    *(&v39 - 9) = a7;
    uint64_t v36 = v41;
    *(&v39 - 8) = v40;
    *(&v39 - 7) = a9;
    *(&v39 - 6) = a10;
    *(&v39 - 5) = v37;
    *(&v39 - 4) = v36;
    *(&v39 - 3) = v33;
    uint64_t v38 = v42;
    *(&v39 - 2) = a2;
    *(&v39 - 1) = v38;
    return (*(uint64_t (**)(uint64_t))(v37 + 16))(v35);
  }
LABEL_10:
  __break(1u);
  return result;
}

uint64_t static vDSP.convolve<A, B>(_:rowCount:columnCount:with5x5Kernel:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  return static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, a3, a4, a5, a6, a7, a8, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with5x5Kernel:));
}

{
  return static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, a3, a4, a5, a6, a7, a8, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with5x5Kernel:));
}

uint64_t static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  return static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, (uint64_t)partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:));
}

{
  return static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, (uint64_t)partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:));
}

uint64_t static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  uint64_t v43 = a4;
  uint64_t v18 = *(void *)(a7 - 8);
  uint64_t v19 = MEMORY[0x1F4188790](a1);
  uint64_t v21 = (char *)&v39 - ((v20 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v23 = *(void *)(v22 - 8);
  uint64_t result = MEMORY[0x1F4188790](v19);
  uint64_t v27 = (char *)&v39 - ((v26 + 15) & 0xFFFFFFFFFFFFFFF0);
  if (v28 < 3 || a3 < 4)
  {
    __break(1u);
    goto LABEL_8;
  }
  uint64_t v29 = *(uint64_t (**)(char *, uint64_t, uint64_t))(v23 + 16);
  uint64_t v41 = v25;
  uint64_t result = v29(v27, v25, a6);
  uint64_t v30 = a3;
  uint64_t v31 = a2 * a3;
  uint64_t v42 = v30;
  if ((unsigned __int128)(a2 * (__int128)v30) >> 64 != v31 >> 63)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v39 = a5;
  uint64_t v40 = a8;
  uint64_t v32 = (*(uint64_t (**)(uint64_t, uint64_t))(a9 + 16))(a6, a9);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v23 + 8))(v27, a6);
  if (v31 != v32)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  uint64_t v33 = v43;
  (*(void (**)(char *, uint64_t, uint64_t))(v18 + 16))(v21, v43, a7);
  uint64_t v34 = (*(uint64_t (**)(uint64_t, uint64_t))(a10 + 16))(a7, a10);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v18 + 8))(v21, a7);
  if (v34 == 25)
  {
    uint64_t v35 = MEMORY[0x1F4188790](a12);
    *(&v39 - 10) = a6;
    *(&v39 - 9) = a7;
    uint64_t v36 = v41;
    *(&v39 - 8) = v40;
    *(&v39 - 7) = a9;
    *(&v39 - 6) = a10;
    *(&v39 - 5) = v37;
    *(&v39 - 4) = v36;
    *(&v39 - 3) = v33;
    uint64_t v38 = v42;
    *(&v39 - 2) = a2;
    *(&v39 - 1) = v38;
    return (*(uint64_t (**)(uint64_t))(v37 + 16))(v35);
  }
LABEL_10:
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, void *a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, void))
{
  if (!a3) {
    goto LABEL_7;
  }
  if ((a6 | a5) < 0)
  {
    __break(1u);
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (!result)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (*a7) {
    return a8(a3, a5, a6, result, *a7);
  }
LABEL_9:
  __break(1u);
  return result;
}

uint64_t static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:));
}

{
  return static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:));
}

uint64_t static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13)
{
  return static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, (uint64_t)partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:));
}

{
  return static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, (uint64_t)partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:));
}

uint64_t static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t (*a11)(void *, uint64_t *))
{
  uint64_t v11 = (*(uint64_t (**)(uint64_t, uint64_t))(a9 + 16))(a7, a9);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v11, a11);
}

uint64_t closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t *a13, unint64_t *a14, void (*a15)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v17 = __swift_instantiateConcreteTypeFromMangledName(a13);
  uint64_t v18 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a14, a13);
  a15(a3, a4, a5, a6, a7, a8, a1, a9, a10, v17, a11, a12, v18);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a11 + 16))(a9, a11);
  *a2 = result;
  return result;
}

uint64_t static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14)
{
  uint64_t v19 = *(void *)(a8 - 8);
  uint64_t result = MEMORY[0x1F4188790](a1);
  uint64_t v25 = (char *)v37 - ((v24 + 15) & 0xFFFFFFFFFFFFFFF0);
  if (v26 < 3 || a3 < 4)
  {
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v27 = v21;
  uint64_t v28 = v22;
  uint64_t v29 = *(uint64_t (**)(char *, uint64_t, uint64_t))(v19 + 16);
  uint64_t v38 = v23;
  uint64_t result = v29(v25, v23, a8);
  if ((unsigned __int128)(a2 * (__int128)a3) >> 64 != (a2 * a3) >> 63)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  v37[0] = a4;
  v37[1] = a7;
  uint64_t v30 = (*(uint64_t (**)(uint64_t, uint64_t))(a11 + 16))(a8, a11);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v19 + 8))(v25, a8);
  if ((v28 & 0x8000000000000001) == 1 && (v27 & 0x8000000000000001) == 1 && a2 * a3 == v30)
  {
    uint64_t v31 = MEMORY[0x1F4188790](a14);
    v37[-12] = a8;
    v37[-11] = v32;
    v37[-10] = v33;
    v37[-9] = a11;
    v37[-8] = v35;
    v37[-7] = v34;
    uint64_t v36 = v37[0];
    v37[-6] = v38;
    v37[-5] = v36;
    v37[-4] = a2;
    v37[-3] = a3;
    v37[-2] = v27;
    v37[-1] = v28;
    return (*(uint64_t (**)(uint64_t))(v34 + 16))(v31);
  }
LABEL_10:
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, void *a7, uint64_t a8, uint64_t a9, uint64_t (*a10)(uint64_t, uint64_t, uint64_t, uint64_t, void, uint64_t))
{
  if (!a3) {
    goto LABEL_9;
  }
  if ((a6 | a5) < 0)
  {
    __break(1u);
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (!result)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (*a7)
  {
    if (((a9 | a8) & 0x8000000000000000) == 0) {
      return a10(a3, a5, a6, result, *a7, a8);
    }
    goto LABEL_8;
  }
LABEL_11:
  __break(1u);
  return result;
}

uint64_t specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(uint64_t a1, uint64_t (*a2)(void *, uint64_t *))
{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2);
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(uint64_t a1, void *a2, uint64_t *a3, unint64_t *a4, uint64_t (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v9 = v5[2];
  uint64_t v10 = v5[3];
  uint64_t v11 = v5[5];
  uint64_t v18 = v5[4];
  uint64_t v13 = v5[6];
  uint64_t v12 = v5[7];
  uint64_t v17 = v5[8];
  uint64_t v14 = __swift_instantiateConcreteTypeFromMangledName(a3);
  uint64_t v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a4, a3);
  uint64_t result = a5(v13, v12, a1, v9, v10, v14, v18, v11, v15);
  *a2 = v17;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.correlate<A, B>(_:withKernel:)(uint64_t a1, void *a2)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vDSP.correlate<A, B, C>(_:withKernel:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:withKernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vDSP.correlate<A, B, C>(_:withKernel:result:));
}

uint64_t partial apply for closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(v2 + 32);
  uint64_t v4 = *(void *)(v2 + 56);
  uint64_t v5 = *(void *)(v2 + 72);
  uint64_t v6 = *(void *)(v2 + 80);
  long long v9 = *(_OWORD *)(v2 + 16);
  uint64_t v10 = v3;
  long long v11 = *(_OWORD *)(v2 + 40);
  uint64_t v12 = v4;
  uint64_t v13 = v5;
  uint64_t v14 = a1;
  uint64_t v15 = v6;
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t, void))(v11 + 24))(a2, &v8, MEMORY[0x1E4FBC848] + 8, v9);
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with5x5Kernel:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  return closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:with3x3Kernel:)(a1, a2, v5[6], v5[7], v5[8], v5[9], v5[2], v5[3], v5[4], v5[5], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v14 = a1;
  uint64_t v3 = *(void *)(v2 + 32);
  uint64_t v4 = *(void *)(v2 + 56);
  uint64_t v5 = *(void *)(v2 + 72);
  long long v8 = *(_OWORD *)(v2 + 16);
  uint64_t v9 = v3;
  long long v10 = *(_OWORD *)(v2 + 40);
  uint64_t v11 = v4;
  uint64_t v12 = v5;
  long long v13 = *(_OWORD *)(v2 + 80);
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t, void))(v10 + 24))(a2, &v7, MEMORY[0x1E4FBC848] + 8, v8);
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:));
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  return closure #1 in static vDSP.convolve<A, B>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:)(a1, a2, v5[6], v5[7], v5[8], v5[9], v5[10], v5[11], v5[2], v5[3], v5[4], v5[5], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(v2 + 32);
  uint64_t v4 = *(void *)(v2 + 56);
  uint64_t v5 = *(void *)(v2 + 72);
  long long v9 = *(_OWORD *)(v2 + 16);
  uint64_t v10 = v3;
  long long v6 = *(_OWORD *)(v2 + 96);
  long long v14 = *(_OWORD *)(v2 + 80);
  long long v11 = *(_OWORD *)(v2 + 40);
  uint64_t v12 = v4;
  uint64_t v13 = v5;
  uint64_t v15 = a1;
  long long v16 = v6;
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t, void))(v11 + 24))(a2, &v8, MEMORY[0x1E4FBC848] + 8, v9);
}

uint64_t specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(uint64_t result, uint64_t (*a2)(void *, uint64_t *))
{
  if (result < 0)
  {
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v4 = result;
  if (result)
  {
    uint64_t v5 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v5 + 16) = v4;
  }
  else
  {
    uint64_t v5 = MEMORY[0x1E4FBC860];
  }
  uint64_t v6 = v5 + 32;
  v7[1] = v4;
  uint64_t v8 = 0;
  v7[0] = v5 + 32;
  uint64_t result = a2(v7, &v8);
  if (v2)
  {
    if (v8 <= v4)
    {
      if (!v7[0])
      {
LABEL_20:
        __break(1u);
        goto LABEL_21;
      }
      if (v6 == v7[0])
      {
        *(void *)(v5 + 16) = v8;
        swift_bridgeObjectRelease();
        return v5;
      }
      goto LABEL_17;
    }
LABEL_16:
    __break(1u);
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (v8 > v4)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (v7[0])
  {
    if (v6 == v7[0])
    {
      *(void *)(v5 + 16) = v8;
      return v5;
    }
    goto LABEL_19;
  }
LABEL_21:
  __break(1u);
  return result;
}

uint64_t specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(uint64_t a1, uint64_t (*a2)(void *, uint64_t *), uint64_t a3)
{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2, a3, type metadata accessor for DSPComplex);
}

{
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(a1, a2, a3, type metadata accessor for DSPDoubleComplex);
}

uint64_t specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(uint64_t result, uint64_t (*a2)(void *, uint64_t *), uint64_t a3, void (*a4)(void))
{
  if (result < 0)
  {
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v6 = result;
  if (result)
  {
    a4(0);
    uint64_t v7 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v7 + 16) = v6;
  }
  else
  {
    uint64_t v7 = MEMORY[0x1E4FBC860];
  }
  uint64_t v8 = v7 + 32;
  v9[1] = v6;
  uint64_t v10 = 0;
  v9[0] = v7 + 32;
  uint64_t result = a2(v9, &v10);
  if (v4)
  {
    if (v10 <= v6)
    {
      if (!v9[0])
      {
LABEL_20:
        __break(1u);
        goto LABEL_21;
      }
      if (v8 == v9[0])
      {
        *(void *)(v7 + 16) = v10;
        swift_bridgeObjectRelease();
        return v7;
      }
      goto LABEL_17;
    }
LABEL_16:
    __break(1u);
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (v10 > v6)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (v9[0])
  {
    if (v8 == v9[0])
    {
      *(void *)(v7 + 16) = v10;
      return v7;
    }
    goto LABEL_19;
  }
LABEL_21:
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, a2, MEMORY[0x1E4F16940]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, a2, MEMORY[0x1E4F16938]);
}

uint64_t __swift_instantiateConcreteTypeFromMangledNameAbstract(uint64_t *a1)
{
  uint64_t result = *a1;
  if (result < 0)
  {
    uint64_t result = swift_getTypeByMangledNameInContextInMetadataState2();
    *a1 = result;
  }
  return result;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 24);
  uint64_t v5 = *(void *)(v3 + 48);
  uint64_t v6 = *(void *)(v3 + 88);
  v8[2] = a1;
  v8[3] = a2;
  long long v9 = *(_OWORD *)(v3 + 72);
  uint64_t v10 = v6;
  long long v11 = *(_OWORD *)(v3 + 96);
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E4FBC848] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, void, uint64_t))
{
  return closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:withKernel:kernelRowCount:kernelColumnCount:result:)(a1, a2, *(void *)(v3 + 16), *(void *)(v3 + 24), *(void *)(v3 + 32), *(void *)(v3 + 40), *(void **)(v3 + 48), *(void *)(v3 + 56), *(void *)(v3 + 64), a3);
}

uint64_t lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(unint64_t *a1, uint64_t *a2)
{
  uint64_t result = *a1;
  if (!result)
  {
    __swift_instantiateConcreteTypeFromMangledNameAbstract(a2);
    uint64_t result = swift_getWitnessTable();
    atomic_store(result, a1);
  }
  return result;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, MEMORY[0x1E4F168F0]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, MEMORY[0x1E4F168E8]);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, void))
{
  return closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(a1, a2, *(void *)(v3 + 16), *(void *)(v3 + 24), *(void *)(v3 + 32), *(void *)(v3 + 40), *(void **)(v3 + 48), a3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with3x3Kernel:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, MEMORY[0x1E4F168E0]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(a1, a2, MEMORY[0x1E4F168D8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:rowCount:columnCount:with5x5Kernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 24);
  uint64_t v5 = *(void *)(v3 + 48);
  uint64_t v6 = *(void *)(v3 + 88);
  v8[2] = a1;
  v8[3] = a2;
  long long v9 = *(_OWORD *)(v3 + 72);
  uint64_t v10 = v6;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E4FBC848] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(a1, a2, MEMORY[0x1E4F168A0]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(a1, a2, MEMORY[0x1E4F16898]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 40);
  uint64_t v5 = *(void *)(v3 + 64);
  uint64_t v6 = *(void *)(v3 + 72);
  uint64_t v7 = *(void *)(v3 + 80);
  void v9[2] = *(void *)(v3 + 16);
  long long v10 = *(_OWORD *)(v3 + 24);
  uint64_t v11 = v4;
  long long v12 = *(_OWORD *)(v3 + 48);
  uint64_t v13 = a1;
  uint64_t v14 = a2;
  uint64_t v15 = v6;
  uint64_t v16 = v7;
  uint64_t v17 = v5;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, void))(v12 + 24))(a3, v9, MEMORY[0x1E4FBC848] + 8, v10);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  return closure #1 in closure #1 in closure #1 in static vDSP.correlate<A, B, C>(_:withKernel:result:)(a1, a2, *(void *)(v3 + 64), *(void *)(v3 + 72), *(uint64_t **)(v3 + 80), *(void *)(v3 + 88), *(void *)(v3 + 96), *(void *)(v3 + 16), *(void *)(v3 + 24), *(void *)(v3 + 32), *(void *)(v3 + 40), *(void *)(v3 + 48), *(void *)(v3 + 56), a3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void))closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void))closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 40);
  uint64_t v5 = *(void *)(v3 + 80);
  v8[2] = *(void *)(v3 + 16);
  long long v9 = *(_OWORD *)(v3 + 24);
  uint64_t v10 = v4;
  long long v6 = *(_OWORD *)(v3 + 64);
  long long v11 = *(_OWORD *)(v3 + 48);
  uint64_t v12 = a1;
  uint64_t v13 = a2;
  long long v14 = v6;
  uint64_t v15 = v5;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, void))(v11 + 24))(a3, v8, MEMORY[0x1E4FBC848] + 8, v9);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void))
{
  return a3(a1, a2, v3[8], v3[9], v3[10], v3[11], v3[12], v3[2], v3[3], v3[4], v3[5], v3[6], v3[7]);
}

BOOL static BNNS.Norm.== infix(_:_:)(float a1, float a2)
{
  return a1 == a2;
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance BNNS.Norm(float *a1, float *a2)
{
  return *a1 == *a2;
}

uint64_t BNNS.EmbeddingLayer.__allocating_init(input:output:dictionary:paddingIndex:maximumNorm:normType:scalesGradientByFrequency:filterParameters:)(_OWORD *a1, long long *a2, long long *a3, uint64_t a4, char a5, int a6, uint64_t a7, uint64_t a8, float a9, float a10, uint64_t a11)
{
  uint64_t v76 = *MEMORY[0x1E4F143B8];
  long long v11 = a3[9];
  long long v69 = a3[8];
  long long v70 = v11;
  long long v71 = a3[10];
  long long v12 = a3[5];
  long long v65 = a3[4];
  long long v66 = v12;
  long long v13 = a3[7];
  long long v67 = a3[6];
  long long v68 = v13;
  long long v14 = a3[1];
  long long v61 = *a3;
  long long v62 = v14;
  long long v15 = a3[3];
  long long v63 = a3[2];
  long long v64 = v15;
  long long v16 = a2[8];
  long long v17 = a2[9];
  long long v18 = a2[6];
  long long v57 = a2[7];
  long long v58 = v16;
  long long v19 = a2[10];
  long long v59 = v17;
  long long v60 = v19;
  long long v20 = a2[4];
  long long v55 = a2[5];
  long long v56 = v18;
  long long v21 = a2[2];
  long long v53 = a2[3];
  long long v54 = v20;
  long long v22 = a2[1];
  long long v50 = *a2;
  long long v51 = v22;
  long long v52 = v21;
  long long v23 = a1[5];
  *(_OWORD *)&v75[68] = a1[4];
  long long v24 = a1[2];
  *(_OWORD *)&v75[52] = a1[3];
  long long v25 = a1[6];
  *(_OWORD *)&v75[116] = a1[7];
  long long v26 = a1[9];
  *(_OWORD *)&v75[132] = a1[8];
  *(_OWORD *)&v75[148] = v26;
  *(_OWORD *)&v75[164] = a1[10];
  *(_OWORD *)&v75[84] = v23;
  *(_OWORD *)&v75[100] = v25;
  long long v27 = a1[1];
  *(_OWORD *)&v75[4] = *a1;
  *(_OWORD *)&v75[20] = v27;
  *(_OWORD *)&v75[36] = v24;
  long long v46 = *(_OWORD *)&v75[128];
  long long v47 = *(_OWORD *)&v75[144];
  long long v48 = *(_OWORD *)&v75[160];
  long long v42 = *(_OWORD *)&v75[64];
  long long v43 = *(_OWORD *)&v75[80];
  long long v44 = *(_OWORD *)&v75[96];
  long long v45 = *(_OWORD *)&v75[112];
  long long v38 = *(_OWORD *)v75;
  long long v39 = *(_OWORD *)&v75[16];
  long long v40 = *(_OWORD *)&v75[32];
  int v37 = a5 & 1;
  int v49 = *(_DWORD *)&v75[176];
  long long v41 = *(_OWORD *)&v75[48];
  uint64_t v72 = a4;
  float v73 = a9;
  float v74 = a10;
  if (a8 == 1)
  {
    uint64_t v28 = 0;
  }
  else
  {
    int v33 = a6;
    uint64_t v34 = a7;
    uint64_t v35 = a8;
    uint64_t v36 = a11;
    uint64_t v28 = &v33;
  }
  uint64_t v29 = MEMORY[0x1D25FFFF0](&v37, v28);
  type metadata accessor for BNNS.EmbeddingLayer();
  uint64_t v30 = swift_allocObject();
  uint64_t v31 = v30;
  if (v29)
  {
    *(void *)(v30 + 16) = v29;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v31;
}

uint64_t BNNS.EmbeddingLayer.apply(batchSize:input:output:)(size_t a1, uint64_t a2, uint64_t a3)
{
  return specialized static BNNS.layerApply(_:batchSize:input:output:)(v3, a1, a2, a3);
}

uint64_t BNNS.EmbeddingLayer.applyBackward(batchSize:input:output:outputGradient:generatingWeightsGradient:)(size_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, _OWORD *a5)
{
  uint64_t v20 = *MEMORY[0x1E4F143B8];
  long long v6 = a4[9];
  *(_OWORD *)&v19.stride[7] = a4[8];
  *(_OWORD *)&v19.data_type = v6;
  *(_OWORD *)&v19.table_data_type = a4[10];
  long long v7 = a4[5];
  *(_OWORD *)&v19.size[7] = a4[4];
  *(_OWORD *)&v19.stride[1] = v7;
  long long v8 = a4[7];
  *(_OWORD *)&v19.stride[3] = a4[6];
  *(_OWORD *)&v19.stride[5] = v8;
  long long v9 = a4[1];
  *(_OWORD *)&v19.flags = *a4;
  *(_OWORD *)&v19.size[1] = v9;
  long long v10 = a4[3];
  *(_OWORD *)&v19.size[3] = a4[2];
  *(_OWORD *)&v19.size[5] = v10;
  long long v11 = a5[9];
  *(_OWORD *)&v18.stride[7] = a5[8];
  *(_OWORD *)&v18.data_type = v11;
  *(_OWORD *)&v18.table_data_type = a5[10];
  long long v12 = a5[5];
  *(_OWORD *)&v18.size[7] = a5[4];
  *(_OWORD *)&v18.stride[1] = v12;
  long long v13 = a5[7];
  *(_OWORD *)&v18.stride[3] = a5[6];
  *(_OWORD *)&v18.stride[5] = v13;
  long long v14 = a5[1];
  *(_OWORD *)&v18.flags = *a5;
  *(_OWORD *)&v18.size[1] = v14;
  long long v15 = a5[3];
  *(_OWORD *)&v18.size[3] = a5[2];
  *(_OWORD *)&v18.size[5] = v15;
  uint64_t result = closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingWeightsGradient:)(&v18, v5, a1, a2, a3, &v19);
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v17 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t BNNS.EmbeddingLayer.__allocating_init(bnnsFilter:)(uint64_t a1)
{
  uint64_t v2 = swift_allocObject();
  uint64_t v3 = v2;
  if (a1)
  {
    *(void *)(v2 + 16) = a1;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v3;
}

uint64_t type metadata accessor for BNNS.EmbeddingLayer()
{
  return self;
}

uint64_t BNNS.EmbeddingLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.EmbeddingLayer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

unint64_t lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error()
{
  unint64_t result = lazy protocol witness table cache variable for type BNNS.Error and conformance BNNS.Error;
  if (!lazy protocol witness table cache variable for type BNNS.Error and conformance BNNS.Error)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.Error and conformance BNNS.Error);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type BNNS.Error and conformance BNNS.Error;
  if (!lazy protocol witness table cache variable for type BNNS.Error and conformance BNNS.Error)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.Error and conformance BNNS.Error);
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.Norm()
{
  return &type metadata for BNNS.Norm;
}

uint64_t method lookup function for BNNS.EmbeddingLayer(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.EmbeddingLayer);
}

uint64_t dispatch thunk of BNNS.EmbeddingLayer.apply(batchSize:input:output:)(uint64_t a1, uint64_t *a2, uint64_t *a3)
{
  uint64_t v4 = a2[17];
  int v5 = *((_DWORD *)a2 + 36);
  uint64_t v6 = a2[19];
  int v7 = *((_DWORD *)a2 + 40);
  uint64_t v8 = a3[17];
  int v9 = *((_DWORD *)a3 + 36);
  uint64_t v10 = a3[19];
  int v11 = *((_DWORD *)a3 + 40);
  long long v12 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *))(*(void *)v3 + 96);
  uint64_t v28 = *a2;
  long long v29 = *(_OWORD *)(a2 + 1);
  long long v30 = *(_OWORD *)(a2 + 3);
  long long v31 = *(_OWORD *)(a2 + 5);
  long long v32 = *(_OWORD *)(a2 + 7);
  long long v33 = *(_OWORD *)(a2 + 9);
  long long v34 = *(_OWORD *)(a2 + 11);
  long long v35 = *(_OWORD *)(a2 + 13);
  long long v36 = *(_OWORD *)(a2 + 15);
  uint64_t v37 = v4;
  int v38 = v5;
  uint64_t v39 = v6;
  int v40 = v7;
  uint64_t v41 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v14 = *a3;
  long long v15 = *(_OWORD *)(a3 + 1);
  long long v16 = *(_OWORD *)(a3 + 3);
  long long v17 = *(_OWORD *)(a3 + 5);
  long long v18 = *(_OWORD *)(a3 + 7);
  long long v19 = *(_OWORD *)(a3 + 9);
  long long v20 = *(_OWORD *)(a3 + 11);
  long long v21 = *(_OWORD *)(a3 + 13);
  long long v22 = *(_OWORD *)(a3 + 15);
  uint64_t v23 = v8;
  int v24 = v9;
  uint64_t v25 = v10;
  int v26 = v11;
  uint64_t v27 = *(uint64_t *)((char *)a3 + 164);
  return v12(a1, &v28, &v14);
}

uint64_t dispatch thunk of BNNS.EmbeddingLayer.applyBackward(batchSize:input:output:outputGradient:generatingWeightsGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5)
{
  uint64_t v6 = a2[17];
  int v7 = *((_DWORD *)a2 + 36);
  uint64_t v8 = a2[19];
  int v9 = *((_DWORD *)a2 + 40);
  uint64_t v10 = a3[17];
  int v11 = *((_DWORD *)a3 + 36);
  uint64_t v12 = a3[19];
  int v13 = *((_DWORD *)a3 + 40);
  uint64_t v14 = a4[17];
  int v15 = *((_DWORD *)a4 + 36);
  uint64_t v16 = a4[19];
  int v17 = *((_DWORD *)a4 + 40);
  uint64_t v18 = a5[17];
  int v19 = *((_DWORD *)a5 + 36);
  uint64_t v20 = a5[19];
  int v21 = *((_DWORD *)a5 + 40);
  v98 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(void *)v5 + 104);
  uint64_t v84 = *a2;
  long long v85 = *(_OWORD *)(a2 + 1);
  long long v86 = *(_OWORD *)(a2 + 3);
  long long v87 = *(_OWORD *)(a2 + 5);
  long long v88 = *(_OWORD *)(a2 + 7);
  long long v89 = *(_OWORD *)(a2 + 9);
  long long v90 = *(_OWORD *)(a2 + 11);
  long long v91 = *(_OWORD *)(a2 + 13);
  long long v92 = *(_OWORD *)(a2 + 15);
  uint64_t v93 = v6;
  int v94 = v7;
  uint64_t v95 = v8;
  int v96 = v9;
  uint64_t v97 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v70 = *a3;
  long long v71 = *(_OWORD *)(a3 + 1);
  long long v72 = *(_OWORD *)(a3 + 3);
  long long v73 = *(_OWORD *)(a3 + 5);
  long long v74 = *(_OWORD *)(a3 + 7);
  long long v75 = *(_OWORD *)(a3 + 9);
  long long v76 = *(_OWORD *)(a3 + 11);
  long long v22 = *(_OWORD *)(a3 + 15);
  long long v77 = *(_OWORD *)(a3 + 13);
  long long v23 = *(_OWORD *)(a4 + 1);
  long long v24 = *(_OWORD *)(a4 + 3);
  long long v25 = *(_OWORD *)(a4 + 5);
  long long v26 = *(_OWORD *)(a4 + 7);
  long long v27 = *(_OWORD *)(a4 + 9);
  long long v28 = *(_OWORD *)(a4 + 11);
  long long v29 = *(_OWORD *)(a4 + 13);
  uint64_t v30 = *a4;
  long long v31 = *(_OWORD *)(a4 + 15);
  long long v32 = *(_OWORD *)(a5 + 1);
  long long v33 = *(_OWORD *)(a5 + 3);
  long long v34 = *(_OWORD *)(a5 + 5);
  long long v35 = *(_OWORD *)(a5 + 7);
  long long v36 = *(_OWORD *)(a5 + 9);
  long long v37 = *(_OWORD *)(a5 + 11);
  long long v38 = *(_OWORD *)(a5 + 13);
  uint64_t v39 = *a5;
  long long v40 = *(_OWORD *)(a5 + 15);
  long long v78 = v22;
  uint64_t v79 = v10;
  int v80 = v11;
  uint64_t v81 = v12;
  int v82 = v13;
  uint64_t v83 = *(uint64_t *)((char *)a3 + 164);
  *(void *)&long long v22 = *(uint64_t *)((char *)a5 + 164);
  uint64_t v56 = v30;
  long long v57 = v23;
  *(void *)&long long v23 = *(uint64_t *)((char *)a4 + 164);
  long long v58 = v24;
  long long v59 = v25;
  long long v60 = v26;
  long long v61 = v27;
  long long v62 = v28;
  long long v63 = v29;
  long long v64 = v31;
  uint64_t v65 = v14;
  int v66 = v15;
  uint64_t v67 = v16;
  int v68 = v17;
  uint64_t v69 = v23;
  uint64_t v42 = v39;
  long long v43 = v32;
  long long v44 = v33;
  long long v45 = v34;
  long long v46 = v35;
  long long v47 = v36;
  long long v48 = v37;
  long long v49 = v38;
  long long v50 = v40;
  uint64_t v51 = v18;
  int v52 = v19;
  uint64_t v53 = v20;
  int v54 = v21;
  uint64_t v55 = v22;
  return v98(a1, &v84, &v70, &v56, &v42);
}

uint64_t static BNNS.transpose(input:output:firstTransposeAxis:secondTransposeAxis:filterParameters:)(long long *a1, long long *a2, uint64_t a3, uint64_t a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v56 = *MEMORY[0x1E4F143B8];
  if (a7 != 1)
  {
    int v30 = a5;
    uint64_t v31 = a6;
    uint64_t v32 = a7;
    uint64_t v33 = a8;
    long long v19 = a1[9];
    long long v53 = a1[8];
    long long v54 = v19;
    long long v55 = a1[10];
    long long v20 = a1[5];
    long long v49 = a1[4];
    long long v50 = v20;
    long long v21 = a1[7];
    long long v51 = a1[6];
    long long v52 = v21;
    long long v22 = a1[1];
    long long v45 = *a1;
    long long v46 = v22;
    long long v23 = a1[3];
    long long v47 = a1[2];
    long long v48 = v23;
    long long v24 = a2[9];
    long long v42 = a2[8];
    long long v43 = v24;
    long long v44 = a2[10];
    long long v25 = a2[5];
    long long v38 = a2[4];
    long long v39 = v25;
    long long v26 = a2[7];
    long long v40 = a2[6];
    long long v41 = v26;
    long long v27 = a2[1];
    long long v34 = *a2;
    long long v35 = v27;
    long long v28 = a2[3];
    long long v36 = a2[2];
    long long v37 = v28;
    uint64_t result = MEMORY[0x1D2600410](&v34, &v45, a3, a4, &v30);
    if (!result) {
      return result;
    }
    goto LABEL_5;
  }
  long long v8 = a1[9];
  long long v53 = a1[8];
  long long v54 = v8;
  long long v55 = a1[10];
  long long v9 = a1[5];
  long long v49 = a1[4];
  long long v50 = v9;
  long long v10 = a1[7];
  long long v51 = a1[6];
  long long v52 = v10;
  long long v11 = a1[1];
  long long v45 = *a1;
  long long v46 = v11;
  long long v12 = a1[3];
  long long v47 = a1[2];
  long long v48 = v12;
  long long v13 = a2[9];
  long long v42 = a2[8];
  long long v43 = v13;
  long long v44 = a2[10];
  long long v14 = a2[5];
  long long v38 = a2[4];
  long long v39 = v14;
  long long v15 = a2[7];
  long long v40 = a2[6];
  long long v41 = v15;
  long long v16 = a2[1];
  long long v34 = *a2;
  long long v35 = v16;
  long long v17 = a2[3];
  long long v36 = a2[2];
  long long v37 = v17;
  uint64_t result = MEMORY[0x1D2600410](&v34, &v45, a3, a4, 0);
  if (result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v29 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static vDSP.downsample<A, B>(_:decimationFactor:filter:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  return static vDSP.downsample<A, B>(_:decimationFactor:filter:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.downsample<A, B>(_:decimationFactor:filter:));
}

{
  return static vDSP.downsample<A, B>(_:decimationFactor:filter:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.downsample<A, B>(_:decimationFactor:filter:));
}

uint64_t partial apply for closure #1 in static vDSP.downsample<A, B>(_:decimationFactor:filter:)(uint64_t a1, void *a2)
{
  return partial apply for closure #1 in static vDSP.downsample<A, B>(_:decimationFactor:filter:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:));
}

{
  return partial apply for closure #1 in static vDSP.downsample<A, B>(_:decimationFactor:filter:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:));
}

uint64_t static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:));
}

{
  return static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:));
}

uint64_t static vDSP.downsample<A, B>(_:decimationFactor:filter:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  uint64_t v18 = v16 - result;
  if (__OFSUB__(v16, result))
  {
    __break(1u);
    goto LABEL_10;
  }
  if (!a2)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (v18 == 0x8000000000000000 && a2 == -1) {
    goto LABEL_12;
  }
  uint64_t v20 = v18 / a2;
  uint64_t result = v20 + 1;
  if (!__OFADD__(v20, 1))
  {
    uint64_t v22 = a4;
    uint64_t v23 = a5;
    uint64_t v24 = a6;
    uint64_t v25 = a7;
    uint64_t v26 = a1;
    uint64_t v27 = a2;
    uint64_t v28 = a3;
    uint64_t v29 = MEMORY[0x1F4188790](result);
    return v21(v29, a8);
  }
LABEL_11:
  __break(1u);
LABEL_12:
  __break(1u);
  return result;
}

uint64_t static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  uint64_t v16 = *(void *)(a6 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v18 = (char *)&v36 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v20 = *(uint64_t (**)(uint64_t, uint64_t))(v19 + 16);
  uint64_t v38 = v19;
  uint64_t v39 = v21;
  uint64_t v37 = v22;
  uint64_t v23 = v20(v22, v19);
  uint64_t v41 = a7;
  uint64_t v42 = v23;
  uint64_t v24 = *(uint64_t (**)(uint64_t))(*(void *)(a10 + 8) + 16);
  uint64_t v40 = a4;
  uint64_t v25 = v24(a7);
  (*(void (**)(char *, uint64_t, uint64_t))(v16 + 16))(v18, a1, a6);
  uint64_t v26 = (*(uint64_t (**)(uint64_t, uint64_t))(a9 + 16))(a6, a9);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v16 + 8))(v18, a6);
  uint64_t v28 = v25 - 1;
  if (__OFSUB__(v25, 1))
  {
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v29 = a2 * v28;
  if ((unsigned __int128)(a2 * (__int128)v28) >> 64 != (a2 * v28) >> 63)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  BOOL v30 = __OFADD__(v29, v42);
  uint64_t v31 = v29 + v42;
  if (v30)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (v26 >= v31)
  {
    uint64_t v32 = MEMORY[0x1F4188790](a11);
    *(&v36 - 12) = v37;
    *(&v36 - 11) = a6;
    uint64_t v33 = v38;
    *(&v36 - 10) = v41;
    *(&v36 - 9) = v33;
    *(&v36 - 8) = a9;
    *(&v36 - 7) = a10;
    uint64_t v34 = v39;
    *(&v36 - 6) = a1;
    *(&v36 - 5) = v34;
    *(&v36 - 4) = a2;
    *(&v36 - 3) = v25;
    *(&v36 - 2) = v35;
    return (*(uint64_t (**)(uint64_t))(a10 + 16))(v32);
  }
LABEL_9:
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, void *a6, uint64_t a7, uint64_t a8, uint64_t (*a9)(uint64_t, uint64_t, uint64_t))
{
  if (!a3)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (!result)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (*a6)
  {
    if (((a8 | a7) & 0x8000000000000000) == 0) {
      return a9(a3, a5, result);
    }
    __break(1u);
    goto LABEL_7;
  }
LABEL_9:
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:));
}

{
  return partial apply for closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:));
}

uint64_t partial apply for closure #1 in static vDSP.downsample<A, B>(_:decimationFactor:filter:)(uint64_t a1, void *a2, uint64_t *a3, unint64_t *a4, uint64_t (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v8 = v5[3];
  uint64_t v17 = v5[2];
  uint64_t v9 = v5[5];
  uint64_t v19 = v5[4];
  uint64_t v10 = v5[6];
  uint64_t v11 = v5[7];
  uint64_t v12 = v5[8];
  uint64_t v13 = v5[9];
  uint64_t v14 = __swift_instantiateConcreteTypeFromMangledName(a3);
  uint64_t v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a4, a3);
  uint64_t result = a5(v10, v11, v12, a1, v17, v8, v14, v19, v9, v15);
  *a2 = v13;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(v2 + 40);
  uint64_t v4 = *(void *)(v2 + 72);
  uint64_t v5 = *(void *)(v2 + 80);
  void v7[2] = *(void *)(v2 + 16);
  long long v8 = *(_OWORD *)(v2 + 24);
  uint64_t v9 = v3;
  long long v10 = *(_OWORD *)(v2 + 48);
  uint64_t v11 = v4;
  uint64_t v12 = v5;
  uint64_t v13 = a1;
  long long v14 = *(_OWORD *)(v2 + 88);
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, void))(v10 + 24))(a2, v7, MEMORY[0x1E4FBC848] + 8, v8);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, a2, MEMORY[0x1E4F168B0]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, a2, MEMORY[0x1E4F168A8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 16);
  uint64_t v5 = *(void *)(v3 + 40);
  uint64_t v6 = *(void *)(v3 + 72);
  uint64_t v7 = *(void *)(v3 + 80);
  void v9[2] = a1;
  void v9[3] = a2;
  v9[4] = v6;
  v9[5] = v7;
  long long v10 = *(_OWORD *)(v3 + 88);
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v5 + 24))(a3, v9, MEMORY[0x1E4FBC848] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t))
{
  return closure #1 in closure #1 in closure #1 in static vDSP.downsample<A, B, C>(_:decimationFactor:filter:result:)(a1, a2, *(void *)(v3 + 16), *(void *)(v3 + 24), *(void *)(v3 + 32), *(void **)(v3 + 40), *(void *)(v3 + 48), *(void *)(v3 + 56), a3);
}

uint64_t vImage.Options.init(rawValue:)@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = result;
  return result;
}

uint64_t vImage.Options.rawValue.getter()
{
  return *v0;
}

void static vImage.Options.noFlags.getter(_DWORD *a1@<X8>)
{
  *a1 = 0;
}

void static vImage.Options.leaveAlphaUnchanged.getter(_DWORD *a1@<X8>)
{
  *a1 = 1;
}

void static vImage.Options.copyInPlace.getter(_DWORD *a1@<X8>)
{
  *a1 = 2;
}

void static vImage.Options.backgroundColorFill.getter(_DWORD *a1@<X8>)
{
  *a1 = 4;
}

void static vImage.Options.imageExtend.getter(_DWORD *a1@<X8>)
{
  *a1 = 8;
}

void static vImage.Options.doNotTile.getter(_DWORD *a1@<X8>)
{
  *a1 = 16;
}

void static vImage.Options.highQualityResampling.getter(_DWORD *a1@<X8>)
{
  *a1 = 32;
}

void static vImage.Options.truncateKernel.getter(_DWORD *a1@<X8>)
{
  *a1 = 64;
}

void static vImage.Options.getTempBufferSize.getter(_DWORD *a1@<X8>)
{
  *a1 = 128;
}

void static vImage.Options.printDiagnosticsToConsole.getter(_DWORD *a1@<X8>)
{
  *a1 = 256;
}

void static vImage.Options.noAllocate.getter(_DWORD *a1@<X8>)
{
  *a1 = 512;
}

void static vImage.Options.hdrContent.getter(_DWORD *a1@<X8>)
{
  *a1 = 1024;
}

void static vImage.Options.doNotClamp.getter(_DWORD *a1@<X8>)
{
  *a1 = 2048;
}

uint64_t vImage.Options.flags.getter()
{
  return *v0;
}

unint64_t lazy protocol witness table accessor for type vImage.Options and conformance vImage.Options()
{
  unint64_t result = lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options;
  if (!lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options;
  if (!lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options;
  if (!lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options;
  if (!lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.Options and conformance vImage.Options);
  }
  return result;
}

_DWORD *protocol witness for OptionSet.init(rawValue:) in conformance vImage.Options@<X0>(_DWORD *result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *result;
  return result;
}

void protocol witness for SetAlgebra.init() in conformance vImage.Options(_DWORD *a1@<X8>)
{
  *a1 = 0;
}

_DWORD *protocol witness for SetAlgebra.union(_:) in conformance vImage.Options@<X0>(_DWORD *result@<X0>, int *a2@<X8>)
{
  *a2 = *v2 | *result;
  return result;
}

_DWORD *protocol witness for SetAlgebra.intersection(_:) in conformance vImage.Options@<X0>(_DWORD *result@<X0>, int *a2@<X8>)
{
  *a2 = *v2 & *result;
  return result;
}

_DWORD *protocol witness for SetAlgebra.symmetricDifference(_:) in conformance vImage.Options@<X0>(_DWORD *result@<X0>, int *a2@<X8>)
{
  *a2 = *v2 ^ *result;
  return result;
}

BOOL protocol witness for SetAlgebra.insert(_:) in conformance vImage.Options(_DWORD *a1, int *a2)
{
  int v3 = *a2;
  int v4 = *v2 & *a2;
  if (v4 != *a2) {
    *v2 |= v3;
  }
  *a1 = v3;
  return v4 != v3;
}

_DWORD *protocol witness for SetAlgebra.remove(_:) in conformance vImage.Options@<X0>(_DWORD *result@<X0>, uint64_t a2@<X8>)
{
  int v3 = *v2 & *result;
  if (v3) {
    *v2 &= ~*result;
  }
  *(_DWORD *)a2 = v3;
  *(unsigned char *)(a2 + 4) = v3 == 0;
  return result;
}

int *protocol witness for SetAlgebra.update(with:) in conformance vImage.Options@<X0>(int *result@<X0>, uint64_t a2@<X8>)
{
  int v3 = *result;
  int v4 = *v2;
  *v2 |= *result;
  int v5 = v4 & v3;
  *(_DWORD *)a2 = v5;
  *(unsigned char *)(a2 + 4) = v5 == 0;
  return result;
}

_DWORD *protocol witness for SetAlgebra.formUnion(_:) in conformance vImage.Options(_DWORD *result)
{
  *v1 |= *result;
  return result;
}

_DWORD *protocol witness for SetAlgebra.formIntersection(_:) in conformance vImage.Options(_DWORD *result)
{
  *v1 &= *result;
  return result;
}

_DWORD *protocol witness for SetAlgebra.formSymmetricDifference(_:) in conformance vImage.Options(_DWORD *result)
{
  *v1 ^= *result;
  return result;
}

_DWORD *protocol witness for SetAlgebra.subtracting(_:) in conformance vImage.Options@<X0>(_DWORD *result@<X0>, int *a2@<X8>)
{
  *a2 = *v2 & ~*result;
  return result;
}

BOOL protocol witness for SetAlgebra.isSubset(of:) in conformance vImage.Options(_DWORD *a1)
{
  return (*v1 & ~*a1) == 0;
}

BOOL protocol witness for SetAlgebra.isDisjoint(with:) in conformance vImage.Options(_DWORD *a1)
{
  return (*v1 & *a1) == 0;
}

BOOL protocol witness for SetAlgebra.isSuperset(of:) in conformance vImage.Options(_DWORD *a1)
{
  return (*a1 & ~*v1) == 0;
}

BOOL protocol witness for SetAlgebra.isEmpty.getter in conformance vImage.Options()
{
  return *v0 == 0;
}

uint64_t protocol witness for SetAlgebra.init<A>(_:) in conformance vImage.Options(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return MEMORY[0x1F4184B08](a1, a4, a2, a5, a3);
}

_DWORD *protocol witness for SetAlgebra.subtract(_:) in conformance vImage.Options(_DWORD *result)
{
  *v1 &= ~*result;
  return result;
}

ValueMetadata *type metadata accessor for vImage.Options()
{
  return &type metadata for vImage.Options;
}

uint64_t vImage.PixelBuffer<>.multiply<A, B>(by:divisor:preBias:postBias:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, void *a5)
{
  uint64_t v6 = *v5;
  v10[2] = *a5;
  v10[3] = v6;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(v10);
  type metadata accessor for vImage.PixelBuffer();
  vImage.PixelBuffer.size.getter(v9);
  swift_bridgeObjectRelease();
  uint64_t result = swift_bridgeObjectRelease();
  if (v10[0] == v9[0] && v10[1] == v9[1])
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(unint64_t (*)(uint64_t, uint64_t)))(v8 + 24))(partial apply for closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:divisor:preBias:postBias:destination:));
  }
  else
  {
    __break(1u);
  }
  return result;
}

unint64_t closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:divisor:preBias:postBias:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, unint64_t a12, uint64_t a13, uint64_t a14)
{
  uint64_t v123 = a8;
  uint64_t v134 = a6;
  *(void *)&long long v135 = a7;
  uint64_t v133 = a5;
  uint64_t v124 = a4;
  uint64_t v15 = *(void *)(a10 - 8);
  uint64_t v16 = (const void *)MEMORY[0x1F4188790](a1);
  uint64_t v18 = (char *)&v120 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  v127 = specialized _copyCollectionToContiguousArray<A>(_:)(v16, v19);
  (*(void (**)(char *, uint64_t, uint64_t))(v15 + 16))(v18, a3, a10);
  uint64_t v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a13 + 16))(a10, a13);
  uint64_t v21 = *(uint64_t (**)(void))(a12 + 32);
  uint64_t v131 = a9;
  unint64_t v132 = a12;
  unint64_t v126 = a12 + 32;
  v125 = v21;
  uint64_t v22 = ((uint64_t (*)(uint64_t, unint64_t))v21)(a9, a12);
  uint64_t v23 = *(uint64_t (**)(void))(a14 + 32);
  uint64_t v130 = a11;
  uint64_t v24 = v23;
  uint64_t v129 = a14;
  uint64_t v25 = ((uint64_t (*)(uint64_t, uint64_t))v23)(a11, a14);
  unint64_t result = (*(uint64_t (**)(char *, uint64_t))(v15 + 8))(v18, a10);
  if ((unsigned __int128)(v22 * (__int128)v25) >> 64 != (v22 * v25) >> 63) {
    goto LABEL_111;
  }
  if (v20 != v22 * v25)
  {
LABEL_112:
    __break(1u);
    goto LABEL_113;
  }
  uint64_t v27 = v124;
  int64_t v28 = *(void *)(v124 + 16);
  unint64_t result = v125();
  if (v28 != result)
  {
LABEL_113:
    __break(1u);
    goto LABEL_114;
  }
  uint64_t v29 = *(void *)(v133 + 16);
  unint64_t result = v24();
  if (v29 != result)
  {
LABEL_114:
    __break(1u);
LABEL_115:
    __break(1u);
LABEL_116:
    __break(1u);
    goto LABEL_117;
  }
  unint64_t v30 = v127[2];
  uint64_t v31 = v134;
  if (!v30) {
    goto LABEL_19;
  }
  int64_t v32 = v127[4];
  unint64_t v33 = v30 - 1;
  if (v30 != 1)
  {
    if (v30 >= 5)
    {
      unint64_t v34 = v33 & 0xFFFFFFFFFFFFFFFCLL | 1;
      int64x2_t v35 = vdupq_n_s64(v32);
      uint64_t v36 = (int64x2_t *)(v127 + 7);
      unint64_t v37 = v33 & 0xFFFFFFFFFFFFFFFCLL;
      int64x2_t v38 = v35;
      do
      {
        int64x2_t v35 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v35, v36[-1]), (int8x16_t)v35, (int8x16_t)v36[-1]);
        int64x2_t v38 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v38, *v36), (int8x16_t)v38, *(int8x16_t *)v36);
        v36 += 2;
        v37 -= 4;
      }
      while (v37);
      int8x16_t v39 = vbslq_s8((int8x16_t)vcgtq_s64(v35, v38), (int8x16_t)v35, (int8x16_t)v38);
      uint64_t v40 = vextq_s8(v39, v39, 8uLL).u64[0];
      int64_t v32 = (int64_t)vbsl_s8((int8x8_t)vcgtd_s64(v39.i64[0], v40), *(int8x8_t *)v39.i8, (int8x8_t)v40);
      if (v33 == (v33 & 0xFFFFFFFFFFFFFFFCLL)) {
        goto LABEL_16;
      }
    }
    else
    {
      unint64_t v34 = 1;
    }
    unint64_t v41 = v30 - v34;
    uint64_t v42 = &v127[v34 + 4];
    do
    {
      int64_t v44 = *v42++;
      int64_t v43 = v44;
      if (v32 <= v44) {
        int64_t v32 = v43;
      }
      --v41;
    }
    while (v41);
  }
LABEL_16:
  if (v32 < 0)
  {
    BOOL v45 = __OFSUB__(0, v32);
    int64_t v32 = -v32;
    if (v45) {
      goto LABEL_123;
    }
  }
  if (v32 >= 0x8000) {
    goto LABEL_108;
  }
LABEL_19:
  uint64_t v46 = v134;
  if (v134 < 0)
  {
    uint64_t v46 = -v134;
    if (__OFSUB__(0, v134)) {
      goto LABEL_122;
    }
  }
  if (v46 > 0x7FFFFFFF) {
    goto LABEL_115;
  }
  unint64_t v47 = *(void *)(v27 + 16);
  if (!v47) {
    goto LABEL_36;
  }
  int64_t v48 = *(void *)(v27 + 32);
  unint64_t v49 = v47 - 1;
  if (v47 != 1)
  {
    if (v47 >= 5)
    {
      unint64_t v50 = v49 & 0xFFFFFFFFFFFFFFFCLL | 1;
      int64x2_t v51 = vdupq_n_s64(v48);
      long long v52 = (int64x2_t *)(v27 + 56);
      unint64_t v53 = v49 & 0xFFFFFFFFFFFFFFFCLL;
      int64x2_t v54 = v51;
      do
      {
        int64x2_t v51 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v51, v52[-1]), (int8x16_t)v51, (int8x16_t)v52[-1]);
        int64x2_t v54 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v54, *v52), (int8x16_t)v54, *(int8x16_t *)v52);
        v52 += 2;
        v53 -= 4;
      }
      while (v53);
      int8x16_t v55 = vbslq_s8((int8x16_t)vcgtq_s64(v51, v54), (int8x16_t)v51, (int8x16_t)v54);
      uint64_t v56 = vextq_s8(v55, v55, 8uLL).u64[0];
      int64_t v48 = (int64_t)vbsl_s8((int8x8_t)vcgtd_s64(v55.i64[0], v56), *(int8x8_t *)v55.i8, (int8x8_t)v56);
      if (v49 == (v49 & 0xFFFFFFFFFFFFFFFCLL)) {
        goto LABEL_33;
      }
    }
    else
    {
      unint64_t v50 = 1;
    }
    unint64_t v57 = v47 - v50;
    long long v58 = (int64_t *)(v27 + 8 * v50 + 32);
    do
    {
      int64_t v60 = *v58++;
      int64_t v59 = v60;
      if (v48 <= v60) {
        int64_t v48 = v59;
      }
      --v57;
    }
    while (v57);
  }
LABEL_33:
  if (v48 < 0)
  {
    BOOL v45 = __OFSUB__(0, v48);
    int64_t v48 = -v48;
    if (v45) {
      goto LABEL_124;
    }
  }
  if (v48 >= 0x8000) {
    goto LABEL_109;
  }
LABEL_36:
  unint64_t v61 = *(void *)(v133 + 16);
  if (!v61) {
    goto LABEL_50;
  }
  int64_t v62 = *(void *)(v133 + 32);
  unint64_t v63 = v61 - 1;
  if (v61 != 1)
  {
    if (v61 >= 5)
    {
      unint64_t v64 = v63 & 0xFFFFFFFFFFFFFFFCLL | 1;
      int64x2_t v65 = vdupq_n_s64(v62);
      int v66 = (int64x2_t *)(v133 + 56);
      unint64_t v67 = v63 & 0xFFFFFFFFFFFFFFFCLL;
      int64x2_t v68 = v65;
      do
      {
        int64x2_t v65 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v65, v66[-1]), (int8x16_t)v65, (int8x16_t)v66[-1]);
        int64x2_t v68 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v68, *v66), (int8x16_t)v68, *(int8x16_t *)v66);
        v66 += 2;
        v67 -= 4;
      }
      while (v67);
      int8x16_t v69 = vbslq_s8((int8x16_t)vcgtq_s64(v65, v68), (int8x16_t)v65, (int8x16_t)v68);
      uint64_t v70 = vextq_s8(v69, v69, 8uLL).u64[0];
      int64_t v62 = (int64_t)vbsl_s8((int8x8_t)vcgtd_s64(v69.i64[0], v70), *(int8x8_t *)v69.i8, (int8x8_t)v70);
      if (v63 == (v63 & 0xFFFFFFFFFFFFFFFCLL)) {
        goto LABEL_47;
      }
    }
    else
    {
      unint64_t v64 = 1;
    }
    unint64_t v71 = v61 - v64;
    long long v72 = (int64_t *)(v133 + 8 * v64 + 32);
    do
    {
      int64_t v74 = *v72++;
      int64_t v73 = v74;
      if (v62 <= v74) {
        int64_t v62 = v73;
      }
      --v71;
    }
    while (v71);
  }
LABEL_47:
  if (v62 < 0)
  {
    BOOL v45 = __OFSUB__(0, v62);
    int64_t v62 = -v62;
    if (v45) {
      goto LABEL_125;
    }
  }
  if (v62 > 0x7FFFFFFF) {
    goto LABEL_110;
  }
LABEL_50:
  v121 = v24;
  uint64_t v128 = v29;
  type metadata accessor for vImage.PixelBuffer();
  uint64_t v75 = vImage.PixelBuffer<>.vImageBuffers.getter();
  int64_t v76 = *(void *)(v75 + 16);
  uint64_t v122 = a14 + 32;
  if (v76)
  {
    v137 = (char *)MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v76, 0);
    long long v77 = v137;
    uint64_t v120 = v75;
    long long v78 = (uint64_t *)(v75 + 56);
    do
    {
      long long v135 = *(_OWORD *)(v78 - 3);
      uint64_t v79 = *(v78 - 1);
      uint64_t v80 = *v78;
      uint64_t v81 = (_OWORD *)swift_slowAlloc();
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage_Buffer>);
      uint64_t inited = swift_initStackObject();
      *(_OWORD *)(inited + 32) = v135;
      *(void *)(inited + 48) = v79;
      *(void *)(inited + 56) = v80;
      long long v83 = *(_OWORD *)(inited + 48);
      *uint64_t v81 = *(_OWORD *)(inited + 32);
      v81[1] = v83;
      swift_setDeallocating();
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v77 + 2) + 1, 1);
        long long v77 = v137;
      }
      unint64_t v85 = *((void *)v77 + 2);
      unint64_t v84 = *((void *)v77 + 3);
      if (v85 >= v84 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v84 > 1), v85 + 1, 1);
        long long v77 = v137;
      }
      v78 += 4;
      *((void *)v77 + 2) = v85 + 1;
      *(void *)&v77[8 * v85 + 32] = v81;
      --v76;
    }
    while (v76);
    swift_bridgeObjectRelease();
    uint64_t v31 = v134;
  }
  else
  {
    swift_bridgeObjectRelease();
    long long v77 = (char *)MEMORY[0x1E4FBC860];
  }
  v137 = v77;
  type metadata accessor for vImage.PixelBuffer();
  uint64_t v86 = vImage.PixelBuffer<>.vImageBuffers.getter();
  int64_t v87 = *(void *)(v86 + 16);
  if (v87)
  {
    v136 = (char *)MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v87, 0);
    long long v88 = v136;
    uint64_t v123 = v86;
    long long v89 = (uint64_t *)(v86 + 56);
    do
    {
      long long v135 = *(_OWORD *)(v89 - 3);
      uint64_t v90 = *(v89 - 1);
      uint64_t v91 = *v89;
      long long v92 = (_OWORD *)swift_slowAlloc();
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage_Buffer>);
      uint64_t v93 = swift_initStackObject();
      *(_OWORD *)(v93 + 32) = v135;
      *(void *)(v93 + 48) = v90;
      *(void *)(v93 + 56) = v91;
      long long v94 = *(_OWORD *)(v93 + 48);
      *long long v92 = *(_OWORD *)(v93 + 32);
      v92[1] = v94;
      swift_setDeallocating();
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v88 + 2) + 1, 1);
        long long v88 = v136;
      }
      unint64_t v96 = *((void *)v88 + 2);
      unint64_t v95 = *((void *)v88 + 3);
      if (v96 >= v95 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v95 > 1), v96 + 1, 1);
        long long v88 = v136;
      }
      v89 += 4;
      *((void *)v88 + 2) = v96 + 1;
      *(void *)&v88[8 * v96 + 32] = v92;
      --v87;
    }
    while (v87);
    swift_bridgeObjectRelease();
    uint64_t v31 = v134;
  }
  else
  {
    swift_bridgeObjectRelease();
    long long v88 = (char *)MEMORY[0x1E4FBC860];
  }
  unint64_t result = ((uint64_t (*)(uint64_t))v125)(v131);
  if ((result & 0x8000000000000000) != 0) {
    goto LABEL_116;
  }
  unint64_t v97 = result;
  if (HIDWORD(result))
  {
LABEL_117:
    __break(1u);
    goto LABEL_118;
  }
  unint64_t result = ((uint64_t (*)(uint64_t))v121)(v130);
  if ((result & 0x8000000000000000) != 0)
  {
LABEL_118:
    __break(1u);
    goto LABEL_119;
  }
  if (HIDWORD(result))
  {
LABEL_119:
    __break(1u);
LABEL_120:
    __break(1u);
    goto LABEL_121;
  }
  unint64_t v132 = result;
  *(void *)&long long v135 = v97;
  int64_t v98 = v127[2];
  if (v98)
  {
    v136 = (char *)MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v98, 0);
    unint64_t result = (unint64_t)v127;
    uint64_t v99 = 0;
    uint64_t v100 = (uint64_t)v136;
    while (1)
    {
      uint64_t v101 = *(void *)(result + 8 * v99 + 32);
      if (v101 < -32768) {
        break;
      }
      if (v101 >= 0x8000) {
        goto LABEL_103;
      }
      v136 = (char *)v100;
      unint64_t v103 = *(void *)(v100 + 16);
      unint64_t v102 = *(void *)(v100 + 24);
      if (v103 >= v102 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v102 > 1), v103 + 1, 1);
        unint64_t result = (unint64_t)v127;
        uint64_t v100 = (uint64_t)v136;
      }
      ++v99;
      *(void *)(v100 + 16) = v103 + 1;
      *(_WORD *)(v100 + 2 * v103 + 32) = v101;
      if (v98 == v99)
      {
        unint64_t result = swift_release();
        uint64_t v31 = v134;
        goto LABEL_81;
      }
    }
    __break(1u);
LABEL_103:
    __break(1u);
    goto LABEL_104;
  }
  unint64_t result = swift_release();
  uint64_t v100 = MEMORY[0x1E4FBC860];
LABEL_81:
  uint64_t v104 = v128;
  uint64_t v105 = v124;
  if (v31 < (uint64_t)0xFFFFFFFF80000000) {
    goto LABEL_120;
  }
  if (v31 > 0x7FFFFFFF)
  {
LABEL_121:
    __break(1u);
LABEL_122:
    __break(1u);
LABEL_123:
    __break(1u);
LABEL_124:
    __break(1u);
LABEL_125:
    __break(1u);
    return result;
  }
  uint64_t v106 = MEMORY[0x1E4FBC860];
  if (v28)
  {
    v136 = (char *)MEMORY[0x1E4FBC860];
    unint64_t result = (unint64_t)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v28, 0);
    uint64_t v104 = v128;
    uint64_t v106 = (uint64_t)v136;
    v107 = (uint64_t *)(v105 + 32);
    while (1)
    {
      uint64_t v109 = *v107++;
      uint64_t v108 = v109;
      if (v109 < -32768) {
        break;
      }
      if (v108 >= 0x8000) {
        goto LABEL_105;
      }
      v136 = (char *)v106;
      unint64_t v111 = *(void *)(v106 + 16);
      unint64_t v110 = *(void *)(v106 + 24);
      if (v111 >= v110 >> 1)
      {
        unint64_t result = (unint64_t)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v110 > 1), v111 + 1, 1);
        uint64_t v104 = v128;
        uint64_t v106 = (uint64_t)v136;
      }
      *(void *)(v106 + 16) = v111 + 1;
      *(_WORD *)(v106 + 2 * v111 + 32) = v108;
      if (!--v28) {
        goto LABEL_90;
      }
    }
LABEL_104:
    __break(1u);
LABEL_105:
    __break(1u);
    goto LABEL_106;
  }
LABEL_90:
  uint64_t v112 = MEMORY[0x1E4FBC860];
  if (v104)
  {
    v136 = (char *)MEMORY[0x1E4FBC860];
    unint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v128, 0);
    uint64_t v113 = v128;
    uint64_t v112 = (uint64_t)v136;
    v114 = (int64_t *)(v133 + 32);
    while (1)
    {
      int64_t v116 = *v114++;
      uint64_t v115 = v116;
      if (v116 < (uint64_t)0xFFFFFFFF80000000) {
        break;
      }
      if (v115 > 0x7FFFFFFF) {
        goto LABEL_107;
      }
      v136 = (char *)v112;
      unint64_t v118 = *(void *)(v112 + 16);
      unint64_t v117 = *(void *)(v112 + 24);
      if (v118 >= v117 >> 1)
      {
        uint64_t v128 = v113;
        unint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v117 > 1, v118 + 1, 1);
        uint64_t v113 = v128;
        uint64_t v112 = (uint64_t)v136;
      }
      *(void *)(v112 + 16) = v118 + 1;
      *(_DWORD *)(v112 + 4 * v118 + 32) = v115;
      if (!--v113) {
        goto LABEL_97;
      }
    }
LABEL_106:
    __break(1u);
LABEL_107:
    __break(1u);
LABEL_108:
    __break(1u);
LABEL_109:
    __break(1u);
LABEL_110:
    __break(1u);
LABEL_111:
    __break(1u);
    goto LABEL_112;
  }
LABEL_97:
  v119 = v137;
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
    v119 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v119 + 2), 0, v119);
  }
  v137 = v119;
  swift_bridgeObjectRetain();
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
    long long v88 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v88 + 2), 0, v88);
  }
  v136 = v88;
  swift_bridgeObjectRetain();
  vImageMatrixMultiply_Planar8((const vImage_Buffer **)v119 + 4, (const vImage_Buffer **)v88 + 4, v135, v132, (const int16_t *)(v100 + 32), v134, (const int16_t *)(v106 + 32), (const int32_t *)(v112 + 32), 0);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  $defer #1 <A><A1, B1>() in closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:divisor:preBias:postBias:destination:)((uint64_t)&v137, (uint64_t *)&v136);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

unint64_t partial apply for closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:divisor:preBias:postBias:destination:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:divisor:preBias:postBias:destination:)(a1, a2, *(void *)(v2 + 64), *(void *)(v2 + 72), *(void *)(v2 + 80), *(void *)(v2 + 88), *(void *)(v2 + 96), *(void *)(v2 + 104), *(void *)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), *(void *)(v2 + 48), *(void *)(v2 + 56));
}

uint64_t specialized Sequence<>.max()(uint64_t a1)
{
  unint64_t v1 = *(void *)(a1 + 16);
  if (!v1)
  {
    LOBYTE(v2) = 0;
    return v2 | ((v1 == 0) << 8);
  }
  unsigned int v2 = *(unsigned __int8 *)(a1 + 32);
  unint64_t v3 = v1 - 1;
  if (v1 != 1)
  {
    if (v1 < 9)
    {
      unint64_t v4 = 1;
      goto LABEL_17;
    }
    if (v1 >= 0x21)
    {
      unint64_t v5 = v3 & 0xFFFFFFFFFFFFFFE0;
      uint8x16_t v6 = (uint8x16_t)vdupq_n_s8(v2);
      uint64_t v7 = (uint8x16_t *)(a1 + 49);
      unint64_t v8 = v3 & 0xFFFFFFFFFFFFFFE0;
      uint8x16_t v9 = v6;
      do
      {
        uint8x16_t v6 = vmaxq_u8(v6, v7[-1]);
        uint8x16_t v9 = vmaxq_u8(v9, *v7);
        v7 += 2;
        v8 -= 32;
      }
      while (v8);
      uint8x16_t v10 = vmaxq_u8(v6, v9);
      v10.i8[0] = vmaxvq_u8(v10);
      unsigned int v2 = v10.i32[0];
      if (v3 == v5) {
        return v2 | ((v1 == 0) << 8);
      }
      if ((v3 & 0x18) == 0)
      {
        unint64_t v4 = v5 | 1;
LABEL_17:
        unint64_t v15 = v1 - v4;
        uint64_t v16 = (unsigned __int8 *)(v4 + a1 + 32);
        do
        {
          unsigned int v18 = *v16++;
          char v17 = v18;
          if (v2 <= v18) {
            LOBYTE(v2) = v17;
          }
          --v15;
        }
        while (v15);
        return v2 | ((v1 == 0) << 8);
      }
    }
    else
    {
      unint64_t v5 = 0;
    }
    unint64_t v4 = v3 & 0xFFFFFFFFFFFFFFF8 | 1;
    uint8x8_t v11 = (uint8x8_t)vdup_n_s8(v2);
    uint64_t v12 = (uint8x8_t *)(v5 + a1 + 33);
    unint64_t v13 = v5 - (v3 & 0xFFFFFFFFFFFFFFF8);
    do
    {
      uint8x8_t v14 = *v12++;
      uint8x8_t v11 = vmax_u8(v11, v14);
      v13 += 8;
    }
    while (v13);
    LOBYTE(v2) = vmaxv_u8(v11);
    if (v3 == (v3 & 0xFFFFFFFFFFFFFFF8)) {
      return v2 | ((v1 == 0) << 8);
    }
    goto LABEL_17;
  }
  return v2 | ((v1 == 0) << 8);
}

uint64_t vImage.PixelBuffer<>.multiply<A, B>(by:preBias:postBias:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, void *a4)
{
  uint64_t v5 = *v4;
  void v9[2] = *a4;
  void v9[3] = v5;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(v9);
  type metadata accessor for vImage.PixelBuffer();
  vImage.PixelBuffer.size.getter(v8);
  swift_bridgeObjectRelease();
  uint64_t result = swift_bridgeObjectRelease();
  if (v9[0] == v8[0] && v9[1] == v8[1])
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(unint64_t (*)(uint64_t, uint64_t)))(v7 + 24))(partial apply for closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:preBias:postBias:destination:));
  }
  else
  {
    __break(1u);
  }
  return result;
}

unint64_t closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:preBias:postBias:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13)
{
  uint64_t v60 = a7;
  *(void *)&long long v70 = a6;
  uint64_t v62 = a5;
  uint64_t v63 = a4;
  uint64_t v15 = *(void *)(a9 - 8);
  uint64_t v16 = (const void *)MEMORY[0x1F4188790](a1);
  unsigned int v18 = (char *)v58 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  int64_t v59 = (const float *)specialized _copyCollectionToContiguousArray<A>(_:)(v16, v19);
  (*(void (**)(char *, uint64_t, uint64_t))(v15 + 16))(v18, a3, a9);
  uint64_t v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a12 + 16))(a9, a12);
  uint64_t v21 = *(uint64_t (**)(uint64_t, uint64_t))(a11 + 32);
  uint64_t v61 = a8;
  uint64_t v69 = a11;
  uint64_t v66 = a11 + 32;
  int64x2_t v65 = v21;
  uint64_t v22 = v21(a8, a11);
  uint64_t v23 = *(uint64_t (**)(void))(a13 + 32);
  uint64_t v67 = a10;
  uint64_t v68 = a13;
  unint64_t v64 = v23;
  uint64_t v24 = ((uint64_t (*)(uint64_t, uint64_t))v23)(a10, a13);
  unint64_t result = (*(uint64_t (**)(char *, uint64_t))(v15 + 8))(v18, a9);
  if ((unsigned __int128)(v22 * (__int128)v24) >> 64 != (v22 * v24) >> 63)
  {
    __break(1u);
    goto LABEL_33;
  }
  if (v20 != v22 * v24)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  uint64_t v26 = *(void *)(v63 + 16);
  uint64_t v27 = v61;
  unint64_t result = v65(v61, v69);
  if (v26 != result)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  uint64_t v28 = *(void *)(v62 + 16);
  unint64_t result = v64();
  if (v28 != result)
  {
LABEL_35:
    __break(1u);
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  v58[1] = a13 + 32;
  type metadata accessor for vImage.PixelBuffer();
  uint64_t v29 = vImage.PixelBuffer<>.vImageBuffers.getter();
  int64_t v30 = *(void *)(v29 + 16);
  uint64_t v31 = MEMORY[0x1E4FBC860];
  if (v30)
  {
    long long v72 = (char *)MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v30, 0);
    int64_t v32 = v72;
    unint64_t v33 = (uint64_t *)(v29 + 56);
    do
    {
      long long v70 = *(_OWORD *)(v33 - 3);
      uint64_t v34 = *(v33 - 1);
      uint64_t v35 = *v33;
      uint64_t v36 = (_OWORD *)swift_slowAlloc();
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage_Buffer>);
      uint64_t inited = swift_initStackObject();
      *(_OWORD *)(inited + 32) = v70;
      *(void *)(inited + 48) = v34;
      *(void *)(inited + 56) = v35;
      long long v38 = *(_OWORD *)(inited + 48);
      *uint64_t v36 = *(_OWORD *)(inited + 32);
      v36[1] = v38;
      swift_setDeallocating();
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v32 + 2) + 1, 1);
        int64_t v32 = v72;
      }
      unint64_t v40 = *((void *)v32 + 2);
      unint64_t v39 = *((void *)v32 + 3);
      if (v40 >= v39 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v39 > 1), v40 + 1, 1);
        int64_t v32 = v72;
      }
      v33 += 4;
      *((void *)v32 + 2) = v40 + 1;
      *(void *)&v32[8 * v40 + 32] = v36;
      --v30;
    }
    while (v30);
    swift_bridgeObjectRelease();
    uint64_t v31 = MEMORY[0x1E4FBC860];
  }
  else
  {
    swift_bridgeObjectRelease();
    int64_t v32 = (char *)MEMORY[0x1E4FBC860];
  }
  long long v72 = v32;
  type metadata accessor for vImage.PixelBuffer();
  uint64_t v41 = vImage.PixelBuffer<>.vImageBuffers.getter();
  int64_t v42 = *(void *)(v41 + 16);
  if (v42)
  {
    unint64_t v71 = (char *)v31;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v42, 0);
    int64_t v43 = v71;
    int64_t v44 = (uint64_t *)(v41 + 56);
    do
    {
      long long v70 = *(_OWORD *)(v44 - 3);
      uint64_t v45 = *(v44 - 1);
      uint64_t v46 = *v44;
      unint64_t v47 = (_OWORD *)swift_slowAlloc();
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage_Buffer>);
      uint64_t v48 = swift_initStackObject();
      *(_OWORD *)(v48 + 32) = v70;
      *(void *)(v48 + 48) = v45;
      *(void *)(v48 + 56) = v46;
      long long v49 = *(_OWORD *)(v48 + 48);
      *unint64_t v47 = *(_OWORD *)(v48 + 32);
      v47[1] = v49;
      swift_setDeallocating();
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v43 + 2) + 1, 1);
        int64_t v43 = v71;
      }
      unint64_t v51 = *((void *)v43 + 2);
      unint64_t v50 = *((void *)v43 + 3);
      if (v51 >= v50 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v50 > 1), v51 + 1, 1);
        int64_t v43 = v71;
      }
      v44 += 4;
      *((void *)v43 + 2) = v51 + 1;
      *(void *)&v43[8 * v51 + 32] = v47;
      --v42;
    }
    while (v42);
    swift_bridgeObjectRelease();
    uint64_t v27 = v61;
  }
  else
  {
    swift_bridgeObjectRelease();
    int64_t v43 = (char *)MEMORY[0x1E4FBC860];
  }
  unint64_t result = v65(v27, v69);
  if ((result & 0x8000000000000000) != 0) {
    goto LABEL_36;
  }
  uint32_t v52 = result;
  if (HIDWORD(result))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  unint64_t result = v64();
  if ((result & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  uint32_t v53 = result;
  if (HIDWORD(result))
  {
LABEL_39:
    __break(1u);
    return result;
  }
  int64x2_t v54 = v72;
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
    int64x2_t v54 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v54 + 2), 0, v54);
  }
  long long v72 = v54;
  swift_bridgeObjectRetain();
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
    int64_t v43 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v43 + 2), 0, v43);
  }
  unint64_t v71 = v43;
  int8x16_t v55 = v59;
  uint64_t v56 = (const float *)(v63 + 32);
  unint64_t v57 = (const float *)(v62 + 32);
  swift_bridgeObjectRetain();
  vImageMatrixMultiply_PlanarF((const vImage_Buffer **)v54 + 4, (const vImage_Buffer **)v43 + 4, v52, v53, v55 + 8, v56, v57, 0);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  swift_release();
  $defer #1 <A><A1, B1>() in closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:divisor:preBias:postBias:destination:)((uint64_t)&v72, (uint64_t *)&v71);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

unint64_t partial apply for closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:preBias:postBias:destination:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:preBias:postBias:destination:)(a1, a2, v2[8], v2[9], v2[10], v2[11], v2[12], v2[2], v2[3], v2[4], v2[5], v2[6], v2[7]);
}

uint64_t $defer #1 <A><A1, B1>() in closure #1 in vImage.PixelBuffer<>.multiply<A, B>(by:divisor:preBias:postBias:destination:)(uint64_t result, uint64_t *a2)
{
  uint64_t v3 = *(void *)result;
  uint64_t v4 = *(void *)(*(void *)result + 16);
  if (v4)
  {
    swift_bridgeObjectRetain();
    for (uint64_t i = 0; i != v4; ++i)
    {
      uint64_t v6 = *(void *)(v3 + 8 * i + 32);
      if (v6) {
        MEMORY[0x1D26009C0](v6, -1, -1);
      }
    }
    unint64_t result = swift_bridgeObjectRelease();
  }
  uint64_t v7 = *a2;
  uint64_t v8 = *(void *)(v7 + 16);
  if (v8)
  {
    swift_bridgeObjectRetain();
    for (uint64_t j = 0; j != v8; ++j)
    {
      uint64_t v10 = *(void *)(v7 + 8 * j + 32);
      if (v10) {
        MEMORY[0x1D26009C0](v10, -1, -1);
      }
    }
    return swift_bridgeObjectRelease();
  }
  return result;
}

vImage_Error vImage.PixelBuffer<>.multiply(by:divisor:preBias:postBias:destination:)(uint64_t a1, uint64_t divisor, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v25 = *MEMORY[0x1E4F143B8];
  uint64_t v6 = *(void **)v5;
  if (!*(void *)(*(void *)v5 + 16))
  {
    __break(1u);
    goto LABEL_31;
  }
  vImagePixelCount v7 = v6[6];
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  vImagePixelCount v8 = v6[5];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v7)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v8)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  uint8x16_t v9 = *(void **)a5;
  if (!*(void *)(*(void *)a5 + 16))
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  uint64_t v10 = v9[6];
  if (v10 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v11 = v9[5];
  if (v11 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v10)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v11)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v7 != v10)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (v8 != v11)
  {
LABEL_41:
    __break(1u);
LABEL_42:
    __break(1u);
LABEL_43:
    __break(1u);
LABEL_44:
    __break(1u);
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  uint64_t v12 = a1;
  if (a1 < 0)
  {
    uint64_t v12 = -a1;
    if (__OFSUB__(0, a1)) {
      goto LABEL_50;
    }
  }
  if (v12 >= 0x8000) {
    goto LABEL_42;
  }
  uint64_t v13 = divisor;
  if (divisor < 0)
  {
    uint64_t v13 = -divisor;
    if (__OFSUB__(0, divisor)) {
      goto LABEL_51;
    }
  }
  if (v13 > 0x7FFFFFFF) {
    goto LABEL_43;
  }
  uint64_t v14 = a3;
  if (a3 < 0)
  {
    uint64_t v14 = -a3;
    if (__OFSUB__(0, a3)) {
      goto LABEL_52;
    }
  }
  if (v14 >= 0x8000) {
    goto LABEL_44;
  }
  uint64_t v15 = a4;
  if (a4 < 0)
  {
    uint64_t v15 = -a4;
    if (__OFSUB__(0, a4)) {
LABEL_53:
    }
      __break(1u);
  }
  if (v15 > 0x7FFFFFFF) {
    goto LABEL_45;
  }
  if (a3 < -32768)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (a3 >= 0x8000)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  int16_t pre_bias = a3;
  if (a4 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (a4 > 0x7FFFFFFF)
  {
LABEL_49:
    __break(1u);
LABEL_50:
    __break(1u);
LABEL_51:
    __break(1u);
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  int32_t post_bias = a4;
  uint64_t v16 = (void *)v6[4];
  size_t v17 = v6[7];
  v24.data = v16;
  v24.height = v8;
  v24.width = v7;
  v24.rowBytes = v17;
  unsigned int v18 = (void *)v9[4];
  size_t v19 = v9[7];
  v23.data = v18;
  v23.height = v8;
  v23.width = v7;
  v23.rowBytes = v19;
  return closure #1 in closure #1 in vImage.PixelBuffer<>.multiply(by:divisor:preBias:postBias:destination:)(&v23, &v24, a1, divisor, &pre_bias, &post_bias);
}

vImage_Error closure #1 in closure #1 in vImage.PixelBuffer<>.multiply(by:divisor:preBias:postBias:destination:)(vImage_Buffer *a1, vImage_Buffer *a2, uint64_t a3, uint64_t divisor, int16_t *pre_bias, int32_t *post_bias)
{
  srcs[1] = *(vImage_Buffer **)MEMORY[0x1E4F143B8];
  dests = a1;
  srcs[0] = a2;
  if (a3 < -32768)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (a3 >= 0x8000)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  int16_t v7 = a3;
  if (divisor < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (divisor > 0x7FFFFFFF) {
    goto LABEL_9;
  }
  return vImageMatrixMultiply_Planar8((const vImage_Buffer **)srcs, (const vImage_Buffer **)&dests, 1u, 1u, &v7, divisor, pre_bias, post_bias, 0);
}

uint64_t vImage.PixelBuffer<>.multiply(by:divisor:preBias:postBias:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t *a11)
{
  uint64_t v53 = *MEMORY[0x1E4F143B8];
  uint64_t v12 = *v11;
  if (!*(void *)(*v11 + 16)) {
    goto LABEL_47;
  }
  uint64_t v13 = *(void *)(v12 + 48);
  if (v13 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  uint64_t v14 = *(void *)(v12 + 40);
  if (v14 < 0)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v13)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (!v14)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  uint64_t v15 = *a11;
  if (!*(void *)(*a11 + 16))
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  uint64_t v16 = *(void *)(v15 + 48);
  if (v16 < 0)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  uint64_t v17 = *(void *)(v15 + 40);
  if (v17 < 0)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (!v16)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (!v17)
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  if (v13 != v16)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  if (v14 != v17)
  {
LABEL_58:
    __break(1u);
LABEL_59:
    __break(1u);
LABEL_60:
    __break(1u);
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  uint64_t v49 = *a11;
  uint64_t v50 = *v11;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135FC0;
  *(void *)(inited + 32) = a1;
  *(void *)(inited + 40) = a2;
  *(void *)(inited + 48) = a3;
  *(void *)(inited + 56) = a4;
  uint64_t v27 = swift_initStackObject();
  *(_OWORD *)(v27 + 16) = xmmword_1D2135FC0;
  *(void *)(v27 + 32) = a6;
  *(void *)(v27 + 40) = a7;
  *(void *)(v27 + 48) = a8;
  *(void *)(v27 + 56) = a9;
  unint64_t v28 = *(void *)(inited + 16);
  if (!v28) {
    goto LABEL_27;
  }
  int64_t v29 = *(void *)(inited + 32);
  unint64_t v30 = v28 - 1;
  if (v28 == 1) {
    goto LABEL_24;
  }
  if (v28 >= 5)
  {
    unint64_t v31 = v30 & 0xFFFFFFFFFFFFFFFCLL | 1;
    int64x2_t v32 = vdupq_n_s64(v29);
    unint64_t v33 = (int64x2_t *)(inited + 56);
    unint64_t v34 = v30 & 0xFFFFFFFFFFFFFFFCLL;
    int64x2_t v35 = v32;
    do
    {
      int64x2_t v32 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v32, v33[-1]), (int8x16_t)v32, (int8x16_t)v33[-1]);
      int64x2_t v35 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v35, *v33), (int8x16_t)v35, *(int8x16_t *)v33);
      v33 += 2;
      v34 -= 4;
    }
    while (v34);
    int8x16_t v36 = vbslq_s8((int8x16_t)vcgtq_s64(v32, v35), (int8x16_t)v32, (int8x16_t)v35);
    uint64_t v37 = vextq_s8(v36, v36, 8uLL).u64[0];
    int64_t v29 = (int64_t)vbsl_s8((int8x8_t)vcgtd_s64(v36.i64[0], v37), *(int8x8_t *)v36.i8, (int8x8_t)v37);
    if (v30 == (v30 & 0xFFFFFFFFFFFFFFFCLL)) {
      goto LABEL_24;
    }
  }
  else
  {
    unint64_t v31 = 1;
  }
  unint64_t v38 = v28 - v31;
  unint64_t v39 = (int64_t *)(inited + 8 * v31 + 32);
  do
  {
    int64_t v41 = *v39++;
    int64_t v40 = v41;
    if (v29 <= v41) {
      int64_t v29 = v40;
    }
    --v38;
  }
  while (v38);
LABEL_24:
  if (v29 < 0)
  {
    BOOL v42 = __OFSUB__(0, v29);
    int64_t v29 = -v29;
    if (v42) {
LABEL_67:
    }
      __break(1u);
  }
  if (v29 >= 0x8000)
  {
    __break(1u);
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
LABEL_27:
  uint64_t v43 = a5;
  if (a5 < 0)
  {
    uint64_t v43 = -a5;
    if (__OFSUB__(0, a5)) {
      goto LABEL_64;
    }
  }
  if (v43 > 0x7FFFFFFF) {
    goto LABEL_59;
  }
  if (a6 <= a7) {
    uint64_t v44 = a7;
  }
  else {
    uint64_t v44 = a6;
  }
  if (v44 <= a8) {
    uint64_t v44 = a8;
  }
  if (v44 <= a9) {
    uint64_t v44 = a9;
  }
  if (v44 < 0)
  {
    BOOL v42 = __OFSUB__(0, v44);
    uint64_t v44 = -v44;
    if (v42) {
      goto LABEL_65;
    }
  }
  if (v44 >= 0x8000) {
    goto LABEL_60;
  }
  uint64_t v45 = a10;
  if (a10 < 0)
  {
    uint64_t v45 = -a10;
    if (__OFSUB__(0, a10)) {
      goto LABEL_66;
    }
  }
  if (v45 > 0x7FFFFFFF) {
    goto LABEL_61;
  }
  if (!*(void *)(v50 + 16))
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  long long v46 = *(_OWORD *)(v50 + 48);
  *(_OWORD *)&v52.data = *(_OWORD *)(v50 + 32);
  *(_OWORD *)&v52.width = v46;
  if (!*(void *)(v49 + 16))
  {
LABEL_63:
    __break(1u);
LABEL_64:
    __break(1u);
LABEL_65:
    __break(1u);
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  long long v47 = *(_OWORD *)(v49 + 48);
  v51[0] = *(_OWORD *)(v49 + 32);
  v51[1] = v47;
  closure #1 in closure #1 in vImage.PixelBuffer<>.multiply(by:divisor:preBias:postBias:destination:)((char *)v51, &v52, inited, a5, v27, a10);
  swift_bridgeObjectRelease();
  return swift_setDeallocating();
}

char *closure #1 in closure #1 in vImage.PixelBuffer<>.multiply(by:divisor:preBias:postBias:destination:)(char *result, const vImage_Buffer *a2, uint64_t a3, uint64_t divisor, uint64_t a5, uint64_t a6)
{
  vImage_Buffer v24 = (const vImage_Buffer *)result;
  int64_t v8 = *(void *)(a3 + 16);
  uint64_t v9 = MEMORY[0x1E4FBC860];
  if (v8)
  {
    uint64_t v26 = MEMORY[0x1E4FBC860];
    unint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v8, 0);
    uint64_t v9 = v26;
    uint64_t v11 = (uint64_t *)(a3 + 32);
    while (1)
    {
      uint64_t v13 = *v11++;
      uint64_t v12 = v13;
      if (v13 < -32768) {
        break;
      }
      if (v12 >= 0x8000) {
        goto LABEL_21;
      }
      unint64_t v15 = *(void *)(v26 + 16);
      unint64_t v14 = *(void *)(v26 + 24);
      if (v15 >= v14 >> 1) {
        unint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v14 > 1), v15 + 1, 1);
      }
      *(void *)(v26 + 16) = v15 + 1;
      *(_WORD *)(v26 + 2 * v15 + 32) = v12;
      if (!--v8) {
        goto LABEL_8;
      }
    }
    __break(1u);
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
LABEL_8:
  if (divisor < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (divisor > 0x7FFFFFFF)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  int64_t v16 = *(void *)(a5 + 16);
  uint64_t v17 = MEMORY[0x1E4FBC860];
  if (v16)
  {
    uint64_t v27 = MEMORY[0x1E4FBC860];
    unint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v16, 0);
    uint64_t v17 = v27;
    unsigned int v18 = (uint64_t *)(a5 + 32);
    while (1)
    {
      uint64_t v20 = *v18++;
      uint64_t v19 = v20;
      if (v20 < -32768) {
        break;
      }
      if (v19 >= 0x8000) {
        goto LABEL_23;
      }
      unint64_t v22 = *(void *)(v27 + 16);
      unint64_t v21 = *(void *)(v27 + 24);
      if (v22 >= v21 >> 1) {
        unint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v21 > 1), v22 + 1, 1);
      }
      *(void *)(v27 + 16) = v22 + 1;
      *(_WORD *)(v27 + 2 * v22 + 32) = v19;
      if (!--v16) {
        goto LABEL_17;
      }
    }
LABEL_22:
    __break(1u);
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
LABEL_17:
  if (a6 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (a6 <= 0x7FFFFFFF)
  {
    vImageMatrixMultiply_ARGB8888ToPlanar8(a2, v24, (const int16_t *)(v9 + 32), divisor, (const int16_t *)(v17 + 32), a6, 0);
    swift_bridgeObjectRelease();
    return (char *)swift_bridgeObjectRelease();
  }
LABEL_27:
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.multiply<A>(by:divisor:preBias:postBias:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t *a11, uint64_t a12, uint64_t a13)
{
  uint64_t v14 = *v13;
  if (!*(void *)(*v13 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v15 = *(void *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v16 = *(void *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v15)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v16)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v17 = *a11;
  if (!*(void *)(*a11 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v18 = *(void *)(v17 + 48);
  if (v18 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v19 = *(void *)(v17 + 40);
  if (v19 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v18)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v19)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v15 != v18)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (v16 == v19)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
    uint64_t inited = swift_initStackObject();
    *(_OWORD *)(inited + 16) = xmmword_1D2135FC0;
    *(void *)(inited + 32) = a3;
    *(void *)(inited + 40) = a4;
    *(void *)(inited + 48) = a5;
    *(void *)(inited + 56) = a6;
    uint64_t v27 = swift_initStackObject();
    *(_OWORD *)(v27 + 16) = xmmword_1D2135FC0;
    *(void *)(v27 + 32) = a7;
    *(void *)(v27 + 40) = a8;
    *(void *)(v27 + 48) = a9;
    *(void *)(v27 + 56) = a10;
    MEMORY[0x1F4188790](v27);
    (*(void (**)(uint64_t (*)(uint64_t, uint64_t)))(a13 + 24))(partial apply for closure #1 in vImage.PixelBuffer<>.multiply<A>(by:divisor:preBias:postBias:destination:));
    swift_bridgeObjectRelease();
    return swift_setDeallocating();
  }
LABEL_25:
  __break(1u);
  return result;
}

uint64_t closure #1 in vImage.PixelBuffer<>.multiply<A>(by:divisor:preBias:postBias:destination:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t *a7, uint64_t *a8, uint64_t a9, uint64_t a10)
{
  unint64_t v71 = a8;
  long long v72 = a7;
  uint64_t v75 = *MEMORY[0x1E4F143B8];
  uint64_t v14 = *(void *)(a9 - 8);
  uint64_t v15 = (const void *)MEMORY[0x1F4188790](a1);
  uint64_t v17 = (char *)&v70 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v19 = specialized _copyCollectionToContiguousArray<A>(_:)(v15, v18);
  (*(void (**)(char *, uint64_t, uint64_t))(v14 + 16))(v17, a3, a9);
  uint64_t v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a10 + 16))(a9, a10);
  (*(void (**)(char *, uint64_t))(v14 + 8))(v17, a9);
  if (v20 != 16) {
    goto LABEL_53;
  }
  unint64_t v21 = v19[2];
  if (!v21) {
    goto LABEL_16;
  }
  int64_t v22 = v19[4];
  unint64_t v23 = v21 - 1;
  if (v21 != 1)
  {
    if (v21 >= 5)
    {
      unint64_t v24 = v23 & 0xFFFFFFFFFFFFFFFCLL | 1;
      int64x2_t v25 = vdupq_n_s64(v22);
      uint64_t v26 = (int64x2_t *)(v19 + 7);
      unint64_t v27 = v23 & 0xFFFFFFFFFFFFFFFCLL;
      int64x2_t v28 = v25;
      do
      {
        int64x2_t v25 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v25, v26[-1]), (int8x16_t)v25, (int8x16_t)v26[-1]);
        int64x2_t v28 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v28, *v26), (int8x16_t)v28, *(int8x16_t *)v26);
        v26 += 2;
        v27 -= 4;
      }
      while (v27);
      int8x16_t v29 = vbslq_s8((int8x16_t)vcgtq_s64(v25, v28), (int8x16_t)v25, (int8x16_t)v28);
      uint64_t v30 = vextq_s8(v29, v29, 8uLL).u64[0];
      int64_t v22 = (int64_t)vbsl_s8((int8x8_t)vcgtd_s64(v29.i64[0], v30), *(int8x8_t *)v29.i8, (int8x8_t)v30);
      if (v23 == (v23 & 0xFFFFFFFFFFFFFFFCLL)) {
        goto LABEL_13;
      }
    }
    else
    {
      unint64_t v24 = 1;
    }
    unint64_t v31 = v21 - v24;
    int64x2_t v32 = &v19[v24 + 4];
    do
    {
      int64_t v34 = *v32++;
      int64_t v33 = v34;
      if (v22 <= v34) {
        int64_t v22 = v33;
      }
      --v31;
    }
    while (v31);
  }
LABEL_13:
  if (v22 < 0)
  {
    BOOL v35 = __OFSUB__(0, v22);
    int64_t v22 = -v22;
    if (v35) {
      goto LABEL_58;
    }
  }
  if (v22 >= 0x8000)
  {
    __break(1u);
    goto LABEL_51;
  }
LABEL_16:
  uint64_t v36 = a4;
  if (a4 < 0)
  {
    uint64_t v36 = -a4;
    if (__OFSUB__(0, a4)) {
      goto LABEL_57;
    }
  }
  if (v36 > 0x7FFFFFFF) {
    goto LABEL_54;
  }
  unint64_t v37 = *(void *)(a5 + 16);
  if (v37)
  {
    int64_t v38 = *(void *)(a5 + 32);
    unint64_t v39 = v37 - 1;
    if (v37 == 1) {
      goto LABEL_30;
    }
    if (v37 >= 5)
    {
      unint64_t v40 = v39 & 0xFFFFFFFFFFFFFFFCLL | 1;
      int64x2_t v41 = vdupq_n_s64(v38);
      BOOL v42 = (int64x2_t *)(a5 + 56);
      unint64_t v43 = v39 & 0xFFFFFFFFFFFFFFFCLL;
      int64x2_t v44 = v41;
      do
      {
        int64x2_t v41 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v41, v42[-1]), (int8x16_t)v41, (int8x16_t)v42[-1]);
        int64x2_t v44 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v44, *v42), (int8x16_t)v44, *(int8x16_t *)v42);
        v42 += 2;
        v43 -= 4;
      }
      while (v43);
      int8x16_t v45 = vbslq_s8((int8x16_t)vcgtq_s64(v41, v44), (int8x16_t)v41, (int8x16_t)v44);
      uint64_t v46 = vextq_s8(v45, v45, 8uLL).u64[0];
      int64_t v38 = (int64_t)vbsl_s8((int8x8_t)vcgtd_s64(v45.i64[0], v46), *(int8x8_t *)v45.i8, (int8x8_t)v46);
      if (v39 == (v39 & 0xFFFFFFFFFFFFFFFCLL)) {
        goto LABEL_30;
      }
    }
    else
    {
      unint64_t v40 = 1;
    }
    unint64_t v47 = v37 - v40;
    uint64_t v48 = (int64_t *)(a5 + 8 * v40 + 32);
    do
    {
      int64_t v50 = *v48++;
      int64_t v49 = v50;
      if (v38 <= v50) {
        int64_t v38 = v49;
      }
      --v47;
    }
    while (v47);
LABEL_30:
    if (v38 < 0)
    {
      BOOL v35 = __OFSUB__(0, v38);
      int64_t v38 = -v38;
      if (v35) {
        goto LABEL_59;
      }
    }
    if (v38 < 0x8000) {
      goto LABEL_33;
    }
LABEL_51:
    __break(1u);
LABEL_52:
    __break(1u);
LABEL_53:
    __break(1u);
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
LABEL_33:
  unint64_t v51 = *(void *)(a6 + 16);
  if (!v51) {
    goto LABEL_47;
  }
  int64_t v52 = *(void *)(a6 + 32);
  unint64_t v53 = v51 - 1;
  if (v51 != 1)
  {
    if (v51 >= 5)
    {
      unint64_t v54 = v53 & 0xFFFFFFFFFFFFFFFCLL | 1;
      int64x2_t v55 = vdupq_n_s64(v52);
      uint64_t v56 = (int64x2_t *)(a6 + 56);
      unint64_t v57 = v53 & 0xFFFFFFFFFFFFFFFCLL;
      int64x2_t v58 = v55;
      do
      {
        int64x2_t v55 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v55, v56[-1]), (int8x16_t)v55, (int8x16_t)v56[-1]);
        int64x2_t v58 = (int64x2_t)vbslq_s8((int8x16_t)vcgtq_s64(v58, *v56), (int8x16_t)v58, *(int8x16_t *)v56);
        v56 += 2;
        v57 -= 4;
      }
      while (v57);
      int8x16_t v59 = vbslq_s8((int8x16_t)vcgtq_s64(v55, v58), (int8x16_t)v55, (int8x16_t)v58);
      uint64_t v60 = vextq_s8(v59, v59, 8uLL).u64[0];
      int64_t v52 = (int64_t)vbsl_s8((int8x8_t)vcgtd_s64(v59.i64[0], v60), *(int8x8_t *)v59.i8, (int8x8_t)v60);
      if (v53 == (v53 & 0xFFFFFFFFFFFFFFFCLL)) {
        goto LABEL_44;
      }
    }
    else
    {
      unint64_t v54 = 1;
    }
    unint64_t v61 = v51 - v54;
    uint64_t v62 = (int64_t *)(a6 + 8 * v54 + 32);
    do
    {
      int64_t v64 = *v62++;
      int64_t v63 = v64;
      if (v52 <= v64) {
        int64_t v52 = v63;
      }
      --v61;
    }
    while (v61);
  }
LABEL_44:
  if (v52 < 0)
  {
    BOOL v35 = __OFSUB__(0, v52);
    int64_t v52 = -v52;
    if (v35) {
LABEL_60:
    }
      __break(1u);
  }
  if (v52 > 0x7FFFFFFF) {
    goto LABEL_52;
  }
LABEL_47:
  uint64_t v65 = *v72;
  if (!*(void *)(*v72 + 16))
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  long long v66 = *(_OWORD *)(v65 + 48);
  *(_OWORD *)&v74.data = *(_OWORD *)(v65 + 32);
  *(_OWORD *)&v74.width = v66;
  uint64_t v67 = *v71;
  if (!*(void *)(*v71 + 16))
  {
LABEL_56:
    __break(1u);
LABEL_57:
    __break(1u);
LABEL_58:
    __break(1u);
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  long long v68 = *(_OWORD *)(v67 + 48);
  v73[0] = *(_OWORD *)(v67 + 32);
  v73[1] = v68;
  closure #1 in closure #1 in closure #1 in vImage.PixelBuffer<>.multiply<A>(by:divisor:preBias:postBias:destination:)((char *)v73, &v74, (uint64_t)v19, a4, a5, a6);
  return swift_release();
}

char *closure #1 in closure #1 in closure #1 in vImage.PixelBuffer<>.multiply<A>(by:divisor:preBias:postBias:destination:)(char *result, const vImage_Buffer *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v6 = a6;
  int64_t v34 = (const vImage_Buffer *)result;
  int64_t v8 = *(void *)(a3 + 16);
  uint64_t v9 = MEMORY[0x1E4FBC860];
  if (v8)
  {
    uint64_t v11 = a4;
    uint64_t v35 = MEMORY[0x1E4FBC860];
    uint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v8, 0);
    uint64_t v9 = v35;
    uint64_t v12 = (uint64_t *)(a3 + 32);
    while (1)
    {
      uint64_t v14 = *v12++;
      uint64_t v13 = v14;
      if (v14 < -32768) {
        break;
      }
      if (v13 >= 0x8000) {
        goto LABEL_27;
      }
      unint64_t v16 = *(void *)(v35 + 16);
      unint64_t v15 = *(void *)(v35 + 24);
      if (v16 >= v15 >> 1) {
        uint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v15 > 1), v16 + 1, 1);
      }
      *(void *)(v35 + 16) = v16 + 1;
      *(_WORD *)(v35 + 2 * v16 + 32) = v13;
      if (!--v8)
      {
        a4 = v11;
        uint64_t v6 = a6;
        goto LABEL_9;
      }
    }
    __break(1u);
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
LABEL_9:
  if (a4 < (uint64_t)0xFFFFFFFF80000000)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (a4 <= 0x7FFFFFFF)
  {
    int32_t divisora = a4;
    int64_t v17 = *(void *)(a5 + 16);
    uint64_t v18 = MEMORY[0x1E4FBC860];
    if (!v17)
    {
LABEL_18:
      uint64_t v24 = *(void *)(v6 + 16);
      uint64_t v25 = MEMORY[0x1E4FBC860];
      if (!v24)
      {
LABEL_25:
        vImageMatrixMultiply_ARGB8888(a2, v34, (const int16_t *)(v9 + 32), divisora, (const int16_t *)(v18 + 32), (const int32_t *)(v25 + 32), 0);
        swift_bridgeObjectRelease();
        swift_bridgeObjectRelease();
        return (char *)swift_bridgeObjectRelease();
      }
      uint64_t v37 = MEMORY[0x1E4FBC860];
      uint64_t result = (char *)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v24, 0);
      uint64_t v25 = v37;
      uint64_t v26 = (int64_t *)(v6 + 32);
      while (1)
      {
        int64_t v28 = *v26++;
        uint64_t v27 = v28;
        if (v28 < (uint64_t)0xFFFFFFFF80000000) {
          goto LABEL_30;
        }
        if (v27 > 0x7FFFFFFF) {
          goto LABEL_31;
        }
        unint64_t v30 = *(void *)(v37 + 16);
        unint64_t v29 = *(void *)(v37 + 24);
        if (v30 >= v29 >> 1) {
          uint64_t result = (char *)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v29 > 1, v30 + 1, 1);
        }
        *(void *)(v37 + 16) = v30 + 1;
        *(_DWORD *)(v37 + 4 * v30 + 32) = v27;
        if (!--v24) {
          goto LABEL_25;
        }
      }
    }
    uint64_t v36 = MEMORY[0x1E4FBC860];
    uint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v17, 0);
    uint64_t v18 = v36;
    uint64_t v19 = (uint64_t *)(a5 + 32);
    while (1)
    {
      uint64_t v21 = *v19++;
      uint64_t v20 = v21;
      if (v21 < -32768) {
        break;
      }
      if (v20 >= 0x8000) {
        goto LABEL_29;
      }
      unint64_t v23 = *(void *)(v36 + 16);
      unint64_t v22 = *(void *)(v36 + 24);
      if (v23 >= v22 >> 1) {
        uint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v22 > 1), v23 + 1, 1);
      }
      *(void *)(v36 + 16) = v23 + 1;
      *(_WORD *)(v36 + 2 * v23 + 32) = v20;
      if (!--v17) {
        goto LABEL_18;
      }
    }
LABEL_28:
    __break(1u);
LABEL_29:
    __break(1u);
LABEL_30:
    __break(1u);
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
LABEL_33:
  __break(1u);
  return result;
}

vImage_Error vImage.PixelBuffer<>.multiply(by:preBias:postBias:destination:)(uint64_t a1, float a2, float a3, float a4)
{
  v22[4] = *MEMORY[0x1E4F143B8];
  uint64_t v5 = *(void **)v4;
  if (!*(void *)(*(void *)v4 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v6)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v7)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  int64_t v8 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v9 = v8[6];
  if (v9 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v10 = v8[5];
  if (v10 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v9)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v10)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v6 != v9)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v7 != v10) {
    goto LABEL_25;
  }
  float post_bias = a4;
  float pre_bias = a3;
  uint64_t v11 = v5[4];
  uint64_t v12 = v5[7];
  v22[0] = v11;
  v22[1] = v7;
  v22[2] = v6;
  v22[3] = v12;
  uint64_t v13 = v8[4];
  uint64_t v14 = v8[7];
  v21[0] = v13;
  v21[1] = v7;
  v21[2] = v6;
  v21[3] = v14;
  srcs = (vImage_Buffer *)v22;
  dests = (vImage_Buffer *)v21;
  float v18 = a2;
  return vImageMatrixMultiply_PlanarF((const vImage_Buffer **)&srcs, (const vImage_Buffer **)&dests, 1u, 1u, &v18, &pre_bias, &post_bias, 0);
}

uint64_t vImage.PixelBuffer<>.multiply(by:preBias:postBias:destination:)(uint64_t *a1, float a2, float a3, float a4, float a5, float a6, float a7, float a8, float a9, float a10)
{
  uint64_t v39 = *MEMORY[0x1E4F143B8];
  uint64_t v18 = *v17;
  if (!*(void *)(*v17 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  vImagePixelCount v19 = *(void *)(v18 + 48);
  if ((v19 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v20 = *(void *)(v18 + 40);
  if ((v20 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v19)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v20)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v21 = *a1;
  if (!*(void *)(*a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v22 = *(void *)(v21 + 48);
  if (v22 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v23 = *(void *)(v21 + 40);
  if (v23 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v22)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v23)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v19 != v22)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v20 != v23) {
    goto LABEL_25;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135FC0;
  *(float *)(inited + 32) = a2;
  *(float *)(inited + 36) = a3;
  *(float *)(inited + 40) = a4;
  *(float *)(inited + 44) = a5;
  v38[0] = a6;
  v38[1] = a7;
  v38[2] = a8;
  v38[3] = a9;
  size_t v33 = *(void *)(v18 + 56);
  src.data = *(void **)(v18 + 32);
  src.height = v20;
  src.width = v19;
  src.rowBytes = v33;
  size_t v34 = *(void *)(v21 + 56);
  dest.data = *(void **)(v21 + 32);
  dest.height = v20;
  dest.width = v19;
  dest.rowBytes = v34;
  vImageMatrixMultiply_ARGBFFFFToPlanarF(&src, &dest, (const float *)(inited + 32), v38, a10, 0);
  return swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.multiply<A>(by:preBias:postBias:destination:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, float a5, float a6, float a7, float a8, float a9, float a10, float a11, float a12)
{
  uint64_t v13 = v12;
  v41[16] = *MEMORY[0x1E4F143B8];
  uint64_t v26 = *(void *)(a3 - 8);
  MEMORY[0x1F4188790](a1);
  int64_t v28 = (char *)&v41[-1] - ((v27 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(char *, uint64_t))(v26 + 16))(v28, a1);
  uint64_t v29 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  (*(void (**)(char *, uint64_t))(v26 + 8))(v28, a3);
  if (v29 != 16)
  {
    __break(1u);
    goto LABEL_16;
  }
  unint64_t v30 = *(void **)v13;
  if (!*(void *)(*(void *)v13 + 16))
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v31 = v30[6];
  if (v31 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v32 = v30[5];
  if (v32 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v31)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v32)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v33 = *a2;
  if (!*(void *)(*a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v34 = *(void *)(v33 + 48);
  if (v34 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v35 = *(void *)(v33 + 40);
  if (v35 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v34)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v35)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v31 != v34)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v32 != v35) {
    goto LABEL_27;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
  uint64_t inited = swift_initStackObject();
  long long v40 = xmmword_1D2135FC0;
  *(_OWORD *)(inited + 16) = xmmword_1D2135FC0;
  *(float *)(inited + 32) = a5;
  *(float *)(inited + 36) = a6;
  *(float *)(inited + 40) = a7;
  *(float *)(inited + 44) = a8;
  uint64_t v37 = swift_initStackObject();
  *(_OWORD *)(v37 + 16) = v40;
  *(float *)(v37 + 32) = a9;
  *(float *)(v37 + 36) = a10;
  *(float *)(v37 + 40) = a11;
  *(float *)(v37 + 44) = a12;
  uint64_t v38 = v30[7];
  v41[0] = v30[4];
  v41[1] = v32;
  v41[2] = v31;
  v41[3] = v38;
  closure #1 in vImage.PixelBuffer<>.multiply<A>(by:preBias:postBias:destination:)((uint64_t)v41, a2, a1, inited, v37, a3, a4);
  swift_bridgeObjectRelease();
  return swift_setDeallocating();
}

uint64_t closure #1 in vImage.PixelBuffer<>.multiply<A>(by:preBias:postBias:destination:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v16 = *MEMORY[0x1E4F143B8];
  uint64_t v7 = *a2;
  if (!*(void *)(*a2 + 16)) {
    __break(1u);
  }
  long long v8 = *(_OWORD *)(v7 + 48);
  v10[0] = *(_OWORD *)(v7 + 32);
  v10[1] = v8;
  uint64_t v12 = a1;
  uint64_t v13 = v10;
  uint64_t v14 = a4;
  uint64_t v15 = a5;
  return (*(uint64_t (**)(const float *(*)(const float *), unsigned char *, uint64_t, uint64_t, uint64_t))(a7 + 24))(partial apply for closure #1 in closure #1 in closure #1 in vImage.PixelBuffer<>.multiply<A>(by:preBias:postBias:destination:), v11, MEMORY[0x1E4FBC848] + 8, a6, a7);
}

uint64_t vImage.PixelBuffer<>.multiply(by:preBias:postBias:destination:)(uint64_t *a1, int8x16_t a2, int8x16_t a3, int8x16_t a4, int8x16_t a5, float a6, float a7, float a8, float a9, float a10, float a11, float a12, float a13)
{
  uint64_t v48 = *MEMORY[0x1E4F143B8];
  uint64_t v21 = *v20;
  if (!*(void *)(*v20 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  vImagePixelCount v22 = *(void *)(v21 + 48);
  if ((v22 & 0x8000000000000000) != 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v23 = *(void *)(v21 + 40);
  if ((v23 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v22)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v23)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v24 = *a1;
  if (!*(void *)(*a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v25 = *(void *)(v24 + 48);
  if (v25 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v26 = *(void *)(v24 + 40);
  if (v26 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v25)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v26)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v22 != v25)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v23 != v26) {
    goto LABEL_25;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135FC0;
  *(float *)(inited + 32) = a6;
  uint64_t v32 = (const float *)(inited + 32);
  *(float *)(inited + 36) = a7;
  *(float *)(inited + 40) = a8;
  *(float *)(inited + 44) = a9;
  v47[0] = a10;
  v47[1] = a11;
  v47[2] = a12;
  v47[3] = a13;
  uint64_t v33 = swift_initStackObject();
  *(int32x2_t *)(v33 + 32) = vzip1_s32(*(int32x2_t *)a2.i8, *(int32x2_t *)a3.i8);
  *(_OWORD *)(v33 + 16) = xmmword_1D2135FD0;
  *(_DWORD *)(v33 + 40) = a4.i32[0];
  *(void *)(v33 + 44) = __PAIR64__(a2.u32[1], a5.u32[0]);
  *(int32x2_t *)(v33 + 52) = vzip2_s32(*(int32x2_t *)a3.i8, *(int32x2_t *)a4.i8);
  int8x8_t v34 = (int8x8_t)vextq_s8(a2, a2, 8uLL).u64[0];
  *(int8x8_t *)(v33 + 60) = vext_s8(*(int8x8_t *)a5.i8, v34, 4uLL);
  int32x2_t v35 = (int32x2_t)vextq_s8(a4, a4, 8uLL).u64[0];
  int32x2_t v36 = (int32x2_t)vextq_s8(a3, a3, 8uLL).u64[0];
  *(int32x2_t *)(v33 + 68) = vzip1_s32(v36, v35);
  LODWORD(v37) = vextq_s8(a5, a5, 8uLL).u32[0];
  HIDWORD(v37) = v34.i32[1];
  *(void *)(v33 + 76) = v37;
  *(int32x2_t *)(v33 + 84) = vzip2_s32(v36, v35);
  *(_DWORD *)(v33 + 92) = a5.i32[3];
  size_t v38 = *(void *)(v21 + 56);
  src.data = *(void **)(v21 + 32);
  src.height = v23;
  src.width = v22;
  src.rowBytes = v38;
  size_t v39 = *(void *)(v24 + 56);
  dest.data = *(void **)(v24 + 32);
  dest.height = v23;
  dest.width = v22;
  dest.rowBytes = v39;
  vImageMatrixMultiply_ARGBFFFF(&src, &dest, (const float *)(v33 + 32), v32, v47, 0);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

uint64_t partial apply for closure #1 in vImage.PixelBuffer<>.multiply<A>(by:divisor:preBias:postBias:destination:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in vImage.PixelBuffer<>.multiply<A>(by:divisor:preBias:postBias:destination:)(a1, a2, *(void *)(v2 + 32), *(void *)(v2 + 40), *(void *)(v2 + 48), *(void *)(v2 + 56), *(uint64_t **)(v2 + 64), *(uint64_t **)(v2 + 72), *(void *)(v2 + 16), *(void *)(v2 + 24));
}

const float *partial apply for closure #1 in closure #1 in closure #1 in vImage.PixelBuffer<>.multiply<A>(by:preBias:postBias:destination:)(const float *matrix)
{
  if (matrix) {
    return (const float *)vImageMatrixMultiply_ARGBFFFF(*(const vImage_Buffer **)(v1 + 16), *(const vImage_Buffer **)(v1 + 24), matrix, (const float *)(*(void *)(v1 + 32) + 32), (const float *)(*(void *)(v1 + 40) + 32), 0);
  }
  __break(1u);
  return matrix;
}

uint64_t vImage.PixelBuffer<>.extractChannel(at:destination:)(uint64_t a1, uint64_t a2)
{
  uint64_t v2 = (uint64_t (*)(void *, void *, uint64_t, void))MEMORY[0x1E4F17010];

  return vImage.PixelBuffer<>.extractChannel(at:destination:)(a1, a2, v2);
}

{
  uint64_t (*v2)(void *, void *, uint64_t, void);
  uint64_t vars8;

  uint64_t v2 = (uint64_t (*)(void *, void *, uint64_t, void))MEMORY[0x1E4F17018];

  return vImage.PixelBuffer<>.extractChannel(at:destination:)(a1, a2, v2);
}

{
  uint64_t (*v2)(void *, void *, uint64_t, void);
  uint64_t vars8;

  uint64_t v2 = (uint64_t (*)(void *, void *, uint64_t, void))MEMORY[0x1E4F17008];

  return vImage.PixelBuffer<>.extractChannel(at:destination:)(a1, a2, v2);
}

uint64_t vImage.PixelBuffer<>._extractChannel<A>(channelIndex:destination:extractFunc:)(uint64_t a1, uint64_t *a2, uint64_t (*a3)(uint64_t *, uint64_t *, uint64_t, void))
{
  uint64_t v25 = *MEMORY[0x1E4F143B8];
  if (a1 > 3)
  {
    __break(1u);
    goto LABEL_6;
  }
  uint64_t v5 = *a2;
  uint64_t v6 = *v3;
  uint64_t v17 = *v3;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&v21);
  uint64_t v8 = v21;
  uint64_t v7 = v22;
  type metadata accessor for vImage.PixelBuffer();
  vImage.PixelBuffer.size.getter(&v17);
  swift_bridgeObjectRelease();
  if (v8 != v17 || v7 != v18) {
LABEL_6:
  }
    __break(1u);
  uint64_t v21 = v6;
  uint64_t v17 = vImage.PixelBuffer<>.vImageBuffer.getter();
  uint64_t v18 = v9;
  uint64_t v19 = v10;
  uint64_t v20 = v11;
  uint64_t v21 = v5;
  uint64_t v21 = vImage.PixelBuffer<>.vImageBuffer.getter();
  uint64_t v22 = v12;
  uint64_t v23 = v13;
  uint64_t v24 = v14;
  return a3(&v17, &v21, a1, 0);
}

uint64_t vImage.PixelBuffer<>.extractChannel(at:destination:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(void *, void *, uint64_t, void))
{
  v16[4] = *MEMORY[0x1E4F143B8];
  if (a1 > 3)
  {
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v4 = *(void **)v3;
  if (!*(void *)(*(void *)v3 + 16))
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v5 = v4[6];
  if (v5 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v6 = v4[5];
  if (v6 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v5)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v6)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = *(void **)a2;
  if (!*(void *)(*(void *)a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v8 = v7[6];
  if (v8 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v9 = v7[5];
  if (v9 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v8)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v9)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v5 != v8)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v6 != v9) {
    goto LABEL_27;
  }
  uint64_t v10 = v4[4];
  uint64_t v11 = v4[7];
  v16[0] = v10;
  v16[1] = v6;
  v16[2] = v5;
  v16[3] = v11;
  uint64_t v12 = v7[4];
  uint64_t v13 = v7[7];
  v15[0] = v12;
  v15[1] = v6;
  v15[2] = v5;
  v15[3] = v13;
  return a3(v16, v15, a1, 0);
}

double BNNS.ActivationFunction.bnnsActivation.getter@<D0>(uint64_t a1@<X8>)
{
  unint64_t v2 = *v1;
  switch(*((unsigned char *)v1 + 8))
  {
    case 1:
      unint64_t v3 = HIDWORD(v2);
      int v4 = 5;
      break;
    case 2:
      LODWORD(v3) = 2143289344;
      int v4 = 7;
      break;
    case 3:
      unint64_t v3 = HIDWORD(v2);
      int v4 = 8;
      break;
    case 4:
      unint64_t v3 = HIDWORD(v2);
      int v4 = 12;
      break;
    case 5:
    case 0x11:
      unint64_t v3 = HIDWORD(v2);
      int v4 = 30;
      break;
    case 6:
      unint64_t v3 = HIDWORD(v2);
      int v4 = 13;
      break;
    case 7:
      unint64_t v3 = HIDWORD(v2);
      int v4 = 14;
      break;
    case 8:
      unint64_t v3 = HIDWORD(v2);
      int v4 = 15;
      break;
    case 9:
      unint64_t v3 = HIDWORD(v2);
      int v4 = 16;
      break;
    case 0xA:
      LODWORD(v3) = 2143289344;
      int v4 = 18;
      break;
    case 0xB:
      LODWORD(v3) = 2143289344;
      int v4 = 24;
      break;
    case 0xC:
      unint64_t v3 = HIDWORD(v2);
      int v4 = 19;
      break;
    case 0xD:
      unint64_t v3 = HIDWORD(v2);
      int v4 = 20;
      break;
    case 0xE:
      LODWORD(v3) = 2143289344;
      int v4 = 25;
      break;
    case 0xF:
      LODWORD(v3) = 2143289344;
      int v4 = 26;
      break;
    case 0x10:
      unint64_t v3 = HIDWORD(v2);
      int v4 = 28;
      break;
    case 0x12:
      int v4 = dword_1D21360A4[v2];
      LODWORD(v2) = 2143289344;
      LODWORD(v3) = 2143289344;
      break;
    default:
      LODWORD(v3) = 2143289344;
      int v4 = 2;
      break;
  }
  *(_DWORD *)a1 = v4;
  *(_DWORD *)(a1 + 4) = v2;
  *(_DWORD *)(a1 + 8) = v3;
  *(_DWORD *)(a1 + 12) = 1;
  double result = 0.0;
  *(_OWORD *)(a1 + 16) = 0u;
  *(_OWORD *)(a1 + 32) = 0u;
  return result;
}

uint64_t BNNS.ActivationLayer.__allocating_init(function:input:output:filterParameters:)(uint64_t *a1, _OWORD *a2, long long *a3, int a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v49 = *MEMORY[0x1E4F143B8];
  long long v11 = a3[8];
  long long v12 = a3[9];
  long long v13 = a3[6];
  __src[18] = a3[7];
  __src[19] = v11;
  long long v14 = a3[10];
  __src[20] = v12;
  __src[21] = v14;
  long long v15 = a3[4];
  long long v16 = a3[5];
  long long v17 = a3[2];
  __src[14] = a3[3];
  __src[15] = v15;
  __src[16] = v16;
  __src[17] = v13;
  long long v18 = *a3;
  __src[12] = a3[1];
  __src[13] = v17;
  long long v19 = a2[9];
  __src[8] = a2[8];
  __src[9] = v19;
  __src[10] = a2[10];
  __src[11] = v18;
  long long v20 = a2[5];
  __src[4] = a2[4];
  __src[5] = v20;
  long long v21 = a2[7];
  __src[6] = a2[6];
  __src[7] = v21;
  long long v22 = a2[1];
  __src[0] = *a2;
  __src[1] = v22;
  long long v23 = a2[3];
  __src[2] = a2[2];
  __src[3] = v23;
  char v24 = *((unsigned char *)a1 + 8);
  uint64_t v30 = *a1;
  LOBYTE(v31) = v24;
  BNNS.ActivationFunction.bnnsActivation.getter((uint64_t)&v42);
  memcpy(__dst, __src, sizeof(__dst));
  int v35 = v42;
  uint64_t v36 = v43;
  uint64_t v37 = v44;
  int v38 = v45;
  long long v39 = v46;
  uint64_t v40 = v47;
  int v41 = 0;
  if (a6 == 1)
  {
    uint64_t v25 = 0;
  }
  else
  {
    LODWORD(v30) = a4;
    uint64_t v31 = a5;
    uint64_t v32 = a6;
    uint64_t v33 = a7;
    uint64_t v25 = &v30;
  }
  uint64_t v26 = MEMORY[0x1D25FFFA0](__dst, v25);
  type metadata accessor for BNNS.ActivationLayer();
  uint64_t v27 = swift_allocObject();
  uint64_t v28 = v27;
  if (v26)
  {
    *(void *)(v27 + 16) = v26;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v28;
}

uint64_t BNNS.ActivationLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.ActivationLayer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

uint64_t static BNNS.applyActivation(activation:input:output:batchSize:filterParameters:)(uint64_t *a1, _OWORD *a2, long long *a3, uint64_t a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v96 = *MEMORY[0x1E4F143B8];
  long long v10 = a3[8];
  long long v11 = a3[9];
  long long v12 = a3[6];
  __src[18] = a3[7];
  __src[19] = v10;
  long long v13 = a3[10];
  __src[20] = v11;
  __src[21] = v13;
  long long v14 = a3[4];
  long long v15 = a3[5];
  long long v16 = a3[2];
  __src[14] = a3[3];
  __src[15] = v14;
  __src[16] = v15;
  __src[17] = v12;
  long long v17 = *a3;
  __src[12] = a3[1];
  __src[13] = v16;
  long long v18 = a2[9];
  __src[8] = a2[8];
  __src[9] = v18;
  long long v19 = a2[10];
  __src[11] = v17;
  __src[10] = v19;
  long long v20 = a2[5];
  __src[4] = a2[4];
  __src[5] = v20;
  long long v21 = a2[6];
  __src[7] = a2[7];
  __src[6] = v21;
  long long v22 = a2[1];
  __src[0] = *a2;
  __src[1] = v22;
  long long v23 = a2[2];
  __src[3] = a2[3];
  __src[2] = v23;
  char v24 = *((unsigned char *)a1 + 8);
  uint64_t v79 = *a1;
  char v80 = v24;
  BNNS.ActivationFunction.bnnsActivation.getter((uint64_t)&v89);
  memcpy(__dst, __src, sizeof(__dst));
  int v82 = v89;
  uint64_t v83 = v90;
  uint64_t v84 = v91;
  int v85 = v92;
  long long v86 = v93;
  uint64_t v87 = v94;
  int v88 = 0;
  if (a7 == 1)
  {
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v77);
    outlined init with take of BNNS.Shape((uint64_t)v77, (uint64_t)v78);
    outlined init with take of BNNS.Shape((uint64_t)v78, (uint64_t)&v79);
    BNNS.Shape.size.getter((uint64_t)&v69);
    unint64_t v25 = v69;
    unint64_t v26 = v70;
    unint64_t v27 = v71;
    unint64_t v28 = v72;
    unint64_t v29 = v73;
    unint64_t v30 = v74;
    unint64_t v31 = v75;
    unint64_t v55 = v76;
    outlined init with take of BNNS.Shape((uint64_t)v78, (uint64_t)&v79);
    BNNS.Shape.stride.getter((uint64_t)&v69);
    unint64_t v56 = specialized static BNNS.calculateBatchStride(size:stride:)(v25, v26, v27, v28, v29, v30, v31, v55, v69, v70, v71, v72, v73, v74, v75, v76);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v69);
    outlined init with take of BNNS.Shape((uint64_t)&v69, (uint64_t)&v79);
    outlined init with take of BNNS.Shape((uint64_t)&v79, (uint64_t)v68);
    BNNS.Shape.size.getter((uint64_t)&v59);
    long long v32 = v59;
    long long v33 = v60;
    long long v34 = v61;
    unint64_t v36 = v62;
    unint64_t v35 = v63;
    outlined init with take of BNNS.Shape((uint64_t)&v79, (uint64_t)v68);
    BNNS.Shape.stride.getter((uint64_t)&v59);
    unint64_t v37 = specialized static BNNS.calculateBatchStride(size:stride:)(v32, *((unint64_t *)&v32 + 1), v33, *((unint64_t *)&v33 + 1), v34, *((unint64_t *)&v34 + 1), v36, v35, v59, *((unint64_t *)&v59 + 1), v60, *((unint64_t *)&v60 + 1), v61, *((unint64_t *)&v61 + 1), v62, v63);
    int v38 = 0;
  }
  else
  {
    int v64 = a5;
    uint64_t v65 = a6;
    uint64_t v66 = a7;
    uint64_t v67 = a8;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v77);
    outlined init with take of BNNS.Shape((uint64_t)v77, (uint64_t)v78);
    outlined init with take of BNNS.Shape((uint64_t)v78, (uint64_t)&v79);
    BNNS.Shape.size.getter((uint64_t)&v69);
    unint64_t v39 = v69;
    unint64_t v40 = v70;
    unint64_t v41 = v71;
    unint64_t v42 = v72;
    unint64_t v44 = v73;
    unint64_t v43 = v74;
    unint64_t v45 = v75;
    unint64_t v57 = v76;
    outlined init with take of BNNS.Shape((uint64_t)v78, (uint64_t)&v79);
    BNNS.Shape.stride.getter((uint64_t)&v69);
    unint64_t v56 = specialized static BNNS.calculateBatchStride(size:stride:)(v39, v40, v41, v42, v44, v43, v45, v57, v69, v70, v71, v72, v73, v74, v75, v76);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v69);
    outlined init with take of BNNS.Shape((uint64_t)&v69, (uint64_t)&v79);
    outlined init with take of BNNS.Shape((uint64_t)&v79, (uint64_t)v68);
    BNNS.Shape.size.getter((uint64_t)&v59);
    long long v46 = v59;
    long long v47 = v60;
    long long v48 = v61;
    unint64_t v50 = v62;
    unint64_t v49 = v63;
    outlined init with take of BNNS.Shape((uint64_t)&v79, (uint64_t)v68);
    BNNS.Shape.stride.getter((uint64_t)&v59);
    unint64_t v37 = specialized static BNNS.calculateBatchStride(size:stride:)(v46, *((unint64_t *)&v46 + 1), v47, *((unint64_t *)&v47 + 1), v48, *((unint64_t *)&v48 + 1), v50, v49, v59, *((unint64_t *)&v59 + 1), v60, *((unint64_t *)&v60 + 1), v61, *((unint64_t *)&v61 + 1), v62, v63);
    int v38 = &v64;
  }
  uint64_t result = MEMORY[0x1D25FFF20](__dst, v38, a4, v56, v37);
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *int64_t v52 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t BNNS.ActivationLayer.__allocating_init(function:axes:input:output:filterParameters:)(uint64_t *a1, uint64_t a2, _OWORD *a3, long long *a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v56 = *MEMORY[0x1E4F143B8];
  long long v11 = a4[8];
  long long v12 = a4[9];
  long long v13 = a4[6];
  __src[18] = a4[7];
  __src[19] = v11;
  long long v14 = a4[10];
  __src[20] = v12;
  __src[21] = v14;
  long long v15 = a4[4];
  long long v16 = a4[5];
  long long v17 = a4[2];
  __src[14] = a4[3];
  __src[15] = v15;
  __src[16] = v16;
  __src[17] = v13;
  long long v18 = *a4;
  __src[12] = a4[1];
  __src[13] = v17;
  long long v19 = a3[9];
  __src[8] = a3[8];
  __src[9] = v19;
  __src[10] = a3[10];
  __src[11] = v18;
  long long v20 = a3[5];
  __src[4] = a3[4];
  __src[5] = v20;
  long long v21 = a3[7];
  __src[6] = a3[6];
  __src[7] = v21;
  long long v22 = a3[1];
  __src[0] = *a3;
  __src[1] = v22;
  long long v23 = a3[3];
  __src[2] = a3[2];
  __src[3] = v23;
  char v24 = *((unsigned char *)a1 + 8);
  uint64_t v37 = *a1;
  LOBYTE(v38) = v24;
  BNNS.ActivationFunction.bnnsActivation.getter((uint64_t)&v49);
  int v25 = v49;
  int v26 = v52;
  long long v36 = v53;
  uint64_t v27 = v54;
  int v28 = specialized static BNNS.computeAxisFlags(_:)(a2);
  swift_bridgeObjectRelease();
  memcpy(__dst, __src, sizeof(__dst));
  int v42 = v25;
  uint64_t v43 = v50;
  uint64_t v44 = v51;
  int v45 = v26;
  long long v46 = v36;
  uint64_t v47 = v27;
  int v48 = v28;
  if (a7 == 1)
  {
    unint64_t v29 = 0;
  }
  else
  {
    LODWORD(v37) = a5;
    uint64_t v38 = a6;
    uint64_t v39 = a7;
    uint64_t v40 = a8;
    unint64_t v29 = &v37;
  }
  uint64_t v30 = MEMORY[0x1D25FFFA0](__dst, v29);
  type metadata accessor for BNNS.ActivationLayer();
  uint64_t v31 = swift_allocObject();
  uint64_t v32 = v31;
  if (v30)
  {
    *(void *)(v31 + 16) = v30;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v32;
}

uint64_t static BNNS.applyActivation(activation:axes:input:output:batchSize:filterParameters:)(uint64_t *a1, uint64_t a2, _OWORD *a3, long long *a4, uint64_t a5, int a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  uint64_t v102 = *MEMORY[0x1E4F143B8];
  long long v10 = a4[8];
  long long v11 = a4[9];
  long long v12 = a4[6];
  __src[18] = a4[7];
  __src[19] = v10;
  long long v13 = a4[10];
  __src[20] = v11;
  __src[21] = v13;
  long long v14 = a4[4];
  long long v15 = a4[5];
  long long v16 = a4[2];
  __src[14] = a4[3];
  __src[15] = v14;
  __src[16] = v15;
  __src[17] = v12;
  long long v17 = *a4;
  __src[12] = a4[1];
  __src[13] = v16;
  long long v18 = a3[9];
  __src[8] = a3[8];
  __src[9] = v18;
  long long v19 = a3[10];
  __src[11] = v17;
  __src[10] = v19;
  long long v20 = a3[5];
  __src[4] = a3[4];
  __src[5] = v20;
  long long v21 = a3[6];
  __src[7] = a3[7];
  __src[6] = v21;
  long long v22 = a3[1];
  __src[0] = *a3;
  __src[1] = v22;
  long long v23 = a3[2];
  __src[3] = a3[3];
  __src[2] = v23;
  char v24 = *((unsigned char *)a1 + 8);
  uint64_t v85 = *a1;
  char v86 = v24;
  BNNS.ActivationFunction.bnnsActivation.getter((uint64_t)&v95);
  int v25 = v95;
  int v26 = v98;
  long long v59 = v99;
  uint64_t v27 = v100;
  int v28 = specialized static BNNS.computeAxisFlags(_:)(a2);
  memcpy(__dst, __src, sizeof(__dst));
  int v88 = v25;
  uint64_t v89 = v96;
  uint64_t v90 = v97;
  int v91 = v26;
  long long v92 = v59;
  uint64_t v93 = v27;
  int v94 = v28;
  if (a8 == 1)
  {
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v83);
    outlined init with take of BNNS.Shape((uint64_t)v83, (uint64_t)v84);
    outlined init with take of BNNS.Shape((uint64_t)v84, (uint64_t)&v85);
    BNNS.Shape.size.getter((uint64_t)&v75);
    unint64_t v29 = v75;
    unint64_t v30 = v76;
    unint64_t v31 = v77;
    unint64_t v32 = v78;
    unint64_t v33 = v79;
    unint64_t v34 = v80;
    unint64_t v35 = v81;
    unint64_t v61 = v82;
    outlined init with take of BNNS.Shape((uint64_t)v84, (uint64_t)&v85);
    BNNS.Shape.stride.getter((uint64_t)&v75);
    unint64_t v62 = specialized static BNNS.calculateBatchStride(size:stride:)(v29, v30, v31, v32, v33, v34, v35, v61, v75, v76, v77, v78, v79, v80, v81, v82);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v75);
    outlined init with take of BNNS.Shape((uint64_t)&v75, (uint64_t)&v85);
    outlined init with take of BNNS.Shape((uint64_t)&v85, (uint64_t)v74);
    BNNS.Shape.size.getter((uint64_t)&v65);
    long long v36 = v65;
    long long v37 = v66;
    long long v38 = v67;
    unint64_t v40 = v68;
    unint64_t v39 = v69;
    outlined init with take of BNNS.Shape((uint64_t)&v85, (uint64_t)v74);
    BNNS.Shape.stride.getter((uint64_t)&v65);
    unint64_t v41 = specialized static BNNS.calculateBatchStride(size:stride:)(v36, *((unint64_t *)&v36 + 1), v37, *((unint64_t *)&v37 + 1), v38, *((unint64_t *)&v38 + 1), v40, v39, v65, *((unint64_t *)&v65 + 1), v66, *((unint64_t *)&v66 + 1), v67, *((unint64_t *)&v67 + 1), v68, v69);
    int v42 = 0;
  }
  else
  {
    int v70 = a6;
    uint64_t v71 = a7;
    uint64_t v72 = a8;
    uint64_t v73 = a9;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v83);
    outlined init with take of BNNS.Shape((uint64_t)v83, (uint64_t)v84);
    outlined init with take of BNNS.Shape((uint64_t)v84, (uint64_t)&v85);
    BNNS.Shape.size.getter((uint64_t)&v75);
    unint64_t v43 = v75;
    unint64_t v44 = v76;
    unint64_t v45 = v77;
    unint64_t v46 = v78;
    unint64_t v48 = v79;
    unint64_t v47 = v80;
    unint64_t v49 = v81;
    unint64_t v63 = v82;
    outlined init with take of BNNS.Shape((uint64_t)v84, (uint64_t)&v85);
    BNNS.Shape.stride.getter((uint64_t)&v75);
    unint64_t v62 = specialized static BNNS.calculateBatchStride(size:stride:)(v43, v44, v45, v46, v48, v47, v49, v63, v75, v76, v77, v78, v79, v80, v81, v82);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v75);
    outlined init with take of BNNS.Shape((uint64_t)&v75, (uint64_t)&v85);
    outlined init with take of BNNS.Shape((uint64_t)&v85, (uint64_t)v74);
    BNNS.Shape.size.getter((uint64_t)&v65);
    long long v50 = v65;
    long long v51 = v66;
    long long v52 = v67;
    unint64_t v54 = v68;
    unint64_t v53 = v69;
    outlined init with take of BNNS.Shape((uint64_t)&v85, (uint64_t)v74);
    BNNS.Shape.stride.getter((uint64_t)&v65);
    unint64_t v41 = specialized static BNNS.calculateBatchStride(size:stride:)(v50, *((unint64_t *)&v50 + 1), v51, *((unint64_t *)&v51 + 1), v52, *((unint64_t *)&v52 + 1), v54, v53, v65, *((unint64_t *)&v65 + 1), v66, *((unint64_t *)&v66 + 1), v67, *((unint64_t *)&v67 + 1), v68, v69);
    int v42 = &v70;
  }
  uint64_t result = MEMORY[0x1D25FFF20](__dst, v42, a5, v62, v41);
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *uint64_t v56 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t type metadata accessor for BNNS.ActivationLayer()
{
  return self;
}

uint64_t outlined init with take of BNNS.Shape(uint64_t a1, uint64_t a2)
{
  return a2;
}

uint64_t __swift_memcpy9_4(uint64_t result, uint64_t *a2)
{
  uint64_t v2 = *a2;
  *(unsigned char *)(result + 8) = *((unsigned char *)a2 + 8);
  *(void *)uint64_t result = v2;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.ActivationFunction(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xEE && *(unsigned char *)(a1 + 9)) {
    return (*(_DWORD *)a1 + 238);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 8);
  if (v3 <= 0x12) {
    int v4 = -1;
  }
  else {
    int v4 = v3 ^ 0xFF;
  }
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.ActivationFunction(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xED)
  {
    *(unsigned char *)(result + 8) = 0;
    *(void *)uint64_t result = a2 - 238;
    if (a3 >= 0xEE) {
      *(unsigned char *)(result + 9) = 1;
    }
  }
  else
  {
    if (a3 >= 0xEE) {
      *(unsigned char *)(result + 9) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 8) = -(char)a2;
    }
  }
  return result;
}

uint64_t getEnumTag for BNNS.ActivationFunction(uint64_t a1)
{
  if (*(unsigned __int8 *)(a1 + 8) <= 0x11u) {
    return *(unsigned __int8 *)(a1 + 8);
  }
  else {
    return (*(_DWORD *)a1 + 18);
  }
}

uint64_t destructiveInjectEnumTag for BNNS.ActivationFunction(uint64_t result, unsigned int a2)
{
  if (a2 >= 0x12)
  {
    *(void *)uint64_t result = a2 - 18;
    LOBYTE(a2) = 18;
  }
  *(unsigned char *)(result + 8) = a2;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.ActivationFunction()
{
  return &type metadata for BNNS.ActivationFunction;
}

uint64_t specialized SetAlgebra<>.init(arrayLiteral:)@<X0>(uint64_t a1@<X0>, _DWORD *a2@<X8>)
{
  uint64_t v3 = *(void *)(a1 + 16);
  if (v3)
  {
    int v4 = 0;
    uint64_t v5 = (int *)(a1 + 32);
    do
    {
      int v7 = *v5++;
      int v6 = v7;
      if ((v7 & ~v4) == 0) {
        int v6 = 0;
      }
      v4 |= v6;
      --v3;
    }
    while (v3);
  }
  else
  {
    int v4 = 0;
  }
  uint64_t result = swift_bridgeObjectRelease();
  *a2 = v4;
  return result;
}

void *static BNNS.DataLayout.allCases.getter()
{
  return &outlined read-only object #0 of static BNNS.DataLayout.allCases.getter;
}

uint64_t BNNS.DataLayout.rank.getter()
{
  unint64_t v1 = *v0;
  if (v1 > 0x14) {
    return 8;
  }
  else {
    return qword_1D21363E8[v1];
  }
}

BOOL static BNNS.DataLayout.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void BNNS.DataLayout.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance BNNS.DataLayout(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void protocol witness for Hashable.hash(into:) in conformance BNNS.DataLayout()
{
  Hasher._combine(_:)(*v0);
}

void protocol witness for static CaseIterable.allCases.getter in conformance BNNS.DataLayout(void *a1@<X8>)
{
  *a1 = &outlined read-only object #0 of static BNNS.DataLayout.allCases.getter;
}

uint64_t static BNNS.defaultLayoutForDimensions(_:)@<X0>(uint64_t result@<X0>, char *a2@<X8>)
{
  if ((unint64_t)(result - 1) >= 8) {
    char v2 = 21;
  }
  else {
    char v2 = 0x110F0D0B09060300uLL >> (8 * (result - 1));
  }
  *a2 = v2;
  return result;
}

Swift::Int BNNS.DataLayout.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance BNNS.DataLayout()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t protocol witness for Error._domain.getter in conformance BNNS.Error()
{
  return MEMORY[0x1F4185E10]();
}

uint64_t protocol witness for Error._code.getter in conformance BNNS.Error()
{
  return MEMORY[0x1F4185E08]();
}

uint64_t protocol witness for Error._userInfo.getter in conformance BNNS.Error()
{
  return MEMORY[0x1F4185E18]();
}

uint64_t protocol witness for Error._getEmbeddedNSError() in conformance BNNS.Error()
{
  return MEMORY[0x1F4185E00]();
}

uint64_t BNNSFilterParameters.options.setter(uint64_t result)
{
  _DWORD *v1 = result;
  return result;
}

uint64_t (*BNNSFilterParameters.options.modify(uint64_t a1))(uint64_t result)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *v1;
  return BNNSFilterParameters.options.modify;
}

uint64_t BNNSFilterParameters.options.modify(uint64_t result)
{
  **(_DWORD **)uint64_t result = *(_DWORD *)(result + 8);
  return result;
}

uint64_t BNNSFilterParameters.threadCount.getter(uint64_t a1, uint64_t a2)
{
  return a2;
}

uint64_t BNNSFilterParameters.threadCount.setter(uint64_t result)
{
  *(void *)(v1 + 8) = result;
  return result;
}

void *(*BNNSFilterParameters.threadCount.modify(void *a1))(void *result)
{
  *a1 = *(void *)(v1 + 8);
  a1[1] = v1;
  return BNNSFilterParameters.threadCount.modify;
}

void *BNNSFilterParameters.threadCount.modify(void *result)
{
  *(void *)(result[1] + 8) = *result;
  return result;
}

uint64_t BNNSFilterParameters.allocator.getter(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return a3;
}

uint64_t BNNSFilterParameters.allocator.setter(uint64_t result)
{
  *(void *)(v1 + 16) = result;
  return result;
}

void *(*BNNSFilterParameters.allocator.modify(void *a1))(void *result)
{
  *a1 = *(void *)(v1 + 16);
  a1[1] = v1;
  return BNNSFilterParameters.allocator.modify;
}

void *BNNSFilterParameters.allocator.modify(void *result)
{
  *(void *)(result[1] + 16) = *result;
  return result;
}

uint64_t BNNSFilterParameters.deallocator.getter(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return a4;
}

uint64_t BNNSFilterParameters.deallocator.setter(uint64_t result)
{
  *(void *)(v1 + 24) = result;
  return result;
}

void *(*BNNSFilterParameters.deallocator.modify(void *a1))(void *result)
{
  *a1 = *(void *)(v1 + 24);
  a1[1] = v1;
  return BNNSFilterParameters.deallocator.modify;
}

void *BNNSFilterParameters.deallocator.modify(void *result)
{
  *(void *)(result[1] + 24) = *result;
  return result;
}

unint64_t lazy protocol witness table accessor for type BNNS.DataLayout and conformance BNNS.DataLayout()
{
  unint64_t result = lazy protocol witness table cache variable for type BNNS.DataLayout and conformance BNNS.DataLayout;
  if (!lazy protocol witness table cache variable for type BNNS.DataLayout and conformance BNNS.DataLayout)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.DataLayout and conformance BNNS.DataLayout);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type [BNNS.DataLayout] and conformance [A]()
{
  unint64_t result = lazy protocol witness table cache variable for type [BNNS.DataLayout] and conformance [A];
  if (!lazy protocol witness table cache variable for type [BNNS.DataLayout] and conformance [A])
  {
    __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for [BNNS.DataLayout]);
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type [BNNS.DataLayout] and conformance [A]);
  }
  return result;
}

uint64_t base witness table accessor for RawRepresentable in BNNSFlags()
{
  return lazy protocol witness table accessor for type BNNSFlags and conformance BNNSFlags(&lazy protocol witness table cache variable for type BNNSFlags and conformance BNNSFlags);
}

uint64_t base witness table accessor for SetAlgebra in BNNSFlags()
{
  return lazy protocol witness table accessor for type BNNSFlags and conformance BNNSFlags(&lazy protocol witness table cache variable for type BNNSFlags and conformance BNNSFlags);
}

uint64_t base witness table accessor for Equatable in BNNSFlags()
{
  return lazy protocol witness table accessor for type BNNSFlags and conformance BNNSFlags(&lazy protocol witness table cache variable for type BNNSFlags and conformance BNNSFlags);
}

uint64_t base witness table accessor for ExpressibleByArrayLiteral in BNNSFlags()
{
  return lazy protocol witness table accessor for type BNNSFlags and conformance BNNSFlags(&lazy protocol witness table cache variable for type BNNSFlags and conformance BNNSFlags);
}

uint64_t lazy protocol witness table accessor for type BNNSFlags and conformance BNNSFlags(unint64_t *a1)
{
  uint64_t result = *a1;
  if (!result)
  {
    type metadata accessor for BNNSFlags(255);
    uint64_t result = swift_getWitnessTable();
    atomic_store(result, a1);
  }
  return result;
}

uint64_t protocol witness for ExpressibleByArrayLiteral.init(arrayLiteral:) in conformance BNNSFlags@<X0>(uint64_t a1@<X0>, _DWORD *a2@<X8>)
{
  int v3 = specialized SetAlgebra<>.init(arrayLiteral:)(a1);
  uint64_t result = swift_bridgeObjectRelease();
  *a2 = v3;
  return result;
}

_DWORD *sub_1D2099E54@<X0>(_DWORD *result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *result;
  return result;
}

_DWORD *sub_1D2099E60(_DWORD *result, _DWORD *a2)
{
  *a2 = *result;
  return result;
}

uint64_t sub_1D2099E6C@<X0>(uint64_t result@<X0>, void *a2@<X8>)
{
  *a2 = *(void *)(result + 8);
  return result;
}

void *sub_1D2099E78(void *result, uint64_t a2)
{
  *(void *)(a2 + 8) = *result;
  return result;
}

uint64_t sub_1D2099E84@<X0>(uint64_t result@<X0>, void *a2@<X8>)
{
  *a2 = *(void *)(result + 16);
  return result;
}

void *sub_1D2099E90(void *result, uint64_t a2)
{
  *(void *)(a2 + 16) = *result;
  return result;
}

uint64_t sub_1D2099E9C@<X0>(uint64_t result@<X0>, void *a2@<X8>)
{
  *a2 = *(void *)(result + 24);
  return result;
}

void *sub_1D2099EA8(void *result, uint64_t a2)
{
  *(void *)(a2 + 24) = *result;
  return result;
}

ValueMetadata *type metadata accessor for BNNS()
{
  return &type metadata for BNNS;
}

unsigned char *__swift_memcpy1_1(unsigned char *result, unsigned char *a2)
{
  *uint64_t result = *a2;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.DataLayout(unsigned __int8 *a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0xEC) {
    goto LABEL_17;
  }
  if (a2 + 20 >= 0xFFFF00) {
    int v2 = 4;
  }
  else {
    int v2 = 2;
  }
  if ((a2 + 20) >> 8 < 0xFF) {
    int v3 = 1;
  }
  else {
    int v3 = v2;
  }
  if (v3 == 4)
  {
    int v4 = *(_DWORD *)(a1 + 1);
    if (v4) {
      return (*a1 | (v4 << 8)) - 20;
    }
  }
  else
  {
    if (v3 == 2)
    {
      int v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1)) {
        goto LABEL_17;
      }
      return (*a1 | (v4 << 8)) - 20;
    }
    int v4 = a1[1];
    if (a1[1]) {
      return (*a1 | (v4 << 8)) - 20;
    }
  }
LABEL_17:
  unsigned int v6 = *a1;
  BOOL v7 = v6 >= 0x15;
  int v8 = v6 - 21;
  if (!v7) {
    int v8 = -1;
  }
  return (v8 + 1);
}

unsigned char *storeEnumTagSinglePayload for BNNS.DataLayout(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 20 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 20) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xEC) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xEB)
  {
    unsigned int v6 = ((a2 - 236) >> 8) + 1;
    *uint64_t result = a2 + 20;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D209A02CLL);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 20;
        break;
    }
  }
  return result;
}

uint64_t getEnumTag for BNNS.DataLayout(unsigned __int8 *a1)
{
  return *a1;
}

unsigned char *destructiveInjectEnumTag for BNNS.DataLayout(unsigned char *result, char a2)
{
  *uint64_t result = a2;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.DataLayout()
{
  return &type metadata for BNNS.DataLayout;
}

uint64_t getEnumTagSinglePayload for BNNS.Error(unsigned __int8 *a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0xFD) {
    goto LABEL_17;
  }
  if (a2 + 3 >= 0xFFFF00) {
    int v2 = 4;
  }
  else {
    int v2 = 2;
  }
  if ((a2 + 3) >> 8 < 0xFF) {
    int v3 = 1;
  }
  else {
    int v3 = v2;
  }
  if (v3 == 4)
  {
    int v4 = *(_DWORD *)(a1 + 1);
    if (v4) {
      return (*a1 | (v4 << 8)) - 3;
    }
  }
  else
  {
    if (v3 == 2)
    {
      int v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1)) {
        goto LABEL_17;
      }
      return (*a1 | (v4 << 8)) - 3;
    }
    int v4 = a1[1];
    if (a1[1]) {
      return (*a1 | (v4 << 8)) - 3;
    }
  }
LABEL_17:
  unsigned int v6 = *a1;
  BOOL v7 = v6 >= 4;
  int v8 = v6 - 4;
  if (!v7) {
    int v8 = -1;
  }
  return (v8 + 1);
}

unsigned char *storeEnumTagSinglePayload for BNNS.Error(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 3 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 3) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFD) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFC)
  {
    unsigned int v6 = ((a2 - 253) >> 8) + 1;
    *uint64_t result = a2 + 3;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D209A1D0);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 3;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.Error()
{
  return &type metadata for BNNS.Error;
}

unint64_t specialized static BNNS.calculateBatchStride(size:stride:)(unint64_t result, unint64_t a2, unint64_t a3, unint64_t a4, unint64_t a5, unint64_t a6, unint64_t a7, unint64_t a8, unint64_t a9, unint64_t a10, unint64_t a11, unint64_t a12, unint64_t a13, unint64_t a14, unint64_t a15, unint64_t a16)
{
  uint64_t v16 = a9;
  if (result <= 1) {
    uint64_t v17 = 1;
  }
  else {
    uint64_t v17 = result;
  }
  if (a9 <= 1) {
    uint64_t v16 = 1;
  }
  uint64_t v18 = v17 * v16;
  if ((unsigned __int128)(v17 * (__int128)v16) >> 64 != (v17 * v16) >> 63)
  {
    __break(1u);
LABEL_64:
    __break(1u);
LABEL_65:
    __break(1u);
LABEL_66:
    __break(1u);
LABEL_67:
    __break(1u);
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
LABEL_70:
    __break(1u);
LABEL_71:
    __break(1u);
LABEL_72:
    __break(1u);
LABEL_73:
    __break(1u);
LABEL_74:
    __break(1u);
LABEL_75:
    __break(1u);
LABEL_76:
    __break(1u);
LABEL_77:
    __break(1u);
    return result;
  }
  if (a2 <= 1) {
    uint64_t v19 = 1;
  }
  else {
    uint64_t v19 = a2;
  }
  uint64_t v20 = v18 * v19;
  if ((unsigned __int128)(v18 * (__int128)v19) >> 64 != (v18 * v19) >> 63) {
    goto LABEL_64;
  }
  if (a10 <= 1) {
    uint64_t v21 = 1;
  }
  else {
    uint64_t v21 = a10;
  }
  uint64_t v22 = v20 * v21;
  if ((unsigned __int128)(v20 * (__int128)v21) >> 64 != (v20 * v21) >> 63) {
    goto LABEL_65;
  }
  if (a3 <= 1) {
    uint64_t v23 = 1;
  }
  else {
    uint64_t v23 = a3;
  }
  uint64_t v24 = v22 * v23;
  if ((unsigned __int128)(v22 * (__int128)v23) >> 64 != (v22 * v23) >> 63) {
    goto LABEL_66;
  }
  if (a11 <= 1) {
    uint64_t v25 = 1;
  }
  else {
    uint64_t v25 = a11;
  }
  uint64_t v26 = v24 * v25;
  if ((unsigned __int128)(v24 * (__int128)v25) >> 64 != (v24 * v25) >> 63) {
    goto LABEL_67;
  }
  if (a4 <= 1) {
    uint64_t v27 = 1;
  }
  else {
    uint64_t v27 = a4;
  }
  uint64_t v28 = v26 * v27;
  if ((unsigned __int128)(v26 * (__int128)v27) >> 64 != (v26 * v27) >> 63) {
    goto LABEL_68;
  }
  if (a12 <= 1) {
    uint64_t v29 = 1;
  }
  else {
    uint64_t v29 = a12;
  }
  uint64_t v30 = v28 * v29;
  if ((unsigned __int128)(v28 * (__int128)v29) >> 64 != (v28 * v29) >> 63) {
    goto LABEL_69;
  }
  if (a5 <= 1) {
    uint64_t v31 = 1;
  }
  else {
    uint64_t v31 = a5;
  }
  uint64_t v32 = v30 * v31;
  if ((unsigned __int128)(v30 * (__int128)v31) >> 64 != (v30 * v31) >> 63) {
    goto LABEL_70;
  }
  if (a13 <= 1) {
    uint64_t v33 = 1;
  }
  else {
    uint64_t v33 = a13;
  }
  uint64_t v34 = v32 * v33;
  if ((unsigned __int128)(v32 * (__int128)v33) >> 64 != (v32 * v33) >> 63) {
    goto LABEL_71;
  }
  if (a6 <= 1) {
    uint64_t v35 = 1;
  }
  else {
    uint64_t v35 = a6;
  }
  uint64_t v36 = v34 * v35;
  if ((unsigned __int128)(v34 * (__int128)v35) >> 64 != (v34 * v35) >> 63) {
    goto LABEL_72;
  }
  if (a14 <= 1) {
    uint64_t v37 = 1;
  }
  else {
    uint64_t v37 = a14;
  }
  uint64_t v38 = v36 * v37;
  if ((unsigned __int128)(v36 * (__int128)v37) >> 64 != (v36 * v37) >> 63) {
    goto LABEL_73;
  }
  if (a7 <= 1) {
    uint64_t v39 = 1;
  }
  else {
    uint64_t v39 = a7;
  }
  uint64_t v40 = v38 * v39;
  if ((unsigned __int128)(v38 * (__int128)v39) >> 64 != (v38 * v39) >> 63) {
    goto LABEL_74;
  }
  if (a15 <= 1) {
    uint64_t v41 = 1;
  }
  else {
    uint64_t v41 = a15;
  }
  uint64_t v42 = v40 * v41;
  if ((unsigned __int128)(v40 * (__int128)v41) >> 64 != (v40 * v41) >> 63) {
    goto LABEL_75;
  }
  if (a8 <= 1) {
    uint64_t v43 = 1;
  }
  else {
    uint64_t v43 = a8;
  }
  uint64_t v44 = v42 * v43;
  if ((unsigned __int128)(v42 * (__int128)v43) >> 64 != (v42 * v43) >> 63) {
    goto LABEL_76;
  }
  uint64_t v45 = a16;
  if (a16 <= 1) {
    uint64_t v45 = 1;
  }
  uint64_t result = v44 * v45;
  if ((unsigned __int128)(v44 * (__int128)v45) >> 64 != (v44 * v45) >> 63) {
    goto LABEL_77;
  }
  return result;
}

uint64_t specialized SetAlgebra<>.init(arrayLiteral:)(uint64_t result)
{
  uint64_t v1 = *(void *)(result + 16);
  if (!v1) {
    return 0;
  }
  uint64_t v2 = result;
  LODWORD(result) = 0;
  int v3 = (int *)(v2 + 32);
  do
  {
    int v5 = *v3++;
    int v4 = v5;
    if ((v5 & ~result) == 0) {
      int v4 = 0;
    }
    uint64_t result = v4 | result;
    --v1;
  }
  while (v1);
  return result;
}

double specialized static BNNS.makeArrayDescriptor(shape:data:dataType:)@<D0>(uint64_t a1@<X0>, uint64_t a2@<X1>, int a3@<W2>, uint64_t a4@<X8>)
{
  outlined init with take of BNNS.Shape(a1, (uint64_t)v57);
  outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v58);
  switch(_s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v58))
  {
    case 1u:
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v18 = (uint64_t *)destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *v18;
      *(void *)&long long v19 = v18[1];
      long long v55 = v19;
      long long v53 = 0u;
      long long v54 = 0u;
      int v12 = 131073;
      goto LABEL_27;
    case 2u:
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v20 = (uint64_t *)destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *v20;
      *(void *)&long long v21 = v20[1];
      long long v55 = v21;
      long long v53 = 0u;
      long long v54 = 0u;
      int v12 = 0x20000;
      goto LABEL_27;
    case 3u:
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v22 = (uint64_t *)destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *v22;
      *(void *)&long long v23 = v22[1];
      long long v55 = v23;
      int v12 = 163841;
      goto LABEL_13;
    case 4u:
      int v12 = 163840;
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v24 = (uint64_t *)destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *v24;
      *(void *)&long long v25 = v24[1];
      goto LABEL_11;
    case 5u:
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v26 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *(void *)v26;
      long long v54 = 0u;
      long long v55 = *(_OWORD *)(v26 + 8);
      int v12 = 196608;
      long long v53 = 0u;
      goto LABEL_27;
    case 6u:
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v27 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *(void *)v27;
      long long v55 = *(_OWORD *)(v27 + 8);
      int v12 = 229377;
      goto LABEL_13;
    case 7u:
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v28 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *(void *)v28;
      long long v54 = 0u;
      long long v55 = *(_OWORD *)(v28 + 8);
      int v12 = 229376;
      long long v53 = 0u;
      goto LABEL_27;
    case 8u:
      int v12 = 196609;
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v29 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *(void *)v29;
      long long v25 = *(_OWORD *)(v29 + 8);
LABEL_11:
      long long v55 = v25;
      goto LABEL_13;
    case 9u:
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v30 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *(void *)v30;
      long long v55 = *(_OWORD *)(v30 + 8);
      int v12 = 196610;
LABEL_13:
      long long v53 = 0u;
      long long v54 = 0u;
      goto LABEL_27;
    case 0xAu:
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v31 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *(void *)v31;
      *(void *)&long long v32 = *(void *)(v31 + 24);
      long long v54 = v32;
      long long v55 = *(_OWORD *)(v31 + 8);
      long long v53 = 0u;
      int v12 = 0x40000;
      goto LABEL_27;
    case 0xBu:
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v33 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *(void *)v33;
      *(void *)&long long v34 = *(void *)(v33 + 24);
      long long v54 = v34;
      long long v55 = *(_OWORD *)(v33 + 8);
      int v12 = 294913;
      goto LABEL_19;
    case 0xCu:
      int v12 = 294912;
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v35 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *(void *)v35;
      *(void *)&long long v36 = *(void *)(v35 + 24);
      long long v54 = v36;
      long long v55 = *(_OWORD *)(v35 + 8);
      goto LABEL_19;
    case 0xDu:
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v37 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *(void *)v37;
      long long v54 = *(_OWORD *)(v37 + 24);
      long long v55 = *(_OWORD *)(v37 + 8);
      long long v53 = 0u;
      int v12 = 360449;
      goto LABEL_27;
    case 0xEu:
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v38 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *(void *)v38;
      long long v54 = *(_OWORD *)(v38 + 24);
      long long v55 = *(_OWORD *)(v38 + 8);
      int v12 = 360448;
LABEL_19:
      long long v53 = 0u;
      goto LABEL_27;
    case 0xFu:
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v39 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *(void *)v39;
      long long v54 = *(_OWORD *)(v39 + 24);
      *((void *)&v40 + 1) = *(void *)(v39 + 32);
      long long v55 = *(_OWORD *)(v39 + 8);
      *(void *)&long long v40 = *(void *)(v39 + 40);
      long long v53 = v40;
      int v12 = 425985;
      goto LABEL_27;
    case 0x10u:
      int v12 = 425984;
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v41 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *(void *)v41;
      long long v54 = *(_OWORD *)(v41 + 24);
      *((void *)&v42 + 1) = *(void *)(v41 + 32);
      long long v55 = *(_OWORD *)(v41 + 8);
      *(void *)&long long v42 = *(void *)(v41 + 40);
      long long v53 = v42;
      goto LABEL_27;
    case 0x11u:
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v43 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *(void *)v43;
      long long v54 = *(_OWORD *)(v43 + 24);
      long long v55 = *(_OWORD *)(v43 + 8);
      long long v53 = *(_OWORD *)(v43 + 40);
      int v12 = 491521;
      goto LABEL_27;
    case 0x12u:
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v44 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *(void *)v44;
      long long v54 = *(_OWORD *)(v44 + 24);
      long long v55 = *(_OWORD *)(v44 + 8);
      long long v53 = *(_OWORD *)(v44 + 40);
      int v12 = 491520;
      goto LABEL_27;
    case 0x13u:
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v45 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v9 = *(void *)v45;
      long long v54 = *(_OWORD *)(v45 + 24);
      long long v55 = *(_OWORD *)(v45 + 8);
      long long v53 = *(_OWORD *)(v45 + 40);
      int v12 = 557057;
      goto LABEL_26;
    case 0x14u:
      int v12 = 557056;
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      uint64_t v45 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v9 = *(void *)v45;
      long long v54 = *(_OWORD *)(v45 + 24);
      long long v55 = *(_OWORD *)(v45 + 8);
      long long v53 = *(_OWORD *)(v45 + 40);
LABEL_26:
      uint64_t v8 = *(void *)(v45 + 56);
LABEL_27:
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      switch(_s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v56))
      {
        case 1u:
        case 2u:
        case 3u:
        case 4u:
          long long v10 = *(_OWORD *)(destructiveProjectEnumData for BNNS.ActivationFunction(v56) + 16);
          goto LABEL_36;
        case 5u:
        case 6u:
        case 7u:
        case 8u:
        case 9u:
          uint64_t v46 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
          long long v10 = *(_OWORD *)(v46 + 24);
          *(void *)&long long v15 = *(void *)(v46 + 40);
          goto LABEL_37;
        case 0xAu:
        case 0xBu:
        case 0xCu:
          uint64_t v47 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
          long long v10 = *(_OWORD *)(v47 + 32);
          long long v15 = *(_OWORD *)(v47 + 48);
          goto LABEL_37;
        case 0xDu:
        case 0xEu:
          uint64_t v48 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
          long long v15 = *(_OWORD *)(v48 + 56);
          long long v10 = *(_OWORD *)(v48 + 40);
          *(void *)&long long v16 = *(void *)(v48 + 72);
          goto LABEL_38;
        case 0xFu:
        case 0x10u:
          unint64_t v49 = (_OWORD *)destructiveProjectEnumData for BNNS.ActivationFunction(v56);
          long long v15 = v49[4];
          long long v16 = v49[5];
          long long v10 = v49[3];
          goto LABEL_38;
        case 0x11u:
        case 0x12u:
          uint64_t v50 = destructiveProjectEnumData for BNNS.ActivationFunction(v56);
          long long v16 = *(_OWORD *)(v50 + 88);
          long long v15 = *(_OWORD *)(v50 + 72);
          long long v10 = *(_OWORD *)(v50 + 56);
          *(void *)&long long v17 = *(void *)(v50 + 104);
          goto LABEL_39;
        case 0x13u:
        case 0x14u:
          long long v51 = (_OWORD *)destructiveProjectEnumData for BNNS.ActivationFunction(v56);
          long long v16 = v51[6];
          long long v17 = v51[7];
          long long v10 = v51[4];
          long long v15 = v51[5];
          goto LABEL_39;
        default:
          *(void *)&long long v10 = *(void *)(destructiveProjectEnumData for BNNS.ActivationFunction(v56) + 8);
LABEL_36:
          long long v15 = 0uLL;
LABEL_37:
          long long v16 = 0uLL;
LABEL_38:
          long long v17 = 0uLL;
LABEL_39:
          long long v13 = v54;
          long long v11 = v55;
          long long v14 = v53;
          break;
      }
      break;
    default:
      destructiveProjectEnumData for BNNS.ActivationFunction(v58);
      outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v56);
      BOOL v7 = (uint64_t *)destructiveProjectEnumData for BNNS.ActivationFunction(v56);
      uint64_t v8 = 0;
      uint64_t v9 = *v7;
      *(void *)&long long v10 = v7[1];
      long long v11 = 0uLL;
      int v12 = 0x10000;
      long long v13 = 0uLL;
      long long v14 = 0uLL;
      long long v15 = 0uLL;
      long long v16 = 0uLL;
      long long v17 = 0uLL;
      break;
  }
  *(_DWORD *)a4 = 0;
  *(_DWORD *)(a4 + 4) = v12;
  *(void *)(a4 + 8) = v9;
  *(_OWORD *)(a4 + 16) = v11;
  *(_OWORD *)(a4 + 32) = v13;
  *(_OWORD *)(a4 + 48) = v14;
  *(void *)(a4 + 64) = v8;
  *(_OWORD *)(a4 + 72) = v10;
  *(_OWORD *)(a4 + 88) = v15;
  *(_OWORD *)(a4 + 104) = v16;
  *(_OWORD *)(a4 + 120) = v17;
  *(void *)(a4 + 136) = a2;
  *(_DWORD *)(a4 + 144) = a3;
  *(void *)(a4 + 152) = 0;
  *(_DWORD *)(a4 + 160) = a3;
  *(void *)&double result = 1065353216;
  *(void *)(a4 + 164) = 1065353216;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOg(uint64_t a1)
{
  return *(unsigned __int8 *)(a1 + 128);
}

uint64_t BNNS.PoolingType.bnnsPoolingFunction.getter()
{
  outlined init with take of BNNS.PoolingType(v0, v3);
  outlined init with take of BNNS.PoolingType(v3, v4);
  int v1 = _s10Accelerate4BNNSO11PoolingTypeOWOg((uint64_t)v4);
  uint64_t result = 4;
  switch(v1)
  {
    case 1:
    case 4:
      destructiveProjectEnumData for BNNS.ActivationFunction(v4);
      uint64_t result = 3;
      break;
    case 2:
      if (*(unsigned char *)destructiveProjectEnumData for BNNS.ActivationFunction(v4)) {
        uint64_t result = 1;
      }
      else {
        uint64_t result = 2;
      }
      break;
    case 5:
      return result;
    default:
      destructiveProjectEnumData for BNNS.ActivationFunction(v4);
      uint64_t result = 0;
      break;
  }
  return result;
}

_OWORD *outlined init with take of BNNS.PoolingType(_OWORD *a1, _OWORD *a2)
{
  *a2 = *a1;
  long long v2 = a1[1];
  long long v3 = a1[2];
  long long v4 = a1[4];
  a2[3] = a1[3];
  a2[4] = v4;
  a2[1] = v2;
  a2[2] = v3;
  long long v5 = a1[5];
  long long v6 = a1[6];
  long long v7 = a1[8];
  a2[7] = a1[7];
  a2[8] = v7;
  a2[5] = v5;
  a2[6] = v6;
  long long v8 = a1[9];
  long long v9 = a1[10];
  long long v10 = a1[11];
  *(_OWORD *)((char *)a2 + 185) = *(_OWORD *)((char *)a1 + 185);
  a2[10] = v9;
  a2[11] = v10;
  a2[9] = v8;
  return a2;
}

uint64_t _s10Accelerate4BNNSO11PoolingTypeOWOg(uint64_t a1)
{
  if (*(unsigned __int8 *)(a1 + 200) <= 4u) {
    return *(unsigned __int8 *)(a1 + 200);
  }
  else {
    return (*(_DWORD *)a1 + 5);
  }
}

uint64_t BNNS.PoolingLayer.__allocating_init(type:input:output:bias:activation:kernelSize:stride:padding:filterParameters:)(_OWORD *a1, long long *a2, long long *a3, uint64_t a4, uint64_t *a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t *a10, int a11, uint64_t a12, uint64_t a13, uint64_t a14)
{
  uint64_t v179 = *MEMORY[0x1E4F143B8];
  long long v16 = a3[9];
  long long v164 = a3[8];
  long long v165 = v16;
  long long v17 = a3[5];
  long long v160 = a3[4];
  long long v161 = v17;
  long long v18 = a3[7];
  long long v162 = a3[6];
  long long v163 = v18;
  long long v19 = a3[1];
  long long v156 = *a3;
  long long v157 = v19;
  long long v20 = a3[3];
  long long v158 = a3[2];
  long long v159 = v20;
  long long v21 = a2[8];
  long long v22 = a2[9];
  long long v23 = a2[6];
  long long v174 = a2[7];
  long long v175 = v21;
  long long v24 = a2[10];
  long long v176 = v22;
  long long v177 = v24;
  long long v25 = a2[4];
  long long v26 = a2[5];
  long long v27 = a2[2];
  long long v170 = a2[3];
  long long v171 = v25;
  long long v28 = a3[10];
  long long v172 = v26;
  long long v173 = v23;
  long long v29 = *a2;
  long long v30 = a2[1];
  long long v166 = v28;
  long long v167 = v29;
  long long v168 = v30;
  long long v169 = v27;
  outlined init with take of BNNS.PoolingType(a1, v178);
  if (*((unsigned char *)a10 + 32) == 1)
  {
    uint64_t v86 = a10[2];
    uint64_t v87 = a10[3];
    uint64_t v84 = *a10;
    uint64_t v85 = a10[1];
    uint64_t v88 = 0;
    uint64_t v89 = 0;
  }
  else
  {
    uint64_t v88 = *a10;
    uint64_t v89 = a10[1];
    uint64_t v86 = 0;
    uint64_t v87 = 0;
    uint64_t v84 = 0;
    uint64_t v85 = 0;
  }
  uint64_t v31 = *a5;
  char v32 = *((unsigned char *)a5 + 8);
  outlined init with take of BNNS.PoolingType(v178, v144);
  switch(_s10Accelerate4BNNSO11PoolingTypeOWOg((uint64_t)v144))
  {
    case 2u:
      destructiveProjectEnumData for BNNS.ActivationFunction(v144);
      goto LABEL_7;
    case 3u:
      long long v34 = (_OWORD *)destructiveProjectEnumData for BNNS.ActivationFunction(v144);
      uint64_t v62 = *((void *)v34 + 24);
      uint64_t v63 = *((void *)v34 + 23);
      goto LABEL_10;
    case 4u:
      uint64_t v35 = destructiveProjectEnumData for BNNS.ActivationFunction(v144);
      uint64_t v62 = *(void *)(v35 + 184);
      uint64_t v63 = *(void *)(v35 + 176);
      long long v37 = *(_OWORD *)(v35 + 16);
      long long v36 = *(_OWORD *)(v35 + 32);
      __dst[0] = *(_OWORD *)v35;
      __dst[1] = v37;
      __dst[2] = v36;
      long long v38 = *(_OWORD *)(v35 + 96);
      long long v40 = *(_OWORD *)(v35 + 48);
      long long v39 = *(_OWORD *)(v35 + 64);
      __dst[5] = *(_OWORD *)(v35 + 80);
      __dst[6] = v38;
      __dst[3] = v40;
      __dst[4] = v39;
      long long v41 = *(_OWORD *)(v35 + 160);
      long long v43 = *(_OWORD *)(v35 + 112);
      long long v42 = *(_OWORD *)(v35 + 128);
      __dst[9] = *(_OWORD *)(v35 + 144);
      __dst[10] = v41;
      __dst[7] = v43;
      __dst[8] = v42;
      _sSo21BNNSNDArrayDescriptoraSgWOi_((uint64_t)__dst);
      long long v34 = __dst;
LABEL_10:
      outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v34, (uint64_t)v143, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      goto LABEL_11;
    case 5u:
LABEL_7:
      _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)__dst);
      outlined init with take of BNNSNDArrayDescriptor?((uint64_t)__dst, (uint64_t)v143, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      uint64_t v62 = 0;
      uint64_t v63 = 0;
LABEL_11:
      uint64_t v59 = 0;
      uint64_t v60 = 0;
      char v61 = 1;
      break;
    default:
      uint64_t v33 = destructiveProjectEnumData for BNNS.ActivationFunction(v144);
      uint64_t v62 = *(void *)(v33 + 32);
      uint64_t v63 = *(void *)(v33 + 24);
      char v61 = *(unsigned char *)(v33 + 16);
      uint64_t v59 = *(void *)v33;
      uint64_t v60 = *(void *)(v33 + 8);
      _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)__dst);
      outlined init with take of BNNSNDArrayDescriptor?((uint64_t)__dst, (uint64_t)v143, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      break;
  }
  outlined init with take of BNNSNDArrayDescriptor?(a4, (uint64_t)v152, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v152) == 1)
  {
    uint64_t v82 = 0;
    uint64_t v83 = 0;
    uint64_t v79 = 0;
    uint64_t v80 = 0;
    uint64_t v77 = 0;
    uint64_t v78 = 0;
    uint64_t v75 = 0;
    uint64_t v76 = 0;
    uint64_t v73 = 0;
    uint64_t v74 = 0;
    uint64_t v71 = 0;
    uint64_t v72 = 0;
    uint64_t v69 = 0;
    uint64_t v70 = 0;
    uint64_t v67 = 0;
    uint64_t v68 = 0;
    uint64_t v65 = 0;
    uint64_t v66 = 0;
    int v44 = 0;
    uint64_t v81 = 0;
    uint64_t v45 = 0;
    uint64_t v64 = 0;
  }
  else
  {
    uint64_t v81 = v152[0];
    uint64_t v82 = v152[2];
    uint64_t v83 = v152[1];
    uint64_t v79 = v152[4];
    uint64_t v80 = v152[3];
    uint64_t v71 = v152[8];
    uint64_t v77 = v152[9];
    uint64_t v78 = v152[5];
    uint64_t v75 = v152[10];
    uint64_t v76 = v152[6];
    uint64_t v73 = v152[11];
    uint64_t v74 = v152[7];
    uint64_t v72 = v152[12];
    uint64_t v69 = v152[14];
    uint64_t v70 = v152[13];
    uint64_t v67 = v152[16];
    uint64_t v68 = v152[15];
    uint64_t v64 = v152[18];
    uint64_t v65 = v152[19];
    uint64_t v66 = v152[17];
    uint64_t v45 = v154;
    int v44 = v153;
    int v58 = v155;
  }
  *(void *)&__src[0] = v31;
  BYTE8(__src[0]) = v32;
  BNNS.ActivationFunction.bnnsActivation.getter((uint64_t)&v145);
  uint64_t v46 = v146;
  uint64_t v47 = v147;
  int v48 = v145;
  int v49 = v148;
  uint64_t v50 = v149;
  uint64_t v51 = v150;
  uint64_t v52 = v151;
  outlined init with take of BNNS.PoolingType(v178, v142);
  int v53 = 4;
  switch(_s10Accelerate4BNNSO11PoolingTypeOWOg((uint64_t)v142))
  {
    case 1u:
    case 4u:
      destructiveProjectEnumData for BNNS.ActivationFunction(v142);
      int v53 = 3;
      break;
    case 2u:
      if (*(unsigned char *)destructiveProjectEnumData for BNNS.ActivationFunction(v142)) {
        int v53 = 1;
      }
      else {
        int v53 = 2;
      }
      break;
    case 5u:
      break;
    default:
      destructiveProjectEnumData for BNNS.ActivationFunction(v142);
      int v53 = 0;
      break;
  }
  __src[8] = v175;
  __src[9] = v176;
  __src[4] = v171;
  __src[5] = v172;
  __src[6] = v173;
  __src[7] = v174;
  __src[0] = v167;
  __src[1] = v168;
  __src[2] = v169;
  __src[3] = v170;
  __src[18] = v163;
  __src[19] = v164;
  __src[20] = v165;
  __src[21] = v166;
  __src[14] = v159;
  __src[15] = v160;
  __src[16] = v161;
  __src[17] = v162;
  __src[10] = v177;
  __src[11] = v156;
  __src[12] = v157;
  __src[13] = v158;
  memcpy(__dst, __src, sizeof(__dst));
  uint64_t v100 = v83;
  uint64_t v101 = v82;
  uint64_t v102 = v80;
  uint64_t v103 = v79;
  uint64_t v104 = v78;
  uint64_t v105 = v76;
  uint64_t v106 = v74;
  uint64_t v107 = v71;
  uint64_t v99 = v81;
  uint64_t v108 = v77;
  uint64_t v109 = v75;
  uint64_t v110 = v73;
  uint64_t v111 = v72;
  uint64_t v112 = v70;
  uint64_t v113 = v69;
  uint64_t v114 = v68;
  uint64_t v115 = v67;
  uint64_t v116 = v66;
  uint64_t v117 = v64;
  uint64_t v118 = v65;
  int v119 = v44;
  uint64_t v120 = v45;
  int v121 = v58;
  int v122 = v48;
  uint64_t v123 = v46;
  uint64_t v124 = v47;
  int v125 = v49;
  uint64_t v126 = v50;
  uint64_t v127 = v51;
  uint64_t v128 = v52;
  int v129 = v53;
  uint64_t v130 = a6;
  uint64_t v131 = a7;
  uint64_t v132 = a8;
  uint64_t v133 = a9;
  uint64_t v134 = v63;
  uint64_t v135 = v62;
  uint64_t v136 = v88;
  uint64_t v137 = v89;
  uint64_t v138 = v84;
  uint64_t v139 = v85;
  uint64_t v140 = v86;
  uint64_t v141 = v87;
  if (a13 == 1)
  {
    long long v54 = 0;
  }
  else
  {
    int v93 = a11;
    uint64_t v94 = a12;
    uint64_t v95 = a13;
    uint64_t v96 = a14;
    long long v54 = &v93;
  }
  uint64_t v55 = MEMORY[0x1D2600060](__dst, v54);
  type metadata accessor for BNNS.PoolingLayer();
  uint64_t v56 = swift_allocObject();
  *(void *)(v56 + 24) = 0;
  *(void *)(v56 + 32) = 0;
  *(unsigned char *)(v56 + 40) = 1;
  _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&v93);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v93, v56 + 48, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  *(void *)(v56 + 232) = 0;
  if (v55)
  {
    *(void *)(v56 + 16) = v55;
    *(void *)(v56 + 24) = v59;
    *(void *)(v56 + 32) = v60;
    *(unsigned char *)(v56 + 40) = v61;
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v143, v56 + 48, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v56;
}

size_t BNNS.PoolingLayer.apply(batchSize:input:output:)(int64_t a1, void *a2, uint64_t a3)
{
  return specialized static BNNS.poolingFilterApply(_:batchSize:input:output:)(v3, a1, a2, a3);
}

size_t BNNS.PoolingLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingBiasGradient:)(uint64_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, _OWORD *a5, uint64_t a6)
{
  return specialized static BNNS.poolingLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingBiasGradient:)(v6, a1, a2, a3, a4, a5, a6);
}

uint64_t BNNS.PoolingLayer.__allocating_init(bnnsFilter:)(uint64_t a1)
{
  uint64_t v2 = swift_allocObject();
  *(void *)(v2 + 24) = 0;
  *(void *)(v2 + 32) = 0;
  *(unsigned char *)(v2 + 40) = 1;
  _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)v4);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v4, v2 + 48, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  *(void *)(v2 + 232) = 0;
  if (a1)
  {
    *(void *)(v2 + 16) = a1;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v2;
}

uint64_t _sSo21BNNSNDArrayDescriptoraSgWOi_(uint64_t result)
{
  *(unsigned char *)(result + 176) = 0;
  return result;
}

uint64_t _sSo21BNNSNDArrayDescriptoraSgWOg(uint64_t a1)
{
  if (*(unsigned char *)(a1 + 176)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t type metadata accessor for BNNS.PoolingLayer()
{
  return self;
}

double _sSo21BNNSNDArrayDescriptoraSgWOi0_(uint64_t a1)
{
  double result = 0.0;
  *(_OWORD *)(a1 + 144) = 0u;
  *(_OWORD *)(a1 + 160) = 0u;
  *(_OWORD *)(a1 + 112) = 0u;
  *(_OWORD *)(a1 + 128) = 0u;
  *(_OWORD *)(a1 + 80) = 0u;
  *(_OWORD *)(a1 + 96) = 0u;
  *(_OWORD *)(a1 + 48) = 0u;
  *(_OWORD *)(a1 + 64) = 0u;
  *(_OWORD *)(a1 + 16) = 0u;
  *(_OWORD *)(a1 + 32) = 0u;
  *(_OWORD *)a1 = 0u;
  *(unsigned char *)(a1 + 176) = 1;
  return result;
}

uint64_t BNNS.PoolingLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.PoolingLayer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

size_t closure #1 in closure #1 in closure #1 in static BNNS.poolingLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingBiasGradient:)(BNNSNDArrayDescriptor *a1, uint64_t a2, uint64_t a3, uint64_t a4, BNNSNDArrayDescriptor *a5, uint64_t a6, uint64_t a7, BNNSNDArrayDescriptor *a8)
{
  outlined init with take of BNNSNDArrayDescriptor?(a2 + 48, (uint64_t)v156, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v156, (uint64_t)v157, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  size_t result = _sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v157);
  size_t v99 = a3;
  uint64_t v100 = a5;
  uint64_t v97 = a1;
  out_delta = a8;
  if (result != 1)
  {
    v153[8] = indices;
    v153[9] = indices_data_type;
    v153[10] = v160;
    v153[4] = v157[4];
    v153[5] = v157[5];
    v153[7] = v157[7];
    v153[6] = v157[6];
    v153[0] = v157[0];
    v153[1] = v157[1];
    v153[3] = v157[3];
    v153[2] = v157[2];
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v149);
    outlined init with take of BNNS.Shape((uint64_t)v149, (uint64_t)v150);
    outlined init with take of BNNS.Shape((uint64_t)v150, (uint64_t)&v141);
    BNNS.Shape.size.getter((uint64_t)&v136);
    long long v16 = v136;
    long long v17 = v137;
    long long v18 = v138;
    unint64_t v19 = v139;
    unint64_t v20 = v140;
    outlined init with take of BNNS.Shape((uint64_t)v150, (uint64_t)&v141);
    BNNS.Shape.stride.getter((uint64_t)&v136);
    size_t v91 = specialized static BNNS.calculateBatchStride(size:stride:)(v16, *((unint64_t *)&v16 + 1), v17, *((unint64_t *)&v17 + 1), v18, *((unint64_t *)&v18 + 1), v19, v20, v136, *((unint64_t *)&v136 + 1), v137, *((unint64_t *)&v137 + 1), v138, *((unint64_t *)&v138 + 1), v139, v140);
    filter = *(void **)(a2 + 16);
    outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v152, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v152, (uint64_t)&v154, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
    uint64_t v89 = v154;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v136);
    outlined init with take of BNNS.Shape((uint64_t)&v136, (uint64_t)&v141);
    outlined init with take of BNNS.Shape((uint64_t)&v141, (uint64_t)v149);
    BNNS.Shape.size.getter((uint64_t)&v128);
    unint64_t v21 = v128;
    unint64_t v22 = v129;
    unint64_t v23 = v130;
    unint64_t v24 = v131;
    unint64_t v25 = v132;
    unint64_t v26 = v133;
    unint64_t v27 = v134;
    unint64_t v28 = v135;
    outlined init with take of BNNS.Shape((uint64_t)&v141, (uint64_t)v149);
    BNNS.Shape.stride.getter((uint64_t)&v128);
    size_t v95 = specialized static BNNS.calculateBatchStride(size:stride:)(v21, v22, v23, v24, v25, v26, v27, v28, v128, v129, v130, v131, v132, v133, v134, v135);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v128);
    outlined init with take of BNNS.Shape((uint64_t)&v128, (uint64_t)v149);
    outlined init with take of BNNS.Shape((uint64_t)v149, (uint64_t)v127);
    BNNS.Shape.size.getter((uint64_t)&v119);
    unint64_t v29 = v119;
    unint64_t v30 = v120;
    unint64_t v31 = v121;
    unint64_t v32 = v122;
    unint64_t v33 = v123;
    unint64_t v34 = v124;
    unint64_t v35 = v125;
    unint64_t v36 = v126;
    outlined init with take of BNNS.Shape((uint64_t)v149, (uint64_t)v127);
    BNNS.Shape.stride.getter((uint64_t)&v119);
    size_t v101 = specialized static BNNS.calculateBatchStride(size:stride:)(v29, v30, v31, v32, v33, v34, v35, v36, v119, v120, v121, v122, v123, v124, v125, v126);
    outlined init with take of BNNSNDArrayDescriptor?(a7 + 136, (uint64_t)v151, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v151, (uint64_t)&v155, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
    out = v155;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v118);
    outlined init with take of BNNS.Shape((uint64_t)v118, (uint64_t)&v119);
    outlined init with take of BNNS.Shape((uint64_t)&v119, (uint64_t)v127);
    BNNS.Shape.size.getter((uint64_t)&v110);
    unint64_t v37 = v110;
    unint64_t v38 = v111;
    unint64_t v39 = v112;
    unint64_t v40 = v113;
    unint64_t v41 = v114;
    unint64_t v42 = v115;
    unint64_t v44 = v116;
    unint64_t v43 = v117;
    outlined init with take of BNNS.Shape((uint64_t)&v119, (uint64_t)v127);
    BNNS.Shape.stride.getter((uint64_t)&v110);
    size_t v45 = specialized static BNNS.calculateBatchStride(size:stride:)(v37, v38, v39, v40, v41, v42, v44, v43, v110, v111, v112, v113, v114, v115, v116, v117);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v110);
    outlined init with take of BNNS.Shape((uint64_t)&v110, (uint64_t)v127);
    outlined init with take of BNNS.Shape((uint64_t)v127, (uint64_t)v109);
    BNNS.Shape.size.getter((uint64_t)&v104);
    long long v46 = v104;
    long long v47 = v105;
    long long v48 = v106;
    unint64_t v50 = v107;
    unint64_t v49 = v108;
    outlined init with take of BNNS.Shape((uint64_t)v127, (uint64_t)v109);
    BNNS.Shape.stride.getter((uint64_t)&v104);
    size_t result = specialized static BNNS.calculateBatchStride(size:stride:)(v46, *((unint64_t *)&v46 + 1), v47, *((unint64_t *)&v47 + 1), v48, *((unint64_t *)&v48 + 1), v50, v49, v104, *((unint64_t *)&v104 + 1), v105, *((unint64_t *)&v105 + 1), v106, *((unint64_t *)&v106 + 1), v107, v108);
    if (*((void *)&indices + 1)) {
      return BNNSPoolingFilterApplyBackwardBatchEx(filter, v99, v89, v95, v100, v101, out, v45, out_delta, result, v97, (const BNNSDataType)indices_data_type, *((const void **)&indices + 1), v91);
    }
    goto LABEL_16;
  }
  if (*(unsigned char *)(a2 + 40)) {
    uint64_t v15 = 1;
  }
  else {
    uint64_t v15 = *(void *)(a2 + 32);
  }
  if (!a3)
  {
    __break(1u);
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
    return result;
  }
  if (v15 == 0x8000000000000000 && a3 == -1) {
    goto LABEL_15;
  }
  size_t filtera = v15 / a3;
  uint64_t v96 = *(void **)(a2 + 16);
  outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)&v155, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v155, (uint64_t)&v104, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  long long v92 = (const void *)v104;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v149);
  outlined init with take of BNNS.Shape((uint64_t)v149, (uint64_t)v150);
  outlined init with take of BNNS.Shape((uint64_t)v150, (uint64_t)v153);
  BNNS.Shape.size.getter((uint64_t)&v141);
  unint64_t v51 = v141;
  unint64_t v52 = v142;
  unint64_t v53 = v143;
  unint64_t v54 = v144;
  unint64_t v55 = v145;
  unint64_t v56 = v146;
  uint64_t v90 = a2;
  unint64_t v57 = v147;
  unint64_t v58 = v148;
  outlined init with take of BNNS.Shape((uint64_t)v150, (uint64_t)v153);
  BNNS.Shape.stride.getter((uint64_t)&v141);
  outa = (void *)specialized static BNNS.calculateBatchStride(size:stride:)(v51, v52, v53, v54, v55, v56, v57, v58, v141, v142, v143, v144, v145, v146, v147, v148);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v141);
  outlined init with take of BNNS.Shape((uint64_t)&v141, (uint64_t)v153);
  outlined init with take of BNNS.Shape((uint64_t)v153, (uint64_t)&v136);
  BNNS.Shape.size.getter((uint64_t)&v128);
  unint64_t v59 = v128;
  unint64_t v60 = v129;
  unint64_t v61 = v130;
  unint64_t v62 = v131;
  unint64_t v63 = v132;
  unint64_t v64 = v133;
  unint64_t v65 = v134;
  unint64_t v66 = v135;
  outlined init with take of BNNS.Shape((uint64_t)v153, (uint64_t)&v136);
  BNNS.Shape.stride.getter((uint64_t)&v128);
  size_t v102 = specialized static BNNS.calculateBatchStride(size:stride:)(v59, v60, v61, v62, v63, v64, v65, v66, v128, v129, v130, v131, v132, v133, v134, v135);
  outlined init with take of BNNSNDArrayDescriptor?(a7 + 136, (uint64_t)&v154, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v154, (uint64_t)v109, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  uint64_t v86 = (const void *)v109[0];
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v127);
  outlined init with take of BNNS.Shape((uint64_t)v127, (uint64_t)&v128);
  outlined init with take of BNNS.Shape((uint64_t)&v128, (uint64_t)&v136);
  BNNS.Shape.size.getter((uint64_t)&v119);
  unint64_t v67 = v119;
  unint64_t v68 = v120;
  unint64_t v69 = v121;
  unint64_t v70 = v122;
  unint64_t v71 = v123;
  unint64_t v72 = v124;
  unint64_t v73 = v125;
  unint64_t v74 = v126;
  outlined init with take of BNNS.Shape((uint64_t)&v128, (uint64_t)&v136);
  BNNS.Shape.stride.getter((uint64_t)&v119);
  size_t v75 = specialized static BNNS.calculateBatchStride(size:stride:)(v67, v68, v69, v70, v71, v72, v73, v74, v119, v120, v121, v122, v123, v124, v125, v126);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v119);
  outlined init with take of BNNS.Shape((uint64_t)&v119, (uint64_t)&v136);
  outlined init with take of BNNS.Shape((uint64_t)&v136, (uint64_t)v118);
  BNNS.Shape.size.getter((uint64_t)&v110);
  unint64_t v76 = v110;
  unint64_t v77 = v111;
  unint64_t v78 = v112;
  unint64_t v79 = v113;
  unint64_t v80 = v114;
  unint64_t v81 = v115;
  unint64_t v83 = v116;
  unint64_t v82 = v117;
  outlined init with take of BNNS.Shape((uint64_t)&v136, (uint64_t)v118);
  BNNS.Shape.stride.getter((uint64_t)&v110);
  size_t v84 = specialized static BNNS.calculateBatchStride(size:stride:)(v76, v77, v78, v79, v80, v81, v83, v82, v110, v111, v112, v113, v114, v115, v116, v117);
  uint64_t v85 = *(const size_t **)(v90 + 24);
  if (*(unsigned char *)(v90 + 40) & 1 | (v85 == 0)) {
    uint64_t v85 = 0;
  }
  return BNNSPoolingFilterApplyBackwardBatch(v96, v99, v92, (size_t)outa, v100, v102, v86, v75, out_delta, v84, v97, v85, filtera);
}

size_t specialized static BNNS.poolingFilterApply(_:batchSize:input:output:)(size_t a1, int64_t a2, void *a3, uint64_t a4)
{
  outlined init with take of BNNSNDArrayDescriptor?(a1 + 48, (uint64_t)v102, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v102, (uint64_t)v103, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  size_t result = _sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v103);
  int64_t v107 = a2;
  size_t in_stride = a1;
  uint64_t v56 = a4;
  if (result != 1)
  {
    long long v95 = v104;
    long long v96 = indices_data_type;
    long long v97 = v106;
    long long v91 = v103[4];
    long long v92 = v103[5];
    long long v94 = v103[7];
    long long v93 = v103[6];
    long long v87 = v103[0];
    long long v88 = v103[1];
    long long v90 = v103[3];
    long long v89 = v103[2];
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v81);
    outlined init with take of BNNS.Shape((uint64_t)v81, (uint64_t)v82);
    outlined init with take of BNNS.Shape((uint64_t)v82, (uint64_t)v80);
    BNNS.Shape.size.getter((uint64_t)&v75);
    long long v10 = v75;
    long long v11 = v76;
    unint64_t v12 = *((void *)&v77 + 1);
    unint64_t v47 = v77;
    outb = a3;
    unint64_t v13 = v78;
    unint64_t v14 = v79;
    outlined init with take of BNNS.Shape((uint64_t)v82, (uint64_t)v80);
    BNNS.Shape.stride.getter((uint64_t)&v75);
    unint64_t v15 = specialized static BNNS.calculateBatchStride(size:stride:)(v10, *((unint64_t *)&v10 + 1), v11, *((unint64_t *)&v11 + 1), v47, v12, v13, v14, v75, *((unint64_t *)&v75 + 1), v76, *((unint64_t *)&v76 + 1), v77, *((unint64_t *)&v77 + 1), v78, v79);
    unint64_t v44 = *(void **)(in_stride + 16);
    size_t idx_stride = v15;
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)outb + 136, (uint64_t)v84, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
    size_t result = outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v84, (uint64_t)&v100, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
    in = v100;
    if (v100)
    {
      BNNSNDArrayDescriptor.shape.getter((uint64_t)v80);
      outlined init with take of BNNS.Shape((uint64_t)v80, (uint64_t)v81);
      outlined init with take of BNNS.Shape((uint64_t)v81, (uint64_t)&v75);
      BNNS.Shape.size.getter((uint64_t)&v67);
      unint64_t v16 = v67;
      size_t in_strideb = v68;
      unint64_t v17 = v69;
      unint64_t v18 = v70;
      unint64_t v20 = v71;
      unint64_t v19 = v72;
      unint64_t v21 = v73;
      unint64_t v22 = v74;
      outlined init with take of BNNS.Shape((uint64_t)v81, (uint64_t)&v75);
      BNNS.Shape.stride.getter((uint64_t)&v67);
      unint64_t in_stridea = specialized static BNNS.calculateBatchStride(size:stride:)(v16, in_strideb, v17, v18, v20, v19, v21, v22, v67, v68, v69, v70, v71, v72, v73, v74);
      outlined init with take of BNNSNDArrayDescriptor?(v56 + 136, (uint64_t)v83, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
      size_t result = outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v83, (uint64_t)&v101, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
      out = v101;
      if (v101)
      {
        BNNSNDArrayDescriptor.shape.getter((uint64_t)&v67);
        outlined init with take of BNNS.Shape((uint64_t)&v67, (uint64_t)&v75);
        outlined init with take of BNNS.Shape((uint64_t)&v75, (uint64_t)&v62);
        BNNS.Shape.size.getter((uint64_t)&v57);
        long long v23 = v57;
        long long v24 = v58;
        long long v25 = v59;
        unint64_t v26 = v60;
        unint64_t v27 = v61;
        outlined init with take of BNNS.Shape((uint64_t)&v75, (uint64_t)&v62);
        BNNS.Shape.stride.getter((uint64_t)&v57);
        size_t result = specialized static BNNS.calculateBatchStride(size:stride:)(v23, *((unint64_t *)&v23 + 1), v24, *((unint64_t *)&v24 + 1), v25, *((unint64_t *)&v25 + 1), v26, v27, v57, *((unint64_t *)&v57 + 1), v58, *((unint64_t *)&v58 + 1), v59, *((unint64_t *)&v59 + 1), v60, v61);
        if (*((void *)&v104 + 1))
        {
          size_t result = BNNSPoolingFilterApplyBatchEx(v44, v107, in, in_stridea, out, result, (const BNNSDataType)indices_data_type, *((void **)&v104 + 1), idx_stride);
          if (!result) {
            return result;
          }
LABEL_8:
          lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
          swift_allocError();
          unsigned char *v28 = 0;
          return swift_willThrow();
        }
        goto LABEL_24;
      }
LABEL_23:
      __break(1u);
LABEL_24:
      __break(1u);
      goto LABEL_25;
    }
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (*(unsigned char *)(a1 + 40)) {
    uint64_t v9 = 1;
  }
  else {
    uint64_t v9 = *(void *)(a1 + 32);
  }
  if (!a2)
  {
    __break(1u);
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (v9 == 0x8000000000000000 && a2 == -1) {
    goto LABEL_21;
  }
  int64_t v45 = v9;
  outa = *(void **)(a1 + 16);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)a3 + 136, (uint64_t)v86, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  size_t result = outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v86, (uint64_t)&v98, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  unint64_t v49 = v98;
  if (!v98)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v81);
  outlined init with take of BNNS.Shape((uint64_t)v81, (uint64_t)v82);
  outlined init with take of BNNS.Shape((uint64_t)v82, (uint64_t)v80);
  BNNS.Shape.size.getter((uint64_t)&v75);
  long long v29 = v75;
  long long v30 = v76;
  long long v31 = v77;
  unint64_t v32 = v78;
  unint64_t v33 = v79;
  outlined init with take of BNNS.Shape((uint64_t)v82, (uint64_t)v80);
  BNNS.Shape.stride.getter((uint64_t)&v75);
  ina = (void *)specialized static BNNS.calculateBatchStride(size:stride:)(v29, *((unint64_t *)&v29 + 1), v30, *((unint64_t *)&v30 + 1), v31, *((unint64_t *)&v31 + 1), v32, v33, v75, *((unint64_t *)&v75 + 1), v76, *((unint64_t *)&v76 + 1), v77, *((unint64_t *)&v77 + 1), v78, v79);
  outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v85, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  size_t result = outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v85, (uint64_t)&v99, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  unint64_t v41 = v99;
  if (!v99)
  {
LABEL_26:
    __break(1u);
    return result;
  }
  uint64_t v46 = v45 / v107;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v75);
  outlined init with take of BNNS.Shape((uint64_t)&v75, (uint64_t)v80);
  outlined init with take of BNNS.Shape((uint64_t)v80, (uint64_t)&v67);
  BNNS.Shape.size.getter((uint64_t)&v62);
  long long v34 = v62;
  long long v35 = v63;
  long long v36 = v64;
  unint64_t v37 = v65;
  unint64_t v38 = v66;
  outlined init with take of BNNS.Shape((uint64_t)v80, (uint64_t)&v67);
  BNNS.Shape.stride.getter((uint64_t)&v62);
  size_t v39 = specialized static BNNS.calculateBatchStride(size:stride:)(v34, *((unint64_t *)&v34 + 1), v35, *((unint64_t *)&v35 + 1), v36, *((unint64_t *)&v36 + 1), v37, v38, v62, *((unint64_t *)&v62 + 1), v63, *((unint64_t *)&v63 + 1), v64, *((unint64_t *)&v64 + 1), v65, v66);
  if (*(unsigned char *)(a1 + 40)) {
    unint64_t v40 = 0;
  }
  else {
    unint64_t v40 = *(size_t **)(a1 + 24);
  }
  size_t result = BNNSPoolingFilterApplyBatch(outa, v107, v49, (size_t)ina, v41, v39, v40, v46);
  if (result) {
    goto LABEL_8;
  }
  return result;
}

size_t specialized static BNNS.poolingLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingBiasGradient:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _OWORD *a5, _OWORD *a6, uint64_t a7)
{
  uint64_t v29 = *MEMORY[0x1E4F143B8];
  long long v12 = a6[9];
  *(_OWORD *)&v27.stride[7] = a6[8];
  *(_OWORD *)&v27.data_type = v12;
  *(_OWORD *)&v27.table_data_type = a6[10];
  long long v13 = a6[5];
  *(_OWORD *)&v27.size[7] = a6[4];
  *(_OWORD *)&v27.stride[1] = v13;
  long long v14 = a6[7];
  *(_OWORD *)&v27.stride[3] = a6[6];
  *(_OWORD *)&v27.stride[5] = v14;
  long long v15 = a6[1];
  *(_OWORD *)&v27.flags = *a6;
  *(_OWORD *)&v27.size[1] = v15;
  long long v16 = a6[3];
  *(_OWORD *)&v27.size[3] = a6[2];
  *(_OWORD *)&v27.size[5] = v16;
  long long v17 = a5[9];
  *(_OWORD *)&v26.stride[7] = a5[8];
  *(_OWORD *)&v26.data_type = v17;
  *(_OWORD *)&v26.table_data_type = a5[10];
  long long v18 = a5[5];
  *(_OWORD *)&v26.size[7] = a5[4];
  *(_OWORD *)&v26.stride[1] = v18;
  long long v19 = a5[7];
  *(_OWORD *)&v26.stride[3] = a5[6];
  *(_OWORD *)&v26.stride[5] = v19;
  long long v20 = a5[1];
  *(_OWORD *)&v26.flags = *a5;
  *(_OWORD *)&v26.size[1] = v20;
  long long v21 = a5[3];
  *(_OWORD *)&v26.size[3] = a5[2];
  *(_OWORD *)&v26.size[5] = v21;
  outlined init with take of BNNSNDArrayDescriptor?(a7, (uint64_t)v28, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v28) == 1)
  {
    unint64_t v22 = 0;
  }
  else
  {
    v25[8] = v28[8];
    v25[9] = v28[9];
    v25[10] = v28[10];
    v25[4] = v28[4];
    v25[5] = v28[5];
    v25[6] = v28[6];
    v25[7] = v28[7];
    v25[0] = v28[0];
    v25[1] = v28[1];
    v25[2] = v28[2];
    v25[3] = v28[3];
    unint64_t v22 = (BNNSNDArrayDescriptor *)v25;
  }
  size_t result = closure #1 in closure #1 in closure #1 in static BNNS.poolingLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingBiasGradient:)(v22, a1, a2, a3, &v27, (uint64_t)a6, a4, &v26);
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v24 = 0;
    return swift_willThrow();
  }
  return result;
}

__n128 __swift_memcpy201_8(uint64_t a1, uint64_t a2)
{
  *(_OWORD *)a1 = *(_OWORD *)a2;
  long long v2 = *(_OWORD *)(a2 + 16);
  long long v3 = *(_OWORD *)(a2 + 32);
  long long v4 = *(_OWORD *)(a2 + 64);
  *(_OWORD *)(a1 + 48) = *(_OWORD *)(a2 + 48);
  *(_OWORD *)(a1 + 64) = v4;
  *(_OWORD *)(a1 + 16) = v2;
  *(_OWORD *)(a1 + 32) = v3;
  long long v5 = *(_OWORD *)(a2 + 80);
  long long v6 = *(_OWORD *)(a2 + 96);
  long long v7 = *(_OWORD *)(a2 + 128);
  *(_OWORD *)(a1 + 112) = *(_OWORD *)(a2 + 112);
  *(_OWORD *)(a1 + 128) = v7;
  *(_OWORD *)(a1 + 80) = v5;
  *(_OWORD *)(a1 + 96) = v6;
  __n128 result = *(__n128 *)(a2 + 144);
  long long v9 = *(_OWORD *)(a2 + 160);
  long long v10 = *(_OWORD *)(a2 + 176);
  *(_OWORD *)(a1 + 185) = *(_OWORD *)(a2 + 185);
  *(_OWORD *)(a1 + 160) = v9;
  *(_OWORD *)(a1 + 176) = v10;
  *(__n128 *)(a1 + 144) = result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.PoolingType(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFB && *(unsigned char *)(a1 + 201)) {
    return (*(_DWORD *)a1 + 251);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 200);
  if (v3 <= 5) {
    int v4 = -1;
  }
  else {
    int v4 = v3 ^ 0xFF;
  }
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.PoolingType(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFA)
  {
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(unsigned char *)(result + 200) = 0;
    *(void *)__n128 result = a2 - 251;
    if (a3 >= 0xFB) {
      *(unsigned char *)(result + 201) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFB) {
      *(unsigned char *)(result + 201) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 200) = -(char)a2;
    }
  }
  return result;
}

uint64_t destructiveInjectEnumTag for BNNS.PoolingType(uint64_t result, unsigned int a2)
{
  if (a2 >= 5)
  {
    *(void *)__n128 result = a2 - 5;
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    LOBYTE(a2) = 5;
    *(_OWORD *)(result + 184) = 0u;
  }
  *(unsigned char *)(result + 200) = a2;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.PoolingType()
{
  return &type metadata for BNNS.PoolingType;
}

uint64_t method lookup function for BNNS.PoolingLayer(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.PoolingLayer);
}

uint64_t dispatch thunk of BNNS.PoolingLayer.apply(batchSize:input:output:)(uint64_t a1, uint64_t *a2, uint64_t *a3)
{
  uint64_t v4 = a2[17];
  int v5 = *((_DWORD *)a2 + 36);
  uint64_t v6 = a2[19];
  int v7 = *((_DWORD *)a2 + 40);
  uint64_t v8 = a3[17];
  int v9 = *((_DWORD *)a3 + 36);
  uint64_t v10 = a3[19];
  int v11 = *((_DWORD *)a3 + 40);
  long long v12 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *))(*(void *)v3 + 192);
  uint64_t v28 = *a2;
  long long v29 = *(_OWORD *)(a2 + 1);
  long long v30 = *(_OWORD *)(a2 + 3);
  long long v31 = *(_OWORD *)(a2 + 5);
  long long v32 = *(_OWORD *)(a2 + 7);
  long long v33 = *(_OWORD *)(a2 + 9);
  long long v34 = *(_OWORD *)(a2 + 11);
  long long v35 = *(_OWORD *)(a2 + 13);
  long long v36 = *(_OWORD *)(a2 + 15);
  uint64_t v37 = v4;
  int v38 = v5;
  uint64_t v39 = v6;
  int v40 = v7;
  uint64_t v41 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v14 = *a3;
  long long v15 = *(_OWORD *)(a3 + 1);
  long long v16 = *(_OWORD *)(a3 + 3);
  long long v17 = *(_OWORD *)(a3 + 5);
  long long v18 = *(_OWORD *)(a3 + 7);
  long long v19 = *(_OWORD *)(a3 + 9);
  long long v20 = *(_OWORD *)(a3 + 11);
  long long v21 = *(_OWORD *)(a3 + 13);
  long long v22 = *(_OWORD *)(a3 + 15);
  uint64_t v23 = v8;
  int v24 = v9;
  uint64_t v25 = v10;
  int v26 = v11;
  uint64_t v27 = *(uint64_t *)((char *)a3 + 164);
  return v12(a1, &v28, &v14);
}

uint64_t dispatch thunk of BNNS.PoolingLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingBiasGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5, uint64_t a6)
{
  uint64_t v7 = a2[17];
  int v8 = *((_DWORD *)a2 + 36);
  uint64_t v9 = a2[19];
  int v10 = *((_DWORD *)a2 + 40);
  uint64_t v11 = a3[17];
  int v12 = *((_DWORD *)a3 + 36);
  uint64_t v13 = a3[19];
  int v14 = *((_DWORD *)a3 + 40);
  uint64_t v15 = a4[17];
  int v16 = *((_DWORD *)a4 + 36);
  uint64_t v17 = a4[19];
  int v18 = *((_DWORD *)a4 + 40);
  uint64_t v19 = a5[17];
  int v20 = *((_DWORD *)a5 + 36);
  uint64_t v21 = a5[19];
  int v22 = *((_DWORD *)a5 + 40);
  char v23 = *(unsigned char *)(a6 + 176);
  int v24 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, _OWORD *))(*(void *)v6 + 200);
  long long v85 = *(_OWORD *)(a2 + 11);
  long long v86 = *(_OWORD *)(a2 + 13);
  long long v87 = *(_OWORD *)(a2 + 15);
  uint64_t v92 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v25 = *a2;
  long long v80 = *(_OWORD *)(a2 + 1);
  long long v81 = *(_OWORD *)(a2 + 3);
  long long v82 = *(_OWORD *)(a2 + 5);
  long long v83 = *(_OWORD *)(a2 + 7);
  long long v84 = *(_OWORD *)(a2 + 9);
  long long v71 = *(_OWORD *)(a3 + 11);
  long long v72 = *(_OWORD *)(a3 + 13);
  long long v73 = *(_OWORD *)(a3 + 15);
  uint64_t v78 = *(uint64_t *)((char *)a3 + 164);
  long long v66 = *(_OWORD *)(a3 + 1);
  long long v67 = *(_OWORD *)(a3 + 3);
  long long v68 = *(_OWORD *)(a3 + 5);
  long long v69 = *(_OWORD *)(a3 + 7);
  long long v70 = *(_OWORD *)(a3 + 9);
  long long v57 = *(_OWORD *)(a4 + 11);
  long long v58 = *(_OWORD *)(a4 + 13);
  long long v59 = *(_OWORD *)(a4 + 15);
  uint64_t v64 = *(uint64_t *)((char *)a4 + 164);
  long long v52 = *(_OWORD *)(a4 + 1);
  long long v53 = *(_OWORD *)(a4 + 3);
  long long v54 = *(_OWORD *)(a4 + 5);
  long long v55 = *(_OWORD *)(a4 + 7);
  long long v56 = *(_OWORD *)(a4 + 9);
  long long v38 = *(_OWORD *)(a5 + 1);
  long long v39 = *(_OWORD *)(a5 + 3);
  long long v40 = *(_OWORD *)(a5 + 5);
  long long v41 = *(_OWORD *)(a5 + 7);
  long long v42 = *(_OWORD *)(a5 + 9);
  long long v43 = *(_OWORD *)(a5 + 11);
  long long v44 = *(_OWORD *)(a5 + 13);
  long long v45 = *(_OWORD *)(a5 + 15);
  uint64_t v50 = *(uint64_t *)((char *)a5 + 164);
  uint64_t v26 = *a3;
  uint64_t v79 = v25;
  uint64_t v27 = *a4;
  uint64_t v65 = v26;
  uint64_t v28 = *a5;
  uint64_t v51 = v27;
  uint64_t v37 = v28;
  long long v29 = *(_OWORD *)(a6 + 16);
  v35[0] = *(_OWORD *)a6;
  v35[1] = v29;
  long long v30 = *(_OWORD *)(a6 + 48);
  v35[2] = *(_OWORD *)(a6 + 32);
  v35[3] = v30;
  long long v31 = *(_OWORD *)(a6 + 80);
  v35[4] = *(_OWORD *)(a6 + 64);
  v35[5] = v31;
  long long v32 = *(_OWORD *)(a6 + 112);
  v35[6] = *(_OWORD *)(a6 + 96);
  v35[7] = v32;
  long long v33 = *(_OWORD *)(a6 + 144);
  v35[8] = *(_OWORD *)(a6 + 128);
  v35[9] = v33;
  v35[10] = *(_OWORD *)(a6 + 160);
  uint64_t v88 = v7;
  int v89 = v8;
  uint64_t v90 = v9;
  int v91 = v10;
  uint64_t v74 = v11;
  int v75 = v12;
  uint64_t v76 = v13;
  int v77 = v14;
  uint64_t v60 = v15;
  int v61 = v16;
  uint64_t v62 = v17;
  int v63 = v18;
  uint64_t v46 = v19;
  int v47 = v20;
  uint64_t v48 = v21;
  int v49 = v22;
  char v36 = v23;
  return v24(a1, &v79, &v65, &v51, &v37, v35);
}

uint64_t outlined init with take of BNNSNDArrayDescriptor?(uint64_t a1, uint64_t a2, uint64_t *a3)
{
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName(a3);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(void *)(v5 - 8) + 32))(a2, a1, v5);
  return a2;
}

uint64_t BNNS.ConvolutionLayer.__allocating_init(type:input:weights:output:bias:padding:activation:groupCount:stride:dilationStride:filterParameters:)(char *a1, _OWORD *a2, long long *a3, _OWORD *a4, uint64_t a5, uint64_t a6, uint64_t *a7, size_t a8, size_t a9, size_t a10, size_t a11, size_t a12, uint32_t a13, size_t a14, int (__cdecl *a15)(void **, size_t, size_t), void (__cdecl *a16)(void *))
{
  uint64_t v88 = *MEMORY[0x1E4F143B8];
  outlined init with take of BNNSNDArrayDescriptor?(a5, (uint64_t)v85);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v85, (uint64_t)v87);
  if (*(unsigned char *)(a6 + 32) == 1)
  {
    size_t v75 = *(void *)(a6 + 16);
    size_t v76 = *(void *)(a6 + 24);
    size_t v74 = *(void *)(a6 + 8);
    size_t v73 = *(void *)a6;
    size_t v77 = 0;
    size_t v78 = 0;
  }
  else
  {
    size_t v77 = *(void *)(a6 + 8);
    size_t v78 = *(void *)a6;
    size_t v75 = 0;
    size_t v76 = 0;
    size_t v74 = 0;
    size_t v73 = 0;
  }
  char v72 = *a1;
  uint64_t v23 = *a7;
  char v24 = *((unsigned char *)a7 + 8);
  outlined init with take of BNNSNDArrayDescriptor?(a5, (uint64_t)v86);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v86) == 1)
  {
    size_t v70 = 0;
    size_t v71 = 0;
    size_t v69 = 0;
    size_t v66 = 0;
    size_t v67 = 0;
    size_t v64 = 0;
    size_t v65 = 0;
    size_t v62 = 0;
    size_t v63 = 0;
    size_t v60 = 0;
    size_t v61 = 0;
    size_t v58 = 0;
    size_t v59 = 0;
    size_t v56 = 0;
    size_t v57 = 0;
    size_t v55 = 0;
    uint64_t v25 = 0;
    uint64_t v26 = 0;
    BNNSDataType v27 = 0;
    uint64_t v68 = 0;
    uint64_t v28 = 0;
    uint64_t v54 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v87, (uint64_t)__src);
    uint64_t v68 = *(void *)&__src[0];
    size_t v70 = *(void *)&__src[1];
    size_t v71 = *((void *)&__src[0] + 1);
    size_t v69 = *((void *)&__src[1] + 1);
    size_t v66 = *(void *)&__src[2];
    size_t v67 = *(void *)&__src[3];
    size_t v64 = *((void *)&__src[2] + 1);
    size_t v65 = *((void *)&__src[3] + 1);
    size_t v62 = *((void *)&__src[4] + 1);
    size_t v63 = *(void *)&__src[4];
    size_t v60 = *((void *)&__src[5] + 1);
    size_t v61 = *(void *)&__src[5];
    size_t v58 = *((void *)&__src[6] + 1);
    size_t v59 = *(void *)&__src[6];
    size_t v56 = *((void *)&__src[7] + 1);
    size_t v57 = *(void *)&__src[7];
    uint64_t v25 = (void *)*((void *)&__src[8] + 1);
    size_t v55 = *(void *)&__src[8];
    uint64_t v26 = (void *)*((void *)&__src[9] + 1);
    uint64_t v54 = *(void *)&__src[9];
    BNNSDataType v27 = __src[10];
    uint64_t v28 = *(void *)((char *)&__src[10] + 4);
    int v53 = HIDWORD(__src[10]);
  }
  *(void *)&__src[0] = v23;
  BYTE8(__src[0]) = v24;
  BNNS.ActivationFunction.bnnsActivation.getter((uint64_t)&v84);
  long long v29 = a2[9];
  __src[8] = a2[8];
  __src[9] = v29;
  long long v30 = a2[5];
  __src[4] = a2[4];
  __src[5] = v30;
  long long v31 = a2[7];
  __src[6] = a2[6];
  __src[7] = v31;
  long long v32 = a2[1];
  __src[0] = *a2;
  __src[1] = v32;
  long long v33 = a2[3];
  __src[2] = a2[2];
  __src[3] = v33;
  long long v34 = a3[8];
  long long v35 = a3[9];
  long long v36 = a3[6];
  __src[18] = a3[7];
  __src[19] = v34;
  long long v37 = a3[10];
  __src[20] = v35;
  __src[21] = v37;
  long long v38 = a3[4];
  long long v39 = a3[5];
  long long v40 = a3[2];
  __src[14] = a3[3];
  __src[15] = v38;
  long long v41 = a2[10];
  __src[16] = v39;
  __src[17] = v36;
  long long v42 = *a3;
  long long v43 = a3[1];
  __src[10] = v41;
  __src[11] = v42;
  __src[12] = v43;
  __src[13] = v40;
  long long v44 = a4[9];
  __src[30] = a4[8];
  __src[31] = v44;
  __src[32] = a4[10];
  long long v45 = a4[5];
  __src[26] = a4[4];
  __src[27] = v45;
  long long v46 = a4[7];
  __src[28] = a4[6];
  __src[29] = v46;
  long long v47 = a4[1];
  _OWORD __src[22] = *a4;
  __src[23] = v47;
  long long v48 = a4[3];
  __src[24] = a4[2];
  __src[25] = v48;
  memcpy(&__dst, __src, 0x210uLL);
  __dst.bias.size[0] = v71;
  __dst.bias.size[1] = v70;
  __dst.bias.size[2] = v69;
  __dst.bias.size[3] = v66;
  __dst.bias.size[4] = v64;
  *(void *)&__dst.bias.flags = v68;
  __dst.bias.size[5] = v67;
  __dst.bias.size[6] = v65;
  __dst.bias.size[7] = v63;
  __dst.bias.stride[0] = v62;
  __dst.bias.stride[1] = v61;
  __dst.bias.stride[2] = v60;
  __dst.bias.stride[3] = v59;
  __dst.bias.stride[4] = v58;
  __dst.bias.stride[5] = v57;
  __dst.bias.stride[6] = v56;
  __dst.bias.stride[7] = v55;
  __dst.bias.data = v25;
  *(void *)&__dst.bias.data_type = v54;
  __dst.bias.table_data = v26;
  __dst.bias.table_data_type = v27;
  *(void *)&__dst.bias.data_scale = v28;
  *((_DWORD *)&__dst.bias.data_bias + 1) = v53;
  __dst.activation = v84;
  __dst.x_stride = a9;
  __dst.y_stride = a10;
  __dst.x_dilation_stride = a11;
  __dst.y_dilation_stride = a12;
  __dst.x_padding = v78;
  __dst.y_padding = v77;
  __dst.groups = a8;
  __dst.pad[0] = v73;
  __dst.pad[1] = v74;
  __dst.pad[2] = v75;
  __dst.pad[3] = v76;
  if (a15 != (int (__cdecl *)(void **, size_t, size_t))1)
  {
    filter_params.flags = a13;
    filter_params.n_threads = a14;
    filter_params.alloc_memory = a15;
    filter_params.free_memory = a16;
    if (v72)
    {
      p_BNNSFilterParameters filter_params = &filter_params;
      goto LABEL_12;
    }
    uint64_t v51 = &filter_params;
LABEL_15:
    uint64_t v50 = (void *)MEMORY[0x1D25FFFD0](&__dst, v51);
    return (*(uint64_t (**)(void *))(v80 + 88))(v50);
  }
  if ((v72 & 1) == 0)
  {
    uint64_t v51 = 0;
    goto LABEL_15;
  }
  p_BNNSFilterParameters filter_params = 0;
LABEL_12:
  uint64_t v50 = BNNSFilterCreateLayerTransposedConvolution(&__dst, p_filter_params);
  return (*(uint64_t (**)(void *))(v80 + 88))(v50);
}

uint64_t outlined init with take of BNNSNDArrayDescriptor?(uint64_t a1, uint64_t a2)
{
  uint64_t v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(void *)(v4 - 8) + 32))(a2, a1, v4);
  return a2;
}

uint64_t BNNS.ConvolutionLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(size_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, _OWORD *a5, uint64_t a6, uint64_t a7)
{
  return specialized static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(v7, a1, a2, a3, a4, a5, a6, a7);
}

uint64_t BNNS.ConvolutionLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.ConvolutionLayer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

double static BNNS.ConvolutionPadding.zero.getter@<D0>(uint64_t a1@<X8>)
{
  *(unsigned char *)(a1 + 32) = 0;
  double result = 0.0;
  *(_OWORD *)a1 = 0u;
  *(_OWORD *)(a1 + 16) = 0u;
  return result;
}

BOOL static BNNS.ConvolutionType.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void BNNS.ConvolutionType.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int BNNS.ConvolutionType.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance BNNS.ConvolutionType(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

unint64_t lazy protocol witness table accessor for type BNNS.ConvolutionType and conformance BNNS.ConvolutionType()
{
  unint64_t result = lazy protocol witness table cache variable for type BNNS.ConvolutionType and conformance BNNS.ConvolutionType;
  if (!lazy protocol witness table cache variable for type BNNS.ConvolutionType and conformance BNNS.ConvolutionType)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.ConvolutionType and conformance BNNS.ConvolutionType);
  }
  return result;
}

uint64_t type metadata accessor for BNNS.ConvolutionLayer()
{
  return self;
}

uint64_t method lookup function for BNNS.ConvolutionLayer(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.ConvolutionLayer);
}

uint64_t dispatch thunk of BNNS.ConvolutionLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v39 = *(void *)(a2 + 136);
  int v8 = *(_DWORD *)(a2 + 144);
  uint64_t v9 = *(void *)(a2 + 152);
  int v10 = *(_DWORD *)(a2 + 160);
  uint64_t v11 = *(void *)(a3 + 136);
  int v12 = *(_DWORD *)(a3 + 144);
  uint64_t v13 = *(void *)(a3 + 152);
  int v14 = *(_DWORD *)(a3 + 160);
  uint64_t v15 = *(void *)(a4 + 136);
  int v16 = *(_DWORD *)(a4 + 144);
  uint64_t v17 = *(void *)(a4 + 152);
  int v18 = *(_DWORD *)(a4 + 160);
  uint64_t v19 = *(void *)(a5 + 136);
  int v20 = *(_DWORD *)(a5 + 144);
  uint64_t v21 = *(void *)(a5 + 152);
  int v22 = *(_DWORD *)(a5 + 160);
  char v23 = *(unsigned char *)(a6 + 176);
  char v24 = *(unsigned char *)(a7 + 176);
  uint64_t v25 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, _OWORD *, _OWORD *))(*(void *)v7 + 112);
  long long v26 = *(_OWORD *)(a2 + 104);
  long long v92 = *(_OWORD *)(a2 + 88);
  long long v27 = *(_OWORD *)(a2 + 120);
  long long v93 = v26;
  long long v94 = v27;
  uint64_t v99 = *(void *)(a2 + 164);
  *(void *)&long long v27 = *(void *)a2;
  long long v87 = *(_OWORD *)(a2 + 8);
  long long v88 = *(_OWORD *)(a2 + 24);
  long long v89 = *(_OWORD *)(a2 + 40);
  long long v90 = *(_OWORD *)(a2 + 56);
  long long v91 = *(_OWORD *)(a2 + 72);
  long long v78 = *(_OWORD *)(a3 + 88);
  long long v79 = *(_OWORD *)(a3 + 104);
  long long v80 = *(_OWORD *)(a3 + 120);
  uint64_t v85 = *(void *)(a3 + 164);
  long long v73 = *(_OWORD *)(a3 + 8);
  long long v74 = *(_OWORD *)(a3 + 24);
  long long v75 = *(_OWORD *)(a3 + 40);
  long long v76 = *(_OWORD *)(a3 + 56);
  long long v77 = *(_OWORD *)(a3 + 72);
  long long v64 = *(_OWORD *)(a4 + 88);
  long long v65 = *(_OWORD *)(a4 + 104);
  long long v66 = *(_OWORD *)(a4 + 120);
  uint64_t v71 = *(void *)(a4 + 164);
  long long v59 = *(_OWORD *)(a4 + 8);
  long long v60 = *(_OWORD *)(a4 + 24);
  long long v61 = *(_OWORD *)(a4 + 40);
  long long v62 = *(_OWORD *)(a4 + 56);
  long long v63 = *(_OWORD *)(a4 + 72);
  long long v45 = *(_OWORD *)(a5 + 8);
  long long v46 = *(_OWORD *)(a5 + 24);
  long long v47 = *(_OWORD *)(a5 + 40);
  long long v48 = *(_OWORD *)(a5 + 56);
  long long v49 = *(_OWORD *)(a5 + 72);
  long long v50 = *(_OWORD *)(a5 + 88);
  long long v51 = *(_OWORD *)(a5 + 104);
  long long v52 = *(_OWORD *)(a5 + 120);
  uint64_t v57 = *(void *)(a5 + 164);
  *(void *)&long long v26 = *(void *)a3;
  uint64_t v86 = v27;
  *(void *)&long long v27 = *(void *)a4;
  uint64_t v72 = v26;
  *(void *)&long long v26 = *(void *)a5;
  uint64_t v58 = v27;
  uint64_t v44 = v26;
  long long v28 = *(_OWORD *)(a6 + 16);
  v42[0] = *(_OWORD *)a6;
  v42[1] = v28;
  long long v29 = *(_OWORD *)(a6 + 48);
  v42[2] = *(_OWORD *)(a6 + 32);
  v42[3] = v29;
  long long v30 = *(_OWORD *)(a6 + 80);
  v42[4] = *(_OWORD *)(a6 + 64);
  v42[5] = v30;
  long long v31 = *(_OWORD *)(a6 + 112);
  v42[6] = *(_OWORD *)(a6 + 96);
  v42[7] = v31;
  long long v32 = *(_OWORD *)(a6 + 144);
  v42[8] = *(_OWORD *)(a6 + 128);
  v42[9] = v32;
  v42[10] = *(_OWORD *)(a6 + 160);
  long long v33 = *(_OWORD *)(a7 + 16);
  v40[0] = *(_OWORD *)a7;
  v40[1] = v33;
  long long v34 = *(_OWORD *)(a7 + 48);
  v40[2] = *(_OWORD *)(a7 + 32);
  v40[3] = v34;
  long long v35 = *(_OWORD *)(a7 + 80);
  v40[4] = *(_OWORD *)(a7 + 64);
  v40[5] = v35;
  long long v36 = *(_OWORD *)(a7 + 112);
  v40[6] = *(_OWORD *)(a7 + 96);
  v40[7] = v36;
  long long v37 = *(_OWORD *)(a7 + 144);
  v40[8] = *(_OWORD *)(a7 + 128);
  v40[9] = v37;
  v40[10] = *(_OWORD *)(a7 + 160);
  uint64_t v95 = v39;
  int v96 = v8;
  uint64_t v97 = v9;
  int v98 = v10;
  uint64_t v81 = v11;
  int v82 = v12;
  uint64_t v83 = v13;
  int v84 = v14;
  uint64_t v67 = v15;
  int v68 = v16;
  uint64_t v69 = v17;
  int v70 = v18;
  uint64_t v53 = v19;
  int v54 = v20;
  uint64_t v55 = v21;
  int v56 = v22;
  char v43 = v23;
  char v41 = v24;
  return v25(a1, &v86, &v72, &v58, &v44, v42, v40);
}

__n128 __swift_memcpy33_8(uint64_t a1, uint64_t a2)
{
  __n128 result = *(__n128 *)a2;
  long long v3 = *(_OWORD *)(a2 + 16);
  *(unsigned char *)(a1 + 32) = *(unsigned char *)(a2 + 32);
  *(__n128 *)a1 = result;
  *(_OWORD *)(a1 + 16) = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.ConvolutionPadding(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFF && *(unsigned char *)(a1 + 33)) {
    return (*(_DWORD *)a1 + 255);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 32);
  if (v3 <= 1) {
    int v4 = -1;
  }
  else {
    int v4 = v3 ^ 0xFF;
  }
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.ConvolutionPadding(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(void *)(result + 16) = 0;
    *(void *)(result + 24) = 0;
    *(unsigned char *)(result + 32) = 0;
    *(void *)__n128 result = a2 - 255;
    *(void *)(result + 8) = 0;
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 33) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 33) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 32) = -(char)a2;
    }
  }
  return result;
}

uint64_t getEnumTag for BNNS.ConvolutionPadding(uint64_t a1)
{
  return *(unsigned __int8 *)(a1 + 32);
}

uint64_t destructiveInjectEnumTag for BNNS.ConvolutionPadding(uint64_t result, char a2)
{
  *(unsigned char *)(result + 32) = a2 & 1;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.ConvolutionPadding()
{
  return &type metadata for BNNS.ConvolutionPadding;
}

uint64_t getEnumTagSinglePayload for BNNS.ConvolutionType(unsigned __int8 *a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0xFF) {
    goto LABEL_17;
  }
  if (a2 + 1 >= 0xFFFF00) {
    int v2 = 4;
  }
  else {
    int v2 = 2;
  }
  if ((a2 + 1) >> 8 < 0xFF) {
    int v3 = 1;
  }
  else {
    int v3 = v2;
  }
  if (v3 == 4)
  {
    int v4 = *(_DWORD *)(a1 + 1);
    if (v4) {
      return (*a1 | (v4 << 8)) - 1;
    }
  }
  else
  {
    if (v3 == 2)
    {
      int v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1)) {
        goto LABEL_17;
      }
      return (*a1 | (v4 << 8)) - 1;
    }
    int v4 = a1[1];
    if (a1[1]) {
      return (*a1 | (v4 << 8)) - 1;
    }
  }
LABEL_17:
  unsigned int v6 = *a1;
  BOOL v7 = v6 >= 2;
  int v8 = v6 - 2;
  if (!v7) {
    int v8 = -1;
  }
  return (v8 + 1);
}

unsigned char *storeEnumTagSinglePayload for BNNS.ConvolutionType(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 1 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFF) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFE)
  {
    unsigned int v6 = ((a2 - 255) >> 8) + 1;
    *__n128 result = a2 + 1;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D209D6E4);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *__n128 result = a2 + 1;
        break;
    }
  }
  return result;
}

unsigned char *destructiveInjectEnumTag for BNNS.ConvolutionType(unsigned char *result, char a2)
{
  *__n128 result = a2 & 1;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.ConvolutionType()
{
  return &type metadata for BNNS.ConvolutionType;
}

uint64_t static vDSP.convert(splitComplexVector:toInterleavedComplexVector:)(uint64_t a1, uint64_t a2, uint64_t *a3)
{
  int v3 = (uint64_t (*)(void *, uint64_t, uint64_t, uint64_t, uint64_t))MEMORY[0x1E4F16EB8];

  return static vDSP.convert(splitComplexVector:toInterleavedComplexVector:)(a1, a2, a3, (uint64_t (*)(void, uint64_t, void, uint64_t))specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:), v3);
}

{
  uint64_t (*v3)(void *, uint64_t, uint64_t, uint64_t, uint64_t);
  uint64_t vars8;

  int v3 = (uint64_t (*)(void *, uint64_t, uint64_t, uint64_t, uint64_t))MEMORY[0x1E4F16EC0];

  return static vDSP.convert(splitComplexVector:toInterleavedComplexVector:)(a1, a2, a3, (uint64_t (*)(void, uint64_t, void, uint64_t))specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:), v3);
}

void static vDSP.convert(interleavedComplexVector:toSplitComplexVector:)(uint64_t a1, DSPSplitComplex *__Z)
{
}

uint64_t static vDSP.convert(splitComplexVector:toInterleavedComplexVector:)(uint64_t a1, uint64_t a2, uint64_t *a3, uint64_t (*a4)(void, uint64_t, void, uint64_t), uint64_t (*a5)(void *, uint64_t, uint64_t, uint64_t, uint64_t))
{
  _OWORD v11[2] = *MEMORY[0x1E4F143B8];
  v11[0] = a1;
  v11[1] = a2;
  uint64_t v8 = *a3;
  uint64_t v9 = *(void *)(*a3 + 16);
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
    uint64_t v8 = a4(0, v9, 0, v8);
  }
  *a3 = v8;
  return a5(v11, 1, v8 + 32, 2, v9);
}

void static vDSP.convert(interleavedComplexVector:toSplitComplexVector:)(uint64_t a1, DSPDoubleSplitComplex *__Z)
{
}

BOOL static BNNS.LearningPhase.== infix(_:_:)(int a1, int a2)
{
  return ((a2 ^ a1) & 1) == 0;
}

void BNNS.LearningPhase.hash(into:)(uint64_t a1, char a2)
{
}

Swift::Int BNNS.LearningPhase.hashValue.getter(char a1)
{
  return Hasher._finalize()();
}

uint64_t BNNS.FusedLayer.apply(batchSize:input:output:for:)(size_t a1, uint64_t a2, uint64_t a3, char a4)
{
  return specialized static BNNS.fusedLayerApply(_:batchSize:input:output:for:)(v4, a1, a2, a3, a4 & 1);
}

uint64_t BNNS.FusedLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingParameterGradients:)(size_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, long long *a5, uint64_t a6)
{
  return specialized static BNNS.fusedLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingParameterGradients:)(v6, a1, a2, a3, a4, a5, a6);
}

uint64_t BNNS.FusedLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t closure #1 in closure #2 in static BNNS.fusedLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingParameterGradients:)@<X0>(BNNSNDArrayDescriptor *a1@<X0>, uint64_t a2@<X1>, size_t a3@<X2>, uint64_t a4@<X3>, BNNSNDArrayDescriptor *a5@<X4>, uint64_t a6@<X6>, _DWORD *a7@<X8>, char **a8)
{
  long long v47 = *(void **)(a2 + 16);
  outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v88, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v88, (uint64_t)&v89, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  in = v89;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v84);
  outlined init with take of BNNS.Shape((uint64_t)v84, (uint64_t)v85);
  outlined init with take of BNNS.Shape((uint64_t)v85, (uint64_t)v86);
  BNNS.Shape.size.getter((uint64_t)&v76);
  unint64_t v8 = v76;
  unint64_t v9 = v77;
  unint64_t v10 = v78;
  unint64_t v11 = v79;
  unint64_t v12 = v80;
  unint64_t v13 = v81;
  unint64_t v14 = v82;
  unint64_t v15 = v83;
  outlined init with take of BNNS.Shape((uint64_t)v85, (uint64_t)v86);
  BNNS.Shape.stride.getter((uint64_t)&v76);
  unint64_t in_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v8, v9, v10, v11, v12, v13, v14, v15, v76, v77, v78, v79, v80, v81, v82, v83);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v76);
  outlined init with take of BNNS.Shape((uint64_t)&v76, (uint64_t)v86);
  outlined init with take of BNNS.Shape((uint64_t)v86, (uint64_t)v75);
  BNNS.Shape.size.getter((uint64_t)&v67);
  unint64_t v16 = v67;
  unint64_t v17 = v68;
  unint64_t v18 = v69;
  unint64_t v19 = v70;
  unint64_t v20 = v71;
  unint64_t v21 = v72;
  unint64_t v22 = v73;
  unint64_t v23 = v74;
  outlined init with take of BNNS.Shape((uint64_t)v86, (uint64_t)v75);
  BNNS.Shape.stride.getter((uint64_t)&v67);
  size_t v44 = specialized static BNNS.calculateBatchStride(size:stride:)(v16, v17, v18, v19, v20, v21, v22, v23, v67, v68, v69, v70, v71, v72, v73, v74);
  outlined init with take of BNNSNDArrayDescriptor?(a6 + 136, (uint64_t)v87, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v87, (uint64_t)&v90, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  char v43 = v90;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v66);
  outlined init with take of BNNS.Shape((uint64_t)v66, (uint64_t)&v67);
  outlined init with take of BNNS.Shape((uint64_t)&v67, (uint64_t)v75);
  BNNS.Shape.size.getter((uint64_t)&v58);
  unint64_t v24 = v58;
  unint64_t v25 = v59;
  unint64_t v26 = v60;
  unint64_t v27 = v61;
  unint64_t v28 = v62;
  unint64_t v29 = v63;
  unint64_t v30 = v64;
  unint64_t v31 = v65;
  outlined init with take of BNNS.Shape((uint64_t)&v67, (uint64_t)v75);
  BNNS.Shape.stride.getter((uint64_t)&v58);
  unint64_t out_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v24, v25, v26, v27, v28, v29, v30, v31, v58, v59, v60, v61, v62, v63, v64, v65);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v58);
  outlined init with take of BNNS.Shape((uint64_t)&v58, (uint64_t)v75);
  outlined init with take of BNNS.Shape((uint64_t)v75, (uint64_t)v57);
  BNNS.Shape.size.getter((uint64_t)&v52);
  long long v32 = v52;
  long long v33 = v53;
  long long v34 = v54;
  unint64_t v36 = v55;
  unint64_t v35 = v56;
  outlined init with take of BNNS.Shape((uint64_t)v75, (uint64_t)v57);
  BNNS.Shape.stride.getter((uint64_t)&v52);
  size_t v37 = specialized static BNNS.calculateBatchStride(size:stride:)(v32, *((unint64_t *)&v32 + 1), v33, *((unint64_t *)&v33 + 1), v34, *((unint64_t *)&v34 + 1), v36, v35, v52, *((unint64_t *)&v52 + 1), v53, *((unint64_t *)&v53 + 1), v54, *((unint64_t *)&v54 + 1), v55, v56);
  long long v38 = *a8;
  char isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
  *a8 = v38;
  if ((isUniquelyReferenced_nonNull_native & 1) == 0) {
    long long v38 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v38 + 2), 0, v38);
  }
  *a8 = v38;
  uint64_t result = BNNSFusedFilterApplyBackwardBatch(v47, a3, in, in_stride, a5, v44, v43, out_stride, a1, v37, (BNNSNDArrayDescriptor **)v38 + 4);
  *a7 = result;
  return result;
}

uint64_t BNNS.FusedConvolutionNormalizationLayer.__allocating_init(input:output:convolutionWeights:convolutionBias:convolutionStride:convolutionDilationStride:convolutionPadding:normalization:normalizationBeta:normalizationGamma:normalizationMomentum:normalizationEpsilon:normalizationActivation:filterParameters:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, uint64_t a10, long long *a11, long long *a12, uint64_t *a13, int a14, uint64_t a15, uint64_t a16, uint64_t a17)
{
  unint64_t v17 = (long long *)MEMORY[0x1F4188790](a1);
  uint64_t v164 = v18;
  uint64_t v163 = v19;
  uint64_t v162 = v20;
  uint64_t v161 = v21;
  uint64_t v23 = v22;
  int v25 = v24;
  int v27 = v26;
  uint64_t v302 = *MEMORY[0x1E4F143B8];
  long long v28 = a12[9];
  long long v255 = a12[8];
  long long v256 = v28;
  long long v29 = a12[10];
  long long v30 = a12[5];
  long long v251 = a12[4];
  long long v252 = v30;
  long long v31 = a12[7];
  long long v253 = a12[6];
  long long v254 = v31;
  long long v32 = a12[1];
  long long v247 = *a12;
  long long v248 = v32;
  long long v33 = a12[3];
  long long v249 = a12[2];
  long long v250 = v33;
  long long v34 = a11[8];
  long long v35 = a11[9];
  long long v36 = a11[6];
  long long v265 = a11[7];
  long long v266 = v34;
  long long v37 = a11[10];
  long long v267 = v35;
  long long v268 = v37;
  long long v38 = a11[4];
  long long v39 = a11[5];
  long long v40 = a11[2];
  long long v261 = a11[3];
  long long v262 = v38;
  long long v263 = v39;
  long long v264 = v36;
  long long v41 = *a11;
  long long v42 = a11[1];
  long long v257 = v29;
  long long v258 = v41;
  long long v259 = v42;
  long long v260 = v40;
  long long v44 = v43[9];
  long long v277 = v43[8];
  long long v278 = v44;
  long long v45 = v43[5];
  long long v273 = v43[4];
  long long v274 = v45;
  long long v46 = v43[7];
  long long v275 = v43[6];
  long long v276 = v46;
  long long v47 = v43[1];
  long long v269 = *v43;
  long long v270 = v47;
  long long v48 = v43[3];
  long long v271 = v43[2];
  long long v272 = v48;
  long long v50 = v49[8];
  long long v51 = v49[9];
  long long v52 = v49[6];
  long long v287 = v49[7];
  long long v288 = v50;
  long long v53 = v49[10];
  long long v289 = v51;
  long long v290 = v53;
  long long v54 = v49[4];
  long long v55 = v49[5];
  long long v56 = v49[2];
  long long v283 = v49[3];
  long long v284 = v54;
  long long v57 = v43[10];
  long long v285 = v55;
  long long v286 = v52;
  long long v58 = *v49;
  long long v59 = v49[1];
  long long v279 = v57;
  long long v280 = v58;
  long long v281 = v59;
  long long v282 = v56;
  long long v60 = v17[9];
  long long v299 = v17[8];
  long long v300 = v60;
  long long v301 = v17[10];
  long long v61 = v17[5];
  long long v295 = v17[4];
  long long v296 = v61;
  long long v62 = v17[7];
  long long v297 = v17[6];
  long long v298 = v62;
  long long v63 = v17[1];
  long long v291 = *v17;
  long long v292 = v63;
  long long v64 = v17[3];
  long long v293 = v17[2];
  long long v294 = v64;
  uint64_t v66 = *a9;
  uint64_t v65 = a9[1];
  uint64_t v67 = a9[2];
  uint64_t v68 = a9[3];
  int v159 = *((unsigned __int8 *)a9 + 32);
  outlined init with take of BNNS.NormalizationType(a10, (uint64_t)v242);
  uint64_t v69 = *a13;
  char v70 = *((unsigned char *)a13 + 8);
  outlined init with take of BNNS.NormalizationType((uint64_t)v242, (uint64_t)v243);
  switch(_s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v243))
  {
    case 2u:
      _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v243);
      uint64_t v114 = 0;
      uint64_t v115 = 0;
      uint64_t v112 = 0;
      uint64_t v113 = 0;
      uint64_t v110 = 0;
      uint64_t v111 = 0;
      uint64_t v108 = 0;
      uint64_t v109 = 0;
      uint64_t v106 = 0;
      uint64_t v107 = 0;
      uint64_t v104 = 0;
      uint64_t v105 = 0;
      uint64_t v102 = 0;
      uint64_t v103 = 0;
      uint64_t v100 = 0;
      uint64_t v101 = 0;
      uint64_t v97 = 0;
      uint64_t v98 = 0;
      int v96 = 0;
      uint64_t v134 = 0;
      uint64_t v135 = 0;
      uint64_t v132 = 0;
      uint64_t v133 = 0;
      uint64_t v129 = 0;
      uint64_t v130 = 0;
      uint64_t v127 = 0;
      uint64_t v128 = 0;
      uint64_t v125 = 0;
      uint64_t v126 = 0;
      uint64_t v123 = 0;
      uint64_t v124 = 0;
      uint64_t v122 = 0;
      uint64_t v119 = 0;
      uint64_t v120 = 0;
      uint64_t v117 = 0;
      uint64_t v118 = 0;
      uint64_t v116 = 0;
      int v121 = 0;
      uint64_t v136 = 0;
      uint64_t v131 = 0;
      goto LABEL_6;
    case 3u:
      uint64_t v114 = 0;
      uint64_t v115 = 0;
      uint64_t v112 = 0;
      uint64_t v113 = 0;
      uint64_t v110 = 0;
      uint64_t v111 = 0;
      uint64_t v108 = 0;
      uint64_t v109 = 0;
      uint64_t v106 = 0;
      uint64_t v107 = 0;
      uint64_t v104 = 0;
      uint64_t v105 = 0;
      uint64_t v102 = 0;
      uint64_t v103 = 0;
      uint64_t v100 = 0;
      uint64_t v101 = 0;
      uint64_t v97 = 0;
      uint64_t v98 = 0;
      int v96 = 0;
      uint64_t v134 = 0;
      uint64_t v135 = 0;
      uint64_t v132 = 0;
      uint64_t v133 = 0;
      uint64_t v129 = 0;
      uint64_t v130 = 0;
      uint64_t v127 = 0;
      uint64_t v128 = 0;
      uint64_t v125 = 0;
      uint64_t v126 = 0;
      uint64_t v123 = 0;
      uint64_t v124 = 0;
      uint64_t v122 = 0;
      uint64_t v119 = 0;
      uint64_t v120 = 0;
      uint64_t v117 = 0;
      uint64_t v118 = 0;
      uint64_t v116 = 0;
      int v121 = 0;
      uint64_t v131 = 0;
      uint64_t v136 = *(void *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v243);
LABEL_6:
      uint64_t v73 = 0;
      uint64_t v72 = 0;
      uint64_t v99 = 0;
      uint64_t v94 = 0;
      uint64_t v95 = 0;
      if (v159) {
        goto LABEL_7;
      }
      goto LABEL_13;
    default:
      unint64_t v71 = (const void *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v243);
      memcpy(__dst, v71, 0x169uLL);
      outlined init with take of BNNSNDArrayDescriptor?((uint64_t)__dst, (uint64_t)v245, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&__dst[23], (uint64_t)v246, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v245, (uint64_t)__dst, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      uint64_t v72 = 0;
      if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)__dst) == 1)
      {
        int v121 = 0;
        uint64_t v122 = 0;
        uint64_t v123 = 0;
        uint64_t v116 = 0;
        uint64_t v117 = 0;
        uint64_t v118 = 0;
        uint64_t v119 = 0;
        uint64_t v120 = 0;
        uint64_t v124 = 0;
        uint64_t v125 = 0;
        uint64_t v126 = 0;
        uint64_t v127 = 0;
        uint64_t v128 = 0;
        uint64_t v129 = 0;
        uint64_t v130 = 0;
        uint64_t v132 = 0;
        uint64_t v133 = 0;
        uint64_t v134 = 0;
        uint64_t v135 = 0;
        uint64_t v131 = 0;
        uint64_t v73 = 0;
        uint64_t v99 = 0;
      }
      else
      {
        outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v245, (uint64_t)v210, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
        uint64_t v131 = v210[0];
        uint64_t v134 = v210[2];
        uint64_t v135 = v210[1];
        uint64_t v132 = v210[4];
        uint64_t v133 = v210[3];
        uint64_t v129 = v210[6];
        uint64_t v130 = v210[5];
        uint64_t v127 = v210[8];
        uint64_t v128 = v210[7];
        uint64_t v125 = v210[10];
        uint64_t v126 = v210[9];
        uint64_t v122 = v210[12];
        uint64_t v119 = v210[14];
        uint64_t v120 = v210[13];
        uint64_t v117 = v210[16];
        uint64_t v118 = v210[15];
        uint64_t v116 = v210[17];
        uint64_t v99 = v210[18];
        uint64_t v123 = v210[19];
        uint64_t v124 = v210[11];
        int v121 = v210[20];
        uint64_t v73 = *(void *)((char *)&v210[20] + 4);
        int v92 = HIDWORD(v210[21]);
      }
      outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v246, (uint64_t)v210, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v210) == 1)
      {
        int v96 = 0;
        uint64_t v97 = 0;
        uint64_t v98 = 0;
        uint64_t v100 = 0;
        uint64_t v101 = 0;
        uint64_t v102 = 0;
        uint64_t v103 = 0;
        uint64_t v104 = 0;
        uint64_t v105 = 0;
        uint64_t v106 = 0;
        uint64_t v107 = 0;
        uint64_t v108 = 0;
        uint64_t v109 = 0;
        uint64_t v110 = 0;
        uint64_t v111 = 0;
        uint64_t v112 = 0;
        uint64_t v113 = 0;
        uint64_t v114 = 0;
        uint64_t v115 = 0;
        uint64_t v94 = 0;
        uint64_t v95 = 0;
      }
      else
      {
        outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v246, (uint64_t)v229, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
        uint64_t v95 = *(void *)&v229[0];
        uint64_t v114 = *(void *)&v229[1];
        uint64_t v115 = *((void *)&v229[0] + 1);
        uint64_t v112 = *(void *)&v229[2];
        uint64_t v113 = *((void *)&v229[1] + 1);
        uint64_t v110 = *(void *)&v229[3];
        uint64_t v111 = *((void *)&v229[2] + 1);
        uint64_t v108 = *(void *)&v229[4];
        uint64_t v109 = *((void *)&v229[3] + 1);
        uint64_t v106 = *(void *)&v229[5];
        uint64_t v107 = *((void *)&v229[4] + 1);
        uint64_t v104 = *(void *)&v229[6];
        uint64_t v105 = *((void *)&v229[5] + 1);
        uint64_t v102 = *(void *)&v229[7];
        uint64_t v103 = *((void *)&v229[6] + 1);
        uint64_t v100 = *(void *)&v229[8];
        uint64_t v101 = *((void *)&v229[7] + 1);
        uint64_t v97 = *((void *)&v229[9] + 1);
        uint64_t v94 = *(void *)&v229[9];
        uint64_t v98 = *((void *)&v229[8] + 1);
        uint64_t v72 = *(void *)((char *)&v229[10] + 4);
        int v96 = v229[10];
        int v91 = HIDWORD(v229[10]);
      }
      uint64_t v136 = 0;
      if (v159)
      {
LABEL_7:
        uint64_t v157 = v65;
        uint64_t v156 = v66;
        uint64_t v65 = 0;
        uint64_t v66 = 0;
      }
      else
      {
LABEL_13:
        uint64_t v68 = 0;
        uint64_t v67 = 0;
        uint64_t v157 = 0;
        uint64_t v156 = 0;
      }
      uint64_t v158 = v66;
      uint64_t v166 = v68;
      outlined init with take of BNNSNDArrayDescriptor?(v23, (uint64_t)v238, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      uint64_t v165 = v67;
      uint64_t v160 = v65;
      if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v238) == 1)
      {
        uint64_t v154 = 0;
        uint64_t v153 = 0;
        uint64_t v152 = 0;
        uint64_t v151 = 0;
        uint64_t v150 = 0;
        uint64_t v149 = 0;
        uint64_t v148 = 0;
        uint64_t v147 = 0;
        uint64_t v146 = 0;
        uint64_t v145 = 0;
        uint64_t v144 = 0;
        uint64_t v142 = 0;
        uint64_t v143 = 0;
        uint64_t v140 = 0;
        uint64_t v141 = 0;
        uint64_t v138 = 0;
        uint64_t v139 = 0;
        uint64_t v74 = 0;
        int v75 = 0;
        uint64_t v155 = 0;
        uint64_t v76 = 0;
        uint64_t v137 = 0;
      }
      else
      {
        uint64_t v155 = v238[0];
        uint64_t v154 = v238[1];
        uint64_t v153 = v238[2];
        uint64_t v152 = v238[3];
        uint64_t v151 = v238[4];
        uint64_t v150 = v238[5];
        uint64_t v149 = v238[6];
        uint64_t v148 = v238[7];
        uint64_t v147 = v238[8];
        uint64_t v146 = v238[9];
        uint64_t v145 = v238[10];
        uint64_t v144 = v238[11];
        uint64_t v142 = v238[13];
        uint64_t v143 = v238[12];
        uint64_t v140 = v238[15];
        uint64_t v141 = v238[14];
        uint64_t v138 = v238[17];
        uint64_t v139 = v238[16];
        uint64_t v137 = v238[18];
        uint64_t v74 = v238[19];
        uint64_t v76 = v240;
        int v75 = v239;
        int v93 = v241;
      }
      __src[8] = v299;
      __src[9] = v300;
      __src[4] = v295;
      __src[5] = v296;
      __src[6] = v297;
      __src[7] = v298;
      __src[0] = v291;
      __src[1] = v292;
      __src[2] = v293;
      __src[3] = v294;
      __src[18] = v287;
      __src[19] = v288;
      __src[20] = v289;
      __src[21] = v290;
      __src[14] = v283;
      __src[15] = v284;
      __src[16] = v285;
      __src[17] = v286;
      __src[10] = v301;
      __src[11] = v280;
      __src[12] = v281;
      __src[13] = v282;
      __src[29] = v276;
      __src[30] = v277;
      __src[31] = v278;
      __src[32] = v279;
      __src[25] = v272;
      __src[26] = v273;
      __src[27] = v274;
      __src[28] = v275;
      _OWORD __src[22] = v269;
      __src[23] = v270;
      __src[24] = v271;
      __dst[0] = v69;
      LOBYTE(__dst[1]) = v70;
      BNNS.ActivationFunction.bnnsActivation.getter((uint64_t)&v231);
      uint64_t v77 = v232;
      uint64_t v78 = v233;
      int v79 = v231;
      int v80 = v234;
      uint64_t v81 = v235;
      uint64_t v82 = v236;
      uint64_t v83 = v237;
      outlined init with take of BNNS.NormalizationType((uint64_t)v242, (uint64_t)v244);
      if (_s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v244) == 2) {
        uint64_t v84 = *(void *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v244);
      }
      else {
        uint64_t v84 = 0;
      }
      v229[8] = v277;
      v229[9] = v278;
      v229[4] = v273;
      v229[5] = v274;
      v229[6] = v275;
      v229[7] = v276;
      v229[0] = v269;
      v229[1] = v270;
      v229[2] = v271;
      v229[3] = v272;
      v229[18] = v276;
      v229[19] = v277;
      v229[20] = v278;
      v229[21] = v279;
      v229[14] = v272;
      v229[15] = v273;
      v229[16] = v274;
      v229[17] = v275;
      v229[10] = v279;
      v229[11] = v269;
      v229[12] = v270;
      v229[13] = v271;
      v229[30] = v266;
      v229[31] = v267;
      v229[26] = v262;
      v229[27] = v263;
      v229[28] = v264;
      v229[29] = v265;
      v229[22] = v258;
      v229[23] = v259;
      v229[24] = v260;
      v229[25] = v261;
      v229[40] = v254;
      v229[41] = v255;
      v229[42] = v256;
      v229[43] = v257;
      v229[36] = v250;
      v229[37] = v251;
      v229[38] = v252;
      v229[39] = v253;
      v229[32] = v268;
      v229[33] = v247;
      v229[34] = v248;
      v229[35] = v249;
      memcpy(v210, __src, 0x210uLL);
      v210[67] = v154;
      v210[68] = v153;
      v210[69] = v152;
      v210[70] = v151;
      v210[71] = v150;
      v210[72] = v149;
      v210[73] = v148;
      v210[74] = v147;
      v210[75] = v146;
      v210[76] = v145;
      v210[77] = v144;
      v210[78] = v143;
      v210[79] = v142;
      v210[80] = v141;
      v210[81] = v140;
      v210[82] = v139;
      v210[83] = v138;
      v210[85] = v74;
      int v211 = v75;
      uint64_t v212 = v76;
      int v213 = v93;
      uint64_t v214 = 0x7FC0000000000000;
      uint64_t v215 = 0x17FC00000;
      uint64_t v218 = v161;
      uint64_t v219 = v162;
      uint64_t v220 = v163;
      uint64_t v221 = v164;
      uint64_t v222 = v158;
      uint64_t v223 = v160;
      uint64_t v225 = v156;
      uint64_t v226 = v157;
      uint64_t v227 = v165;
      uint64_t v228 = v166;
      v210[66] = v155;
      v210[84] = v137;
      long long v217 = 0u;
      long long v216 = 0u;
      uint64_t v224 = v136;
      memcpy(__dst, v229, 0x2C0uLL);
      __dst[89] = v135;
      __dst[90] = v134;
      __dst[91] = v133;
      __dst[92] = v132;
      __dst[93] = v130;
      __dst[94] = v129;
      __dst[95] = v128;
      __dst[96] = v127;
      __dst[97] = v126;
      __dst[98] = v125;
      __dst[99] = v124;
      __dst[100] = v122;
      __dst[101] = v120;
      __dst[102] = v119;
      __dst[103] = v118;
      __dst[104] = v117;
      __dst[105] = v116;
      __dst[88] = v131;
      __dst[106] = v99;
      __dst[107] = v123;
      int v173 = v121;
      uint64_t v174 = v73;
      int v175 = v92;
      uint64_t v176 = v95;
      uint64_t v177 = v115;
      uint64_t v178 = v114;
      uint64_t v179 = v113;
      uint64_t v180 = v112;
      uint64_t v181 = v111;
      uint64_t v182 = v110;
      uint64_t v183 = v109;
      uint64_t v184 = v108;
      uint64_t v185 = v107;
      uint64_t v186 = v106;
      uint64_t v187 = v105;
      uint64_t v188 = v104;
      uint64_t v189 = v103;
      uint64_t v190 = v102;
      uint64_t v191 = v101;
      uint64_t v192 = v100;
      uint64_t v193 = v98;
      uint64_t v194 = v94;
      uint64_t v195 = v97;
      int v196 = v96;
      uint64_t v197 = v72;
      int v198 = v91;
      int v199 = v27;
      int v200 = v25;
      int v201 = v79;
      uint64_t v202 = v77;
      uint64_t v203 = v78;
      int v204 = v80;
      uint64_t v205 = v81;
      uint64_t v206 = v82;
      uint64_t v207 = v83;
      uint64_t v208 = v136;
      uint64_t v209 = v84;
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
      uint64_t v85 = swift_allocObject();
      *(_OWORD *)(v85 + 16) = xmmword_1D2135290;
      *(void *)(v85 + 32) = v210;
      *(void *)(v85 + 40) = __dst;
      long long v167 = (char *)v85;
      if (a16 == 1)
      {
        uint64_t v86 = 0;
      }
      else
      {
        int v168 = a14;
        uint64_t v169 = a15;
        uint64_t v170 = a16;
        uint64_t v171 = a17;
        uint64_t v86 = (const BNNSFilterParameters *)&v168;
      }
      long long v87 = closure #1 in closure #1 in closure #1 in BNNS.FusedConvolutionNormalizationLayer.init(input:output:convolutionWeights:convolutionBias:convolutionStride:convolutionDilationStride:convolutionPadding:normalization:normalizationBeta:normalizationGamma:normalizationMomentum:normalizationEpsilon:normalizationActivation:filterParameters:)(v86, (uint64_t)v242, &v167, 0);
      swift_bridgeObjectRelease();
      type metadata accessor for BNNS.FusedConvolutionNormalizationLayer();
      uint64_t v88 = swift_allocObject();
      uint64_t v89 = v88;
      if (v87)
      {
        *(void *)(v88 + 16) = v87;
      }
      else
      {
        type metadata accessor for BNNS.Layer();
        swift_deallocPartialClassInstance();
        return 0;
      }
      return v89;
  }
}

uint64_t BNNS.FusedFullyConnectedNormalizationLayer.__allocating_init(input:output:fullyConnectedWeights:fullyConnectedBias:normalization:normalizationBeta:normalizationGamma:normalizationMomentum:normalizationEpsilon:normalizationActivation:filterParameters:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, int a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  uint64_t v12 = MEMORY[0x1F4188790](a1);
  unint64_t v14 = v13;
  uint64_t v16 = v15;
  uint64_t v18 = v17;
  int v20 = v19;
  int v22 = v21;
  int v24 = v23;
  int v25 = (_OWORD *)v12;
  uint64_t v258 = *MEMORY[0x1E4F143B8];
  long long v27 = v26[9];
  long long v233 = v26[8];
  long long v234 = v27;
  long long v235 = v26[10];
  long long v28 = v26[5];
  long long v229 = v26[4];
  long long v230 = v28;
  long long v29 = v26[6];
  long long v232 = v26[7];
  long long v231 = v29;
  long long v30 = v26[1];
  long long v225 = *v26;
  long long v226 = v30;
  long long v31 = v26[2];
  long long v228 = v26[3];
  long long v227 = v31;
  long long v33 = v32[9];
  long long v244 = v32[8];
  long long v245 = v33;
  long long v246 = v32[10];
  long long v34 = v32[5];
  long long v240 = v32[4];
  long long v241 = v34;
  long long v35 = v32[6];
  long long v243 = v32[7];
  long long v242 = v35;
  long long v36 = v32[1];
  long long v236 = *v32;
  long long v237 = v36;
  long long v37 = v32[2];
  long long v239 = v32[3];
  long long v238 = v37;
  long long v38 = v23[9];
  long long v255 = v23[8];
  long long v256 = v38;
  long long v257 = v23[10];
  long long v39 = v23[5];
  long long v251 = v23[4];
  long long v252 = v39;
  long long v40 = v23[6];
  long long v254 = v23[7];
  long long v253 = v40;
  long long v41 = v23[1];
  long long v247 = *v23;
  long long v248 = v41;
  long long v42 = v23[2];
  long long v250 = v23[3];
  long long v249 = v42;
  outlined init with take of BNNS.NormalizationType(v43, (uint64_t)v222);
  uint64_t v44 = *v14;
  char v135 = *((unsigned char *)v14 + 8);
  outlined init with take of BNNSNDArrayDescriptor?(v16, (uint64_t)v218, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v218) == 1)
  {
    uint64_t v45 = 0;
    uint64_t v46 = 0;
    uint64_t v47 = 0;
    uint64_t v153 = 0;
    uint64_t v152 = 0;
    uint64_t v151 = 0;
    uint64_t v149 = 0;
    uint64_t v148 = 0;
    uint64_t v147 = 0;
    uint64_t v146 = 0;
    uint64_t v145 = 0;
    uint64_t v144 = 0;
    uint64_t v143 = 0;
    uint64_t v141 = 0;
    uint64_t v142 = 0;
    uint64_t v139 = 0;
    uint64_t v140 = 0;
    uint64_t v138 = 0;
    int v137 = 0;
    uint64_t v150 = 0;
    uint64_t v48 = 0;
    uint64_t v136 = 0;
  }
  else
  {
    uint64_t v150 = v218[0];
    uint64_t v45 = v218[1];
    uint64_t v46 = v218[2];
    uint64_t v47 = v218[3];
    uint64_t v153 = v218[4];
    uint64_t v152 = v218[5];
    uint64_t v151 = v218[6];
    uint64_t v149 = v218[7];
    uint64_t v148 = v218[8];
    uint64_t v147 = v218[9];
    uint64_t v146 = v218[10];
    uint64_t v145 = v218[11];
    uint64_t v144 = v218[12];
    uint64_t v143 = v218[13];
    uint64_t v141 = v218[15];
    uint64_t v142 = v218[14];
    uint64_t v139 = v218[17];
    uint64_t v140 = v218[16];
    uint64_t v136 = v218[18];
    uint64_t v138 = v218[19];
    uint64_t v48 = v220;
    int v137 = v219;
    int v92 = v221;
  }
  long long v49 = v25[9];
  __src[8] = v25[8];
  __src[9] = v49;
  long long v50 = v25[5];
  __src[4] = v25[4];
  __src[5] = v50;
  long long v51 = v25[6];
  __src[7] = v25[7];
  __src[6] = v51;
  long long v52 = v25[1];
  __src[0] = *v25;
  __src[1] = v52;
  long long v53 = v25[2];
  __src[3] = v25[3];
  __src[2] = v53;
  long long v54 = v18[8];
  long long v55 = v18[9];
  long long v56 = v18[6];
  __src[18] = v18[7];
  __src[19] = v54;
  long long v57 = v18[10];
  __src[20] = v55;
  __src[21] = v57;
  long long v58 = v18[4];
  long long v59 = v18[5];
  long long v60 = v18[2];
  __src[14] = v18[3];
  __src[15] = v58;
  long long v61 = v25[10];
  __src[16] = v59;
  __src[17] = v56;
  long long v62 = *v18;
  long long v63 = v18[1];
  __src[10] = v61;
  __src[11] = v62;
  __src[12] = v63;
  __src[13] = v60;
  long long v64 = v24[9];
  __src[30] = v24[8];
  __src[31] = v64;
  __src[32] = v24[10];
  long long v65 = v24[5];
  __src[26] = v24[4];
  __src[27] = v65;
  long long v66 = v24[6];
  __src[29] = v24[7];
  __src[28] = v66;
  long long v67 = v24[1];
  _OWORD __src[22] = *v24;
  __src[23] = v67;
  long long v68 = v24[2];
  __src[25] = v24[3];
  __src[24] = v68;
  outlined init with take of BNNS.NormalizationType((uint64_t)v222, (uint64_t)v223);
  uint64_t v155 = v46;
  uint64_t v154 = v45;
  switch(_s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v223))
  {
    case 2u:
      uint64_t v73 = v47;
      _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v223);
      uint64_t v113 = 0;
      uint64_t v114 = 0;
      uint64_t v111 = 0;
      uint64_t v112 = 0;
      uint64_t v109 = 0;
      uint64_t v110 = 0;
      uint64_t v107 = 0;
      uint64_t v108 = 0;
      uint64_t v105 = 0;
      uint64_t v106 = 0;
      uint64_t v103 = 0;
      uint64_t v104 = 0;
      uint64_t v101 = 0;
      uint64_t v102 = 0;
      uint64_t v99 = 0;
      uint64_t v100 = 0;
      uint64_t v96 = 0;
      uint64_t v97 = 0;
      int v95 = 0;
      uint64_t v133 = 0;
      uint64_t v134 = 0;
      uint64_t v131 = 0;
      uint64_t v132 = 0;
      uint64_t v128 = 0;
      uint64_t v129 = 0;
      uint64_t v126 = 0;
      uint64_t v127 = 0;
      uint64_t v124 = 0;
      uint64_t v125 = 0;
      uint64_t v122 = 0;
      uint64_t v123 = 0;
      uint64_t v121 = 0;
      uint64_t v118 = 0;
      uint64_t v119 = 0;
      uint64_t v116 = 0;
      uint64_t v117 = 0;
      uint64_t v115 = 0;
      int v120 = 0;
      uint64_t v74 = 0;
      uint64_t v130 = 0;
      goto LABEL_9;
    case 3u:
      uint64_t v73 = v47;
      uint64_t v113 = 0;
      uint64_t v114 = 0;
      uint64_t v111 = 0;
      uint64_t v112 = 0;
      uint64_t v109 = 0;
      uint64_t v110 = 0;
      uint64_t v107 = 0;
      uint64_t v108 = 0;
      uint64_t v105 = 0;
      uint64_t v106 = 0;
      uint64_t v103 = 0;
      uint64_t v104 = 0;
      uint64_t v101 = 0;
      uint64_t v102 = 0;
      uint64_t v99 = 0;
      uint64_t v100 = 0;
      uint64_t v96 = 0;
      uint64_t v97 = 0;
      int v95 = 0;
      uint64_t v133 = 0;
      uint64_t v134 = 0;
      uint64_t v131 = 0;
      uint64_t v132 = 0;
      uint64_t v128 = 0;
      uint64_t v129 = 0;
      uint64_t v126 = 0;
      uint64_t v127 = 0;
      uint64_t v124 = 0;
      uint64_t v125 = 0;
      uint64_t v122 = 0;
      uint64_t v123 = 0;
      uint64_t v121 = 0;
      uint64_t v118 = 0;
      uint64_t v119 = 0;
      uint64_t v116 = 0;
      uint64_t v117 = 0;
      uint64_t v115 = 0;
      int v120 = 0;
      uint64_t v130 = 0;
      uint64_t v74 = *(void *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v223);
LABEL_9:
      uint64_t v72 = 0;
      uint64_t v71 = 0;
      uint64_t v98 = 0;
      uint64_t v93 = 0;
      uint64_t v94 = 0;
      char v75 = v135;
      break;
    default:
      uint64_t v69 = (const void *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v223);
      memcpy(__dst, v69, 0x169uLL);
      char v70 = &demangling cache variable for type metadata for BNNSNDArrayDescriptor?;
      outlined init with take of BNNSNDArrayDescriptor?((uint64_t)__dst, (uint64_t)v208, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&__dst[23], (uint64_t)v209, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v208, (uint64_t)__dst, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      uint64_t v71 = 0;
      if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)__dst) == 1)
      {
        int v120 = 0;
        uint64_t v121 = 0;
        uint64_t v122 = 0;
        uint64_t v115 = 0;
        uint64_t v116 = 0;
        uint64_t v117 = 0;
        uint64_t v118 = 0;
        uint64_t v119 = 0;
        uint64_t v123 = 0;
        uint64_t v124 = 0;
        uint64_t v125 = 0;
        uint64_t v126 = 0;
        uint64_t v127 = 0;
        uint64_t v128 = 0;
        uint64_t v129 = 0;
        uint64_t v131 = 0;
        uint64_t v132 = 0;
        uint64_t v133 = 0;
        uint64_t v134 = 0;
        uint64_t v130 = 0;
        uint64_t v72 = 0;
        uint64_t v98 = 0;
      }
      else
      {
        outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v208, (uint64_t)v199, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
        uint64_t v130 = v199[0];
        uint64_t v133 = v199[2];
        uint64_t v134 = v199[1];
        uint64_t v131 = v199[4];
        uint64_t v132 = v199[3];
        uint64_t v128 = v199[6];
        uint64_t v129 = v199[5];
        uint64_t v126 = v199[8];
        uint64_t v127 = v199[7];
        uint64_t v124 = v199[10];
        uint64_t v125 = v199[9];
        uint64_t v121 = v199[12];
        uint64_t v118 = v199[14];
        uint64_t v119 = v199[13];
        uint64_t v116 = v199[16];
        uint64_t v117 = v199[15];
        uint64_t v115 = v199[17];
        uint64_t v98 = v199[18];
        uint64_t v122 = v199[19];
        uint64_t v123 = v199[11];
        int v120 = v199[20];
        uint64_t v72 = *(void *)((char *)&v199[20] + 4);
        LODWORD(v70) = HIDWORD(v199[21]);
      }
      outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v209, (uint64_t)v199, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      int v91 = (int)v70;
      if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v199) == 1)
      {
        int v95 = 0;
        uint64_t v96 = 0;
        uint64_t v97 = 0;
        uint64_t v99 = 0;
        uint64_t v100 = 0;
        uint64_t v101 = 0;
        uint64_t v102 = 0;
        uint64_t v103 = 0;
        uint64_t v104 = 0;
        uint64_t v105 = 0;
        uint64_t v106 = 0;
        uint64_t v107 = 0;
        uint64_t v108 = 0;
        uint64_t v109 = 0;
        uint64_t v110 = 0;
        uint64_t v111 = 0;
        uint64_t v112 = 0;
        uint64_t v113 = 0;
        uint64_t v114 = 0;
        uint64_t v93 = 0;
        uint64_t v94 = 0;
      }
      else
      {
        outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v209, (uint64_t)v207, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
        uint64_t v94 = *(void *)&v207[0];
        uint64_t v113 = *(void *)&v207[1];
        uint64_t v114 = *((void *)&v207[0] + 1);
        uint64_t v111 = *(void *)&v207[2];
        uint64_t v112 = *((void *)&v207[1] + 1);
        uint64_t v109 = *(void *)&v207[3];
        uint64_t v110 = *((void *)&v207[2] + 1);
        uint64_t v107 = *(void *)&v207[4];
        uint64_t v108 = *((void *)&v207[3] + 1);
        uint64_t v105 = *(void *)&v207[5];
        uint64_t v106 = *((void *)&v207[4] + 1);
        uint64_t v103 = *(void *)&v207[6];
        uint64_t v104 = *((void *)&v207[5] + 1);
        uint64_t v101 = *(void *)&v207[7];
        uint64_t v102 = *((void *)&v207[6] + 1);
        uint64_t v99 = *(void *)&v207[8];
        uint64_t v100 = *((void *)&v207[7] + 1);
        uint64_t v96 = *((void *)&v207[9] + 1);
        uint64_t v93 = *(void *)&v207[9];
        uint64_t v97 = *((void *)&v207[8] + 1);
        int v95 = v207[10];
        uint64_t v71 = *(void *)((char *)&v207[10] + 4);
        int v90 = HIDWORD(v207[10]);
      }
      char v75 = v135;
      uint64_t v73 = v47;
      uint64_t v74 = 0;
      break;
  }
  __dst[0] = v44;
  LOBYTE(__dst[1]) = v75;
  BNNS.ActivationFunction.bnnsActivation.getter((uint64_t)&v211);
  uint64_t v76 = v212;
  uint64_t v77 = v213;
  int v78 = v211;
  int v79 = v214;
  uint64_t v80 = v215;
  uint64_t v81 = v216;
  uint64_t v82 = v217;
  outlined init with take of BNNS.NormalizationType((uint64_t)v222, (uint64_t)v224);
  if (_s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v224) == 2) {
    uint64_t v83 = *(void *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v224);
  }
  else {
    uint64_t v83 = 0;
  }
  v207[8] = v255;
  v207[9] = v256;
  v207[4] = v251;
  v207[5] = v252;
  v207[7] = v254;
  v207[6] = v253;
  v207[0] = v247;
  v207[1] = v248;
  v207[3] = v250;
  v207[2] = v249;
  v207[18] = v254;
  v207[19] = v255;
  v207[20] = v256;
  v207[21] = v257;
  v207[14] = v250;
  v207[15] = v251;
  v207[16] = v252;
  v207[17] = v253;
  v207[10] = v257;
  v207[11] = v247;
  v207[12] = v248;
  v207[13] = v249;
  v207[30] = v244;
  v207[31] = v245;
  v207[26] = v240;
  v207[27] = v241;
  v207[29] = v243;
  v207[28] = v242;
  v207[22] = v236;
  v207[23] = v237;
  v207[25] = v239;
  v207[24] = v238;
  v207[40] = v232;
  v207[41] = v233;
  v207[42] = v234;
  v207[43] = v235;
  v207[36] = v228;
  v207[37] = v229;
  v207[38] = v230;
  v207[39] = v231;
  v207[32] = v246;
  v207[33] = v225;
  v207[34] = v226;
  v207[35] = v227;
  memcpy(v199, __src, 0x210uLL);
  v199[66] = v150;
  v199[67] = v154;
  v199[68] = v155;
  v199[69] = v73;
  v199[70] = v153;
  v199[71] = v152;
  v199[72] = v151;
  v199[73] = v149;
  v199[74] = v148;
  v199[75] = v147;
  v199[76] = v146;
  v199[77] = v145;
  v199[78] = v144;
  v199[79] = v143;
  v199[80] = v142;
  v199[81] = v141;
  v199[82] = v140;
  v199[83] = v139;
  v199[84] = v136;
  v199[85] = v138;
  int v200 = v137;
  uint64_t v201 = v48;
  int v202 = v92;
  uint64_t v203 = 0x7FC0000000000000;
  uint64_t v204 = 0x17FC00000;
  long long v206 = 0u;
  long long v205 = 0u;
  memcpy(__dst, v207, 0x2C0uLL);
  __dst[89] = v134;
  __dst[90] = v133;
  __dst[91] = v132;
  __dst[92] = v131;
  __dst[93] = v129;
  __dst[94] = v128;
  __dst[95] = v127;
  __dst[96] = v126;
  __dst[97] = v125;
  __dst[98] = v124;
  __dst[99] = v123;
  __dst[100] = v121;
  __dst[101] = v119;
  __dst[102] = v118;
  __dst[103] = v117;
  __dst[104] = v116;
  __dst[105] = v115;
  __dst[88] = v130;
  __dst[106] = v98;
  __dst[107] = v122;
  int v162 = v120;
  uint64_t v163 = v72;
  int v164 = v91;
  uint64_t v165 = v94;
  uint64_t v166 = v114;
  uint64_t v167 = v113;
  uint64_t v168 = v112;
  uint64_t v169 = v111;
  uint64_t v170 = v110;
  uint64_t v171 = v109;
  uint64_t v172 = v108;
  uint64_t v173 = v107;
  uint64_t v174 = v106;
  uint64_t v175 = v105;
  uint64_t v176 = v104;
  uint64_t v177 = v103;
  uint64_t v178 = v102;
  uint64_t v179 = v101;
  uint64_t v180 = v100;
  uint64_t v181 = v99;
  uint64_t v182 = v97;
  uint64_t v183 = v93;
  uint64_t v184 = v96;
  int v185 = v95;
  uint64_t v186 = v71;
  int v187 = v90;
  int v188 = v22;
  int v189 = v20;
  int v190 = v78;
  uint64_t v191 = v76;
  uint64_t v192 = v77;
  int v193 = v79;
  uint64_t v194 = v80;
  uint64_t v195 = v81;
  uint64_t v196 = v82;
  uint64_t v197 = v74;
  uint64_t v198 = v83;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
  uint64_t v84 = swift_allocObject();
  *(_OWORD *)(v84 + 16) = xmmword_1D2135290;
  *(void *)(v84 + 32) = v199;
  *(void *)(v84 + 40) = __dst;
  uint64_t v156 = (char *)v84;
  if (a11 == 1)
  {
    uint64_t v85 = 0;
  }
  else
  {
    int v157 = a9;
    uint64_t v158 = a10;
    uint64_t v159 = a11;
    uint64_t v160 = a12;
    uint64_t v85 = (const BNNSFilterParameters *)&v157;
  }
  uint64_t v86 = closure #1 in closure #1 in closure #1 in BNNS.FusedConvolutionNormalizationLayer.init(input:output:convolutionWeights:convolutionBias:convolutionStride:convolutionDilationStride:convolutionPadding:normalization:normalizationBeta:normalizationGamma:normalizationMomentum:normalizationEpsilon:normalizationActivation:filterParameters:)(v85, (uint64_t)v222, &v156, 1);
  swift_bridgeObjectRelease();
  type metadata accessor for BNNS.FusedFullyConnectedNormalizationLayer();
  uint64_t v87 = swift_allocObject();
  uint64_t v88 = v87;
  if (v86)
  {
    *(void *)(v87 + 16) = v86;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v88;
}

void *closure #1 in closure #1 in closure #1 in BNNS.FusedConvolutionNormalizationLayer.init(input:output:convolutionWeights:convolutionBias:convolutionStride:convolutionDilationStride:convolutionPadding:normalization:normalizationBeta:normalizationGamma:normalizationMomentum:normalizationEpsilon:normalizationActivation:filterParameters:)(const BNNSFilterParameters *a1, uint64_t a2, char **a3, int a4)
{
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<BNNSFilterType>);
  uint64_t v8 = swift_allocObject();
  *(_DWORD *)(v8 + 32) = a4;
  outlined init with take of BNNS.NormalizationType(a2, (uint64_t)v14);
  int v9 = _s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v14) + 2;
  _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v14);
  *(_DWORD *)(v8 + 36) = v9;
  unint64_t v10 = *a3;
  char isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
  *a3 = v10;
  if ((isUniquelyReferenced_nonNull_native & 1) == 0) {
    unint64_t v10 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v10 + 2), 0, v10);
  }
  *a3 = v10;
  FusedLayer = BNNSFilterCreateFusedLayer(2uLL, (const BNNSFilterType *)(v8 + 32), (const void **)v10 + 4, a1);
  swift_setDeallocating();
  swift_deallocClassInstance();
  return FusedLayer;
}

uint64_t BNNS.FusedLayer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

uint64_t specialized static BNNS.fusedLayerApply(_:batchSize:input:output:for:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4, char a5)
{
  uint64_t v48 = a4;
  int v25 = *(void **)(a1 + 16);
  outlined init with take of BNNSNDArrayDescriptor?(a3 + 136, (uint64_t)v45, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  uint64_t result = outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v45, (uint64_t)&v46, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  in = v46;
  if (!v46)
  {
    __break(1u);
    goto LABEL_7;
  }
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v42);
  outlined init with take of BNNS.Shape((uint64_t)v42, (uint64_t)v43);
  outlined init with take of BNNS.Shape((uint64_t)v43, (uint64_t)v41);
  BNNS.Shape.size.getter((uint64_t)&v33);
  unint64_t v6 = v33;
  unint64_t v7 = v34;
  unint64_t v8 = v35;
  unint64_t v9 = v36;
  unint64_t v10 = v37;
  unint64_t v11 = v38;
  unint64_t v12 = v39;
  unint64_t v13 = v40;
  outlined init with take of BNNS.Shape((uint64_t)v43, (uint64_t)v41);
  BNNS.Shape.stride.getter((uint64_t)&v33);
  size_t v22 = specialized static BNNS.calculateBatchStride(size:stride:)(v6, v7, v8, v9, v10, v11, v12, v13, v33, v34, v35, v36, v37, v38, v39, v40);
  outlined init with take of BNNSNDArrayDescriptor?(v48 + 136, (uint64_t)v44, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  uint64_t result = outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v44, (uint64_t)&v47, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  int v21 = v47;
  if (!v47)
  {
LABEL_7:
    __break(1u);
    return result;
  }
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v33);
  outlined init with take of BNNS.Shape((uint64_t)&v33, (uint64_t)v41);
  outlined init with take of BNNS.Shape((uint64_t)v41, (uint64_t)v32);
  BNNS.Shape.size.getter((uint64_t)&v27);
  long long v14 = v27;
  long long v15 = v28;
  long long v16 = v29;
  unint64_t v17 = v30;
  unint64_t v18 = v31;
  outlined init with take of BNNS.Shape((uint64_t)v41, (uint64_t)v32);
  BNNS.Shape.stride.getter((uint64_t)&v27);
  size_t v19 = specialized static BNNS.calculateBatchStride(size:stride:)(v14, *((unint64_t *)&v14 + 1), v15, *((unint64_t *)&v15 + 1), v16, *((unint64_t *)&v16 + 1), v17, v18, v27, *((unint64_t *)&v27 + 1), v28, *((unint64_t *)&v28 + 1), v29, *((unint64_t *)&v29 + 1), v30, v31);
  uint64_t result = BNNSFusedFilterApplyBatch(v25, a2, in, v22, v21, v19, (a5 & 1) == 0);
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *int v20 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t specialized static BNNS.fusedLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingParameterGradients:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4, _OWORD *a5, long long *a6, uint64_t a7)
{
  uint64_t v13 = a1;
  uint64_t v61 = *MEMORY[0x1E4F143B8];
  int64_t v14 = *(void *)(a7 + 16);
  uint64_t v15 = MEMORY[0x1E4FBC860];
  if (v14)
  {
    size_t v54 = a2;
    uint64_t v55 = a3;
    uint64_t v56 = a4;
    *(void *)&v59.flags = MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v14, 0);
    long long v16 = (_OWORD *)(a7 + 32);
    uint64_t v15 = *(void *)&v59.flags;
    do
    {
      long long v17 = v16[9];
      *(_OWORD *)&v60.stride[7] = v16[8];
      *(_OWORD *)&v60.data_type = v17;
      *(_OWORD *)&v60.table_data_type = v16[10];
      long long v18 = v16[5];
      *(_OWORD *)&v60.size[7] = v16[4];
      *(_OWORD *)&v60.stride[1] = v18;
      long long v19 = v16[7];
      *(_OWORD *)&v60.stride[3] = v16[6];
      *(_OWORD *)&v60.stride[5] = v19;
      long long v20 = v16[1];
      *(_OWORD *)&v60.flags = *v16;
      *(_OWORD *)&v60.size[1] = v20;
      long long v21 = v16[3];
      *(_OWORD *)&v60.size[3] = v16[2];
      *(_OWORD *)&v60.size[5] = v21;
      size_t v22 = (_OWORD *)swift_slowAlloc();
      long long v23 = *(_OWORD *)&v60.data_type;
      v22[8] = *(_OWORD *)&v60.stride[7];
      v22[9] = v23;
      v22[10] = *(_OWORD *)&v60.table_data_type;
      long long v24 = *(_OWORD *)&v60.stride[1];
      v22[4] = *(_OWORD *)&v60.size[7];
      void v22[5] = v24;
      long long v25 = *(_OWORD *)&v60.stride[5];
      v22[6] = *(_OWORD *)&v60.stride[3];
      v22[7] = v25;
      long long v26 = *(_OWORD *)&v60.size[1];
      *size_t v22 = *(_OWORD *)&v60.flags;
      v22[1] = v26;
      long long v27 = *(_OWORD *)&v60.size[5];
      v22[2] = *(_OWORD *)&v60.size[3];
      v22[3] = v27;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(void *)(v15 + 16) + 1, 1);
        uint64_t v15 = *(void *)&v59.flags;
      }
      unint64_t v29 = *(void *)(v15 + 16);
      unint64_t v28 = *(void *)(v15 + 24);
      if (v29 >= v28 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v28 > 1), v29 + 1, 1);
        uint64_t v15 = *(void *)&v59.flags;
      }
      *(void *)(v15 + 16) = v29 + 1;
      *(void *)(v15 + 8 * v29 + 32) = v22;
      v16 += 11;
      --v14;
    }
    while (v14);
    a4 = v56;
    a2 = v54;
    a3 = v55;
    uint64_t v13 = a1;
  }
  long long v30 = a6[8];
  long long v31 = a6[9];
  long long v32 = a6[6];
  *(_OWORD *)&v60.stride[5] = a6[7];
  *(_OWORD *)&v60.stride[7] = v30;
  long long v33 = a6[10];
  *(_OWORD *)&v60.data_type = v31;
  *(_OWORD *)&v60.table_data_type = v33;
  long long v34 = a6[4];
  long long v35 = a6[5];
  long long v36 = a6[2];
  *(_OWORD *)&v60.size[5] = a6[3];
  *(_OWORD *)&v60.size[7] = v34;
  long long v58 = (char *)v15;
  *(_OWORD *)&v60.stride[1] = v35;
  *(_OWORD *)&v60.stride[3] = v32;
  long long v37 = *a6;
  *(_OWORD *)&v60.size[1] = a6[1];
  *(_OWORD *)&v60.size[3] = v36;
  long long v38 = a5[9];
  *(_OWORD *)&v59.stride[7] = a5[8];
  *(_OWORD *)&v59.data_type = v38;
  *(_OWORD *)&v59.table_data_type = a5[10];
  *(_OWORD *)&v60.flags = v37;
  long long v39 = a5[5];
  *(_OWORD *)&v59.size[7] = a5[4];
  *(_OWORD *)&v59.stride[1] = v39;
  long long v40 = a5[7];
  *(_OWORD *)&v59.stride[3] = a5[6];
  *(_OWORD *)&v59.stride[5] = v40;
  long long v41 = a5[1];
  *(_OWORD *)&v59.flags = *a5;
  *(_OWORD *)&v59.size[1] = v41;
  long long v42 = a5[3];
  *(_OWORD *)&v59.size[3] = a5[2];
  *(_OWORD *)&v59.size[5] = v42;
  closure #1 in closure #2 in static BNNS.fusedLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingParameterGradients:)(&v59, v13, a2, a3, &v60, a4, &v57, &v58);
  if (v57)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *uint64_t v43 = 0;
    swift_willThrow();
    uint64_t v44 = v58;
    uint64_t v45 = *((void *)v58 + 2);
    if (v45)
    {
      swift_bridgeObjectRetain();
      for (uint64_t i = 0; i != v45; ++i)
      {
        uint64_t v47 = *(void *)&v44[8 * i + 32];
        if (v47) {
          MEMORY[0x1D26009C0](v47, -1, -1);
        }
      }
LABEL_20:
      swift_bridgeObjectRelease();
    }
  }
  else
  {
    uint64_t v48 = v58;
    uint64_t v49 = *((void *)v58 + 2);
    if (v49)
    {
      swift_bridgeObjectRetain();
      for (uint64_t j = 0; j != v49; ++j)
      {
        uint64_t v51 = *(void *)&v48[8 * j + 32];
        if (v51) {
          MEMORY[0x1D26009C0](v51, -1, -1);
        }
      }
      goto LABEL_20;
    }
  }
  return swift_bridgeObjectRelease();
}

uint64_t outlined init with take of BNNS.NormalizationType(uint64_t a1, uint64_t a2)
{
  return a2;
}

uint64_t _s10Accelerate4BNNSO17NormalizationTypeOWOg(uint64_t a1)
{
  return *(unsigned __int8 *)(a1 + 360) >> 6;
}

uint64_t type metadata accessor for BNNS.FusedConvolutionNormalizationLayer()
{
  return self;
}

uint64_t _s10Accelerate4BNNSO17NormalizationTypeOWOj0_(uint64_t result)
{
  *(unsigned char *)(result + 360) &= 0x3Fu;
  return result;
}

uint64_t type metadata accessor for BNNS.FusedFullyConnectedNormalizationLayer()
{
  return self;
}

unint64_t lazy protocol witness table accessor for type BNNS.LearningPhase and conformance BNNS.LearningPhase()
{
  unint64_t result = lazy protocol witness table cache variable for type BNNS.LearningPhase and conformance BNNS.LearningPhase;
  if (!lazy protocol witness table cache variable for type BNNS.LearningPhase and conformance BNNS.LearningPhase)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.LearningPhase and conformance BNNS.LearningPhase);
  }
  return result;
}

unsigned char *storeEnumTagSinglePayload for BNNS.LearningPhase(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 1 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFF) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFE)
  {
    unsigned int v6 = ((a2 - 255) >> 8) + 1;
    *unint64_t result = a2 + 1;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D20A002CLL);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 1;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.LearningPhase()
{
  return &type metadata for BNNS.LearningPhase;
}

uint64_t type metadata accessor for BNNS.FusedLayer()
{
  return self;
}

uint64_t method lookup function for BNNS.FusedLayer(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.FusedLayer);
}

uint64_t dispatch thunk of BNNS.FusedLayer.apply(batchSize:input:output:for:)(uint64_t a1, uint64_t *a2, uint64_t *a3, char a4)
{
  uint64_t v5 = a2[17];
  int v6 = *((_DWORD *)a2 + 36);
  uint64_t v7 = a2[19];
  int v8 = *((_DWORD *)a2 + 40);
  uint64_t v9 = a3[17];
  int v10 = *((_DWORD *)a3 + 36);
  uint64_t v11 = a3[19];
  int v12 = *((_DWORD *)a3 + 40);
  uint64_t v13 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, void))(*(void *)v4 + 96);
  uint64_t v29 = *a2;
  long long v30 = *(_OWORD *)(a2 + 1);
  long long v31 = *(_OWORD *)(a2 + 3);
  long long v32 = *(_OWORD *)(a2 + 5);
  long long v33 = *(_OWORD *)(a2 + 7);
  long long v34 = *(_OWORD *)(a2 + 9);
  long long v35 = *(_OWORD *)(a2 + 11);
  long long v36 = *(_OWORD *)(a2 + 13);
  long long v37 = *(_OWORD *)(a2 + 15);
  uint64_t v38 = v5;
  int v39 = v6;
  uint64_t v40 = v7;
  int v41 = v8;
  uint64_t v42 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v15 = *a3;
  long long v16 = *(_OWORD *)(a3 + 1);
  long long v17 = *(_OWORD *)(a3 + 3);
  long long v18 = *(_OWORD *)(a3 + 5);
  long long v19 = *(_OWORD *)(a3 + 7);
  long long v20 = *(_OWORD *)(a3 + 9);
  long long v21 = *(_OWORD *)(a3 + 11);
  long long v22 = *(_OWORD *)(a3 + 13);
  long long v23 = *(_OWORD *)(a3 + 15);
  uint64_t v24 = v9;
  int v25 = v10;
  uint64_t v26 = v11;
  int v27 = v12;
  uint64_t v28 = *(uint64_t *)((char *)a3 + 164);
  return v13(a1, &v29, &v15, a4 & 1);
}

uint64_t dispatch thunk of BNNS.FusedLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingParameterGradients:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5, uint64_t a6)
{
  uint64_t v7 = a2[17];
  int v8 = *((_DWORD *)a2 + 36);
  uint64_t v9 = a2[19];
  int v10 = *((_DWORD *)a2 + 40);
  uint64_t v11 = a3[17];
  int v12 = *((_DWORD *)a3 + 36);
  uint64_t v13 = a3[19];
  int v14 = *((_DWORD *)a3 + 40);
  uint64_t v15 = a4[17];
  int v16 = *((_DWORD *)a4 + 36);
  uint64_t v17 = a4[19];
  int v18 = *((_DWORD *)a4 + 40);
  uint64_t v19 = a5[17];
  int v20 = *((_DWORD *)a5 + 36);
  uint64_t v21 = a5[19];
  int v22 = *((_DWORD *)a5 + 40);
  uint64_t v44 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t))(*(void *)v6 + 104);
  uint64_t v87 = *a2;
  long long v88 = *(_OWORD *)(a2 + 1);
  long long v89 = *(_OWORD *)(a2 + 3);
  long long v90 = *(_OWORD *)(a2 + 5);
  long long v91 = *(_OWORD *)(a2 + 7);
  long long v92 = *(_OWORD *)(a2 + 9);
  long long v93 = *(_OWORD *)(a2 + 11);
  long long v94 = *(_OWORD *)(a2 + 13);
  long long v95 = *(_OWORD *)(a2 + 15);
  uint64_t v96 = v7;
  int v97 = v8;
  uint64_t v98 = v9;
  int v99 = v10;
  uint64_t v100 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v73 = *a3;
  long long v74 = *(_OWORD *)(a3 + 1);
  long long v75 = *(_OWORD *)(a3 + 3);
  long long v76 = *(_OWORD *)(a3 + 5);
  long long v77 = *(_OWORD *)(a3 + 7);
  long long v78 = *(_OWORD *)(a3 + 9);
  long long v79 = *(_OWORD *)(a3 + 11);
  long long v23 = *(_OWORD *)(a3 + 15);
  long long v80 = *(_OWORD *)(a3 + 13);
  long long v24 = *(_OWORD *)(a4 + 1);
  long long v25 = *(_OWORD *)(a4 + 3);
  long long v26 = *(_OWORD *)(a4 + 5);
  long long v27 = *(_OWORD *)(a4 + 7);
  long long v28 = *(_OWORD *)(a4 + 9);
  long long v29 = *(_OWORD *)(a4 + 11);
  long long v30 = *(_OWORD *)(a4 + 13);
  uint64_t v31 = *a4;
  long long v32 = *(_OWORD *)(a4 + 15);
  long long v33 = *(_OWORD *)(a5 + 1);
  long long v34 = *(_OWORD *)(a5 + 3);
  long long v35 = *(_OWORD *)(a5 + 5);
  long long v36 = *(_OWORD *)(a5 + 7);
  long long v37 = *(_OWORD *)(a5 + 9);
  long long v38 = *(_OWORD *)(a5 + 11);
  long long v39 = *(_OWORD *)(a5 + 13);
  uint64_t v40 = *a5;
  long long v41 = *(_OWORD *)(a5 + 15);
  long long v81 = v23;
  uint64_t v82 = v11;
  int v83 = v12;
  uint64_t v84 = v13;
  int v85 = v14;
  uint64_t v86 = *(uint64_t *)((char *)a3 + 164);
  *(void *)&long long v23 = *(uint64_t *)((char *)a5 + 164);
  uint64_t v59 = v31;
  long long v60 = v24;
  *(void *)&long long v24 = *(uint64_t *)((char *)a4 + 164);
  long long v61 = v25;
  long long v62 = v26;
  long long v63 = v27;
  long long v64 = v28;
  long long v65 = v29;
  long long v66 = v30;
  long long v67 = v32;
  uint64_t v68 = v15;
  int v69 = v16;
  uint64_t v70 = v17;
  int v71 = v18;
  uint64_t v72 = v24;
  uint64_t v45 = v40;
  long long v46 = v33;
  long long v47 = v34;
  long long v48 = v35;
  long long v49 = v36;
  long long v50 = v37;
  long long v51 = v38;
  long long v52 = v39;
  long long v53 = v41;
  uint64_t v54 = v19;
  int v55 = v20;
  uint64_t v56 = v21;
  int v57 = v22;
  uint64_t v58 = v23;
  return v44(a1, &v87, &v73, &v59, &v45, a6);
}

uint64_t vImage.PixelBuffer<>.flatten(channelOrdering:backgroundColor:isPremultiplied:destination:)(unsigned char *a1, char a2, char a3, char a4, char a5, char a6, uint64_t a7)
{
  void v21[4] = *MEMORY[0x1E4F143B8];
  int v8 = *(void **)v7;
  if (!*(void *)(*(void *)v7 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v9 = v8[6];
  if (v9 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  uint64_t v10 = v8[5];
  if (v10 < 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v9)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v10)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v11 = *(void **)a7;
  if (!*(void *)(*(void *)a7 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v12 = v11[6];
  if (v12 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v13 = v11[5];
  if (v13 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v12)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v13)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v9 != v12)
  {
LABEL_27:
    __break(1u);
LABEL_28:
    __break(1u);
  }
  if (v10 != v13) {
    goto LABEL_28;
  }
  uint64_t v14 = v8[4];
  uint64_t v15 = v8[7];
  if (*a1) {
    int v16 = @nonobjc vImageFlatten_RGBA8888ToRGB888(_:_:_:_:_:);
  }
  else {
    int v16 = @nonobjc vImageFlatten_ARGB8888ToRGB888(_:_:_:_:_:);
  }
  v21[0] = v14;
  v21[1] = v10;
  v21[2] = v9;
  v21[3] = v15;
  uint64_t v17 = v11[7];
  v20[0] = v11[4];
  v20[1] = v10;
  v20[2] = v9;
  v20[3] = v17;
  v19[0] = a2;
  v19[1] = a3;
  v19[2] = a4;
  v19[3] = a5;
  return v16((const vImage_Buffer *)v21, (const vImage_Buffer *)v20, v19, a6 & 1, 0);
}

vImage_Error @nonobjc vImageFlatten_ARGB8888ToRGB888(_:_:_:_:_:)(const vImage_Buffer *a1, const vImage_Buffer *a2, const uint8_t *a3, char a4, vImage_Flags a5)
{
  return vImageFlatten_ARGB8888ToRGB888(a1, a2, a3, a4 & 1, a5);
}

vImage_Error @nonobjc vImageFlatten_RGBA8888ToRGB888(_:_:_:_:_:)(const vImage_Buffer *a1, const vImage_Buffer *a2, const uint8_t *a3, char a4, vImage_Flags a5)
{
  return vImageFlatten_RGBA8888ToRGB888(a1, a2, a3, a4 & 1, a5);
}

uint64_t vImage.PixelBuffer<>.flatten(channelOrdering:backgroundColor:isPremultiplied:destination:)(unsigned char *a1, char a2, uint64_t a3, float a4, float a5, float a6, float a7)
{
  v22[4] = *MEMORY[0x1E4F143B8];
  int v8 = *(void **)v7;
  if (!*(void *)(*(void *)v7 + 16))
  {
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v9 = v8[6];
  if (v9 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v10 = v8[5];
  if (v10 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v9)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v10)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v11 = *(void **)a3;
  if (!*(void *)(*(void *)a3 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v12 = v11[6];
  if (v12 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v13 = v11[5];
  if (v13 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v12)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v13)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v9 != v12)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v10 != v13) {
    goto LABEL_27;
  }
  uint64_t v14 = @nonobjc vImageFlatten_ARGBFFFFToRGBFFF(_:_:_:_:_:);
  uint64_t v15 = v8[4];
  uint64_t v16 = v8[7];
  if (*a1) {
    uint64_t v14 = @nonobjc vImageFlatten_RGBAFFFFToRGBFFF(_:_:_:_:_:);
  }
  v22[0] = v15;
  v22[1] = v10;
  v22[2] = v9;
  v22[3] = v16;
  uint64_t v17 = v11[4];
  uint64_t v18 = v11[7];
  v21[0] = v17;
  v21[1] = v10;
  v21[2] = v9;
  v21[3] = v18;
  *(float *)int v20 = a4;
  *(float *)&v20[1] = a5;
  *(float *)&v20[2] = a6;
  *(float *)&v20[3] = a7;
  return v14((const vImage_Buffer *)v22, (const vImage_Buffer *)v21, (const float *)v20, a2 & 1, 0);
}

vImage_Error @nonobjc vImageFlatten_ARGBFFFFToRGBFFF(_:_:_:_:_:)(const vImage_Buffer *a1, const vImage_Buffer *a2, const float *a3, char a4, vImage_Flags a5)
{
  return vImageFlatten_ARGBFFFFToRGBFFF(a1, a2, a3, a4 & 1, a5);
}

vImage_Error @nonobjc vImageFlatten_RGBAFFFFToRGBFFF(_:_:_:_:_:)(const vImage_Buffer *a1, const vImage_Buffer *a2, const float *a3, char a4, vImage_Flags a5)
{
  return vImageFlatten_RGBAFFFFToRGBFFF(a1, a2, a3, a4 & 1, a5);
}

BOOL static vImage.ChannelOrdering.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void vImage.ChannelOrdering.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int vImage.ChannelOrdering.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

unint64_t lazy protocol witness table accessor for type vImage.ChannelOrdering and conformance vImage.ChannelOrdering()
{
  unint64_t result = lazy protocol witness table cache variable for type vImage.ChannelOrdering and conformance vImage.ChannelOrdering;
  if (!lazy protocol witness table cache variable for type vImage.ChannelOrdering and conformance vImage.ChannelOrdering)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.ChannelOrdering and conformance vImage.ChannelOrdering);
  }
  return result;
}

unsigned char *storeEnumTagSinglePayload for vImage.ChannelOrdering(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 1 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFF) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFE)
  {
    unsigned int v6 = ((a2 - 255) >> 8) + 1;
    *unint64_t result = a2 + 1;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D20A08B4);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 1;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vImage.ChannelOrdering()
{
  return &type metadata for vImage.ChannelOrdering;
}

uint64_t static BNNS.tile(input:output:filterParameters:)(long long *a1, long long *a2, int a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v54 = *MEMORY[0x1E4F143B8];
  if (a5 != 1)
  {
    int v28 = a3;
    uint64_t v29 = a4;
    uint64_t v30 = a5;
    uint64_t v31 = a6;
    long long v17 = a1[9];
    long long v51 = a1[8];
    long long v52 = v17;
    long long v53 = a1[10];
    long long v18 = a1[5];
    long long v47 = a1[4];
    long long v48 = v18;
    long long v19 = a1[7];
    long long v49 = a1[6];
    long long v50 = v19;
    long long v20 = a1[1];
    long long v43 = *a1;
    long long v44 = v20;
    long long v21 = a1[3];
    long long v45 = a1[2];
    long long v46 = v21;
    long long v22 = a2[9];
    long long v40 = a2[8];
    long long v41 = v22;
    long long v42 = a2[10];
    long long v23 = a2[5];
    long long v36 = a2[4];
    long long v37 = v23;
    long long v24 = a2[7];
    long long v38 = a2[6];
    long long v39 = v24;
    long long v25 = a2[1];
    long long v32 = *a2;
    long long v33 = v25;
    long long v26 = a2[3];
    long long v34 = a2[2];
    long long v35 = v26;
    uint64_t result = MEMORY[0x1D26003F0](&v43, &v32, &v28);
    if (!result) {
      return result;
    }
    goto LABEL_5;
  }
  long long v6 = a1[9];
  long long v51 = a1[8];
  long long v52 = v6;
  long long v53 = a1[10];
  long long v7 = a1[5];
  long long v47 = a1[4];
  long long v48 = v7;
  long long v8 = a1[7];
  long long v49 = a1[6];
  long long v50 = v8;
  long long v9 = a1[1];
  long long v43 = *a1;
  long long v44 = v9;
  long long v10 = a1[3];
  long long v45 = a1[2];
  long long v46 = v10;
  long long v11 = a2[9];
  long long v40 = a2[8];
  long long v41 = v11;
  long long v42 = a2[10];
  long long v12 = a2[5];
  long long v36 = a2[4];
  long long v37 = v12;
  long long v13 = a2[7];
  long long v38 = a2[6];
  long long v39 = v13;
  long long v14 = a2[1];
  long long v32 = *a2;
  long long v33 = v14;
  long long v15 = a2[3];
  long long v34 = a2[2];
  long long v35 = v15;
  uint64_t result = MEMORY[0x1D26003F0](&v43, &v32, 0);
  if (result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v27 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.tileBackward(outputGradient:generatingInputGradient:filterParameters:)(long long *a1, long long *a2, int a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v54 = *MEMORY[0x1E4F143B8];
  if (a5 != 1)
  {
    int v28 = a3;
    uint64_t v29 = a4;
    uint64_t v30 = a5;
    uint64_t v31 = a6;
    long long v17 = a1[9];
    long long v51 = a1[8];
    long long v52 = v17;
    long long v53 = a1[10];
    long long v18 = a1[5];
    long long v47 = a1[4];
    long long v48 = v18;
    long long v19 = a1[7];
    long long v49 = a1[6];
    long long v50 = v19;
    long long v20 = a1[1];
    long long v43 = *a1;
    long long v44 = v20;
    long long v21 = a1[3];
    long long v45 = a1[2];
    long long v46 = v21;
    long long v22 = a2[9];
    long long v40 = a2[8];
    long long v41 = v22;
    long long v42 = a2[10];
    long long v23 = a2[5];
    long long v36 = a2[4];
    long long v37 = v23;
    long long v24 = a2[7];
    long long v38 = a2[6];
    long long v39 = v24;
    long long v25 = a2[1];
    long long v32 = *a2;
    long long v33 = v25;
    long long v26 = a2[3];
    long long v34 = a2[2];
    long long v35 = v26;
    uint64_t result = MEMORY[0x1D2600400](&v32, &v43, &v28);
    if (!result) {
      return result;
    }
    goto LABEL_5;
  }
  long long v6 = a1[9];
  long long v51 = a1[8];
  long long v52 = v6;
  long long v53 = a1[10];
  long long v7 = a1[5];
  long long v47 = a1[4];
  long long v48 = v7;
  long long v8 = a1[7];
  long long v49 = a1[6];
  long long v50 = v8;
  long long v9 = a1[1];
  long long v43 = *a1;
  long long v44 = v9;
  long long v10 = a1[3];
  long long v45 = a1[2];
  long long v46 = v10;
  long long v11 = a2[9];
  long long v40 = a2[8];
  long long v41 = v11;
  long long v42 = a2[10];
  long long v12 = a2[5];
  long long v36 = a2[4];
  long long v37 = v12;
  long long v13 = a2[7];
  long long v38 = a2[6];
  long long v39 = v13;
  long long v14 = a2[1];
  long long v32 = *a2;
  long long v33 = v14;
  long long v15 = a2[3];
  long long v34 = a2[2];
  long long v35 = v15;
  uint64_t result = MEMORY[0x1D2600400](&v32, &v43, 0);
  if (result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v27 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.copyBandPart(_:to:lowerBandCount:upperBandCount:filterParameters:)(_OWORD *a1, _OWORD *a2, uint64_t a3, char a4, uint64_t a5, char a6, uint32_t a7, size_t a8, int (__cdecl *a9)(void **, size_t, size_t), void (__cdecl *a10)(void *))
{
  uint64_t v39 = *MEMORY[0x1E4F143B8];
  if (a4) {
    a3 = -1;
  }
  if (a3 < (uint64_t)0xFFFFFFFF80000000)
  {
    __break(1u);
    goto LABEL_18;
  }
  if (a3 > 0x7FFFFFFF)
  {
LABEL_18:
    __break(1u);
LABEL_19:
    __break(1u);
LABEL_20:
    __break(1u);
  }
  if (a6) {
    a5 = -1;
  }
  if (a5 < (uint64_t)0xFFFFFFFF80000000) {
    goto LABEL_19;
  }
  if (a5 > 0x7FFFFFFF) {
    goto LABEL_20;
  }
  if (a9 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    long long v14 = a1[9];
    *(_OWORD *)&input.stride[7] = a1[8];
    *(_OWORD *)&input.data_type = v14;
    *(_OWORD *)&input.table_data_type = a1[10];
    long long v15 = a1[5];
    *(_OWORD *)&input.size[7] = a1[4];
    *(_OWORD *)&input.stride[1] = v15;
    long long v16 = a1[7];
    *(_OWORD *)&input.stride[3] = a1[6];
    *(_OWORD *)&input.stride[5] = v16;
    long long v17 = a1[1];
    *(_OWORD *)&input.flags = *a1;
    *(_OWORD *)&input.size[1] = v17;
    long long v18 = a1[3];
    *(_OWORD *)&input.size[3] = a1[2];
    *(_OWORD *)&input.size[5] = v18;
    long long v19 = a2[9];
    *(_OWORD *)&output.stride[7] = a2[8];
    *(_OWORD *)&output.data_type = v19;
    *(_OWORD *)&output.table_data_type = a2[10];
    long long v20 = a2[5];
    *(_OWORD *)&output.size[7] = a2[4];
    *(_OWORD *)&output.stride[1] = v20;
    long long v21 = a2[7];
    *(_OWORD *)&output.stride[3] = a2[6];
    *(_OWORD *)&output.stride[5] = v21;
    long long v22 = a2[1];
    *(_OWORD *)&output.flags = *a2;
    *(_OWORD *)&output.size[1] = v22;
    long long v23 = a2[3];
    *(_OWORD *)&output.size[3] = a2[2];
    *(_OWORD *)&output.size[5] = v23;
    uint64_t result = BNNSBandPart(a3, a5, &input, &output, 0);
    if (!result) {
      return result;
    }
  }
  else
  {
    v36.flags = a7;
    v36.n_threads = a8;
    v36.alloc_memory = a9;
    v36.free_memory = a10;
    long long v25 = a1[9];
    *(_OWORD *)&input.stride[7] = a1[8];
    *(_OWORD *)&input.data_type = v25;
    *(_OWORD *)&input.table_data_type = a1[10];
    long long v26 = a1[5];
    *(_OWORD *)&input.size[7] = a1[4];
    *(_OWORD *)&input.stride[1] = v26;
    long long v27 = a1[7];
    *(_OWORD *)&input.stride[3] = a1[6];
    *(_OWORD *)&input.stride[5] = v27;
    long long v28 = a1[1];
    *(_OWORD *)&input.flags = *a1;
    *(_OWORD *)&input.size[1] = v28;
    long long v29 = a1[3];
    *(_OWORD *)&input.size[3] = a1[2];
    *(_OWORD *)&input.size[5] = v29;
    long long v30 = a2[9];
    *(_OWORD *)&output.stride[7] = a2[8];
    *(_OWORD *)&output.data_type = v30;
    *(_OWORD *)&output.table_data_type = a2[10];
    long long v31 = a2[5];
    *(_OWORD *)&output.size[7] = a2[4];
    *(_OWORD *)&output.stride[1] = v31;
    long long v32 = a2[7];
    *(_OWORD *)&output.stride[3] = a2[6];
    *(_OWORD *)&output.stride[5] = v32;
    long long v33 = a2[1];
    *(_OWORD *)&output.flags = *a2;
    *(_OWORD *)&output.size[1] = v33;
    long long v34 = a2[3];
    *(_OWORD *)&output.size[3] = a2[2];
    *(_OWORD *)&output.size[5] = v34;
    uint64_t result = BNNSBandPart(a3, a5, &input, &output, &v36);
    if (!result) {
      return result;
    }
  }
  lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
  swift_allocError();
  unsigned char *v35 = 0;
  return swift_willThrow();
}

BOOL static BNNS.CropResizeLayer.BoxCoordinateMode.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void BNNS.CropResizeLayer.BoxCoordinateMode.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

uint64_t BNNS.CropResizeLayer.__allocating_init(coordinatesAreNormalized:spatialScale:extrapolationValue:samplingMode:boxCoordinateMode:)(char a1, unsigned __int8 *a2, unsigned __int8 *a3, float a4, float a5)
{
  uint64_t result = swift_allocObject();
  int v11 = *a2;
  int v12 = *a3;
  *(unsigned char *)(result + 16) = a1;
  *(float *)(result + 20) = a4;
  *(float *)(result + 24) = a5;
  *(_DWORD *)(result + 28) = v11;
  *(_DWORD *)(result + 32) = v12;
  *(_DWORD *)(result + 36) = 1;
  return result;
}

uint64_t BNNS.CropResizeLayer.init(coordinatesAreNormalized:spatialScale:extrapolationValue:samplingMode:boxCoordinateMode:)(char a1, unsigned __int8 *a2, unsigned __int8 *a3, float a4, float a5)
{
  int v6 = *a2;
  int v7 = *a3;
  *(unsigned char *)(v5 + 16) = a1;
  *(float *)(v5 + 20) = a4;
  *(float *)(v5 + 24) = a5;
  *(_DWORD *)(v5 + 28) = v6;
  *(_DWORD *)(v5 + 32) = v7;
  *(_DWORD *)(v5 + 36) = 1;
  return v5;
}

uint64_t BNNS.CropResizeLayer.apply(input:regionOfInterest:output:filterParameters:)(long long *a1, long long *a2, long long *a3, int a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v94 = *MEMORY[0x1E4F143B8];
  if (a6 != 1)
  {
    uint64_t v55 = *(void *)(v7 + 20);
    uint64_t v56 = *(void *)(v7 + 28);
    long long v31 = a1[8];
    long long v32 = a1[9];
    long long v33 = a1[6];
    long long v90 = a1[7];
    long long v91 = v31;
    long long v34 = a1[10];
    long long v92 = v32;
    long long v93 = v34;
    long long v35 = a1[4];
    long long v36 = a1[5];
    long long v37 = a1[2];
    long long v86 = a1[3];
    long long v87 = v35;
    long long v88 = v36;
    long long v89 = v33;
    long long v38 = *a1;
    long long v84 = a1[1];
    long long v85 = v37;
    long long v39 = a2[9];
    long long v80 = a2[8];
    long long v81 = v39;
    long long v82 = a2[10];
    long long v83 = v38;
    long long v40 = a2[5];
    long long v76 = a2[4];
    long long v77 = v40;
    long long v41 = a2[7];
    long long v78 = a2[6];
    long long v79 = v41;
    long long v42 = a2[1];
    long long v72 = *a2;
    long long v73 = v42;
    long long v43 = a2[3];
    long long v74 = a2[2];
    long long v75 = v43;
    long long v44 = a3[8];
    long long v45 = a3[9];
    long long v46 = a3[6];
    long long v68 = a3[7];
    long long v69 = v44;
    long long v47 = a3[10];
    long long v70 = v45;
    long long v71 = v47;
    long long v49 = a3[4];
    long long v48 = a3[5];
    *(void *)&v59[4] = a5;
    *(void *)&v59[12] = a6;
    uint64_t v60 = a7;
    int v50 = *(_DWORD *)(v7 + 36);
    v54[0] = *(unsigned char *)(v7 + 16);
    int v57 = v50;
    int v58 = a4;
    long long v66 = v48;
    long long v67 = v46;
    long long v51 = a3[1];
    long long v61 = *a3;
    long long v62 = v51;
    long long v52 = a3[2];
    long long v64 = a3[3];
    long long v65 = v49;
    long long v63 = v52;
    uint64_t result = MEMORY[0x1D25FFEE0](v54, &v83, &v72, &v61, &v58);
    if (!result) {
      return result;
    }
    goto LABEL_5;
  }
  *(void *)uint64_t v59 = *(void *)(v7 + 20);
  *(void *)&v59[8] = *(void *)(v7 + 28);
  long long v8 = a1[8];
  long long v9 = a1[9];
  long long v10 = a1[6];
  long long v90 = a1[7];
  long long v91 = v8;
  long long v11 = a1[10];
  long long v92 = v9;
  long long v93 = v11;
  long long v12 = a1[4];
  long long v13 = a1[5];
  long long v14 = a1[2];
  long long v86 = a1[3];
  long long v87 = v12;
  long long v88 = v13;
  long long v89 = v10;
  long long v15 = *a1;
  long long v84 = a1[1];
  long long v85 = v14;
  long long v16 = a2[9];
  long long v80 = a2[8];
  long long v81 = v16;
  long long v82 = a2[10];
  long long v83 = v15;
  long long v17 = a2[5];
  long long v76 = a2[4];
  long long v77 = v17;
  long long v18 = a2[7];
  long long v78 = a2[6];
  long long v79 = v18;
  long long v19 = a2[1];
  long long v72 = *a2;
  long long v73 = v19;
  long long v20 = a2[3];
  long long v74 = a2[2];
  long long v75 = v20;
  long long v21 = a3[8];
  long long v22 = a3[9];
  long long v23 = a3[6];
  long long v68 = a3[7];
  long long v69 = v21;
  long long v24 = a3[10];
  long long v70 = v22;
  long long v71 = v24;
  long long v26 = a3[4];
  long long v25 = a3[5];
  int v27 = *(_DWORD *)(v7 + 36);
  LOBYTE(v58) = *(unsigned char *)(v7 + 16);
  *(_DWORD *)&v59[16] = v27;
  long long v66 = v25;
  long long v67 = v23;
  long long v28 = a3[1];
  long long v61 = *a3;
  long long v62 = v28;
  long long v29 = a3[2];
  long long v64 = a3[3];
  long long v65 = v26;
  long long v63 = v29;
  uint64_t result = MEMORY[0x1D25FFEE0](&v58, &v83, &v72, &v61, 0);
  if (result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v53 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t BNNS.CropResizeLayer.applyBackward(regionOfInterest:outputGradient:generatingInputGradient:filterParameters:)(long long *a1, long long *a2, long long *a3, int a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v94 = *MEMORY[0x1E4F143B8];
  if (a6 != 1)
  {
    uint64_t v55 = *(void *)(v7 + 20);
    uint64_t v56 = *(void *)(v7 + 28);
    long long v31 = a3[8];
    long long v32 = a3[9];
    long long v33 = a3[6];
    long long v90 = a3[7];
    long long v91 = v31;
    long long v34 = a3[10];
    long long v92 = v32;
    long long v93 = v34;
    long long v35 = a3[4];
    long long v36 = a3[5];
    long long v37 = a3[2];
    long long v86 = a3[3];
    long long v87 = v35;
    long long v88 = v36;
    long long v89 = v33;
    long long v38 = *a3;
    long long v84 = a3[1];
    long long v85 = v37;
    long long v39 = a1[9];
    long long v80 = a1[8];
    long long v81 = v39;
    long long v82 = a1[10];
    long long v83 = v38;
    long long v40 = a1[5];
    long long v76 = a1[4];
    long long v77 = v40;
    long long v41 = a1[7];
    long long v78 = a1[6];
    long long v79 = v41;
    long long v42 = a1[1];
    long long v72 = *a1;
    long long v73 = v42;
    long long v43 = a1[3];
    long long v74 = a1[2];
    long long v75 = v43;
    long long v44 = a2[8];
    long long v45 = a2[9];
    long long v46 = a2[6];
    long long v68 = a2[7];
    long long v69 = v44;
    long long v47 = a2[10];
    long long v70 = v45;
    long long v71 = v47;
    long long v49 = a2[4];
    long long v48 = a2[5];
    *(void *)&v59[4] = a5;
    *(void *)&v59[12] = a6;
    uint64_t v60 = a7;
    int v50 = *(_DWORD *)(v7 + 36);
    v54[0] = *(unsigned char *)(v7 + 16);
    int v57 = v50;
    int v58 = a4;
    long long v66 = v48;
    long long v67 = v46;
    long long v51 = a2[1];
    long long v61 = *a2;
    long long v62 = v51;
    long long v52 = a2[2];
    long long v64 = a2[3];
    long long v65 = v49;
    long long v63 = v52;
    uint64_t result = MEMORY[0x1D25FFEF0](v54, &v83, &v72, &v61, &v58);
    if (!result) {
      return result;
    }
    goto LABEL_5;
  }
  *(void *)uint64_t v59 = *(void *)(v7 + 20);
  *(void *)&v59[8] = *(void *)(v7 + 28);
  long long v8 = a3[8];
  long long v9 = a3[9];
  long long v10 = a3[6];
  long long v90 = a3[7];
  long long v91 = v8;
  long long v11 = a3[10];
  long long v92 = v9;
  long long v93 = v11;
  long long v12 = a3[4];
  long long v13 = a3[5];
  long long v14 = a3[2];
  long long v86 = a3[3];
  long long v87 = v12;
  long long v88 = v13;
  long long v89 = v10;
  long long v15 = *a3;
  long long v84 = a3[1];
  long long v85 = v14;
  long long v16 = a1[9];
  long long v80 = a1[8];
  long long v81 = v16;
  long long v82 = a1[10];
  long long v83 = v15;
  long long v17 = a1[5];
  long long v76 = a1[4];
  long long v77 = v17;
  long long v18 = a1[7];
  long long v78 = a1[6];
  long long v79 = v18;
  long long v19 = a1[1];
  long long v72 = *a1;
  long long v73 = v19;
  long long v20 = a1[3];
  long long v74 = a1[2];
  long long v75 = v20;
  long long v21 = a2[8];
  long long v22 = a2[9];
  long long v23 = a2[6];
  long long v68 = a2[7];
  long long v69 = v21;
  long long v24 = a2[10];
  long long v70 = v22;
  long long v71 = v24;
  long long v26 = a2[4];
  long long v25 = a2[5];
  int v27 = *(_DWORD *)(v7 + 36);
  LOBYTE(v58) = *(unsigned char *)(v7 + 16);
  *(_DWORD *)&v59[16] = v27;
  long long v66 = v25;
  long long v67 = v23;
  long long v28 = a2[1];
  long long v61 = *a2;
  long long v62 = v28;
  long long v29 = a2[2];
  long long v64 = a2[3];
  long long v65 = v26;
  long long v63 = v29;
  uint64_t result = MEMORY[0x1D25FFEF0](&v58, &v83, &v72, &v61, 0);
  if (result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v53 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t BNNS.CropResizeLayer.deinit()
{
  return v0;
}

uint64_t BNNS.CropResizeLayer.__deallocating_deinit()
{
  return swift_deallocClassInstance();
}

BOOL static BNNS.ShuffleType.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void BNNS.ShuffleType.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int BNNS.ShuffleType.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t static BNNS.shuffle(_:input:output:filterParameters:)(unsigned __int8 *a1, _OWORD *a2, _OWORD *a3, uint32_t a4, size_t a5, int (__cdecl *a6)(void **, size_t, size_t), void (__cdecl *a7)(void *))
{
  uint64_t v33 = *MEMORY[0x1E4F143B8];
  BNNSShuffleType v7 = *a1;
  if (a6 != (int (__cdecl *)(void **, size_t, size_t))1)
  {
    v30.flags = a4;
    v30.n_threads = a5;
    v30.alloc_memory = a6;
    v30.free_memory = a7;
    long long v19 = a2[9];
    *(_OWORD *)&input.stride[7] = a2[8];
    *(_OWORD *)&input.data_type = v19;
    *(_OWORD *)&input.table_data_type = a2[10];
    long long v20 = a2[5];
    *(_OWORD *)&input.size[7] = a2[4];
    *(_OWORD *)&input.stride[1] = v20;
    long long v21 = a2[7];
    *(_OWORD *)&input.stride[3] = a2[6];
    *(_OWORD *)&input.stride[5] = v21;
    long long v22 = a2[1];
    *(_OWORD *)&input.flags = *a2;
    *(_OWORD *)&input.size[1] = v22;
    long long v23 = a2[3];
    *(_OWORD *)&input.size[3] = a2[2];
    *(_OWORD *)&input.size[5] = v23;
    long long v24 = a3[9];
    *(_OWORD *)&output.stride[7] = a3[8];
    *(_OWORD *)&output.data_type = v24;
    *(_OWORD *)&output.table_data_type = a3[10];
    long long v25 = a3[5];
    *(_OWORD *)&output.size[7] = a3[4];
    *(_OWORD *)&output.stride[1] = v25;
    long long v26 = a3[7];
    *(_OWORD *)&output.stride[3] = a3[6];
    *(_OWORD *)&output.stride[5] = v26;
    long long v27 = a3[1];
    *(_OWORD *)&output.flags = *a3;
    *(_OWORD *)&output.size[1] = v27;
    long long v28 = a3[3];
    *(_OWORD *)&output.size[3] = a3[2];
    *(_OWORD *)&output.size[5] = v28;
    uint64_t result = BNNSShuffle(v7, &input, &output, &v30);
    if (!result) {
      return result;
    }
    goto LABEL_5;
  }
  long long v8 = a2[9];
  *(_OWORD *)&input.stride[7] = a2[8];
  *(_OWORD *)&input.data_type = v8;
  *(_OWORD *)&input.table_data_type = a2[10];
  long long v9 = a2[5];
  *(_OWORD *)&input.size[7] = a2[4];
  *(_OWORD *)&input.stride[1] = v9;
  long long v10 = a2[7];
  *(_OWORD *)&input.stride[3] = a2[6];
  *(_OWORD *)&input.stride[5] = v10;
  long long v11 = a2[1];
  *(_OWORD *)&input.flags = *a2;
  *(_OWORD *)&input.size[1] = v11;
  long long v12 = a2[3];
  *(_OWORD *)&input.size[3] = a2[2];
  *(_OWORD *)&input.size[5] = v12;
  long long v13 = a3[9];
  *(_OWORD *)&output.stride[7] = a3[8];
  *(_OWORD *)&output.data_type = v13;
  *(_OWORD *)&output.table_data_type = a3[10];
  long long v14 = a3[5];
  *(_OWORD *)&output.size[7] = a3[4];
  *(_OWORD *)&output.stride[1] = v14;
  long long v15 = a3[7];
  *(_OWORD *)&output.stride[3] = a3[6];
  *(_OWORD *)&output.stride[5] = v15;
  long long v16 = a3[1];
  *(_OWORD *)&output.flags = *a3;
  *(_OWORD *)&output.size[1] = v16;
  long long v17 = a3[3];
  *(_OWORD *)&output.size[3] = a3[2];
  *(_OWORD *)&output.size[5] = v17;
  uint64_t result = BNNSShuffle(v7, &input, &output, 0);
  if (result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v29 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t BNNSNDArrayDescriptor.dataSize.getter()
{
  uint64_t v8 = *MEMORY[0x1E4F143B8];
  long long v1 = v0[9];
  v7[8] = v0[8];
  v7[9] = v1;
  v7[10] = v0[10];
  long long v2 = v0[5];
  v7[4] = v0[4];
  v7[5] = v2;
  long long v3 = v0[7];
  v7[6] = v0[6];
  v7[7] = v3;
  long long v4 = v0[1];
  v7[0] = *v0;
  v7[1] = v4;
  long long v5 = v0[3];
  void v7[2] = v0[2];
  void v7[3] = v5;
  return MEMORY[0x1D26002C0](v7);
}

uint64_t static BNNS.gather(input:indices:output:axis:filterParameters:)(_OWORD *a1, _OWORD *a2, _OWORD *a3, uint64_t a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v32 = *MEMORY[0x1E4F143B8];
  long long v8 = a1[9];
  v31[8] = a1[8];
  v31[9] = v8;
  v31[10] = a1[10];
  long long v9 = a1[5];
  v31[4] = a1[4];
  v31[5] = v9;
  long long v10 = a1[7];
  v31[6] = a1[6];
  v31[7] = v10;
  long long v11 = a1[1];
  v31[0] = *a1;
  v31[1] = v11;
  long long v12 = a1[3];
  v31[2] = a1[2];
  v31[3] = v12;
  long long v13 = a2[9];
  v30[8] = a2[8];
  v30[9] = v13;
  v30[10] = a2[10];
  long long v14 = a2[5];
  v30[4] = a2[4];
  v30[5] = v14;
  long long v15 = a2[7];
  v30[6] = a2[6];
  v30[7] = v15;
  long long v16 = a2[1];
  v30[0] = *a2;
  v30[1] = v16;
  long long v17 = a2[3];
  v30[2] = a2[2];
  v30[3] = v17;
  long long v18 = a3[9];
  v29[8] = a3[8];
  v29[9] = v18;
  v29[10] = a3[10];
  long long v19 = a3[5];
  v29[4] = a3[4];
  v29[5] = v19;
  long long v20 = a3[7];
  v29[6] = a3[6];
  v29[7] = v20;
  long long v21 = a3[1];
  v29[0] = *a3;
  v29[1] = v21;
  long long v22 = a3[3];
  v29[2] = a3[2];
  v29[3] = v22;
  if (a7 != 1)
  {
    int v25 = a5;
    uint64_t v26 = a6;
    uint64_t v27 = a7;
    uint64_t v28 = a8;
    uint64_t result = MEMORY[0x1D26000F0](a4, v31, v30, v29, &v25);
    if (!result) {
      return result;
    }
    goto LABEL_5;
  }
  uint64_t result = MEMORY[0x1D26000F0](a4, v31, v30, v29, 0);
  if (result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v24 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.gatherND(input:indices:output:filterParameters:)(_OWORD *a1, _OWORD *a2, _OWORD *a3, int a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v31 = *MEMORY[0x1E4F143B8];
  long long v7 = a1[9];
  v30[8] = a1[8];
  v30[9] = v7;
  v30[10] = a1[10];
  long long v8 = a1[5];
  v30[4] = a1[4];
  v30[5] = v8;
  long long v9 = a1[7];
  v30[6] = a1[6];
  v30[7] = v9;
  long long v10 = a1[1];
  v30[0] = *a1;
  v30[1] = v10;
  long long v11 = a1[3];
  v30[2] = a1[2];
  v30[3] = v11;
  long long v12 = a2[9];
  v29[8] = a2[8];
  v29[9] = v12;
  v29[10] = a2[10];
  long long v13 = a2[5];
  v29[4] = a2[4];
  v29[5] = v13;
  long long v14 = a2[7];
  v29[6] = a2[6];
  v29[7] = v14;
  long long v15 = a2[1];
  v29[0] = *a2;
  v29[1] = v15;
  long long v16 = a2[3];
  v29[2] = a2[2];
  v29[3] = v16;
  long long v17 = a3[9];
  v28[8] = a3[8];
  v28[9] = v17;
  v28[10] = a3[10];
  long long v18 = a3[5];
  v28[4] = a3[4];
  v28[5] = v18;
  long long v19 = a3[7];
  v28[6] = a3[6];
  v28[7] = v19;
  long long v20 = a3[1];
  v28[0] = *a3;
  v28[1] = v20;
  long long v21 = a3[3];
  v28[2] = a3[2];
  v28[3] = v21;
  if (a6 != 1)
  {
    int v24 = a4;
    uint64_t v25 = a5;
    uint64_t v26 = a6;
    uint64_t v27 = a7;
    uint64_t result = MEMORY[0x1D2600100](v30, v29, v28, &v24);
    if (!result) {
      return result;
    }
    goto LABEL_5;
  }
  uint64_t result = MEMORY[0x1D2600100](v30, v29, v28, 0);
  if (result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v23 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.scatter(input:indices:output:axis:reductionFunction:filterParameters:)(_OWORD *a1, _OWORD *a2, _OWORD *a3, uint64_t a4, int *a5, int a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  uint64_t v37 = *MEMORY[0x1E4F143B8];
  uint64_t v9 = *a5;
  int v10 = *((unsigned __int8 *)a5 + 4);
  long long v11 = a1[9];
  v36[8] = a1[8];
  v36[9] = v11;
  v36[10] = a1[10];
  long long v12 = a1[5];
  v36[4] = a1[4];
  v36[5] = v12;
  long long v13 = a1[7];
  v36[6] = a1[6];
  v36[7] = v13;
  long long v14 = a1[1];
  v36[0] = *a1;
  v36[1] = v14;
  long long v15 = a1[3];
  v36[2] = a1[2];
  v36[3] = v15;
  long long v16 = a2[9];
  v35[8] = a2[8];
  v35[9] = v16;
  v35[10] = a2[10];
  long long v17 = a2[5];
  v35[4] = a2[4];
  v35[5] = v17;
  long long v18 = a2[7];
  v35[6] = a2[6];
  v35[7] = v18;
  long long v19 = a2[1];
  v35[0] = *a2;
  v35[1] = v19;
  long long v20 = a2[3];
  v35[2] = a2[2];
  v35[3] = v20;
  long long v21 = a3[9];
  v34[8] = a3[8];
  v34[9] = v21;
  v34[10] = a3[10];
  long long v22 = a3[5];
  v34[4] = a3[4];
  v34[5] = v22;
  long long v23 = a3[7];
  v34[6] = a3[6];
  v34[7] = v23;
  long long v24 = a3[1];
  v34[0] = *a3;
  v34[1] = v24;
  long long v25 = a3[3];
  v34[2] = a3[2];
  v34[3] = v25;
  if (a8 == 1)
  {
    if (v10) {
      uint64_t v26 = dword_1D2136950[v9];
    }
    else {
      uint64_t v26 = 8;
    }
    uint64_t result = MEMORY[0x1D26003C0](a4, v26, v36, v35, v34, 0);
    if (!result) {
      return result;
    }
LABEL_11:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v29 = 0;
    return swift_willThrow();
  }
  int v30 = a6;
  uint64_t v31 = a7;
  uint64_t v32 = a8;
  uint64_t v33 = a9;
  if (v10) {
    uint64_t v27 = dword_1D2136950[v9];
  }
  else {
    uint64_t v27 = 8;
  }
  uint64_t result = MEMORY[0x1D26003C0](a4, v27, v36, v35, v34, &v30);
  if (result) {
    goto LABEL_11;
  }
  return result;
}

uint64_t static BNNS.scatterND(input:indices:output:reductionFunction:filterParameters:)(_OWORD *a1, _OWORD *a2, _OWORD *a3, int *a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v36 = *MEMORY[0x1E4F143B8];
  uint64_t v8 = *a4;
  int v9 = *((unsigned __int8 *)a4 + 4);
  long long v10 = a1[9];
  v35[8] = a1[8];
  v35[9] = v10;
  v35[10] = a1[10];
  long long v11 = a1[5];
  v35[4] = a1[4];
  v35[5] = v11;
  long long v12 = a1[7];
  v35[6] = a1[6];
  v35[7] = v12;
  long long v13 = a1[1];
  v35[0] = *a1;
  v35[1] = v13;
  long long v14 = a1[3];
  v35[2] = a1[2];
  v35[3] = v14;
  long long v15 = a2[9];
  v34[8] = a2[8];
  v34[9] = v15;
  v34[10] = a2[10];
  long long v16 = a2[5];
  v34[4] = a2[4];
  v34[5] = v16;
  long long v17 = a2[7];
  v34[6] = a2[6];
  v34[7] = v17;
  long long v18 = a2[1];
  v34[0] = *a2;
  v34[1] = v18;
  long long v19 = a2[3];
  v34[2] = a2[2];
  v34[3] = v19;
  long long v20 = a3[9];
  v33[8] = a3[8];
  v33[9] = v20;
  v33[10] = a3[10];
  long long v21 = a3[5];
  v33[4] = a3[4];
  v33[5] = v21;
  long long v22 = a3[7];
  v33[6] = a3[6];
  v33[7] = v22;
  long long v23 = a3[1];
  v33[0] = *a3;
  v33[1] = v23;
  long long v24 = a3[3];
  v33[2] = a3[2];
  v33[3] = v24;
  if (a7 == 1)
  {
    if (v9) {
      uint64_t v25 = dword_1D2136950[v8];
    }
    else {
      uint64_t v25 = 8;
    }
    uint64_t result = MEMORY[0x1D26003D0](v25, v35, v34, v33, 0);
    if (!result) {
      return result;
    }
LABEL_11:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    unsigned char *v28 = 0;
    return swift_willThrow();
  }
  int v29 = a5;
  uint64_t v30 = a6;
  uint64_t v31 = a7;
  uint64_t v32 = a8;
  if (v9) {
    uint64_t v26 = dword_1D2136950[v8];
  }
  else {
    uint64_t v26 = 8;
  }
  uint64_t result = MEMORY[0x1D26003D0](v26, v35, v34, v33, &v29);
  if (result) {
    goto LABEL_11;
  }
  return result;
}

uint64_t static BNNS.scatter(input:indices:output:axis:filterParameters:)(_OWORD *a1, _OWORD *a2, _OWORD *a3, uint64_t a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v32 = *MEMORY[0x1E4F143B8];
  long long v8 = a1[9];
  v31[8] = a1[8];
  v31[9] = v8;
  v31[10] = a1[10];
  long long v9 = a1[5];
  v31[4] = a1[4];
  v31[5] = v9;
  long long v10 = a1[7];
  v31[6] = a1[6];
  v31[7] = v10;
  long long v11 = a1[1];
  v31[0] = *a1;
  v31[1] = v11;
  long long v12 = a1[3];
  v31[2] = a1[2];
  v31[3] = v12;
  long long v13 = a2[9];
  v30[8] = a2[8];
  v30[9] = v13;
  v30[10] = a2[10];
  long long v14 = a2[5];
  v30[4] = a2[4];
  v30[5] = v14;
  long long v15 = a2[7];
  v30[6] = a2[6];
  v30[7] = v15;
  long long v16 = a2[1];
  v30[0] = *a2;
  v30[1] = v16;
  long long v17 = a2[3];
  v30[2] = a2[2];
  v30[3] = v17;
  long long v18 = a3[9];
  v29[8] = a3[8];
  v29[9] = v18;
  v29[10] = a3[10];
  long long v19 = a3[5];
  v29[4] = a3[4];
  v29[5] = v19;
  long long v20 = a3[7];
  v29[6] = a3[6];
  v29[7] = v20;
  long long v21 = a3[1];
  v29[0] = *a3;
  v29[1] = v21;
  long long v22 = a3[3];
  v29[2] = a3[2];
  v29[3] = v22;
  if (a7 != 1)
  {
    int v25 = a5;
    uint64_t v26 = a6;
    uint64_t v27 = a7;
    uint64_t v28 = a8;
    uint64_t result = MEMORY[0x1D26003C0](a4, 15, v31, v30, v29, &v25);
    if (!result) {
      return result;
    }
    goto LABEL_5;
  }
  uint64_t result = MEMORY[0x1D26003C0](a4, 15, v31, v30, v29, 0);
  if (result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v24 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNS.scatterND(input:indices:output:filterParameters:)(_OWORD *a1, _OWORD *a2, _OWORD *a3, int a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v31 = *MEMORY[0x1E4F143B8];
  long long v7 = a1[9];
  v30[8] = a1[8];
  v30[9] = v7;
  v30[10] = a1[10];
  long long v8 = a1[5];
  v30[4] = a1[4];
  v30[5] = v8;
  long long v9 = a1[7];
  v30[6] = a1[6];
  v30[7] = v9;
  long long v10 = a1[1];
  v30[0] = *a1;
  v30[1] = v10;
  long long v11 = a1[3];
  v30[2] = a1[2];
  v30[3] = v11;
  long long v12 = a2[9];
  v29[8] = a2[8];
  v29[9] = v12;
  v29[10] = a2[10];
  long long v13 = a2[5];
  v29[4] = a2[4];
  v29[5] = v13;
  long long v14 = a2[7];
  v29[6] = a2[6];
  v29[7] = v14;
  long long v15 = a2[1];
  v29[0] = *a2;
  v29[1] = v15;
  long long v16 = a2[3];
  v29[2] = a2[2];
  v29[3] = v16;
  long long v17 = a3[9];
  v28[8] = a3[8];
  v28[9] = v17;
  v28[10] = a3[10];
  long long v18 = a3[5];
  v28[4] = a3[4];
  v28[5] = v18;
  long long v19 = a3[7];
  v28[6] = a3[6];
  v28[7] = v19;
  long long v20 = a3[1];
  v28[0] = *a3;
  v28[1] = v20;
  long long v21 = a3[3];
  v28[2] = a3[2];
  v28[3] = v21;
  if (a6 != 1)
  {
    int v24 = a4;
    uint64_t v25 = a5;
    uint64_t v26 = a6;
    uint64_t v27 = a7;
    uint64_t result = MEMORY[0x1D26003D0](15, v30, v29, v28, &v24);
    if (!result) {
      return result;
    }
    goto LABEL_5;
  }
  uint64_t result = MEMORY[0x1D26003D0](15, v30, v29, v28, 0);
  if (result)
  {
LABEL_5:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v23 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t static BNNSNDArrayDescriptor.allocate<A>(randomNormalUsing:mean:standardDeviation:shape:batchSize:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X4>, uint64_t a5@<X5>, uint64_t a6@<X6>, uint64_t a7@<X8>)
{
  uint64_t v21 = a3;
  uint64_t v22 = a7;
  uint64_t v27 = *MEMORY[0x1E4F143B8];
  uint64_t v12 = *(void *)(a5 - 8);
  MEMORY[0x1F4188790](a1);
  long long v14 = (char *)&v21 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  outlined init with take of BNNS.Shape(v15, (uint64_t)v25);
  int v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 8))(a5, a6);
  helper #1 <A>(_:) in static BNNSNDArrayDescriptor.allocateUninitialized(scalarType:shape:batchSize:)(a4, (uint64_t)v25, v16, &v24);
  long long v17 = *(void **)(a1 + 16);
  long long v18 = *(void (**)(char *, uint64_t, uint64_t))(v12 + 16);
  v18(v14, a2, a5);
  lazy protocol witness table accessor for type Float and conformance Float();
  BinaryFloatingPoint.init<A>(_:)();
  float v19 = *(float *)v26;
  v18(v14, v21, a5);
  BinaryFloatingPoint.init<A>(_:)();
  if (BNNSRandomFillNormalFloat(v17, &v24, v19, *(float *)&v23.flags))
  {
    if (v24.data) {
      MEMORY[0x1D26009C0](v24.data, -1, -1);
    }
    _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)&v23);
  }
  else
  {
    BNNSNDArrayDescriptor v23 = v24;
    _sSo21BNNSNDArrayDescriptoraSgWOi_((uint64_t)&v23);
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)&v23, (uint64_t)v26);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v26, v22);
}

unint64_t lazy protocol witness table accessor for type Float and conformance Float()
{
  unint64_t result = lazy protocol witness table cache variable for type Float and conformance Float;
  if (!lazy protocol witness table cache variable for type Float and conformance Float)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Float and conformance Float);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type BNNS.CropResizeLayer.BoxCoordinateMode and conformance BNNS.CropResizeLayer.BoxCoordinateMode()
{
  unint64_t result = lazy protocol witness table cache variable for type BNNS.CropResizeLayer.BoxCoordinateMode and conformance BNNS.CropResizeLayer.BoxCoordinateMode;
  if (!lazy protocol witness table cache variable for type BNNS.CropResizeLayer.BoxCoordinateMode and conformance BNNS.CropResizeLayer.BoxCoordinateMode)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.CropResizeLayer.BoxCoordinateMode and conformance BNNS.CropResizeLayer.BoxCoordinateMode);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type BNNS.CropResizeLayer.LinearSamplingMode and conformance BNNS.CropResizeLayer.LinearSamplingMode()
{
  unint64_t result = lazy protocol witness table cache variable for type BNNS.CropResizeLayer.LinearSamplingMode and conformance BNNS.CropResizeLayer.LinearSamplingMode;
  if (!lazy protocol witness table cache variable for type BNNS.CropResizeLayer.LinearSamplingMode and conformance BNNS.CropResizeLayer.LinearSamplingMode)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.CropResizeLayer.LinearSamplingMode and conformance BNNS.CropResizeLayer.LinearSamplingMode);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type BNNS.ShuffleType and conformance BNNS.ShuffleType()
{
  unint64_t result = lazy protocol witness table cache variable for type BNNS.ShuffleType and conformance BNNS.ShuffleType;
  if (!lazy protocol witness table cache variable for type BNNS.ShuffleType and conformance BNNS.ShuffleType)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.ShuffleType and conformance BNNS.ShuffleType);
  }
  return result;
}

uint64_t type metadata accessor for BNNS.CropResizeLayer()
{
  return self;
}

uint64_t method lookup function for BNNS.CropResizeLayer(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.CropResizeLayer);
}

uint64_t dispatch thunk of BNNS.CropResizeLayer.__allocating_init(coordinatesAreNormalized:spatialScale:extrapolationValue:samplingMode:boxCoordinateMode:)()
{
  return (*(uint64_t (**)(void))(v0 + 88))();
}

uint64_t dispatch thunk of BNNS.CropResizeLayer.apply(input:regionOfInterest:output:filterParameters:)(uint64_t *a1, uint64_t *a2, uint64_t *a3)
{
  uint64_t v4 = a1[17];
  int v5 = *((_DWORD *)a1 + 36);
  uint64_t v6 = a1[19];
  int v7 = *((_DWORD *)a1 + 40);
  uint64_t v8 = a2[17];
  int v9 = *((_DWORD *)a2 + 36);
  uint64_t v10 = a2[19];
  int v11 = *((_DWORD *)a2 + 40);
  uint64_t v12 = a3[17];
  int v13 = *((_DWORD *)a3 + 36);
  uint64_t v14 = a3[19];
  int v15 = *((_DWORD *)a3 + 40);
  int v16 = *(uint64_t (**)(uint64_t *, uint64_t *, uint64_t *))(*(void *)v3 + 96);
  uint64_t v52 = *a1;
  long long v53 = *(_OWORD *)(a1 + 1);
  long long v54 = *(_OWORD *)(a1 + 3);
  long long v55 = *(_OWORD *)(a1 + 5);
  long long v56 = *(_OWORD *)(a1 + 7);
  long long v57 = *(_OWORD *)(a1 + 9);
  long long v58 = *(_OWORD *)(a1 + 11);
  long long v59 = *(_OWORD *)(a1 + 13);
  long long v60 = *(_OWORD *)(a1 + 15);
  uint64_t v61 = v4;
  int v62 = v5;
  uint64_t v63 = v6;
  int v64 = v7;
  uint64_t v65 = *(uint64_t *)((char *)a1 + 164);
  uint64_t v38 = *a2;
  long long v39 = *(_OWORD *)(a2 + 1);
  long long v40 = *(_OWORD *)(a2 + 3);
  long long v41 = *(_OWORD *)(a2 + 5);
  long long v42 = *(_OWORD *)(a2 + 7);
  long long v43 = *(_OWORD *)(a2 + 9);
  long long v44 = *(_OWORD *)(a2 + 11);
  long long v45 = *(_OWORD *)(a2 + 13);
  long long v46 = *(_OWORD *)(a2 + 15);
  uint64_t v47 = v8;
  int v48 = v9;
  uint64_t v49 = v10;
  int v50 = v11;
  uint64_t v51 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v24 = *a3;
  long long v17 = *(_OWORD *)(a3 + 3);
  long long v25 = *(_OWORD *)(a3 + 1);
  long long v26 = v17;
  long long v18 = *(_OWORD *)(a3 + 7);
  long long v19 = *(_OWORD *)(a3 + 9);
  long long v20 = *(_OWORD *)(a3 + 11);
  long long v21 = *(_OWORD *)(a3 + 13);
  long long v22 = *(_OWORD *)(a3 + 15);
  long long v27 = *(_OWORD *)(a3 + 5);
  long long v28 = v18;
  long long v29 = v19;
  long long v30 = v20;
  long long v31 = v21;
  long long v32 = v22;
  uint64_t v33 = v12;
  int v34 = v13;
  uint64_t v35 = v14;
  int v36 = v15;
  uint64_t v37 = *(uint64_t *)((char *)a3 + 164);
  return v16(&v52, &v38, &v24);
}

uint64_t dispatch thunk of BNNS.CropResizeLayer.applyBackward(regionOfInterest:outputGradient:generatingInputGradient:filterParameters:)(uint64_t *a1, uint64_t *a2, uint64_t *a3)
{
  uint64_t v4 = a1[17];
  int v5 = *((_DWORD *)a1 + 36);
  uint64_t v6 = a1[19];
  int v7 = *((_DWORD *)a1 + 40);
  uint64_t v8 = a2[17];
  int v9 = *((_DWORD *)a2 + 36);
  uint64_t v10 = a2[19];
  int v11 = *((_DWORD *)a2 + 40);
  uint64_t v12 = a3[17];
  int v13 = *((_DWORD *)a3 + 36);
  uint64_t v14 = a3[19];
  int v15 = *((_DWORD *)a3 + 40);
  int v16 = *(uint64_t (**)(uint64_t *, uint64_t *, uint64_t *))(*(void *)v3 + 104);
  uint64_t v52 = *a1;
  long long v53 = *(_OWORD *)(a1 + 1);
  long long v54 = *(_OWORD *)(a1 + 3);
  long long v55 = *(_OWORD *)(a1 + 5);
  long long v56 = *(_OWORD *)(a1 + 7);
  long long v57 = *(_OWORD *)(a1 + 9);
  long long v58 = *(_OWORD *)(a1 + 11);
  long long v59 = *(_OWORD *)(a1 + 13);
  long long v60 = *(_OWORD *)(a1 + 15);
  uint64_t v61 = v4;
  int v62 = v5;
  uint64_t v63 = v6;
  int v64 = v7;
  uint64_t v65 = *(uint64_t *)((char *)a1 + 164);
  uint64_t v38 = *a2;
  long long v39 = *(_OWORD *)(a2 + 1);
  long long v40 = *(_OWORD *)(a2 + 3);
  long long v41 = *(_OWORD *)(a2 + 5);
  long long v42 = *(_OWORD *)(a2 + 7);
  long long v43 = *(_OWORD *)(a2 + 9);
  long long v44 = *(_OWORD *)(a2 + 11);
  long long v45 = *(_OWORD *)(a2 + 13);
  long long v46 = *(_OWORD *)(a2 + 15);
  uint64_t v47 = v8;
  int v48 = v9;
  uint64_t v49 = v10;
  int v50 = v11;
  uint64_t v51 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v24 = *a3;
  long long v17 = *(_OWORD *)(a3 + 3);
  long long v25 = *(_OWORD *)(a3 + 1);
  long long v26 = v17;
  long long v18 = *(_OWORD *)(a3 + 7);
  long long v19 = *(_OWORD *)(a3 + 9);
  long long v20 = *(_OWORD *)(a3 + 11);
  long long v21 = *(_OWORD *)(a3 + 13);
  long long v22 = *(_OWORD *)(a3 + 15);
  long long v27 = *(_OWORD *)(a3 + 5);
  long long v28 = v18;
  long long v29 = v19;
  long long v30 = v20;
  long long v31 = v21;
  long long v32 = v22;
  uint64_t v33 = v12;
  int v34 = v13;
  uint64_t v35 = v14;
  int v36 = v15;
  uint64_t v37 = *(uint64_t *)((char *)a3 + 164);
  return v16(&v52, &v38, &v24);
}

unsigned char *storeEnumTagSinglePayload for BNNS.CropResizeLayer.BoxCoordinateMode(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 3 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 3) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFD) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFC)
  {
    unsigned int v6 = ((a2 - 253) >> 8) + 1;
    *unint64_t result = a2 + 3;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D20A26DCLL);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 3;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.CropResizeLayer.BoxCoordinateMode()
{
  return &type metadata for BNNS.CropResizeLayer.BoxCoordinateMode;
}

uint64_t getEnumTagSinglePayload for BNNS.CropResizeLayer.LinearSamplingMode(unsigned __int8 *a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0xFC) {
    goto LABEL_17;
  }
  if (a2 + 4 >= 0xFFFF00) {
    int v2 = 4;
  }
  else {
    int v2 = 2;
  }
  if ((a2 + 4) >> 8 < 0xFF) {
    int v3 = 1;
  }
  else {
    int v3 = v2;
  }
  if (v3 == 4)
  {
    int v4 = *(_DWORD *)(a1 + 1);
    if (v4) {
      return (*a1 | (v4 << 8)) - 4;
    }
  }
  else
  {
    if (v3 == 2)
    {
      int v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1)) {
        goto LABEL_17;
      }
      return (*a1 | (v4 << 8)) - 4;
    }
    int v4 = a1[1];
    if (a1[1]) {
      return (*a1 | (v4 << 8)) - 4;
    }
  }
LABEL_17:
  unsigned int v6 = *a1;
  BOOL v7 = v6 >= 5;
  int v8 = v6 - 5;
  if (!v7) {
    int v8 = -1;
  }
  return (v8 + 1);
}

unsigned char *storeEnumTagSinglePayload for BNNS.CropResizeLayer.LinearSamplingMode(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 4 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 4) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFC) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFB)
  {
    unsigned int v6 = ((a2 - 252) >> 8) + 1;
    *unint64_t result = a2 + 4;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D20A2870);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 4;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.CropResizeLayer.LinearSamplingMode()
{
  return &type metadata for BNNS.CropResizeLayer.LinearSamplingMode;
}

unsigned char *storeEnumTagSinglePayload for BNNS.ShuffleType(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 1 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFF) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFE)
  {
    unsigned int v6 = ((a2 - 255) >> 8) + 1;
    *unint64_t result = a2 + 1;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D20A2974);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 1;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.ShuffleType()
{
  return &type metadata for BNNS.ShuffleType;
}

__n128 BNNS.FusedNormalizationParameters.layerParameters(input:output:)(uint64_t a1)
{
  int v2 = (long long *)MEMORY[0x1F4188790](a1);
  uint64_t v128 = v3;
  long long v5 = v4[9];
  long long v176 = v4[8];
  long long v177 = v5;
  long long v178 = v4[10];
  long long v6 = v4[5];
  long long v172 = v4[4];
  long long v173 = v6;
  long long v7 = v4[6];
  long long v175 = v4[7];
  long long v174 = v7;
  long long v8 = v4[1];
  long long v168 = *v4;
  long long v169 = v8;
  long long v9 = v4[2];
  long long v171 = v4[3];
  long long v170 = v9;
  long long v10 = v2[9];
  long long v187 = v2[8];
  long long v188 = v10;
  long long v189 = v2[10];
  long long v11 = v2[5];
  long long v183 = v2[4];
  long long v184 = v11;
  long long v12 = v2[6];
  long long v186 = v2[7];
  long long v185 = v12;
  long long v13 = v2[1];
  long long v179 = *v2;
  long long v180 = v13;
  long long v14 = v2[2];
  long long v182 = v2[3];
  long long v181 = v14;
  outlined init with take of BNNS.NormalizationType(v1, (uint64_t)v190);
  outlined init with take of BNNSNDArrayDescriptor?(v1 + 368, (uint64_t)v191);
  outlined init with take of BNNSNDArrayDescriptor?(v1 + 552, (uint64_t)v192);
  uint64_t v15 = *(void *)(v1 + 732);
  uint64_t v16 = *(void *)(v1 + 740);
  char v17 = *(unsigned char *)(v1 + 748);
  _sSo21BNNSNDArrayDescriptoraSgWOi0_((uint64_t)v193);
  outlined init with take of BNNS.NormalizationType((uint64_t)v190, (uint64_t)v194);
  unsigned int v18 = _s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v194);
  if (v18 >= 2)
  {
    if (v18 == 2)
    {
      _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v194);
      uint64_t v127 = 0;
    }
    else
    {
      uint64_t v127 = *(void *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v194);
    }
    long long v22 = v193;
    long long v21 = v193;
  }
  else
  {
    uint64_t v19 = _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v194);
    uint64_t v20 = v19 + 184;
    long long v21 = v164;
    outlined init with take of BNNSNDArrayDescriptor?(v19, (uint64_t)v164);
    long long v22 = v165;
    outlined init with take of BNNSNDArrayDescriptor?(v20, (uint64_t)v165);
    uint64_t v127 = 0;
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v22, (uint64_t)v166);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v21, (uint64_t)v167);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v191, (uint64_t)v156);
  uint64_t v23 = 0;
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v156) == 1)
  {
    uint64_t v125 = 0;
    uint64_t v124 = 0;
    uint64_t v123 = 0;
    uint64_t v122 = 0;
    uint64_t v121 = 0;
    uint64_t v120 = 0;
    uint64_t v119 = 0;
    uint64_t v118 = 0;
    uint64_t v117 = 0;
    uint64_t v116 = 0;
    uint64_t v115 = 0;
    uint64_t v114 = 0;
    uint64_t v113 = 0;
    uint64_t v112 = 0;
    uint64_t v111 = 0;
    uint64_t v110 = 0;
    uint64_t v109 = 0;
    uint64_t v107 = 0;
    int v106 = 0;
    uint64_t v126 = 0;
    uint64_t v24 = 0;
    unint64_t v108 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v191, (uint64_t)&v130);
    uint64_t v126 = v130;
    uint64_t v125 = v131;
    uint64_t v124 = v132;
    uint64_t v123 = v133;
    uint64_t v122 = v134;
    uint64_t v121 = v135;
    uint64_t v120 = v136;
    uint64_t v119 = v137;
    uint64_t v118 = v138;
    uint64_t v117 = v139;
    uint64_t v116 = v140;
    uint64_t v115 = v141;
    uint64_t v114 = v142;
    uint64_t v113 = v143;
    uint64_t v112 = v144;
    uint64_t v111 = v145;
    uint64_t v110 = v146;
    uint64_t v109 = v147;
    unint64_t v108 = v148;
    uint64_t v107 = v149;
    uint64_t v24 = v151;
    int v106 = v150;
    int v43 = v152;
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v192, (uint64_t)v155);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v155) == 1)
  {
    uint64_t v104 = 0;
    uint64_t v103 = 0;
    uint64_t v102 = 0;
    uint64_t v101 = 0;
    uint64_t v100 = 0;
    uint64_t v99 = 0;
    uint64_t v98 = 0;
    uint64_t v96 = 0;
    uint64_t v97 = 0;
    uint64_t v94 = 0;
    uint64_t v95 = 0;
    uint64_t v92 = 0;
    uint64_t v93 = 0;
    uint64_t v90 = 0;
    uint64_t v91 = 0;
    uint64_t v87 = 0;
    uint64_t v88 = 0;
    uint64_t v86 = 0;
    int v85 = 0;
    uint64_t v105 = 0;
    unint64_t v89 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v192, (uint64_t)&v130);
    uint64_t v105 = v130;
    uint64_t v104 = v131;
    uint64_t v103 = v132;
    uint64_t v102 = v133;
    uint64_t v101 = v134;
    uint64_t v100 = v135;
    uint64_t v99 = v136;
    uint64_t v98 = v137;
    uint64_t v96 = v139;
    uint64_t v97 = v138;
    uint64_t v94 = v141;
    uint64_t v95 = v140;
    uint64_t v92 = v143;
    uint64_t v93 = v142;
    uint64_t v90 = v145;
    uint64_t v91 = v144;
    uint64_t v87 = v147;
    uint64_t v88 = v146;
    unint64_t v89 = v148;
    uint64_t v86 = v149;
    uint64_t v23 = v151;
    int v85 = v150;
    int v42 = v152;
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v167, (uint64_t)v154);
  uint64_t v25 = 0;
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v154) == 1)
  {
    uint64_t v81 = 0;
    uint64_t v82 = 0;
    uint64_t v79 = 0;
    uint64_t v80 = 0;
    uint64_t v77 = 0;
    uint64_t v78 = 0;
    uint64_t v75 = 0;
    uint64_t v76 = 0;
    uint64_t v72 = 0;
    uint64_t v73 = 0;
    uint64_t v70 = 0;
    uint64_t v71 = 0;
    uint64_t v68 = 0;
    uint64_t v69 = 0;
    uint64_t v66 = 0;
    uint64_t v67 = 0;
    uint64_t v64 = 0;
    uint64_t v65 = 0;
    int v83 = 0;
    uint64_t v84 = 0;
    uint64_t v26 = 0;
    unint64_t v74 = 0;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v167, (uint64_t)&v130);
    uint64_t v84 = v130;
    uint64_t v81 = v132;
    uint64_t v82 = v131;
    uint64_t v79 = v134;
    uint64_t v80 = v133;
    uint64_t v77 = v136;
    uint64_t v78 = v135;
    uint64_t v75 = v138;
    uint64_t v76 = v137;
    uint64_t v72 = v140;
    uint64_t v73 = v139;
    uint64_t v70 = v142;
    uint64_t v71 = v141;
    uint64_t v68 = v144;
    uint64_t v69 = v143;
    uint64_t v66 = v146;
    uint64_t v67 = v145;
    unint64_t v74 = v148;
    uint64_t v64 = v149;
    uint64_t v65 = v147;
    uint64_t v26 = v151;
    int v83 = v150;
    int v41 = v152;
  }
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v166, (uint64_t)v153);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v153) == 1)
  {
    int v27 = 0;
    uint64_t v45 = 0;
    uint64_t v46 = 0;
    uint64_t v47 = 0;
    uint64_t v48 = 0;
    uint64_t v49 = 0;
    uint64_t v50 = 0;
    uint64_t v51 = 0;
    uint64_t v52 = 0;
    uint64_t v53 = 0;
    uint64_t v54 = 0;
    uint64_t v55 = 0;
    uint64_t v56 = 0;
    uint64_t v57 = 0;
    uint64_t v58 = 0;
    uint64_t v59 = 0;
    uint64_t v60 = 0;
    uint64_t v61 = 0;
    uint64_t v63 = 0;
    uint64_t v62 = 0;
    __n128 v44 = 0u;
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v166, (uint64_t)&v130);
    uint64_t v62 = v130;
    uint64_t v63 = v131;
    uint64_t v60 = v133;
    uint64_t v61 = v132;
    uint64_t v58 = v135;
    uint64_t v59 = v134;
    uint64_t v56 = v137;
    uint64_t v57 = v136;
    uint64_t v54 = v139;
    uint64_t v55 = v138;
    uint64_t v52 = v141;
    uint64_t v53 = v140;
    uint64_t v50 = v143;
    uint64_t v51 = v142;
    uint64_t v48 = v145;
    uint64_t v49 = v144;
    uint64_t v46 = v147;
    uint64_t v47 = v146;
    v28.n128_u64[0] = v148;
    __n128 v44 = v28;
    uint64_t v45 = v149;
    uint64_t v25 = v151;
    int v27 = v150;
    int v40 = v152;
  }
  uint64_t v130 = v16;
  LOBYTE(v131) = v17;
  BNNS.ActivationFunction.bnnsActivation.getter((uint64_t)&v157);
  uint64_t v29 = v158;
  uint64_t v30 = v159;
  int v31 = v157;
  int v32 = v160;
  uint64_t v33 = v161;
  uint64_t v34 = v162;
  uint64_t v35 = v163;
  outlined init with take of BNNS.NormalizationType((uint64_t)v190, (uint64_t)&v130);
  if (_s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)&v130) == 2) {
    uint64_t v36 = *(void *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)&v130);
  }
  else {
    uint64_t v36 = 0;
  }
  __src[8] = v187;
  __src[9] = v188;
  __src[4] = v183;
  __src[5] = v184;
  __src[6] = v185;
  __src[7] = v186;
  __src[0] = v179;
  __src[1] = v180;
  __src[2] = v181;
  __src[3] = v182;
  __src[18] = v175;
  __src[19] = v176;
  __src[20] = v177;
  __src[21] = v178;
  __src[13] = v170;
  __src[14] = v171;
  __src[15] = v172;
  __src[16] = v173;
  __src[17] = v174;
  __src[10] = v189;
  __src[11] = v168;
  __src[12] = v169;
  type metadata accessor for BNNSLayerParametersNormalization(0);
  v128[3] = v37;
  v128[4] = (uint64_t)&protocol witness table for BNNSLayerParametersNormalization;
  uint64_t v38 = swift_allocObject();
  *uint64_t v128 = v38;
  memcpy((void *)(v38 + 16), __src, 0x160uLL);
  *(void *)(v38 + 376) = v125;
  *(void *)(v38 + 384) = v124;
  *(void *)(v38 + 392) = v123;
  *(void *)(v38 + 400) = v122;
  *(void *)(v38 + 408) = v121;
  *(void *)(v38 + 416) = v120;
  *(void *)(v38 + 424) = v119;
  *(void *)(v38 + 432) = v118;
  *(void *)(v38 + 440) = v117;
  *(void *)(v38 + 448) = v116;
  *(void *)(v38 + 456) = v115;
  *(void *)(v38 + 464) = v114;
  *(void *)(v38 + 472) = v113;
  *(void *)(v38 + 480) = v112;
  *(void *)(v38 + 488) = v111;
  *(void *)(v38 + 496) = v110;
  *(void *)(v38 + 504) = v109;
  *(void *)(v38 + 520) = v107;
  *(_DWORD *)(v38 + 528) = v106;
  *(_DWORD *)(v38 + 540) = v43;
  *(void *)(v38 + 552) = v104;
  *(void *)(v38 + 560) = v103;
  *(void *)(v38 + 568) = v102;
  *(void *)(v38 + 576) = v101;
  *(void *)(v38 + 584) = v100;
  *(void *)(v38 + 592) = v99;
  *(void *)(v38 + 600) = v98;
  *(void *)(v38 + 608) = v97;
  *(void *)(v38 + 616) = v96;
  *(void *)(v38 + 624) = v95;
  *(void *)(v38 + 632) = v94;
  *(void *)(v38 + 640) = v93;
  *(void *)(v38 + 648) = v92;
  *(void *)(v38 + 656) = v91;
  *(void *)(v38 + 664) = v90;
  *(void *)(v38 + 672) = v88;
  *(void *)(v38 + 680) = v87;
  *(void *)(v38 + 696) = v86;
  *(_DWORD *)(v38 + 704) = v85;
  *(_DWORD *)(v38 + 716) = v42;
  *(void *)(v38 + 728) = v82;
  *(void *)(v38 + 736) = v81;
  *(void *)(v38 + 744) = v80;
  *(void *)(v38 + 752) = v79;
  *(void *)(v38 + 760) = v78;
  *(void *)(v38 + 768) = v77;
  *(void *)(v38 + 776) = v76;
  *(void *)(v38 + 784) = v75;
  *(void *)(v38 + 792) = v73;
  *(void *)(v38 + 800) = v72;
  *(void *)(v38 + 808) = v71;
  *(void *)(v38 + 816) = v70;
  *(void *)(v38 + 824) = v69;
  *(void *)(v38 + 832) = v68;
  *(void *)(v38 + 840) = v67;
  *(void *)(v38 + 848) = v66;
  *(void *)(v38 + 856) = v65;
  *(void *)(v38 + 872) = v64;
  *(void *)(v38 + 368) = v126;
  *(void *)(v38 + 512) = v108;
  *(void *)(v38 + 532) = v24;
  *(void *)(v38 + 544) = v105;
  *(void *)(v38 + 688) = v89;
  *(void *)(v38 + 708) = v23;
  *(void *)(v38 + 720) = v84;
  *(void *)(v38 + 864) = v74;
  *(_DWORD *)(v38 + 880) = v83;
  *(void *)(v38 + 884) = v26;
  *(_DWORD *)(v38 + 892) = v41;
  *(void *)(v38 + 896) = v62;
  *(void *)(v38 + 904) = v63;
  *(void *)(v38 + 912) = v61;
  *(void *)(v38 + 920) = v60;
  *(void *)(v38 + 928) = v59;
  *(void *)(v38 + 936) = v58;
  *(void *)(v38 + 944) = v57;
  *(void *)(v38 + 952) = v56;
  *(void *)(v38 + 960) = v55;
  *(void *)(v38 + 968) = v54;
  *(void *)(v38 + 976) = v53;
  *(void *)(v38 + 984) = v52;
  *(void *)(v38 + 992) = v51;
  *(void *)(v38 + 1000) = v50;
  *(void *)(v38 + 1008) = v49;
  *(void *)(v38 + 1016) = v48;
  *(void *)(v38 + 1024) = v47;
  *(void *)(v38 + 1032) = v46;
  __n128 result = v44;
  *(void *)(v38 + 1040) = v44.n128_u64[0];
  *(void *)(v38 + 1048) = v45;
  *(_DWORD *)(v38 + 1056) = v27;
  *(void *)(v38 + 1060) = v25;
  *(_DWORD *)(v38 + 1068) = v40;
  *(void *)(v38 + 1072) = v15;
  *(_DWORD *)(v38 + 1080) = v31;
  *(void *)(v38 + 1084) = v29;
  *(void *)(v38 + 1092) = v30;
  *(_DWORD *)(v38 + 1100) = v32;
  *(void *)(v38 + 1104) = v33;
  *(void *)(v38 + 1112) = v34;
  *(void *)(v38 + 1120) = v35;
  *(void *)(v38 + 1128) = v127;
  *(void *)(v38 + 1136) = v36;
  return result;
}

void *BNNS.FusedNormalizationParameters.init(type:beta:gamma:momentum:epsilon:activation:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t *a4@<X3>, void *a5@<X8>, float a6@<S0>, float a7@<S1>)
{
  outlined init with take of BNNSNDArrayDescriptor?(a3, (uint64_t)v16);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v16, (uint64_t)v17);
  outlined init with take of BNNSNDArrayDescriptor?(a2, (uint64_t)v15);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v15, (uint64_t)v18);
  outlined init with take of BNNS.NormalizationType(a1, (uint64_t)v19);
  uint64_t v13 = *a4;
  LOBYTE(a4) = *((unsigned char *)a4 + 8);
  outlined init with take of BNNS.NormalizationType((uint64_t)v19, (uint64_t)__src);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v18, (uint64_t)&__src[92]);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v17, (uint64_t)&__src[138]);
  *(float *)&__src[183] = a6;
  *(float *)&__src[184] = a7;
  *(void *)&__src[185] = v13;
  LOBYTE(__src[187]) = (_BYTE)a4;
  return memcpy(a5, __src, 0x2EDuLL);
}

uint64_t BNNS.FusedNormalizationParameters.type.getter@<X0>(uint64_t a1@<X8>)
{
  outlined init with take of BNNS.NormalizationType(v1, (uint64_t)v4);
  return outlined init with take of BNNS.NormalizationType((uint64_t)v4, a1);
}

uint64_t BNNS.FusedNormalizationParameters.type.setter(uint64_t a1)
{
  outlined init with take of BNNS.NormalizationType(a1, (uint64_t)v3);
  return outlined init with take of BNNS.NormalizationType((uint64_t)v3, v1);
}

uint64_t (*BNNS.FusedNormalizationParameters.type.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedNormalizationParameters.beta.getter@<X0>(uint64_t a1@<X8>)
{
  outlined init with take of BNNSNDArrayDescriptor?(v1 + 368, (uint64_t)v4);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v4, a1);
}

uint64_t BNNS.FusedNormalizationParameters.beta.setter(uint64_t a1)
{
  return outlined init with take of BNNSNDArrayDescriptor?(a1, v1 + 368);
}

uint64_t (*BNNS.FusedNormalizationParameters.beta.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedNormalizationParameters.gamma.getter@<X0>(uint64_t a1@<X8>)
{
  outlined init with take of BNNSNDArrayDescriptor?(v1 + 552, (uint64_t)v4);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v4, a1);
}

uint64_t BNNS.FusedNormalizationParameters.gamma.setter(uint64_t a1)
{
  return outlined init with take of BNNSNDArrayDescriptor?(a1, v1 + 552);
}

uint64_t (*BNNS.FusedNormalizationParameters.gamma.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

float BNNS.FusedNormalizationParameters.momentum.getter()
{
  return *(float *)(v0 + 732);
}

void BNNS.FusedNormalizationParameters.momentum.setter(float a1)
{
  *(float *)(v1 + 732) = a1;
}

uint64_t (*BNNS.FusedNormalizationParameters.momentum.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

float BNNS.FusedNormalizationParameters.epsilon.getter()
{
  return *(float *)(v0 + 736);
}

void BNNS.FusedNormalizationParameters.epsilon.setter(float a1)
{
  *(float *)(v1 + 736) = a1;
}

uint64_t (*BNNS.FusedNormalizationParameters.epsilon.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedNormalizationParameters.activation.getter(uint64_t a1@<X8>)
{
  char v2 = *(unsigned char *)(v1 + 748);
  *(void *)a1 = *(void *)(v1 + 740);
  *(unsigned char *)(a1 + 8) = v2;
}

uint64_t BNNS.FusedNormalizationParameters.activation.setter(uint64_t result)
{
  char v2 = *(unsigned char *)(result + 8);
  *(void *)(v1 + 740) = *(void *)result;
  *(unsigned char *)(v1 + 748) = v2;
  return result;
}

uint64_t (*BNNS.FusedNormalizationParameters.activation.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t protocol witness for FusableLayerParametersWrapper.filterType.getter in conformance BNNS.FusedNormalizationParameters()
{
  outlined init with take of BNNS.NormalizationType(v0, (uint64_t)v3);
  outlined init with take of BNNS.NormalizationType((uint64_t)v3, (uint64_t)v4);
  uint64_t v1 = _s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v4) + 2;
  _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v4);
  return v1;
}

void *__swift_memcpy749_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x2EDuLL);
}

uint64_t getEnumTagSinglePayload for BNNS.FusedNormalizationParameters(uint64_t a1, int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0 && *(unsigned char *)(a1 + 749)) {
    return *(_DWORD *)a1 + 0x80000000;
  }
  uint64_t v2 = *(void *)(a1 + 176) >> 1;
  if (v2 > 0x80000000) {
    int v3 = ~v2;
  }
  else {
    int v3 = -1;
  }
  return (v3 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.FusedNormalizationParameters(uint64_t result, int a2, int a3)
{
  if (a2 < 0)
  {
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(unsigned char *)(result + 748) = 0;
    *(_DWORD *)(result + 744) = 0;
    *(_OWORD *)(result + 728) = 0u;
    *(_OWORD *)(result + 712) = 0u;
    *(_OWORD *)(result + 696) = 0u;
    *(_OWORD *)(result + 680) = 0u;
    *(_OWORD *)(result + 664) = 0u;
    *(_OWORD *)(result + 648) = 0u;
    *(_OWORD *)(result + 632) = 0u;
    *(_OWORD *)(result + 616) = 0u;
    *(_OWORD *)(result + 600) = 0u;
    *(_OWORD *)(result + 584) = 0u;
    *(_OWORD *)(result + 568) = 0u;
    *(_OWORD *)(result + 552) = 0u;
    *(_OWORD *)(result + 536) = 0u;
    *(_OWORD *)(result + 520) = 0u;
    *(_OWORD *)(result + 504) = 0u;
    *(_OWORD *)(result + 488) = 0u;
    *(_OWORD *)(result + 472) = 0u;
    *(_OWORD *)(result + 456) = 0u;
    *(_OWORD *)(result + 440) = 0u;
    *(_OWORD *)(result + 424) = 0u;
    *(_OWORD *)(result + 408) = 0u;
    *(_OWORD *)(result + 392) = 0u;
    *(_OWORD *)(result + 376) = 0u;
    *(_OWORD *)(result + 360) = 0u;
    *(_OWORD *)(result + 344) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(void *)__n128 result = a2 ^ 0x80000000;
    if (a3 < 0) {
      *(unsigned char *)(result + 749) = 1;
    }
  }
  else
  {
    if ((a3 & 0x80000000) == 0)
    {
      if (!a2) {
        return result;
      }
LABEL_8:
      *(_OWORD *)(result + 144) = 0u;
      *(_OWORD *)(result + 160) = 0u;
      *(_OWORD *)(result + 112) = 0u;
      *(_OWORD *)(result + 128) = 0u;
      *(_OWORD *)(result + 80) = 0u;
      *(_OWORD *)(result + 96) = 0u;
      *(_OWORD *)(result + 48) = 0u;
      *(_OWORD *)(result + 64) = 0u;
      *(_OWORD *)(result + 16) = 0u;
      *(_OWORD *)(result + 32) = 0u;
      *(_OWORD *)__n128 result = 0u;
      *(void *)(result + 176) = 2 * -a2;
      *(_OWORD *)(result + 200) = 0u;
      *(_OWORD *)(result + 216) = 0u;
      *(_OWORD *)(result + 232) = 0u;
      *(_OWORD *)(result + 248) = 0u;
      *(unsigned char *)(result + 360) = 0;
      *(_OWORD *)(result + 184) = 0u;
      result += 184;
      *(_OWORD *)(result + 80) = 0u;
      *(_OWORD *)(result + 96) = 0u;
      *(_OWORD *)(result + 112) = 0u;
      *(_OWORD *)(result + 128) = 0u;
      *(_OWORD *)(result + 144) = 0u;
      *(_OWORD *)(result + 160) = 0u;
      return result;
    }
    *(unsigned char *)(result + 749) = 0;
    if (a2) {
      goto LABEL_8;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.FusedNormalizationParameters()
{
  return &type metadata for BNNS.FusedNormalizationParameters;
}

uint64_t sub_1D20A3904()
{
  return MEMORY[0x1F4186498](v0, 1144, 7);
}

float static vDSP.maximum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.maximum<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.maximum<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F16968]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F16978]);
}

double static vDSP.maximum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.maximum<A>(_:));
}

float static vDSP.maximumMagnitude<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.maximumMagnitude<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.maximumMagnitude<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F16948]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F16950]);
}

double static vDSP.maximumMagnitude<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.maximumMagnitude<A>(_:));
}

float static vDSP.minimum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.minimum<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.minimum<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F169C8]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F169D0]);
}

double static vDSP.minimum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.minimum<A>(_:));
}

float static vDSP.sum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.sum<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.sum<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F16A38]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F16A48]);
}

double static vDSP.sum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.sum<A>(_:));
}

float static vDSP.sumOfSquares<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.sumOfSquares<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.sumOfSquares<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F16A80]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F16A88]);
}

double static vDSP.sumOfSquares<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.sumOfSquares<A>(_:));
}

float static vDSP.sumAndSumOfSquares<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  if (v4 < 0) {
    __break(1u);
  }
  MEMORY[0x1F4188790](v4);
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t)))(a3 + 24))(partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:));
  return NAN;
}

uint64_t partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(a1, a2, MEMORY[0x1E4F16A50]);
}

{
  return partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(a1, a2, MEMORY[0x1E4F16A58]);
}

double static vDSP.sumAndSumOfSquares<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  if (v4 < 0) {
    __break(1u);
  }
  MEMORY[0x1F4188790](v4);
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t)))(a3 + 24))(partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:));
  return NAN;
}

float static vDSP.sumOfMagnitudes<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.sumOfMagnitudes<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.sumOfMagnitudes<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F16A60]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F16A70]);
}

double static vDSP.sumOfMagnitudes<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.sumOfMagnitudes<A>(_:));
}

float static vDSP.indexOfMaximum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.indexOfMaximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.indexOfMaximum<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.indexOfMaximum<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(a1, a2, MEMORY[0x1E4F16980]);
}

{
  return partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(a1, a2, MEMORY[0x1E4F16988]);
}

double static vDSP.indexOfMaximum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.indexOfMaximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.indexOfMaximum<A>(_:));
}

float static vDSP.indexOfMaximumMagnitude<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.indexOfMaximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.indexOfMaximumMagnitude<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.indexOfMaximumMagnitude<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(a1, a2, MEMORY[0x1E4F16958]);
}

{
  return partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(a1, a2, MEMORY[0x1E4F16960]);
}

double static vDSP.indexOfMaximumMagnitude<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.indexOfMaximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.indexOfMaximumMagnitude<A>(_:));
}

float static vDSP.indexOfMinimum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.indexOfMaximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.indexOfMinimum<A>(_:));
}

float static vDSP.indexOfMaximum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v6 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  if (v6 < 0) {
    __break(1u);
  }
  MEMORY[0x1F4188790](v6);
  (*(void (**)(uint64_t))(a3 + 24))(a4);
  return NAN;
}

uint64_t partial apply for closure #1 in static vDSP.indexOfMinimum<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(a1, a2, MEMORY[0x1E4F169D8]);
}

{
  return partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(a1, a2, MEMORY[0x1E4F169E0]);
}

double static vDSP.indexOfMinimum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.indexOfMaximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.indexOfMinimum<A>(_:));
}

double static vDSP.indexOfMaximum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v6 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  if (v6 < 0) {
    __break(1u);
  }
  MEMORY[0x1F4188790](v6);
  (*(void (**)(uint64_t))(a3 + 24))(a4);
  return NAN;
}

uint64_t partial apply for closure #1 in static vDSP.sumAndSumOfSquares<A>(_:)(uint64_t result, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, void, void, void))
{
  if (result) {
    return a3(result, 1, v3[2], v3[3], v3[4]);
  }
  __break(1u);
  return result;
}

float static vDSP.meanSquare<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.meanSquare<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.meanSquare<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F169B0]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F169B8]);
}

double static vDSP.meanSquare<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.meanSquare<A>(_:));
}

float static vDSP.meanMagnitude<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.meanMagnitude<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.meanMagnitude<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F16990]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F16998]);
}

double static vDSP.meanMagnitude<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.meanMagnitude<A>(_:));
}

float static vDSP.mean<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.mean<A>(_:));
}

uint64_t partial apply for closure #1 in static vDSP.mean<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F169A0]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F169A8]);
}

double static vDSP.mean<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.mean<A>(_:));
}

float static vDSP.rootMeanSquare<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.rootMeanSquare<A>(_:));
}

float static vDSP.maximum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v6 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  if (v6 < 0) {
    __break(1u);
  }
  MEMORY[0x1F4188790](v6);
  (*(void (**)(uint64_t))(a3 + 24))(a4);
  return NAN;
}

uint64_t partial apply for closure #1 in static vDSP.rootMeanSquare<A>(_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F16A28]);
}

{
  return partial apply for closure #1 in static vDSP.maximum<A>(_:)(a1, a2, MEMORY[0x1E4F16A30]);
}

double static vDSP.rootMeanSquare<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.maximum<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.rootMeanSquare<A>(_:));
}

double static vDSP.maximum<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v6 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  if (v6 < 0) {
    __break(1u);
  }
  MEMORY[0x1F4188790](v6);
  (*(void (**)(uint64_t))(a3 + 24))(a4);
  return NAN;
}

uint64_t partial apply for closure #1 in static vDSP.maximum<A>(_:)(uint64_t result, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, void, void))
{
  if (result) {
    return a3(result, 1, *(void *)(v3 + 16), *(void *)(v3 + 24));
  }
  __break(1u);
  return result;
}

uint64_t BNNS.BroadcastMatrixMultiplyLayer.__allocating_init(inputA:transposed:isWeights:inputB:transposed:isWeights:output:alpha:accumulatesToOutput:isQuadratic:filterParameters:)(_OWORD *a1, char a2, char a3, _OWORD *a4, char a5, char a6, long long *a7, char a8, float a9, char a10, int a11, int a12, uint64_t a13, uint64_t a14, uint64_t a15)
{
  uint64_t v49 = *MEMORY[0x1E4F143B8];
  if (a3 & 1) != 0 && (a6) {
    return 0;
  }
  long long v15 = a1[6];
  *(_OWORD *)&v47[115] = a1[7];
  if (a8) {
    float v16 = 1.0;
  }
  else {
    float v16 = 0.0;
  }
  long long v17 = a1[9];
  *(_OWORD *)&v47[131] = a1[8];
  *(_OWORD *)&v47[147] = v17;
  *(_OWORD *)&v47[163] = a1[10];
  long long v18 = a1[2];
  *(_OWORD *)&v47[51] = a1[3];
  long long v19 = a1[5];
  *(_OWORD *)&v47[67] = a1[4];
  *(_OWORD *)&v47[83] = v19;
  *(_OWORD *)&v47[99] = v15;
  long long v20 = a1[1];
  *(_OWORD *)&v47[3] = *a1;
  *(_OWORD *)&v47[19] = v20;
  *(_OWORD *)&v47[35] = v18;
  long long v21 = a4[9];
  *(_OWORD *)&layer_params.iB_desc.stride[7] = a4[8];
  *(_OWORD *)&layer_params.iB_desc.data_type = v21;
  long long v22 = a4[5];
  *(_OWORD *)&layer_params.iB_desc.size[7] = a4[4];
  *(_OWORD *)&layer_params.iB_desc.stride[1] = v22;
  long long v23 = a4[7];
  *(_OWORD *)&layer_params.iB_desc.stride[3] = a4[6];
  *(_OWORD *)&layer_params.iB_desc.stride[5] = v23;
  long long v24 = a4[1];
  *(_OWORD *)&layer_params.iB_desc.flags = *a4;
  *(_OWORD *)&layer_params.iB_desc.size[1] = v24;
  long long v25 = a4[3];
  *(_OWORD *)&layer_params.iB_desc.size[3] = a4[2];
  *(_OWORD *)&layer_params.iB_desc.size[5] = v25;
  long long v26 = a7[8];
  long long v27 = a7[9];
  long long v28 = a7[6];
  *(_OWORD *)&layer_params.o_desc.stride[5] = a7[7];
  *(_OWORD *)&layer_params.o_desc.stride[7] = v26;
  long long v29 = a7[10];
  *(_OWORD *)&layer_params.o_desc.data_type = v27;
  *(_OWORD *)&layer_params.o_desc.table_data_type = v29;
  long long v30 = a7[4];
  long long v31 = a7[5];
  long long v32 = a7[2];
  *(_OWORD *)&layer_params.o_desc.size[5] = a7[3];
  *(_OWORD *)&layer_params.o_desc.size[7] = v30;
  long long v33 = a4[10];
  *(_OWORD *)&layer_params.o_desc.stride[1] = v31;
  *(_OWORD *)&layer_params.o_desc.stride[3] = v28;
  long long v34 = *a7;
  long long v35 = a7[1];
  *(_OWORD *)&layer_params.iB_desc.table_data_type = v33;
  *(_OWORD *)&layer_params.o_desc.flags = v34;
  char v36 = a3 & 1;
  char v37 = a6 & 1;
  *(_OWORD *)&layer_params.o_desc.size[1] = v35;
  *(_OWORD *)&layer_params.o_desc.size[3] = v32;
  layer_params.alpha = a9;
  layer_params.beta = v16;
  layer_params.transA = a2 & 1;
  layer_params.transB = a5 & 1;
  layer_params.quadratic = a10 & 1;
  layer_params.a_is_weights = a3 & 1;
  layer_params.b_is_weights = a6 & 1;
  *(_OWORD *)((char *)&layer_params.iA_desc.stride[6] + 5) = *(_OWORD *)&v47[128];
  *(_OWORD *)((char *)&layer_params.iA_desc.data + 5) = *(_OWORD *)&v47[144];
  *(_OWORD *)((char *)&layer_params.iA_desc.table_data + 5) = *(_OWORD *)&v47[160];
  *((_DWORD *)&layer_params.iA_desc.data_bias + 1) = *(_DWORD *)&v47[175];
  *(_OWORD *)((char *)&layer_params.iA_desc.size[6] + 5) = *(_OWORD *)&v47[64];
  *(_OWORD *)((char *)layer_params.iA_desc.stride + 5) = *(_OWORD *)&v47[80];
  *(_OWORD *)((char *)&layer_params.iA_desc.stride[2] + 5) = *(_OWORD *)&v47[96];
  *(_OWORD *)((char *)&layer_params.iA_desc.stride[4] + 5) = *(_OWORD *)&v47[112];
  *(_OWORD *)(&layer_params.b_is_weights + 1) = *(_OWORD *)v47;
  *(_OWORD *)((char *)layer_params.iA_desc.size + 5) = *(_OWORD *)&v47[16];
  *(_OWORD *)((char *)&layer_params.iA_desc.size[2] + 5) = *(_OWORD *)&v47[32];
  *(_OWORD *)((char *)&layer_params.iA_desc.size[4] + 5) = *(_OWORD *)&v47[48];
  if (a14 == 1)
  {
    uint64_t v38 = 0;
  }
  else
  {
    int v43 = a12;
    uint64_t v44 = a13;
    uint64_t v45 = a14;
    uint64_t v46 = a15;
    uint64_t v38 = (const BNNSFilterParameters *)&v43;
  }
  long long v39 = BNNSFilterCreateLayerBroadcastMatMul(&layer_params, v38);
  type metadata accessor for BNNS.BroadcastMatrixMultiplyLayer();
  uint64_t v40 = swift_allocObject();
  uint64_t v41 = v40;
  *(unsigned char *)(v40 + 24) = v36;
  *(unsigned char *)(v40 + 25) = v37;
  if (!v39)
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  *(void *)(v40 + 16) = v39;
  return v41;
}

uint64_t type metadata accessor for BNNS.BroadcastMatrixMultiplyLayer()
{
  return self;
}

uint64_t BNNS.BroadcastMatrixMultiplyLayer.apply(batchSize:inputA:inputB:output:)(void *a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  int v8 = *(unsigned __int8 *)(v4 + 24);
  uint64_t v9 = a3;
  if (v8 & 1) != 0 || (uint64_t v9 = a2, (*(unsigned char *)(v4 + 25)))
  {
    long long v10 = *(const void **)(v9 + 136);
    if (v10)
    {
      outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v99);
      outlined init with take of UnsafeMutableRawPointer?((uint64_t)v99, (uint64_t)&v106);
      if (v106)
      {
        uint64_t v65 = v106;
        uint64_t v67 = v10;
        uint64_t v107 = a1;
        BNNSNDArrayDescriptor.shape.getter((uint64_t)v97);
        outlined init with take of BNNS.Shape((uint64_t)v97, (uint64_t)v98);
        outlined init with take of BNNS.Shape((uint64_t)v98, (uint64_t)v96);
        BNNS.Shape.size.getter((uint64_t)&v88);
        if (v8)
        {
          unint64_t v11 = v88;
          unint64_t v12 = v89;
          unint64_t v13 = v90;
          unint64_t v14 = v91;
          unint64_t v15 = v92;
          unint64_t v57 = v94;
          unint64_t v62 = v93;
          unint64_t v53 = v95;
          outlined init with take of BNNS.Shape((uint64_t)v98, (uint64_t)v96);
          BNNS.Shape.stride.getter((uint64_t)&v88);
          unint64_t v16 = specialized static BNNS.calculateBatchStride(size:stride:)(v11, v12, v13, v14, v15, v62, v57, v53, v88, v89, v90, v91, v92, v93, v94, v95);
        }
        else
        {
          unint64_t v32 = v88;
          unint64_t v33 = v89;
          unint64_t v34 = v90;
          unint64_t v35 = v91;
          unint64_t v36 = v92;
          unint64_t v60 = v94;
          unint64_t v63 = v93;
          unint64_t v55 = v95;
          outlined init with take of BNNS.Shape((uint64_t)v98, (uint64_t)v96);
          BNNS.Shape.stride.getter((uint64_t)&v88);
          unint64_t v16 = specialized static BNNS.calculateBatchStride(size:stride:)(v32, v33, v34, v35, v36, v63, v60, v55, v88, v89, v90, v91, v92, v93, v94, v95);
        }
        uint64_t v61 = *(void **)(v4 + 16);
        size_t v64 = v16;
        BNNSNDArrayDescriptor.shape.getter((uint64_t)v97);
        outlined init with take of BNNS.Shape((uint64_t)v97, (uint64_t)v98);
        outlined init with take of BNNS.Shape((uint64_t)v98, (uint64_t)v96);
        BNNS.Shape.size.getter((uint64_t)&v88);
        unint64_t v37 = v88;
        unint64_t v38 = v89;
        unint64_t v39 = v90;
        unint64_t v40 = v91;
        unint64_t v41 = v92;
        unint64_t v56 = v93;
        unint64_t v42 = v94;
        unint64_t v43 = v95;
        outlined init with take of BNNS.Shape((uint64_t)v98, (uint64_t)v96);
        BNNS.Shape.stride.getter((uint64_t)&v88);
        size_t v44 = specialized static BNNS.calculateBatchStride(size:stride:)(v37, v38, v39, v40, v41, v56, v42, v43, v88, v89, v90, v91, v92, v93, v94, v95);
        uint64_t result = BNNSFilterApplyBatch(v61, (size_t)v107, v67, v64, v65, v44);
        if (result) {
          goto LABEL_15;
        }
        return result;
      }
    }
LABEL_12:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v31 = 2;
    return swift_willThrow();
  }
  outlined init with take of UnsafeMutableRawPointer?(a2 + 136, (uint64_t)v102);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v102, (uint64_t)&v103);
  long long v17 = v103;
  if (!v103) {
    goto LABEL_12;
  }
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v101);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v101, (uint64_t)&v104);
  long long v18 = v104;
  if (!v104) {
    goto LABEL_12;
  }
  outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v100);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v100, (uint64_t)&v105);
  if (!v105) {
    goto LABEL_12;
  }
  uint64_t v68 = *(void **)(v4 + 16);
  uint64_t v107 = v105;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v88);
  outlined init with take of BNNS.Shape((uint64_t)&v88, (uint64_t)v96);
  outlined init with take of BNNS.Shape((uint64_t)v96, (uint64_t)v87);
  BNNS.Shape.size.getter((uint64_t)&v79);
  unint64_t v19 = v79;
  unint64_t v20 = v80;
  uint64_t v66 = v18;
  unint64_t v22 = v81;
  unint64_t v21 = v82;
  unint64_t v58 = v83;
  unint64_t v47 = v85;
  size_t inB_stride = v84;
  unint64_t v46 = v86;
  outlined init with take of BNNS.Shape((uint64_t)v96, (uint64_t)v87);
  BNNS.Shape.stride.getter((uint64_t)&v79);
  size_t v59 = specialized static BNNS.calculateBatchStride(size:stride:)(v19, v20, v22, v21, v58, inB_stride, v47, v46, v79, v80, v81, v82, v83, v84, v85, v86);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v87);
  outlined init with take of BNNS.Shape((uint64_t)v87, (uint64_t)v97);
  outlined init with take of BNNS.Shape((uint64_t)v97, (uint64_t)&v79);
  BNNS.Shape.size.getter((uint64_t)&v74);
  long long v23 = v74;
  long long v24 = v75;
  long long v25 = v76;
  unint64_t v48 = v78;
  size_t inB_stridea = v77;
  outlined init with take of BNNS.Shape((uint64_t)v97, (uint64_t)&v79);
  BNNS.Shape.stride.getter((uint64_t)&v74);
  unint64_t inB_strideb = specialized static BNNS.calculateBatchStride(size:stride:)(v23, *((unint64_t *)&v23 + 1), v24, *((unint64_t *)&v24 + 1), v25, *((unint64_t *)&v25 + 1), inB_stridea, v48, v74, *((unint64_t *)&v74 + 1), v75, *((unint64_t *)&v75 + 1), v76, *((unint64_t *)&v76 + 1), v77, v78);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v79);
  outlined init with take of BNNS.Shape((uint64_t)&v79, (uint64_t)v98);
  outlined init with take of BNNS.Shape((uint64_t)v98, (uint64_t)&v74);
  BNNS.Shape.size.getter((uint64_t)&v69);
  long long v26 = v69;
  long long v27 = v70;
  long long v28 = v71;
  unint64_t v54 = v72;
  unint64_t v49 = v73;
  outlined init with take of BNNS.Shape((uint64_t)v98, (uint64_t)&v74);
  BNNS.Shape.stride.getter((uint64_t)&v69);
  size_t v29 = specialized static BNNS.calculateBatchStride(size:stride:)(v26, *((unint64_t *)&v26 + 1), v27, *((unint64_t *)&v27 + 1), v28, *((unint64_t *)&v28 + 1), v54, v49, v69, *((unint64_t *)&v69 + 1), v70, *((unint64_t *)&v70 + 1), v71, *((unint64_t *)&v71 + 1), v72, v73);
  uint64_t result = BNNSFilterApplyTwoInputBatch(v68, (size_t)a1, v17, v59, v66, inB_strideb, v107, v29);
  if (result)
  {
LABEL_15:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    unsigned char *v45 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t outlined init with take of UnsafeMutableRawPointer?(uint64_t a1, uint64_t a2)
{
  uint64_t v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(void *)(v4 - 8) + 32))(a2, a1, v4);
  return a2;
}

uint64_t BNNS.BroadcastMatrixMultiplyLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _OWORD *a5, long long *a6, _OWORD *a7)
{
  uint64_t v119 = *MEMORY[0x1E4F143B8];
  long long v11 = a6[9];
  long long v115 = a6[8];
  long long v116 = v11;
  long long v117 = a6[10];
  long long v12 = a6[5];
  long long v111 = a6[4];
  long long v112 = v12;
  long long v13 = a6[7];
  long long v113 = a6[6];
  long long v114 = v13;
  long long v14 = a6[1];
  long long v107 = *a6;
  long long v108 = v14;
  long long v15 = a6[3];
  long long v109 = a6[2];
  long long v110 = v15;
  long long v16 = a7[6];
  *(_OWORD *)&v118.stride[5] = a7[7];
  long long v17 = a7[9];
  *(_OWORD *)&v118.stride[7] = a7[8];
  *(_OWORD *)&v118.data_type = v17;
  *(_OWORD *)&v118.table_data_type = a7[10];
  long long v18 = a7[2];
  *(_OWORD *)&v118.size[5] = a7[3];
  long long v19 = a7[5];
  *(_OWORD *)&v118.size[7] = a7[4];
  *(_OWORD *)&v118.stride[1] = v19;
  *(_OWORD *)&v118.stride[3] = v16;
  long long v20 = a7[1];
  *(_OWORD *)&v118.flags = *a7;
  *(_OWORD *)&v118.size[1] = v20;
  *(_OWORD *)&v118.size[3] = v18;
  if (*(unsigned char *)(v7 + 24))
  {
    uint64_t v87 = a4;
    size_t v88 = a1;
    unint64_t v86 = *(const void **)(a3 + 136);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v104);
    outlined init with take of BNNS.Shape((uint64_t)&v104, (uint64_t)&v103);
    outlined init with take of BNNS.Shape((uint64_t)&v103, (uint64_t)&v102);
    BNNS.Shape.size.getter((uint64_t)&v96);
    long long v21 = v96;
    unint64_t v83 = *((void *)&v97 + 1);
    unint64_t v22 = v97;
    unint64_t v23 = v98;
    unint64_t v75 = v100;
    unint64_t v79 = v99;
    unint64_t v71 = v101;
    outlined init with take of BNNS.Shape((uint64_t)&v103, (uint64_t)&v102);
    BNNS.Shape.stride.getter((uint64_t)&v96);
    size_t v84 = specialized static BNNS.calculateBatchStride(size:stride:)(v21, *((unint64_t *)&v21 + 1), v22, v83, v23, v79, v75, v71, v96, *((unint64_t *)&v96 + 1), v97, *((unint64_t *)&v97 + 1), v98, v99, v100, v101);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v102);
    outlined init with take of BNNS.Shape((uint64_t)&v102, (uint64_t)&v104);
    outlined init with take of BNNS.Shape((uint64_t)&v104, (uint64_t)&v96);
    BNNS.Shape.size.getter((uint64_t)&v90);
    unint64_t v24 = v91;
    long long v25 = v92;
    long long v26 = v93;
    unint64_t v76 = v94;
    unint64_t v80 = v90;
    unint64_t v72 = v95;
    outlined init with take of BNNS.Shape((uint64_t)&v104, (uint64_t)&v96);
    BNNS.Shape.stride.getter((uint64_t)&v90);
    size_t v27 = specialized static BNNS.calculateBatchStride(size:stride:)(v80, v24, v25, *((unint64_t *)&v25 + 1), v26, *((unint64_t *)&v26 + 1), v76, v72, v90, v91, v92, *((unint64_t *)&v92 + 1), v93, *((unint64_t *)&v93 + 1), v94, v95);
    long long v28 = a6[9];
    *(_OWORD *)&v105.stride[7] = a6[8];
    *(_OWORD *)&v105.data_type = v28;
    long long v29 = a6[10];
    long long v30 = a6[5];
    *(_OWORD *)&v105.size[7] = a6[4];
    *(_OWORD *)&v105.stride[1] = v30;
    long long v31 = a6[7];
    *(_OWORD *)&v105.stride[3] = a6[6];
    *(_OWORD *)&v105.stride[5] = v31;
    long long v32 = a6[1];
    *(_OWORD *)&v105.flags = *a6;
    *(_OWORD *)&v105.size[1] = v32;
    long long v33 = a6[3];
    *(_OWORD *)&v105.size[3] = a6[2];
    *(_OWORD *)&v105.size[5] = v33;
    BNNSNDArrayDescriptor v106 = v118;
    *(_OWORD *)&v105.table_data_type = v29;
    goto LABEL_5;
  }
  if (*(unsigned char *)(v7 + 25))
  {
    uint64_t v87 = a4;
    size_t v88 = a1;
    uint64_t v89 = v7;
    unint64_t v86 = *(const void **)(a2 + 136);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v104);
    outlined init with take of BNNS.Shape((uint64_t)&v104, (uint64_t)&v103);
    outlined init with take of BNNS.Shape((uint64_t)&v103, (uint64_t)&v102);
    BNNS.Shape.size.getter((uint64_t)&v96);
    unint64_t v34 = *((void *)&v96 + 1);
    unint64_t v85 = v96;
    long long v35 = v97;
    unint64_t v36 = v98;
    unint64_t v77 = v100;
    unint64_t v81 = v99;
    unint64_t v73 = v101;
    outlined init with take of BNNS.Shape((uint64_t)&v103, (uint64_t)&v102);
    BNNS.Shape.stride.getter((uint64_t)&v96);
    size_t v84 = specialized static BNNS.calculateBatchStride(size:stride:)(v85, v34, v35, *((unint64_t *)&v35 + 1), v36, v81, v77, v73, v96, *((unint64_t *)&v96 + 1), v97, *((unint64_t *)&v97 + 1), v98, v99, v100, v101);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v102);
    outlined init with take of BNNS.Shape((uint64_t)&v102, (uint64_t)&v104);
    outlined init with take of BNNS.Shape((uint64_t)&v104, (uint64_t)&v96);
    BNNS.Shape.size.getter((uint64_t)&v90);
    unint64_t v37 = v91;
    long long v38 = v92;
    long long v39 = v93;
    unint64_t v78 = v94;
    unint64_t v82 = v90;
    unint64_t v74 = v95;
    outlined init with take of BNNS.Shape((uint64_t)&v104, (uint64_t)&v96);
    BNNS.Shape.stride.getter((uint64_t)&v90);
    size_t v27 = specialized static BNNS.calculateBatchStride(size:stride:)(v82, v37, v38, *((unint64_t *)&v38 + 1), v39, *((unint64_t *)&v39 + 1), v78, v74, v90, v91, v92, *((unint64_t *)&v92 + 1), v93, *((unint64_t *)&v93 + 1), v94, v95);
    long long v40 = a7[9];
    *(_OWORD *)&v105.stride[7] = a7[8];
    *(_OWORD *)&v105.data_type = v40;
    long long v41 = a7[5];
    *(_OWORD *)&v105.size[7] = a7[4];
    *(_OWORD *)&v105.stride[1] = v41;
    long long v42 = a7[7];
    *(_OWORD *)&v105.stride[3] = a7[6];
    *(_OWORD *)&v105.stride[5] = v42;
    long long v43 = a7[1];
    *(_OWORD *)&v105.flags = *a7;
    *(_OWORD *)&v105.size[1] = v43;
    long long v44 = a7[3];
    *(_OWORD *)&v105.size[3] = a7[2];
    *(_OWORD *)&v105.size[5] = v44;
    *(_OWORD *)&v106.stride[5] = v114;
    *(_OWORD *)&v106.stride[7] = v115;
    *(_OWORD *)&v106.data_type = v116;
    *(_OWORD *)&v106.table_data_type = v117;
    *(_OWORD *)&v106.size[5] = v110;
    *(_OWORD *)&v106.size[7] = v111;
    long long v45 = a7[10];
    *(_OWORD *)&v106.stride[1] = v112;
    *(_OWORD *)&v106.stride[3] = v113;
    *(_OWORD *)&v105.table_data_type = v45;
    *(_OWORD *)&v106.flags = v107;
    *(_OWORD *)&v106.size[1] = v108;
    *(_OWORD *)&v106.size[3] = v109;
LABEL_5:
    long long v46 = a5[9];
    *(_OWORD *)&v104.stride[7] = a5[8];
    *(_OWORD *)&v104.data_type = v46;
    *(_OWORD *)&v104.table_data_type = a5[10];
    long long v47 = a5[5];
    *(_OWORD *)&v104.size[7] = a5[4];
    *(_OWORD *)&v104.stride[1] = v47;
    long long v48 = a5[7];
    *(_OWORD *)&v104.stride[3] = a5[6];
    *(_OWORD *)&v104.stride[5] = v48;
    long long v49 = a5[1];
    *(_OWORD *)&v104.flags = *a5;
    *(_OWORD *)&v104.size[1] = v49;
    long long v50 = a5[3];
    *(_OWORD *)&v104.size[3] = a5[2];
    *(_OWORD *)&v104.size[5] = v50;
    BNNSNDArrayDescriptor v103 = v106;
    BNNSNDArrayDescriptor v102 = v105;
    uint64_t result = closure #1 in closure #1 in closure #2 in BNNS.BroadcastMatrixMultiplyLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(&v102, v89, v88, v86, v84, &v103, v27, v87, &v104);
    if (!result) {
      return result;
    }
    goto LABEL_6;
  }
  long long v53 = a5[9];
  *(_OWORD *)&v104.stride[7] = a5[8];
  *(_OWORD *)&v104.data_type = v53;
  *(_OWORD *)&v104.table_data_type = a5[10];
  long long v54 = a5[5];
  *(_OWORD *)&v104.size[7] = a5[4];
  *(_OWORD *)&v104.stride[1] = v54;
  long long v55 = a5[7];
  *(_OWORD *)&v104.stride[3] = a5[6];
  *(_OWORD *)&v104.stride[5] = v55;
  long long v56 = a5[1];
  *(_OWORD *)&v104.flags = *a5;
  *(_OWORD *)&v104.size[1] = v56;
  long long v57 = a5[3];
  *(_OWORD *)&v104.size[3] = a5[2];
  *(_OWORD *)&v104.size[5] = v57;
  long long v58 = a6[8];
  long long v59 = a6[9];
  long long v60 = a6[6];
  *(_OWORD *)&v103.stride[5] = a6[7];
  *(_OWORD *)&v103.stride[7] = v58;
  long long v61 = a6[10];
  *(_OWORD *)&v103.data_type = v59;
  *(_OWORD *)&v103.table_data_type = v61;
  long long v62 = a6[4];
  long long v63 = a6[5];
  long long v64 = a6[2];
  *(_OWORD *)&v103.size[5] = a6[3];
  *(_OWORD *)&v103.size[7] = v62;
  *(_OWORD *)&v103.stride[1] = v63;
  *(_OWORD *)&v103.stride[3] = v60;
  long long v65 = *a6;
  *(_OWORD *)&v103.size[1] = a6[1];
  *(_OWORD *)&v103.size[3] = v64;
  long long v66 = a7[9];
  *(_OWORD *)&v102.stride[7] = a7[8];
  *(_OWORD *)&v102.data_type = v66;
  *(_OWORD *)&v102.table_data_type = a7[10];
  *(_OWORD *)&v103.flags = v65;
  long long v67 = a7[5];
  *(_OWORD *)&v102.size[7] = a7[4];
  *(_OWORD *)&v102.stride[1] = v67;
  long long v68 = a7[7];
  *(_OWORD *)&v102.stride[3] = a7[6];
  *(_OWORD *)&v102.stride[5] = v68;
  long long v69 = a7[1];
  *(_OWORD *)&v102.flags = *a7;
  *(_OWORD *)&v102.size[1] = v69;
  long long v70 = a7[3];
  *(_OWORD *)&v102.size[3] = a7[2];
  *(_OWORD *)&v102.size[5] = v70;
  uint64_t result = closure #1 in closure #1 in closure #1 in BNNS.BroadcastMatrixMultiplyLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(&v102, v7, a1, a2, &v103, (uint64_t)a6, a3, (uint64_t)a7, a4, &v104);
  if (result)
  {
LABEL_6:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *uint64_t v52 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in BNNS.BroadcastMatrixMultiplyLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(BNNSNDArrayDescriptor *a1, uint64_t a2, size_t a3, uint64_t a4, BNNSNDArrayDescriptor *a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, const BNNSNDArrayDescriptor *out_delta)
{
  long long v66 = *(void **)(a2 + 16);
  outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v125);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v125, (uint64_t)&v126);
  long long v65 = v126;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v120);
  outlined init with take of BNNS.Shape((uint64_t)v120, (uint64_t)v121);
  outlined init with take of BNNS.Shape((uint64_t)v121, (uint64_t)v122);
  BNNS.Shape.size.getter((uint64_t)&v112);
  unint64_t v10 = v112;
  unint64_t v11 = v113;
  unint64_t v12 = v114;
  unint64_t v13 = v115;
  unint64_t v14 = v116;
  unint64_t v15 = v117;
  unint64_t v16 = v118;
  unint64_t v17 = v119;
  outlined init with take of BNNS.Shape((uint64_t)v121, (uint64_t)v122);
  BNNS.Shape.stride.getter((uint64_t)&v112);
  size_t v64 = specialized static BNNS.calculateBatchStride(size:stride:)(v10, v11, v12, v13, v14, v15, v16, v17, v112, v113, v114, v115, v116, v117, v118, v119);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v112);
  outlined init with take of BNNS.Shape((uint64_t)&v112, (uint64_t)v122);
  outlined init with take of BNNS.Shape((uint64_t)v122, (uint64_t)v111);
  BNNS.Shape.size.getter((uint64_t)&v103);
  unint64_t v18 = v103;
  unint64_t v19 = v104;
  unint64_t v20 = v105;
  unint64_t v21 = v106;
  unint64_t v22 = v107;
  unint64_t v23 = v108;
  unint64_t v24 = v109;
  unint64_t v25 = v110;
  outlined init with take of BNNS.Shape((uint64_t)v122, (uint64_t)v111);
  BNNS.Shape.stride.getter((uint64_t)&v103);
  size_t v63 = specialized static BNNS.calculateBatchStride(size:stride:)(v18, v19, v20, v21, v22, v23, v24, v25, v103, v104, v105, v106, v107, v108, v109, v110);
  outlined init with take of UnsafeMutableRawPointer?(a7 + 136, (uint64_t)v124);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v124, (uint64_t)&v127);
  inB = v127;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v102);
  outlined init with take of BNNS.Shape((uint64_t)v102, (uint64_t)&v103);
  outlined init with take of BNNS.Shape((uint64_t)&v103, (uint64_t)v111);
  BNNS.Shape.size.getter((uint64_t)&v94);
  unint64_t v26 = v94;
  unint64_t v27 = v95;
  unint64_t v28 = v96;
  unint64_t v29 = v97;
  unint64_t v30 = v98;
  unint64_t v31 = v99;
  unint64_t v32 = v100;
  unint64_t v33 = v101;
  outlined init with take of BNNS.Shape((uint64_t)&v103, (uint64_t)v111);
  BNNS.Shape.stride.getter((uint64_t)&v94);
  size_t v60 = specialized static BNNS.calculateBatchStride(size:stride:)(v26, v27, v28, v29, v30, v31, v32, v33, v94, v95, v96, v97, v98, v99, v100, v101);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v94);
  outlined init with take of BNNS.Shape((uint64_t)&v94, (uint64_t)v111);
  outlined init with take of BNNS.Shape((uint64_t)v111, (uint64_t)v93);
  BNNS.Shape.size.getter((uint64_t)&v85);
  unint64_t v34 = v85;
  unint64_t v35 = v86;
  unint64_t v36 = v87;
  unint64_t v37 = v88;
  unint64_t v38 = v89;
  unint64_t v39 = v90;
  unint64_t v40 = v91;
  unint64_t v41 = v92;
  outlined init with take of BNNS.Shape((uint64_t)v111, (uint64_t)v93);
  BNNS.Shape.stride.getter((uint64_t)&v85);
  size_t v61 = specialized static BNNS.calculateBatchStride(size:stride:)(v34, v35, v36, v37, v38, v39, v40, v41, v85, v86, v87, v88, v89, v90, v91, v92);
  outlined init with take of UnsafeMutableRawPointer?(a9 + 136, (uint64_t)v123);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v123, (uint64_t)&v128);
  out = v128;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v84);
  outlined init with take of BNNS.Shape((uint64_t)v84, (uint64_t)&v85);
  outlined init with take of BNNS.Shape((uint64_t)&v85, (uint64_t)v93);
  BNNS.Shape.size.getter((uint64_t)&v76);
  unint64_t v42 = v76;
  unint64_t v43 = v77;
  unint64_t v44 = v78;
  unint64_t v45 = v79;
  unint64_t v46 = v80;
  unint64_t v47 = v81;
  unint64_t v49 = v82;
  unint64_t v48 = v83;
  outlined init with take of BNNS.Shape((uint64_t)&v85, (uint64_t)v93);
  BNNS.Shape.stride.getter((uint64_t)&v76);
  size_t v50 = specialized static BNNS.calculateBatchStride(size:stride:)(v42, v43, v44, v45, v46, v47, v49, v48, v76, v77, v78, v79, v80, v81, v82, v83);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v76);
  outlined init with take of BNNS.Shape((uint64_t)&v76, (uint64_t)v93);
  outlined init with take of BNNS.Shape((uint64_t)v93, (uint64_t)v75);
  BNNS.Shape.size.getter((uint64_t)&v70);
  long long v51 = v70;
  long long v52 = v71;
  long long v53 = v72;
  unint64_t v55 = v73;
  unint64_t v54 = v74;
  outlined init with take of BNNS.Shape((uint64_t)v93, (uint64_t)v75);
  BNNS.Shape.stride.getter((uint64_t)&v70);
  size_t v56 = specialized static BNNS.calculateBatchStride(size:stride:)(v51, *((unint64_t *)&v51 + 1), v52, *((unint64_t *)&v52 + 1), v53, *((unint64_t *)&v53 + 1), v55, v54, v70, *((unint64_t *)&v70 + 1), v71, *((unint64_t *)&v71 + 1), v72, *((unint64_t *)&v72 + 1), v73, v74);
  return BNNSFilterApplyBackwardTwoInputBatch(v66, a3, v65, v64, a5, v63, inB, v60, a1, v61, out, v50, out_delta, v56, 0, 0);
}

uint64_t closure #1 in closure #1 in closure #2 in BNNS.BroadcastMatrixMultiplyLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(BNNSNDArrayDescriptor *a1, uint64_t a2, size_t a3, const void *a4, size_t a5, BNNSNDArrayDescriptor *a6, size_t a7, uint64_t a8, const BNNSNDArrayDescriptor *out_delta)
{
  unint64_t v26 = *(void **)(a2 + 16);
  outlined init with take of UnsafeMutableRawPointer?(a8 + 136, (uint64_t)v50);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v50, (uint64_t)&v51);
  unint64_t v25 = v51;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v47);
  outlined init with take of BNNS.Shape((uint64_t)v47, (uint64_t)v48);
  outlined init with take of BNNS.Shape((uint64_t)v48, (uint64_t)v49);
  BNNS.Shape.size.getter((uint64_t)&v39);
  unint64_t v9 = v39;
  unint64_t v10 = v40;
  unint64_t v11 = v41;
  unint64_t v12 = v42;
  unint64_t v13 = v43;
  unint64_t v14 = v44;
  unint64_t v15 = v45;
  unint64_t v16 = v46;
  outlined init with take of BNNS.Shape((uint64_t)v48, (uint64_t)v49);
  BNNS.Shape.stride.getter((uint64_t)&v39);
  size_t v17 = specialized static BNNS.calculateBatchStride(size:stride:)(v9, v10, v11, v12, v13, v14, v15, v16, v39, v40, v41, v42, v43, v44, v45, v46);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v39);
  outlined init with take of BNNS.Shape((uint64_t)&v39, (uint64_t)v49);
  outlined init with take of BNNS.Shape((uint64_t)v49, (uint64_t)v38);
  BNNS.Shape.size.getter((uint64_t)&v33);
  long long v18 = v33;
  long long v19 = v34;
  long long v20 = v35;
  unint64_t v21 = v36;
  unint64_t v22 = v37;
  outlined init with take of BNNS.Shape((uint64_t)v49, (uint64_t)v38);
  BNNS.Shape.stride.getter((uint64_t)&v33);
  size_t v23 = specialized static BNNS.calculateBatchStride(size:stride:)(v18, *((unint64_t *)&v18 + 1), v19, *((unint64_t *)&v19 + 1), v20, *((unint64_t *)&v20 + 1), v21, v22, v33, *((unint64_t *)&v33 + 1), v34, *((unint64_t *)&v34 + 1), v35, *((unint64_t *)&v35 + 1), v36, v37);
  return BNNSFilterApplyBackwardBatch(v26, a3, a4, a5, a6, a7, v25, v17, out_delta, v23, a1, 0);
}

void BNNS.BroadcastMatrixMultiplyLayer.__allocating_init(bnnsFilter:)()
{
}

uint64_t BNNS.BroadcastMatrixMultiplyLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.BroadcastMatrixMultiplyLayer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

uint64_t method lookup function for BNNS.BroadcastMatrixMultiplyLayer(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.BroadcastMatrixMultiplyLayer);
}

uint64_t dispatch thunk of BNNS.BroadcastMatrixMultiplyLayer.apply(batchSize:inputA:inputB:output:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4)
{
  uint64_t v5 = a2[17];
  int v6 = *((_DWORD *)a2 + 36);
  uint64_t v7 = a2[19];
  int v8 = *((_DWORD *)a2 + 40);
  uint64_t v9 = a3[17];
  int v10 = *((_DWORD *)a3 + 36);
  uint64_t v11 = a3[19];
  int v12 = *((_DWORD *)a3 + 40);
  uint64_t v13 = a4[17];
  int v14 = *((_DWORD *)a4 + 36);
  uint64_t v15 = a4[19];
  int v16 = *((_DWORD *)a4 + 40);
  size_t v17 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *))(*(void *)v4 + 120);
  uint64_t v52 = *a2;
  long long v53 = *(_OWORD *)(a2 + 1);
  long long v54 = *(_OWORD *)(a2 + 3);
  long long v55 = *(_OWORD *)(a2 + 5);
  long long v56 = *(_OWORD *)(a2 + 7);
  long long v57 = *(_OWORD *)(a2 + 9);
  long long v58 = *(_OWORD *)(a2 + 11);
  long long v59 = *(_OWORD *)(a2 + 13);
  long long v60 = *(_OWORD *)(a2 + 15);
  uint64_t v61 = v5;
  int v62 = v6;
  uint64_t v63 = v7;
  int v64 = v8;
  uint64_t v65 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v38 = *a3;
  long long v39 = *(_OWORD *)(a3 + 1);
  long long v40 = *(_OWORD *)(a3 + 3);
  long long v41 = *(_OWORD *)(a3 + 5);
  long long v42 = *(_OWORD *)(a3 + 7);
  long long v43 = *(_OWORD *)(a3 + 9);
  long long v44 = *(_OWORD *)(a3 + 11);
  long long v45 = *(_OWORD *)(a3 + 13);
  long long v46 = *(_OWORD *)(a3 + 15);
  uint64_t v47 = v9;
  int v48 = v10;
  uint64_t v49 = v11;
  int v50 = v12;
  uint64_t v51 = *(uint64_t *)((char *)a3 + 164);
  uint64_t v24 = *a4;
  long long v25 = *(_OWORD *)(a4 + 1);
  long long v26 = *(_OWORD *)(a4 + 3);
  long long v18 = *(_OWORD *)(a4 + 7);
  long long v19 = *(_OWORD *)(a4 + 9);
  long long v20 = *(_OWORD *)(a4 + 11);
  long long v21 = *(_OWORD *)(a4 + 13);
  long long v22 = *(_OWORD *)(a4 + 15);
  long long v27 = *(_OWORD *)(a4 + 5);
  long long v28 = v18;
  long long v29 = v19;
  long long v30 = v20;
  long long v31 = v21;
  long long v32 = v22;
  uint64_t v33 = v13;
  int v34 = v14;
  uint64_t v35 = v15;
  int v36 = v16;
  uint64_t v37 = *(uint64_t *)((char *)a4 + 164);
  return v17(a1, &v52, &v38, &v24);
}

uint64_t dispatch thunk of BNNS.BroadcastMatrixMultiplyLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t a4, uint64_t a5, uint64_t *a6, uint64_t *a7)
{
  uint64_t v76 = a2[17];
  int v75 = *((_DWORD *)a2 + 36);
  uint64_t v74 = a2[19];
  int v73 = *((_DWORD *)a2 + 40);
  uint64_t v72 = a3[17];
  int v71 = *((_DWORD *)a3 + 36);
  uint64_t v8 = a3[19];
  int v9 = *((_DWORD *)a3 + 40);
  uint64_t v10 = *(void *)(a4 + 136);
  int v11 = *(_DWORD *)(a4 + 144);
  uint64_t v12 = *(void *)(a4 + 152);
  int v13 = *(_DWORD *)(a4 + 160);
  uint64_t v14 = *(void *)(a5 + 136);
  int v15 = *(_DWORD *)(a5 + 144);
  uint64_t v16 = *(void *)(a5 + 152);
  int v17 = *(_DWORD *)(a5 + 160);
  uint64_t v18 = a6[17];
  int v19 = *((_DWORD *)a6 + 36);
  uint64_t v20 = a6[19];
  int v21 = *((_DWORD *)a6 + 40);
  uint64_t v22 = a7[17];
  int v23 = *((_DWORD *)a7 + 36);
  uint64_t v24 = a7[19];
  int v25 = *((_DWORD *)a7 + 40);
  long long v26 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(void *)v7 + 128);
  uint64_t v27 = *a2;
  long long v28 = *(_OWORD *)(a2 + 1);
  long long v29 = *(_OWORD *)(a2 + 3);
  long long v30 = *(_OWORD *)(a2 + 5);
  long long v31 = *(_OWORD *)(a2 + 7);
  long long v32 = *(_OWORD *)(a2 + 9);
  long long v33 = *(_OWORD *)(a2 + 13);
  long long v34 = *(_OWORD *)(a2 + 15);
  uint64_t v35 = *(uint64_t *)((char *)a2 + 164);
  long long v36 = *(_OWORD *)(a3 + 1);
  long long v37 = *(_OWORD *)(a3 + 3);
  long long v38 = *(_OWORD *)(a3 + 5);
  long long v39 = *(_OWORD *)(a3 + 7);
  long long v40 = *(_OWORD *)(a3 + 9);
  long long v41 = *(_OWORD *)(a3 + 11);
  long long v42 = *(_OWORD *)(a3 + 13);
  uint64_t v43 = *a3;
  long long v44 = *(_OWORD *)(a3 + 15);
  uint64_t v45 = *(uint64_t *)((char *)a3 + 164);
  long long v154 = *(_OWORD *)(a2 + 11);
  long long v155 = v33;
  long long v156 = v34;
  uint64_t v161 = v35;
  long long v149 = v28;
  long long v150 = v29;
  long long v151 = v30;
  long long v152 = v31;
  long long v153 = v32;
  long long v140 = v41;
  long long v141 = v42;
  long long v142 = v44;
  uint64_t v147 = v45;
  long long v46 = *(_OWORD *)(a4 + 8);
  long long v47 = *(_OWORD *)(a4 + 24);
  long long v48 = *(_OWORD *)(a4 + 40);
  long long v49 = *(_OWORD *)(a4 + 56);
  long long v50 = *(_OWORD *)(a4 + 72);
  long long v51 = *(_OWORD *)(a4 + 88);
  long long v52 = *(_OWORD *)(a4 + 104);
  *(void *)&long long v34 = *(void *)a4;
  long long v53 = *(_OWORD *)(a4 + 120);
  *(void *)&long long v41 = *(void *)(a4 + 164);
  long long v135 = v36;
  long long v136 = v37;
  long long v137 = v38;
  long long v138 = v39;
  long long v139 = v40;
  long long v126 = v51;
  long long v127 = v52;
  long long v128 = v53;
  uint64_t v133 = v41;
  long long v54 = *(_OWORD *)(a5 + 88);
  long long v121 = v46;
  long long v55 = *(_OWORD *)(a5 + 104);
  long long v122 = v47;
  long long v56 = *(_OWORD *)(a5 + 120);
  long long v123 = v48;
  *(void *)&long long v48 = *(void *)(a5 + 164);
  long long v124 = v49;
  long long v125 = v50;
  long long v112 = v54;
  long long v113 = v55;
  long long v114 = v56;
  uint64_t v119 = v48;
  long long v57 = *(_OWORD *)(a5 + 24);
  long long v58 = *(_OWORD *)(a5 + 40);
  long long v59 = *(_OWORD *)(a5 + 56);
  long long v60 = *(_OWORD *)(a5 + 72);
  *(void *)&long long v54 = *(void *)a5;
  long long v107 = *(_OWORD *)(a5 + 8);
  long long v108 = v57;
  long long v109 = v58;
  long long v110 = v59;
  long long v111 = v60;
  long long v98 = *(_OWORD *)(a6 + 11);
  long long v99 = *(_OWORD *)(a6 + 13);
  long long v100 = *(_OWORD *)(a6 + 15);
  uint64_t v105 = *(uint64_t *)((char *)a6 + 164);
  uint64_t v148 = v27;
  long long v61 = *(_OWORD *)(a6 + 1);
  long long v62 = *(_OWORD *)(a6 + 3);
  uint64_t v134 = v43;
  long long v63 = *(_OWORD *)(a6 + 5);
  uint64_t v120 = v34;
  long long v64 = *(_OWORD *)(a6 + 7);
  uint64_t v106 = v54;
  uint64_t v92 = *a6;
  long long v65 = *(_OWORD *)(a6 + 9);
  long long v93 = v61;
  long long v94 = v62;
  long long v95 = v63;
  long long v96 = v64;
  long long v97 = v65;
  uint64_t v78 = *a7;
  long long v66 = *(_OWORD *)(a7 + 3);
  long long v79 = *(_OWORD *)(a7 + 1);
  long long v80 = v66;
  long long v67 = *(_OWORD *)(a7 + 7);
  long long v81 = *(_OWORD *)(a7 + 5);
  long long v82 = v67;
  long long v68 = *(_OWORD *)(a7 + 11);
  long long v83 = *(_OWORD *)(a7 + 9);
  long long v84 = v68;
  long long v69 = *(_OWORD *)(a7 + 15);
  long long v85 = *(_OWORD *)(a7 + 13);
  long long v86 = v69;
  uint64_t v91 = *(uint64_t *)((char *)a7 + 164);
  uint64_t v157 = v76;
  int v158 = v75;
  uint64_t v159 = v74;
  int v160 = v73;
  uint64_t v143 = v72;
  int v144 = v71;
  uint64_t v145 = v8;
  int v146 = v9;
  uint64_t v129 = v10;
  int v130 = v11;
  uint64_t v131 = v12;
  int v132 = v13;
  uint64_t v115 = v14;
  int v116 = v15;
  uint64_t v117 = v16;
  int v118 = v17;
  uint64_t v101 = v18;
  int v102 = v19;
  uint64_t v103 = v20;
  int v104 = v21;
  uint64_t v87 = v22;
  int v88 = v23;
  uint64_t v89 = v24;
  int v90 = v25;
  return v26(a1, &v148, &v134, &v120, &v106, &v92, &v78);
}

uint64_t static vDSP.float16ToFloat<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.float16ToFloat<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vDSP.float16ToFloat<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.float16ToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

uint64_t closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4)
{
  if (!a2) {
    goto LABEL_10;
  }
  if (a4 + 0x4000000000000000 < 0)
  {
    __break(1u);
    goto LABEL_8;
  }
  int v6 = (uint64_t *)result;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t result = swift_initStackObject();
  *(_OWORD *)(result + 16) = xmmword_1D2135280;
  if (a4 < 0)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  *(void *)(result + 32) = a2;
  *(void *)(result + 40) = 1;
  *(void *)(result + 48) = a4;
  *(void *)(result + 56) = 2 * a4;
  *(void *)(result + 64) = 0;
  v9[10] = result;
  uint64_t v7 = *v6;
  if (v7)
  {
    if ((unint64_t)(a4 - 0x2000000000000000) >> 62 == 3)
    {
      uint64_t inited = swift_initStackObject();
      *(_OWORD *)(inited + 16) = xmmword_1D2135280;
      *(void *)(inited + 32) = v7;
      *(void *)(inited + 40) = 1;
      *(void *)(inited + 48) = a4;
      *(void *)(inited + 56) = 4 * a4;
      *(void *)(inited + 64) = 0;
      v9[0] = inited;
      vImage.PixelBuffer<>.convert(to:)((uint64_t)v9);
      swift_bridgeObjectRelease();
      return swift_bridgeObjectRelease();
    }
    goto LABEL_9;
  }
LABEL_11:
  __break(1u);
  return result;
}

{
  uint64_t *v6;
  uint64_t v7;
  uint64_t inited;
  void v9[12];

  if (!a2) {
    goto LABEL_8;
  }
  if ((unint64_t)(a4 - 0x2000000000000000) >> 62 != 3)
  {
    __break(1u);
    goto LABEL_7;
  }
  int v6 = (uint64_t *)result;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t result = swift_initStackObject();
  *(_OWORD *)(result + 16) = xmmword_1D2135280;
  if (a4 < 0)
  {
LABEL_7:
    __break(1u);
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  *(void *)(result + 32) = a2;
  *(void *)(result + 40) = 1;
  *(void *)(result + 48) = a4;
  *(void *)(result + 56) = 4 * a4;
  *(void *)(result + 64) = 0;
  void v9[11] = result;
  uint64_t v7 = *v6;
  if (*v6)
  {
    uint64_t inited = swift_initStackObject();
    *(_OWORD *)(inited + 16) = xmmword_1D2135280;
    *(void *)(inited + 32) = v7;
    *(void *)(inited + 40) = 1;
    *(void *)(inited + 48) = a4;
    *(void *)(inited + 56) = 2 * a4;
    *(void *)(inited + 64) = 0;
    v9[0] = inited;
    vImage.PixelBuffer<>.convert(to:)((uint64_t)v9);
    swift_bridgeObjectRelease();
    return swift_bridgeObjectRelease();
  }
LABEL_9:
  __break(1u);
  return result;
}

uint64_t static vDSP.convertElements<A, B>(of:to:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return static vDSP.convertElements<A, B>(of:to:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

uint64_t closure #1 in static vDSP.float16ToFloat<A>(_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t *a6, unint64_t *a7, void (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(a6);
  uint64_t v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a7, a6);
  a8(a3, a1, a4, v16, a5, v17);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.floatToFloat16<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.floatToFloat16<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.convertElements<A, B>(of:to:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v29 = a7;
  uint64_t v30 = a2;
  uint64_t v11 = *(void *)(a3 - 8);
  MEMORY[0x1F4188790](a1);
  int v13 = (char *)&v26 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v14 = *(void (**)(char *))(v11 + 16);
  uint64_t v26 = v15;
  v14(v13);
  uint64_t v16 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  uint64_t v17 = v16(a3, a5);
  uint64_t v27 = a4;
  uint64_t v28 = a6;
  uint64_t v18 = *(void *)(a6 + 8);
  uint64_t v19 = v30;
  uint64_t v20 = (*(uint64_t (**)(uint64_t))(v18 + 16))(a4);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v11 + 8))(v13, a3);
  if (v17 == v20)
  {
    uint64_t v22 = v16(a3, a5);
    uint64_t v23 = MEMORY[0x1F4188790](v22);
    uint64_t v24 = v27;
    *(&v26 - 6) = a3;
    *(&v26 - 5) = v24;
    uint64_t v25 = v28;
    *(&v26 - 4) = a5;
    *(&v26 - 3) = v25;
    *(&v26 - 2) = v19;
    *(&v26 - 1) = v23;
    return (*(uint64_t (**)(uint64_t))(a5 + 24))(v29);
  }
  else
  {
    __break(1u);
  }
  return result;
}

{
  uint64_t v12;
  uint64_t v13;
  char *v14;
  void (*v15)(char *);
  uint64_t v16;
  uint64_t (*v17)(uint64_t, uint64_t);
  uint64_t v18;
  uint64_t (*v19)(uint64_t);
  uint64_t v20;
  uint64_t result;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;

  uint64_t v30 = a7;
  uint64_t v12 = *(void *)(a3 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v14 = (char *)&v27 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v15 = *(void (**)(char *))(v12 + 16);
  long long v32 = v16;
  v15(v14);
  uint64_t v17 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  long long v33 = a5;
  uint64_t v18 = v17(a3, a5);
  uint64_t v28 = a6;
  uint64_t v29 = a2;
  uint64_t v19 = *(uint64_t (**)(uint64_t))(*(void *)(a6 + 8) + 16);
  long long v31 = a4;
  uint64_t v20 = v19(a4);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v12 + 8))(v14, a3);
  if (v18 == v20)
  {
    uint64_t v23 = v32;
    uint64_t v22 = v33;
    uint64_t result = v17(a3, v33);
    if ((result & 0x8000000000000000) == 0)
    {
      uint64_t v24 = MEMORY[0x1F4188790](result);
      uint64_t v25 = v31;
      *(&v27 - 6) = a3;
      *(&v27 - 5) = v25;
      uint64_t v26 = v28;
      *(&v27 - 4) = v22;
      *(&v27 - 3) = v26;
      *(&v27 - 2) = v23;
      *(&v27 - 1) = v24;
      return (*(uint64_t (**)(uint64_t))(v26 + 16))(v30);
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

{
  uint64_t v10;
  uint64_t result;
  uint64_t v12;

  uint64_t v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a6 + 8) + 16))(a4);
  if (result >= v10) {
    uint64_t v12 = v10;
  }
  else {
    uint64_t v12 = result;
  }
  if (v12 < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(uint64_t))(a6 + 16))(a7);
  }
  return result;
}

BOOL static vDSP.RoundingMode.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void vDSP.RoundingMode.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int vDSP.RoundingMode.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t static vDSP.convertElements<A, B>(of:to:rounding:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, a3, a4, a5, a6, a7, (uint64_t)partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

uint64_t static vDSP.convertElements<A, B>(of:to:rounding:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v11 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a7 + 8) + 16))(a5);
  if (result >= v11) {
    uint64_t v13 = v11;
  }
  else {
    uint64_t v13 = result;
  }
  if (v13 < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(uint64_t))(a7 + 16))(a8);
  }
  return result;
}

uint64_t closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(uint64_t result, uint64_t a2, char a3, void *a4, uint64_t a5, uint64_t (*a6)(void), uint64_t (*a7)(void))
{
  if ((a3 & 1) == 0)
  {
    if (result)
    {
      if (*a4) {
        return a6();
      }
    }
    else
    {
      __break(1u);
    }
    __break(1u);
    goto LABEL_10;
  }
  if (!result)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (*a4) {
    return a7();
  }
LABEL_11:
  __break(1u);
  return result;
}

uint64_t static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:), (uint64_t (*)(void *, uint64_t *))partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:));
}

{
  return static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:), (uint64_t (*)(void *, uint64_t *))partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:));
}

{
  return static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:), (uint64_t (*)(void *, uint64_t *))partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:));
}

{
  return static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:), (uint64_t (*)(void *, uint64_t *))partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:));
}

{
  return static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:), (uint64_t (*)(void *, uint64_t *))partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:));
}

{
  return static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:), (uint64_t (*)(void *, uint64_t *))partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:));
}

uint64_t closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t *a8, unint64_t *a9, void (*a10)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(a8);
  uint64_t v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a9, a8);
  a10(a3, a1, a4, v16, a6, v17);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  *a2 = result;
  return result;
}

uint64_t static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(void *, uint64_t *), uint64_t (*a8)(void *, uint64_t *))
{
  if (swift_dynamicCastMetatype())
  {
    uint64_t v12 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
    uint64_t v13 = MEMORY[0x1F4188790](v12);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v13, a7);
LABEL_5:
    uint64_t v16 = _arrayForceCast<A, B>(_:)();
    swift_bridgeObjectRelease();
    return v16;
  }
  if (swift_dynamicCastMetatype())
  {
    uint64_t v14 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
    uint64_t v15 = MEMORY[0x1F4188790](v14);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v15, a8);
    goto LABEL_5;
  }
  _StringGuts.grow(_:)(39);
  _typeName(_:qualified:)();
  swift_bridgeObjectRelease();
  v18._object = (void *)0x80000001D213C2A0;
  v18._countAndFlagsBits = 0xD000000000000025;
  String.append(_:)(v18);
  uint64_t result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  if (swift_dynamicCastMetatype())
  {
    uint64_t v8 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    uint64_t v9 = MEMORY[0x1F4188790](v8);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v9, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
LABEL_13:
    uint64_t v20 = _arrayForceCast<A, B>(_:)();
    swift_bridgeObjectRelease();
    return v20;
  }
  if (swift_dynamicCastMetatype())
  {
    uint64_t v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    uint64_t v11 = MEMORY[0x1F4188790](v10);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v11, (uint64_t (*)(void *, uint64_t *))partial apply for closure #2 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  if (swift_dynamicCastMetatype())
  {
    uint64_t v12 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    uint64_t v13 = MEMORY[0x1F4188790](v12);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v13, (uint64_t (*)(void *, uint64_t *))partial apply for closure #3 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  if (swift_dynamicCastMetatype())
  {
    uint64_t v14 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    uint64_t v15 = MEMORY[0x1F4188790](v14);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v15, (uint64_t (*)(void *, uint64_t *))partial apply for closure #4 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  if (swift_dynamicCastMetatype())
  {
    uint64_t v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    uint64_t v17 = MEMORY[0x1F4188790](v16);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v17, (uint64_t (*)(void *, uint64_t *))partial apply for closure #5 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  if (swift_dynamicCastMetatype())
  {
    uint64_t v18 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    uint64_t v19 = MEMORY[0x1F4188790](v18);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v19, (uint64_t (*)(void *, uint64_t *))partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  _StringGuts.grow(_:)(39);
  _typeName(_:qualified:)();
  swift_bridgeObjectRelease();
  v22._object = (void *)0x80000001D213C2A0;
  v22._countAndFlagsBits = 0xD000000000000025;
  String.append(_:)(v22);
  uint64_t result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

{
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t result;
  Swift::String v22;

  if (swift_dynamicCastMetatype())
  {
    uint64_t v8 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    uint64_t v9 = MEMORY[0x1F4188790](v8);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v9, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
LABEL_13:
    uint64_t v20 = _arrayForceCast<A, B>(_:)();
    swift_bridgeObjectRelease();
    return v20;
  }
  if (swift_dynamicCastMetatype())
  {
    uint64_t v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    uint64_t v11 = MEMORY[0x1F4188790](v10);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v11, (uint64_t (*)(void *, uint64_t *))partial apply for closure #2 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  if (swift_dynamicCastMetatype())
  {
    uint64_t v12 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    uint64_t v13 = MEMORY[0x1F4188790](v12);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v13, (uint64_t (*)(void *, uint64_t *))partial apply for closure #3 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  if (swift_dynamicCastMetatype())
  {
    uint64_t v14 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    uint64_t v15 = MEMORY[0x1F4188790](v14);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v15, (uint64_t (*)(void *, uint64_t *))partial apply for closure #4 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  if (swift_dynamicCastMetatype())
  {
    uint64_t v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    uint64_t v17 = MEMORY[0x1F4188790](v16);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v17, (uint64_t (*)(void *, uint64_t *))partial apply for closure #5 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  if (swift_dynamicCastMetatype())
  {
    uint64_t v18 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
    uint64_t v19 = MEMORY[0x1F4188790](v18);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v19, (uint64_t (*)(void *, uint64_t *))partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:));
    goto LABEL_13;
  }
  _StringGuts.grow(_:)(39);
  _typeName(_:qualified:)();
  swift_bridgeObjectRelease();
  v22._object = (void *)0x80000001D213C2A0;
  v22._countAndFlagsBits = 0xD000000000000025;
  String.append(_:)(v22);
  uint64_t result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t closure #1 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, unint64_t *a10, void (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(a9);
  uint64_t v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a10, a9);
  a11(a3, a1, a4, a5, v16, a7, v17);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  *a2 = result;
  return result;
}

uint64_t static vDSP.floatToDouble<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.floatToDouble<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.doubleToFloat<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.float16ToFloat<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.doubleToFloat<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.float16ToFloat<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(uint64_t, uint64_t, void *))
{
  uint64_t v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  _OWORD v12[2] = a2;
  v12[3] = a3;
  v12[4] = a1;
  return a5(v10, a4, v12);
}

uint64_t closure #1 in static vDSP.floatToDouble<A>(_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t *a6, unint64_t *a7, void (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(a6);
  uint64_t v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a7, a6);
  a8(a3, a1, a4, v16, a5, v17);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  void v6[3];

  uint64_t v3 = *(void *)(v2 + 16);
  uint64_t v4 = *(void *)(v2 + 32);
  v6[2] = a1;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v4 + 24))(a2, v6, MEMORY[0x1E4FBC848] + 8, v3);
}

uint64_t partial apply for closure #1 in static vDSP.floatToFloat16<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.float16ToFloat<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float16>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float16> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

uint64_t partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = v3[3];
  uint64_t v5 = v3[5];
  uint64_t v6 = v3[7];
  void v8[2] = a1;
  void v8[3] = a2;
  void v8[4] = v6;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v5 + 16))(a3, v8, MEMORY[0x1E4FBC848] + 8, v4);
}

uint64_t partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:));
}

uint64_t partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:));
}

uint64_t partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(v2 + 16);
  uint64_t v4 = *(void *)(v2 + 32);
  uint64_t v5 = *(void *)(v2 + 64);
  v7[16] = *(unsigned char *)(v2 + 56);
  uint64_t v8 = a1;
  uint64_t v9 = v5;
  return (*(uint64_t (**)(uint64_t, unsigned char *, uint64_t, uint64_t))(v4 + 24))(a2, v7, MEMORY[0x1E4FBC848] + 8, v3);
}

uint64_t partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

uint64_t partial apply for closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

{
  return partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

uint64_t partial apply for closure #2 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  return closure #1 in static vDSP.integerToFloatingPoint<A, B>(_:floatingPointType:)(a1, a2, v5[6], v5[2], v5[3], v5[4], v5[5], a3, a4, a5);
}

uint64_t partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Int32>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Int32> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Int32>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Int32> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

uint64_t partial apply for closure #5 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Int16>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Int16> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Int16>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Int16> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

uint64_t partial apply for closure #4 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Int8>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Int8> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Int8>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Int8> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

uint64_t partial apply for closure #3 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<UInt32>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<UInt32> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<UInt32>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<UInt32> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

uint64_t partial apply for closure #2 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<UInt16>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<UInt16> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<UInt16>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<UInt16> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

uint64_t partial apply for closure #1 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<UInt8>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<UInt8> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

{
  return partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<UInt8>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<UInt8> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:rounding:));
}

uint64_t partial apply for closure #6 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  return closure #1 in static vDSP.floatingPointToInteger<A, B>(_:integerType:rounding:)(a1, a2, v5[6], v5[7], v5[2], v5[3], v5[4], v5[5], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.floatToDouble<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.floatToDouble<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

uint64_t partial apply for closure #1 in static vDSP.doubleToFloat<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.floatToDouble<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convertElements<A, B>(of:to:));
}

unint64_t lazy protocol witness table accessor for type vDSP.RoundingMode and conformance vDSP.RoundingMode()
{
  unint64_t result = lazy protocol witness table cache variable for type vDSP.RoundingMode and conformance vDSP.RoundingMode;
  if (!lazy protocol witness table cache variable for type vDSP.RoundingMode and conformance vDSP.RoundingMode)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.RoundingMode and conformance vDSP.RoundingMode);
  }
  return result;
}

unsigned char *storeEnumTagSinglePayload for vDSP.RoundingMode(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 1 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFF) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFE)
  {
    unsigned int v6 = ((a2 - 255) >> 8) + 1;
    *unint64_t result = a2 + 1;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D20A95CCLL);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 1;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vDSP.RoundingMode()
{
  return &type metadata for vDSP.RoundingMode;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E4F16B50]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E4F16E30]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E4F16C30]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E4F16C28]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E4F16C20]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E4F16C18]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E4F16C40]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E4F16C38]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E4F16C60]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E4F16C58]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E4F16C50]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E4F16C48]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E4F16C70]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, a2, MEMORY[0x1E4F16C68]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(unsigned char *)(v2 + 16), *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16C10], MEMORY[0x1E4F16BE0]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(unsigned char *)(v2 + 16), *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16C08], MEMORY[0x1E4F16BD8]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(unsigned char *)(v2 + 16), *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16B80], MEMORY[0x1E4F16BB0]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(unsigned char *)(v2 + 16), *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16B78], MEMORY[0x1E4F16BA8]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(unsigned char *)(v2 + 16), *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16B60], MEMORY[0x1E4F16B90]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(unsigned char *)(v2 + 16), *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16B58], MEMORY[0x1E4F16B88]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(unsigned char *)(v2 + 16), *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16C00], MEMORY[0x1E4F16BD0]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(unsigned char *)(v2 + 16), *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16BF8], MEMORY[0x1E4F16BC8]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(unsigned char *)(v2 + 16), *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16BF0], MEMORY[0x1E4F16BC0]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(unsigned char *)(v2 + 16), *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16BE8], MEMORY[0x1E4F16BB8]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(unsigned char *)(v2 + 16), *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16B70], MEMORY[0x1E4F16BA0]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:rounding:)(a1, a2, *(unsigned char *)(v2 + 16), *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16B68], MEMORY[0x1E4F16B98]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(uint64_t result, uint64_t a2, uint64_t (*a3)(void))
{
  if (result)
  {
    if (**(void **)(v3 + 16)) {
      return a3();
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(uint64_t a1)
{
  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, v1[2], v1[3], v1[4]);
}

{
  uint64_t *v1;

  return closure #1 in closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, v1[2], v1[3], v1[4]);
}

uint64_t static BNNS.applyTopK(k:input:bestValues:bestIndices:axis:batchSize:filterParameters:)(uint64_t a1, _OWORD *a2, _OWORD *a3, _OWORD *a4, uint64_t a5, uint64_t a6, int a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return static BNNS.applyTopK(k:input:bestValues:bestIndices:axis:batchSize:filterParameters:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, MEMORY[0x1E4F16788]);
}

uint64_t static BNNS.applyInTopK(k:input:testIndices:output:axis:batchSize:filterParameters:)(uint64_t a1, _OWORD *a2, _OWORD *a3, _OWORD *a4, uint64_t a5, uint64_t a6, int a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return static BNNS.applyTopK(k:input:bestValues:bestIndices:axis:batchSize:filterParameters:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, MEMORY[0x1E4F16780]);
}

uint64_t static BNNS.applyTopK(k:input:bestValues:bestIndices:axis:batchSize:filterParameters:)(uint64_t a1, _OWORD *a2, _OWORD *a3, _OWORD *a4, uint64_t a5, uint64_t a6, int a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, unint64_t, uint64_t, unint64_t, uint64_t, unint64_t, uint64_t))
{
  uint64_t v35 = *MEMORY[0x1E4F143B8];
  long long v11 = a2[9];
  v34[8] = a2[8];
  v34[9] = v11;
  v34[10] = a2[10];
  long long v12 = a2[5];
  v34[4] = a2[4];
  v34[5] = v12;
  long long v13 = a2[7];
  v34[6] = a2[6];
  v34[7] = v13;
  long long v14 = a2[1];
  v34[0] = *a2;
  v34[1] = v14;
  long long v15 = a2[3];
  v34[2] = a2[2];
  v34[3] = v15;
  long long v16 = a3[9];
  v33[8] = a3[8];
  v33[9] = v16;
  v33[10] = a3[10];
  long long v17 = a3[5];
  v33[4] = a3[4];
  v33[5] = v17;
  long long v18 = a3[7];
  v33[6] = a3[6];
  v33[7] = v18;
  long long v19 = a3[1];
  v33[0] = *a3;
  v33[1] = v19;
  long long v20 = a3[3];
  v33[2] = a3[2];
  v33[3] = v20;
  long long v21 = a4[9];
  v32[8] = a4[8];
  v32[9] = v21;
  v32[10] = a4[10];
  long long v22 = a4[5];
  v32[4] = a4[4];
  v32[5] = v22;
  long long v23 = a4[7];
  v32[6] = a4[6];
  v32[7] = v23;
  long long v24 = a4[1];
  v32[0] = *a4;
  v32[1] = v24;
  long long v25 = a4[3];
  v32[2] = a4[2];
  v32[3] = v25;
  if (a9 == 1)
  {
    uint64_t result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.applyTopK(k:input:bestValues:bestIndices:axis:batchSize:filterParameters:)(0, a1, a5, a6, (uint64_t)v34, (uint64_t)a2, (uint64_t)v33, (uint64_t)a3, (uint64_t)v32, (uint64_t)a4, a11);
  }
  else
  {
    int v28 = a7;
    uint64_t v29 = a8;
    uint64_t v30 = a9;
    uint64_t v31 = a10;
    uint64_t result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.applyTopK(k:input:bestValues:bestIndices:axis:batchSize:filterParameters:)((uint64_t)&v28, a1, a5, a6, (uint64_t)v34, (uint64_t)a2, (uint64_t)v33, (uint64_t)a3, (uint64_t)v32, (uint64_t)a4, a11);
  }
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *uint64_t v27 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.applyTopK(k:input:bestValues:bestIndices:axis:batchSize:filterParameters:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, unint64_t, uint64_t, unint64_t, uint64_t, unint64_t, uint64_t))
{
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v57);
  outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v58);
  outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)v67);
  BNNS.Shape.size.getter((uint64_t)&v59);
  unint64_t v11 = v59;
  unint64_t v12 = v60;
  unint64_t v13 = v61;
  unint64_t v14 = v62;
  unint64_t v15 = v63;
  unint64_t v16 = v64;
  unint64_t v17 = v65;
  unint64_t v18 = v66;
  outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)v67);
  BNNS.Shape.stride.getter((uint64_t)&v59);
  unint64_t v35 = specialized static BNNS.calculateBatchStride(size:stride:)(v11, v12, v13, v14, v15, v16, v17, v18, v59, v60, v61, v62, v63, v64, v65, v66);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v56);
  outlined init with take of BNNS.Shape((uint64_t)v56, (uint64_t)&v59);
  outlined init with take of BNNS.Shape((uint64_t)&v59, (uint64_t)v67);
  BNNS.Shape.size.getter((uint64_t)&v48);
  unint64_t v19 = v48;
  unint64_t v20 = v49;
  unint64_t v21 = v50;
  unint64_t v22 = v51;
  unint64_t v23 = v52;
  unint64_t v24 = v53;
  unint64_t v25 = v54;
  unint64_t v26 = v55;
  outlined init with take of BNNS.Shape((uint64_t)&v59, (uint64_t)v67);
  BNNS.Shape.stride.getter((uint64_t)&v48);
  unint64_t v27 = specialized static BNNS.calculateBatchStride(size:stride:)(v19, v20, v21, v22, v23, v24, v25, v26, v48, v49, v50, v51, v52, v53, v54, v55);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v48);
  outlined init with take of BNNS.Shape((uint64_t)&v48, (uint64_t)v67);
  outlined init with take of BNNS.Shape((uint64_t)v67, (uint64_t)v47);
  BNNS.Shape.size.getter((uint64_t)&v42);
  long long v28 = v42;
  long long v29 = v43;
  long long v30 = v44;
  unint64_t v32 = v45;
  unint64_t v31 = v46;
  outlined init with take of BNNS.Shape((uint64_t)v67, (uint64_t)v47);
  BNNS.Shape.stride.getter((uint64_t)&v42);
  unint64_t v33 = specialized static BNNS.calculateBatchStride(size:stride:)(v28, *((unint64_t *)&v28 + 1), v29, *((unint64_t *)&v29 + 1), v30, *((unint64_t *)&v30 + 1), v32, v31, v42, *((unint64_t *)&v42 + 1), v43, *((unint64_t *)&v43 + 1), v44, *((unint64_t *)&v44 + 1), v45, v46);
  return a11(a2, a3, a4, a5, v35, a7, v27, a9, v33, a1);
}

uint64_t static vDSP.taperedMerge<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  char *v22;
  void (*v23)(char *, uint64_t, uint64_t);
  uint64_t v24;
  void (*v25)(char *, uint64_t, uint64_t);
  uint64_t (*v26)(uint64_t, uint64_t);
  uint64_t v27;
  uint64_t v28;
  void (*v29)(char *, uint64_t);
  uint64_t (*v30)(char *, uint64_t);
  uint64_t result;
  char *v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  void (*v43)(char *, uint64_t, uint64_t);
  char *v44;
  uint64_t v45;
  uint64_t v46;
  uint64_t v47;
  uint64_t v48;
  uint64_t (*v49)(uint64_t, uint64_t);
  uint64_t v50;

  unint64_t v50 = a8;
  long long v47 = a6;
  unint64_t v48 = a3;
  unint64_t v13 = *(void *)(a5 - 8);
  unint64_t v14 = MEMORY[0x1F4188790](a1);
  unint64_t v16 = (char *)&v40 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  unint64_t v18 = *(void *)(v17 - 8);
  unint64_t v19 = MEMORY[0x1F4188790](v14);
  long long v44 = (char *)&v40 - ((v20 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1F4188790](v19);
  unint64_t v22 = (char *)&v40 - v21;
  unint64_t v23 = *(void (**)(char *, uint64_t, uint64_t))(v18 + 16);
  unint64_t v45 = v24;
  long long v43 = v23;
  ((void (*)(char *))v23)((char *)&v40 - v21);
  unint64_t v25 = *(void (**)(char *, uint64_t, uint64_t))(v13 + 16);
  long long v42 = a2;
  v25(v16, a2, a5);
  unint64_t v26 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  unint64_t v46 = a7;
  unint64_t v49 = v26;
  unint64_t v27 = v26(a4, a7);
  long long v28 = (*(uint64_t (**)(uint64_t))(v50 + 16))(a5);
  long long v29 = *(void (**)(char *, uint64_t))(v13 + 8);
  long long v41 = a5;
  v29(v16, a5);
  long long v30 = *(uint64_t (**)(char *, uint64_t))(v18 + 8);
  uint64_t result = v30(v22, a4);
  if (v27 != v28)
  {
    __break(1u);
    goto LABEL_6;
  }
  unint64_t v32 = v44;
  v43(v44, v45, a4);
  unint64_t v33 = v46;
  long long v34 = v49(a4, v46);
  unint64_t v35 = (*(uint64_t (**)(uint64_t))(*(void *)(a9 + 8) + 16))(v47);
  uint64_t result = v30(v32, a4);
  if (v34 != v35)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  uint64_t result = v49(a4, v33);
  if ((result & 0x8000000000000000) == 0)
  {
    long long v36 = MEMORY[0x1F4188790](result);
    long long v37 = v41;
    *(&v40 - 10) = a4;
    *(&v40 - 9) = v37;
    *(&v40 - 8) = v47;
    *(&v40 - 7) = v33;
    *(&v40 - 6) = v50;
    *(&v40 - 5) = a9;
    long long v38 = v48;
    *(&v40 - 4) = v42;
    *(&v40 - 3) = v38;
    *(&v40 - 2) = v36;
    return (*(uint64_t (**)(uint64_t))(v33 + 24))(v39);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 40);
  uint64_t v5 = *(void *)(v3 + 72);
  void v7[2] = *(void *)(v3 + 16);
  long long v8 = *(_OWORD *)(v3 + 24);
  uint64_t v9 = v4;
  long long v10 = *(_OWORD *)(v3 + 48);
  uint64_t v11 = v5;
  uint64_t v12 = a1;
  uint64_t v13 = a2;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, void))(v10 + 24))(a3, v7, MEMORY[0x1E4FBC848] + 8, v8);
}

void *closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(void *result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, uint64_t, uint64_t, void, uint64_t, uint64_t))
{
  if (!a2)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!a4)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*result) {
    return (void *)a7(a2, 1, a4, 1, *result, 1, a6);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vDSP.taperedMerge<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.taperedMerge<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.taperedMerge<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.taperedMerge<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.taperedMerge<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vDSP.taperedMerge<A, B>(_:_:)(uint64_t a1, void *a2)
{
  return partial apply for closure #1 in static vDSP.taperedMerge<A, B>(_:_:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vDSP.taperedMerge<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.taperedMerge<A, B>(_:_:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vDSP.taperedMerge<A, B, C>(_:_:result:));
}

uint64_t static vDSP.taperedMerge<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, void *))
{
  uint64_t v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  v18[2] = a3;
  v18[3] = a4;
  v18[4] = a5;
  v18[5] = a6;
  v18[6] = a1;
  v18[7] = a2;
  v18[8] = v16;
  return a8(v16, a7, v18);
}

uint64_t static vDSP.swapElements<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.swapElements<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.swapElements<A, B>(_:_:));
}

{
  return static vDSP.swapElements<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.swapElements<A, B>(_:_:));
}

uint64_t static vDSP.swapElements<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v11 = *(void *)(a5 + 8);
  uint64_t v12 = *(uint64_t (**)(uint64_t, uint64_t))(v11 + 16);
  uint64_t v13 = v12(a3, v11);
  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a6 + 8) + 16))(a4);
  if (v13 == result)
  {
    uint64_t result = v12(a3, v11);
    if ((result & 0x8000000000000000) == 0)
    {
      MEMORY[0x1F4188790](result);
      return (*(uint64_t (**)(uint64_t))(a5 + 16))(a7);
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in static vDSP.swapElements<A, B>(_:_:)(void *a1, uint64_t *a2, uint64_t a3, uint64_t (*a4)(void))
{
  uint64_t result = *a2;
  if (*a2)
  {
    if (*a1) {
      return a4();
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t static vDSP.gather<A, B, C>(_:indices:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.gather<A, B, C>(_:indices:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vDSP.gather<A, B, C>(_:indices:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v11;
  uint64_t v12;
  char *v13;
  void (*v14)(char *);
  uint64_t v15;
  uint64_t (*v16)(uint64_t, uint64_t);
  uint64_t v17;
  uint64_t v18;
  uint64_t (*v19)(uint64_t, uint64_t);
  uint64_t v20;
  uint64_t v21;
  uint64_t result;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;

  unint64_t v31 = a7;
  long long v34 = a6;
  long long v28 = a4;
  long long v29 = a1;
  uint64_t v11 = *(void *)(a5 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v13 = (char *)&v28 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  unint64_t v14 = *(void (**)(char *))(v11 + 16);
  long long v30 = v15;
  v14(v13);
  uint64_t v16 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  unint64_t v32 = a8;
  unint64_t v17 = v16(a5, a8);
  unint64_t v33 = a9;
  unint64_t v18 = *(void *)(a9 + 8);
  unint64_t v19 = *(uint64_t (**)(uint64_t, uint64_t))(v18 + 16);
  unint64_t v20 = v34;
  unint64_t v21 = v19(v34, v18);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v11 + 8))(v13, a5);
  if (v17 == v21)
  {
    uint64_t result = v19(v20, v18);
    if ((result & 0x8000000000000000) == 0)
    {
      unint64_t v23 = MEMORY[0x1F4188790](result);
      *(&v28 - 10) = v28;
      *(&v28 - 9) = a5;
      unint64_t v24 = v31;
      *(&v28 - 8) = v20;
      *(&v28 - 7) = v24;
      unint64_t v25 = v33;
      *(&v28 - 6) = v32;
      *(&v28 - 5) = v25;
      unint64_t v26 = v30;
      *(&v28 - 4) = v29;
      *(&v28 - 3) = v26;
      *(&v28 - 2) = v23;
      return (*(uint64_t (**)(uint64_t))(v25 + 16))(v27);
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, void *a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, uint64_t, void, uint64_t))
{
  if (!a3)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!result)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*a5) {
    return a7(a3, result, 1, *a5, 1);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vDSP.gather<A, B>(_:indices:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.gather<A, B>(_:indices:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.gather<A, B>(_:indices:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.gather<A, B>(_:indices:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.gather<A, B>(_:indices:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.gather<A, B>(_:indices:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, void *))
{
  uint64_t v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  v18[2] = a3;
  v18[3] = a4;
  v18[4] = a5;
  v18[5] = a6;
  v18[6] = a1;
  v18[7] = a2;
  v18[8] = v16;
  return a8(v16, a7, v18);
}

uint64_t static vDSP.compress<A, B, C>(_:gatingVector:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  return static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, a3, a4, a5, a6, a7, a8);
}

{
  return static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, a3, a4, a5, a6, a7, a8);
}

{
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  char *v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  char *v19;
  void (*v20)(char *);
  uint64_t v21;
  void (*v22)(char *, uint64_t, uint64_t);
  uint64_t (*v23)(uint64_t, uint64_t);
  uint64_t v24;
  uint64_t v25;
  uint64_t result;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;

  long long v39 = a8;
  unint64_t v35 = a3;
  long long v36 = a6;
  uint64_t v12 = *(void *)(a5 - 8);
  uint64_t v13 = MEMORY[0x1F4188790](a1);
  unint64_t v15 = (char *)&v33 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0);
  unint64_t v17 = *(void *)(v16 - 8);
  MEMORY[0x1F4188790](v13);
  unint64_t v19 = (char *)&v33 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  unint64_t v20 = *(void (**)(char *))(v17 + 16);
  long long v37 = v21;
  v20(v19);
  unint64_t v22 = *(void (**)(char *, uint64_t, uint64_t))(v12 + 16);
  long long v34 = a2;
  v22(v15, a2, a5);
  unint64_t v23 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  long long v38 = a7;
  unint64_t v24 = v23(a4, a7);
  unint64_t v25 = (*(uint64_t (**)(uint64_t))(v39 + 16))(a5);
  (*(void (**)(char *, uint64_t))(v12 + 8))(v15, a5);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v17 + 8))(v19, a4);
  if (v24 == v25)
  {
    unint64_t v27 = v37;
    long long v28 = v38;
    uint64_t result = v23(a4, v38);
    if ((result & 0x8000000000000000) == 0)
    {
      long long v29 = MEMORY[0x1F4188790](result);
      *(&v33 - 10) = a4;
      *(&v33 - 9) = a5;
      *(&v33 - 8) = v36;
      *(&v33 - 7) = v28;
      *(&v33 - 6) = v39;
      *(&v33 - 5) = v30;
      unint64_t v31 = v34;
      *(&v33 - 4) = v27;
      *(&v33 - 3) = v31;
      *(&v33 - 2) = v29;
      return (*(uint64_t (**)(uint64_t))(v30 + 16))(v32);
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t *a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  if (!a3)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!result)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v7 = *a5;
  if (v7) {
    return a7(a3, 1, result, 1, v7, 1, a6);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(uint64_t a1, uint64_t a2, uint64_t a3, char a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  if (a4)
  {
    uint64_t v8 = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
    uint64_t v9 = MEMORY[0x1F4188790](v8);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v9, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:));
    a1 = swift_bridgeObjectRelease();
  }
  MEMORY[0x1F4188790](a1);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v10, (uint64_t (*)(void *, uint64_t *))partial apply for closure #2 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:));
}

{
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;

  if (a4)
  {
    uint64_t v8 = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
    uint64_t v9 = MEMORY[0x1F4188790](v8);
    specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v9, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:));
    a1 = swift_bridgeObjectRelease();
  }
  MEMORY[0x1F4188790](a1);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v10, (uint64_t (*)(void *, uint64_t *))partial apply for closure #2 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:));
}

void closure #1 in closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(const float *a1, uint64_t a2, const float *a3, float **a4, uint64_t a5, vDSP_Length *a6, vDSP_Length *a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  if (!a1)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v11 = *a4;
  if (*a4)
  {
    vDSP_Length v16 = (*(uint64_t (**)(uint64_t))(a11 + 16))(a9);
    if ((v16 & 0x8000000000000000) == 0)
    {
      vDSP_vclipc(a1, 1, a3, a3, v11, 1, v16, a6, a7);
      return;
    }
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
}

void closure #1 in closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(const double *a1, uint64_t a2, const double *a3, double **a4, uint64_t a5, vDSP_Length *a6, vDSP_Length *a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  if (!a1)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v11 = *a4;
  if (*a4)
  {
    vDSP_Length v16 = (*(uint64_t (**)(uint64_t))(a11 + 16))(a9);
    if ((v16 & 0x8000000000000000) == 0)
    {
      vDSP_vclipcD(a1, 1, a3, a3, v11, 1, v16, a6, a7);
      return;
    }
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
}

uint64_t partial apply for closure #1 in static vDSP.taperedMerge<A, B>(_:_:)(uint64_t a1, void *a2, uint64_t *a3, unint64_t *a4, uint64_t (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v9 = v5[2];
  uint64_t v10 = v5[3];
  uint64_t v11 = v5[5];
  uint64_t v18 = v5[4];
  uint64_t v13 = v5[6];
  uint64_t v12 = v5[7];
  uint64_t v17 = v5[8];
  uint64_t v14 = __swift_instantiateConcreteTypeFromMangledName(a3);
  uint64_t v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a4, a3);
  uint64_t result = a5(v13, v12, a1, v9, v10, v14, v18, v11, v15);
  *a2 = v17;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.swapElements<A, B>(_:_:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.swapElements<A, B>(_:_:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.swapElements<A, B>(_:_:));
}

{
  return partial apply for closure #1 in static vDSP.swapElements<A, B>(_:_:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.swapElements<A, B>(_:_:));
}

uint64_t partial apply for closure #1 in static vDSP.swapElements<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(v2 + 24);
  uint64_t v4 = *(void *)(v2 + 40);
  v6[2] = a1;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v4 + 16))(a2, v6, MEMORY[0x1E4FBC848] + 8, v3);
}

uint64_t partial apply for closure #1 in static vDSP.gather<A, B, C>(_:indices:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:));
}

uint64_t partial apply for closure #1 in static vDSP.gather<A, B>(_:indices:)(uint64_t a1, void *a2)
{
  return partial apply for closure #1 in static vDSP.taperedMerge<A, B>(_:_:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, static vDSP.gather<A, B, C>(_:indices:result:));
}

{
  return partial apply for closure #1 in static vDSP.taperedMerge<A, B>(_:_:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, static vDSP.gather<A, B, C>(_:indices:result:));
}

uint64_t partial apply for closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:));
}

uint64_t partial apply for closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:));
}

{
  return partial apply for closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:));
}

uint64_t partial apply for closure #2 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(uint64_t a1, void *a2)
{
  return partial apply for closure #2 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.compress<A, B, C>(_:gatingVector:result:));
}

{
  return partial apply for closure #2 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.compress<A, B, C>(_:gatingVector:result:));
}

uint64_t partial apply for closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 40);
  uint64_t v5 = *(void *)(v3 + 48);
  uint64_t v6 = *(void *)(v3 + 56);
  void v8[2] = *(void *)(v3 + 16);
  long long v9 = *(_OWORD *)(v3 + 24);
  uint64_t v10 = v4;
  uint64_t v11 = v6;
  uint64_t v12 = a1;
  uint64_t v13 = v5;
  long long v14 = *(_OWORD *)(v3 + 64);
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, void))(v4 + 24))(a3, v8, MEMORY[0x1E4FBC848] + 8, v9);
}

uint64_t partial apply for closure #2 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(uint64_t a1, void *a2, uint64_t *a3, unint64_t *a4, uint64_t (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v9 = v5[2];
  uint64_t v10 = v5[3];
  uint64_t v11 = v5[5];
  uint64_t v18 = v5[4];
  uint64_t v13 = v5[6];
  uint64_t v12 = v5[7];
  uint64_t v17 = v5[8];
  uint64_t v14 = __swift_instantiateConcreteTypeFromMangledName(a3);
  uint64_t v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a4, a3);
  uint64_t result = a5(v13, v12, a1, v9, v10, v14, v18, v11, v15);
  *a2 = v17;
  return result;
}

void partial apply for closure #1 in closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(const double *a1, uint64_t a2)
{
  closure #1 in closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(a1, a2, *(const double **)(v2 + 48), *(double ***)(v2 + 56), *(void *)(v2 + 64), *(vDSP_Length **)(v2 + 72), *(vDSP_Length **)(v2 + 80), *(void *)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40));
}

void partial apply for closure #1 in closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(const float *a1, uint64_t a2)
{
  closure #1 in closure #1 in static vDSP.compress<A, B>(_:gatingVector:nonZeroGatingCount:)(a1, a2, *(const float **)(v2 + 48), *(float ***)(v2 + 56), *(void *)(v2 + 64), *(vDSP_Length **)(v2 + 72), *(vDSP_Length **)(v2 + 80), *(void *)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(uint64_t **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16B28]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(uint64_t **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16B20]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16C90]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.gather<A, B, C>(_:indices:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16C88]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = v3[3];
  uint64_t v5 = v3[6];
  uint64_t v6 = v3[9];
  void v8[2] = a1;
  void v8[3] = a2;
  void v8[4] = v6;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E4FBC848] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.swapElements<A, B>(_:_:)(void *a1)
{
  return closure #1 in closure #1 in static vDSP.swapElements<A, B>(_:_:)(a1, *(uint64_t **)(v1 + 16), *(void *)(v1 + 24), MEMORY[0x1E4F16E70]);
}

{
  uint64_t v1;

  return closure #1 in closure #1 in static vDSP.swapElements<A, B>(_:_:)(a1, *(uint64_t **)(v1 + 16), *(void *)(v1 + 24), MEMORY[0x1E4F16E68]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:));
}

void *partial apply for closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(void *a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, MEMORY[0x1E4F16E90]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, MEMORY[0x1E4F16E88]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 32);
  uint64_t v5 = *(void *)(v3 + 56);
  uint64_t v6 = *(void *)(v3 + 88);
  v8[1] = *(_OWORD *)(v3 + 72);
  uint64_t v9 = a1;
  uint64_t v10 = a2;
  uint64_t v11 = v6;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, uint64_t))(v5 + 16))(a3, v8, MEMORY[0x1E4FBC848] + 8, v4);
}

void *partial apply for closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(void *a1, uint64_t (*a2)(uint64_t, uint64_t, uint64_t, uint64_t, void, uint64_t, uint64_t))
{
  return closure #1 in closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, v2[2], v2[3], v2[4], v2[5], v2[6], a2);
}

unint64_t BNNS.Shape.batchStride.getter()
{
  outlined init with take of BNNS.Shape(v0, (uint64_t)v13);
  outlined init with take of BNNS.Shape((uint64_t)v13, (uint64_t)v12);
  BNNS.Shape.size.getter((uint64_t)&v7);
  long long v1 = v7;
  long long v2 = v8;
  long long v3 = v9;
  unint64_t v4 = v10;
  unint64_t v5 = v11;
  outlined init with take of BNNS.Shape((uint64_t)v13, (uint64_t)v12);
  BNNS.Shape.stride.getter((uint64_t)&v7);
  return specialized static BNNS.calculateBatchStride(size:stride:)(v1, *((unint64_t *)&v1 + 1), v2, *((unint64_t *)&v2 + 1), v3, *((unint64_t *)&v3 + 1), v4, v5, v7, *((unint64_t *)&v7 + 1), v8, *((unint64_t *)&v8 + 1), v9, *((unint64_t *)&v9 + 1), v10, v11);
}

uint64_t BNNS.Shape.layout.getter()
{
  outlined init with take of BNNS.Shape(v0, (uint64_t)v3);
  outlined init with take of BNNS.Shape((uint64_t)v3, (uint64_t)v4);
  uint64_t v1 = dword_1D2136D70[(int)_s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v4)];
  destructiveProjectEnumData for BNNS.ActivationFunction(v4);
  return v1;
}

void *BNNS.Shape.size.getter@<X0>(uint64_t a1@<X8>)
{
  outlined init with take of BNNS.Shape(v1, (uint64_t)v8);
  outlined init with take of BNNS.Shape((uint64_t)v8, (uint64_t)v9);
  switch(_s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v9))
  {
    case 1u:
    case 2u:
    case 3u:
    case 4u:
      uint64_t result = (void *)destructiveProjectEnumData for BNNS.ActivationFunction(v9);
      uint64_t v4 = 0;
      *(void *)&long long v5 = result[1];
      goto LABEL_10;
    case 5u:
    case 6u:
    case 7u:
    case 8u:
    case 9u:
      uint64_t result = (void *)destructiveProjectEnumData for BNNS.ActivationFunction(v9);
      uint64_t v4 = 0;
      long long v5 = *(_OWORD *)(result + 1);
      goto LABEL_10;
    case 0xAu:
    case 0xBu:
    case 0xCu:
      uint64_t result = (void *)destructiveProjectEnumData for BNNS.ActivationFunction(v9);
      uint64_t v4 = 0;
      long long v5 = *(_OWORD *)(result + 1);
      *(void *)&long long v6 = result[3];
      goto LABEL_11;
    case 0xDu:
    case 0xEu:
      uint64_t result = (void *)destructiveProjectEnumData for BNNS.ActivationFunction(v9);
      uint64_t v4 = 0;
      long long v5 = *(_OWORD *)(result + 1);
      long long v6 = *(_OWORD *)(result + 3);
      goto LABEL_11;
    case 0xFu:
    case 0x10u:
      uint64_t result = (void *)destructiveProjectEnumData for BNNS.ActivationFunction(v9);
      uint64_t v4 = 0;
      long long v5 = *(_OWORD *)(result + 1);
      long long v6 = *(_OWORD *)(result + 3);
      *(void *)&long long v7 = result[5];
      goto LABEL_12;
    case 0x11u:
    case 0x12u:
      uint64_t result = (void *)destructiveProjectEnumData for BNNS.ActivationFunction(v9);
      uint64_t v4 = 0;
      long long v5 = *(_OWORD *)(result + 1);
      long long v6 = *(_OWORD *)(result + 3);
      long long v7 = *(_OWORD *)(result + 5);
      goto LABEL_12;
    case 0x13u:
    case 0x14u:
      uint64_t result = (void *)destructiveProjectEnumData for BNNS.ActivationFunction(v9);
      long long v5 = *(_OWORD *)(result + 1);
      long long v6 = *(_OWORD *)(result + 3);
      long long v7 = *(_OWORD *)(result + 5);
      uint64_t v4 = result[7];
      goto LABEL_12;
    default:
      uint64_t result = (void *)destructiveProjectEnumData for BNNS.ActivationFunction(v9);
      uint64_t v4 = 0;
      long long v5 = 0uLL;
LABEL_10:
      long long v6 = 0uLL;
LABEL_11:
      long long v7 = 0uLL;
LABEL_12:
      *(void *)a1 = *result;
      *(_OWORD *)(a1 + 8) = v5;
      *(_OWORD *)(a1 + 24) = v6;
      *(_OWORD *)(a1 + 40) = v7;
      *(void *)(a1 + 56) = v4;
      return result;
  }
}

uint64_t BNNS.Shape.stride.getter@<X0>(uint64_t a1@<X8>)
{
  outlined init with take of BNNS.Shape(v1, (uint64_t)v9);
  outlined init with take of BNNS.Shape((uint64_t)v9, (uint64_t)v10);
  switch(_s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v10))
  {
    case 1u:
    case 2u:
    case 3u:
    case 4u:
      uint64_t result = destructiveProjectEnumData for BNNS.ActivationFunction(v10);
      uint64_t v4 = 0;
      long long v5 = (void *)(result + 16);
      *(void *)&long long v6 = *(void *)(result + 24);
      goto LABEL_10;
    case 5u:
    case 6u:
    case 7u:
    case 8u:
    case 9u:
      uint64_t result = destructiveProjectEnumData for BNNS.ActivationFunction(v10);
      uint64_t v4 = 0;
      long long v5 = (void *)(result + 24);
      long long v6 = *(_OWORD *)(result + 32);
      goto LABEL_10;
    case 0xAu:
    case 0xBu:
    case 0xCu:
      uint64_t result = destructiveProjectEnumData for BNNS.ActivationFunction(v10);
      uint64_t v4 = 0;
      long long v5 = (void *)(result + 32);
      long long v6 = *(_OWORD *)(result + 40);
      *(void *)&long long v7 = *(void *)(result + 56);
      goto LABEL_11;
    case 0xDu:
    case 0xEu:
      uint64_t result = destructiveProjectEnumData for BNNS.ActivationFunction(v10);
      uint64_t v4 = 0;
      long long v5 = (void *)(result + 40);
      long long v6 = *(_OWORD *)(result + 48);
      long long v7 = *(_OWORD *)(result + 64);
      goto LABEL_11;
    case 0xFu:
    case 0x10u:
      uint64_t result = destructiveProjectEnumData for BNNS.ActivationFunction(v10);
      uint64_t v4 = 0;
      long long v5 = (void *)(result + 48);
      long long v6 = *(_OWORD *)(result + 56);
      long long v7 = *(_OWORD *)(result + 72);
      *(void *)&long long v8 = *(void *)(result + 88);
      goto LABEL_12;
    case 0x11u:
    case 0x12u:
      uint64_t result = destructiveProjectEnumData for BNNS.ActivationFunction(v10);
      uint64_t v4 = 0;
      long long v5 = (void *)(result + 56);
      long long v6 = *(_OWORD *)(result + 64);
      long long v7 = *(_OWORD *)(result + 80);
      long long v8 = *(_OWORD *)(result + 96);
      goto LABEL_12;
    case 0x13u:
    case 0x14u:
      uint64_t result = destructiveProjectEnumData for BNNS.ActivationFunction(v10);
      long long v5 = (void *)(result + 64);
      long long v6 = *(_OWORD *)(result + 72);
      long long v7 = *(_OWORD *)(result + 88);
      long long v8 = *(_OWORD *)(result + 104);
      uint64_t v4 = *(void *)(result + 120);
      goto LABEL_12;
    default:
      uint64_t result = destructiveProjectEnumData for BNNS.ActivationFunction(v10);
      uint64_t v4 = 0;
      long long v5 = (void *)(result + 8);
      long long v6 = 0uLL;
LABEL_10:
      long long v7 = 0uLL;
LABEL_11:
      long long v8 = 0uLL;
LABEL_12:
      *(void *)a1 = *v5;
      *(_OWORD *)(a1 + 8) = v6;
      *(_OWORD *)(a1 + 24) = v7;
      *(_OWORD *)(a1 + 40) = v8;
      *(void *)(a1 + 56) = v4;
      return result;
  }
}

uint64_t BNNS.Shape.init(_:dataLayout:stride:)@<X0>(void *a1@<X0>, unsigned __int8 *a2@<X1>, void *a3@<X2>, uint64_t a4@<X8>)
{
  unsigned int v7 = *a2;
  if (v7 == 21)
  {
    static BNNS.defaultLayoutForDimensions(_:)(a1[2], v13);
    unsigned int v7 = v13[0];
    if (v13[0] == 21) {
      goto LABEL_15;
    }
  }
  if (a3)
  {
    uint64_t v8 = a1[2];
    if (v8 != a3[2])
    {
LABEL_14:
      __break(1u);
LABEL_15:
      uint64_t result = swift_bridgeObjectRelease();
      __break(1u);
      return result;
    }
    if (v7 <= 0x14) {
      goto LABEL_6;
    }
  }
  else
  {
    uint64_t v8 = a1[2];
    if (v7 <= 0x14)
    {
LABEL_6:
      uint64_t v9 = qword_1D2136C30[(char)v7];
      goto LABEL_9;
    }
  }
  uint64_t v9 = 8;
LABEL_9:
  if (v8 != v9)
  {
    __break(1u);
    goto LABEL_14;
  }
  char v12 = v7;
  if (!a3) {
    a3 = &outlined read-only object #0 of BNNS.Shape.init(_:dataLayout:stride:);
  }
  static BNNS.makeShape(size:dataLayout:stride:)(a1, &v12, (uint64_t)a3, (uint64_t)v13);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  outlined init with take of BNNS.Shape((uint64_t)v13, (uint64_t)v11);
  return outlined init with take of BNNS.Shape((uint64_t)v11, a4);
}

uint64_t static BNNS.makeShape(size:dataLayout:stride:)@<X0>(void *a1@<X0>, unsigned char *a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X8>)
{
  switch(*a2)
  {
    case 1:
      uint64_t v6 = a1[2];
      if (!v6) {
        goto LABEL_202;
      }
      if (v6 == 1) {
        goto LABEL_203;
      }
      uint64_t v7 = *(void *)(a3 + 16);
      if (!v7) {
        goto LABEL_204;
      }
      if (v7 == 1) {
        goto LABEL_205;
      }
      uint64_t v8 = a1[5];
      uint64_t v165 = a1[4];
      uint64_t v166 = v8;
      long long v167 = *(_OWORD *)(a3 + 32);
      _s10Accelerate4BNNSO5ShapeOWOi1_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 2:
      uint64_t v9 = a1[2];
      if (!v9) {
        goto LABEL_206;
      }
      if (v9 == 1) {
        goto LABEL_207;
      }
      uint64_t v10 = *(void *)(a3 + 16);
      if (!v10) {
        goto LABEL_208;
      }
      if (v10 == 1) {
        goto LABEL_209;
      }
      uint64_t v11 = a1[5];
      uint64_t v165 = a1[4];
      uint64_t v166 = v11;
      long long v167 = *(_OWORD *)(a3 + 32);
      _s10Accelerate4BNNSO5ShapeOWOi0_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 3:
      uint64_t v12 = a1[2];
      if (!v12) {
        goto LABEL_210;
      }
      if (v12 == 1) {
        goto LABEL_211;
      }
      uint64_t v13 = *(void *)(a3 + 16);
      if (!v13) {
        goto LABEL_212;
      }
      if (v13 == 1) {
        goto LABEL_213;
      }
      uint64_t v14 = a1[5];
      uint64_t v165 = a1[4];
      uint64_t v166 = v14;
      long long v167 = *(_OWORD *)(a3 + 32);
      _s10Accelerate4BNNSO5ShapeOWOi2_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 4:
      uint64_t v15 = a1[2];
      if (!v15) {
        goto LABEL_214;
      }
      if (v15 == 1) {
        goto LABEL_215;
      }
      uint64_t v16 = *(void *)(a3 + 16);
      if (!v16) {
        goto LABEL_216;
      }
      if (v16 == 1) {
        goto LABEL_217;
      }
      uint64_t v17 = a1[5];
      uint64_t v165 = a1[4];
      uint64_t v166 = v17;
      long long v167 = *(_OWORD *)(a3 + 32);
      _s10Accelerate4BNNSO5ShapeOWOi3_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 5:
      unint64_t v18 = a1[2];
      if (!v18) {
        goto LABEL_218;
      }
      if (v18 == 1) {
        goto LABEL_219;
      }
      if (v18 < 3) {
        goto LABEL_220;
      }
      unint64_t v19 = *(void *)(a3 + 16);
      if (!v19) {
        goto LABEL_221;
      }
      if (v19 == 1) {
        goto LABEL_222;
      }
      if (v19 < 3) {
        goto LABEL_223;
      }
      uint64_t v20 = a1[5];
      uint64_t v21 = a1[6];
      uint64_t v22 = *(void *)(a3 + 32);
      uint64_t v165 = a1[4];
      uint64_t v166 = v20;
      *(void *)&long long v167 = v21;
      *((void *)&v167 + 1) = v22;
      long long v168 = *(_OWORD *)(a3 + 40);
      _s10Accelerate4BNNSO5ShapeOWOi4_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 6:
      unint64_t v23 = a1[2];
      if (!v23) {
        goto LABEL_224;
      }
      if (v23 == 1) {
        goto LABEL_225;
      }
      if (v23 < 3) {
        goto LABEL_226;
      }
      unint64_t v24 = *(void *)(a3 + 16);
      if (!v24) {
        goto LABEL_227;
      }
      if (v24 == 1) {
        goto LABEL_228;
      }
      if (v24 < 3) {
        goto LABEL_229;
      }
      uint64_t v25 = a1[5];
      uint64_t v26 = a1[6];
      uint64_t v27 = *(void *)(a3 + 32);
      uint64_t v165 = a1[4];
      uint64_t v166 = v25;
      *(void *)&long long v167 = v26;
      *((void *)&v167 + 1) = v27;
      long long v168 = *(_OWORD *)(a3 + 40);
      _s10Accelerate4BNNSO5ShapeOWOi5_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 7:
      unint64_t v28 = a1[2];
      if (!v28) {
        goto LABEL_230;
      }
      if (v28 == 1) {
        goto LABEL_231;
      }
      if (v28 < 3) {
        goto LABEL_232;
      }
      unint64_t v29 = *(void *)(a3 + 16);
      if (!v29) {
        goto LABEL_233;
      }
      if (v29 == 1) {
        goto LABEL_234;
      }
      if (v29 < 3) {
        goto LABEL_235;
      }
      uint64_t v30 = a1[5];
      uint64_t v31 = a1[6];
      uint64_t v32 = *(void *)(a3 + 32);
      uint64_t v165 = a1[4];
      uint64_t v166 = v30;
      *(void *)&long long v167 = v31;
      *((void *)&v167 + 1) = v32;
      long long v168 = *(_OWORD *)(a3 + 40);
      _s10Accelerate4BNNSO5ShapeOWOi6_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 8:
      unint64_t v33 = a1[2];
      if (!v33) {
        goto LABEL_236;
      }
      if (v33 == 1) {
        goto LABEL_237;
      }
      if (v33 < 3) {
        goto LABEL_238;
      }
      if (v33 == 3) {
        goto LABEL_239;
      }
      unint64_t v34 = *(void *)(a3 + 16);
      if (!v34) {
        goto LABEL_240;
      }
      if (v34 == 1) {
        goto LABEL_241;
      }
      if (v34 < 3) {
        goto LABEL_242;
      }
      if (v34 == 3) {
        goto LABEL_243;
      }
      uint64_t v35 = a1[5];
      uint64_t v36 = a1[6];
      uint64_t v37 = a1[7];
      uint64_t v38 = *(void *)(a3 + 32);
      uint64_t v39 = *(void *)(a3 + 40);
      uint64_t v165 = a1[4];
      uint64_t v166 = v35;
      *(void *)&long long v167 = v36;
      *((void *)&v167 + 1) = v37;
      *(void *)&long long v168 = v38;
      *((void *)&v168 + 1) = v39;
      long long v169 = *(_OWORD *)(a3 + 48);
      _s10Accelerate4BNNSO5ShapeOWOi9_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 9:
      unint64_t v40 = a1[2];
      if (!v40) {
        goto LABEL_244;
      }
      if (v40 == 1) {
        goto LABEL_245;
      }
      if (v40 < 3) {
        goto LABEL_246;
      }
      if (v40 == 3) {
        goto LABEL_247;
      }
      unint64_t v41 = *(void *)(a3 + 16);
      if (!v41) {
        goto LABEL_248;
      }
      if (v41 == 1) {
        goto LABEL_249;
      }
      if (v41 < 3) {
        goto LABEL_250;
      }
      if (v41 == 3) {
        goto LABEL_251;
      }
      uint64_t v42 = a1[5];
      uint64_t v43 = a1[6];
      uint64_t v44 = a1[7];
      uint64_t v45 = *(void *)(a3 + 32);
      uint64_t v46 = *(void *)(a3 + 40);
      uint64_t v165 = a1[4];
      uint64_t v166 = v42;
      *(void *)&long long v167 = v43;
      *((void *)&v167 + 1) = v44;
      *(void *)&long long v168 = v45;
      *((void *)&v168 + 1) = v46;
      long long v169 = *(_OWORD *)(a3 + 48);
      _s10Accelerate4BNNSO5ShapeOWOi10_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 0xA:
      unint64_t v47 = a1[2];
      if (!v47) {
        goto LABEL_252;
      }
      if (v47 == 1) {
        goto LABEL_253;
      }
      if (v47 < 3) {
        goto LABEL_254;
      }
      if (v47 == 3) {
        goto LABEL_255;
      }
      unint64_t v48 = *(void *)(a3 + 16);
      if (!v48) {
        goto LABEL_256;
      }
      if (v48 == 1) {
        goto LABEL_257;
      }
      if (v48 < 3) {
        goto LABEL_258;
      }
      if (v48 == 3) {
        goto LABEL_259;
      }
      uint64_t v49 = a1[5];
      uint64_t v50 = a1[6];
      uint64_t v51 = a1[7];
      uint64_t v52 = *(void *)(a3 + 32);
      uint64_t v53 = *(void *)(a3 + 40);
      uint64_t v165 = a1[4];
      uint64_t v166 = v49;
      *(void *)&long long v167 = v50;
      *((void *)&v167 + 1) = v51;
      *(void *)&long long v168 = v52;
      *((void *)&v168 + 1) = v53;
      long long v169 = *(_OWORD *)(a3 + 48);
      _s10Accelerate4BNNSO5ShapeOWOi11_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 0xB:
      unint64_t v54 = a1[2];
      if (!v54) {
        goto LABEL_260;
      }
      if (v54 == 1) {
        goto LABEL_261;
      }
      if (v54 < 3) {
        goto LABEL_262;
      }
      if (v54 == 3) {
        goto LABEL_263;
      }
      if (v54 < 5) {
        goto LABEL_264;
      }
      unint64_t v55 = *(void *)(a3 + 16);
      if (!v55) {
        goto LABEL_265;
      }
      if (v55 == 1) {
        goto LABEL_266;
      }
      if (v55 < 3) {
        goto LABEL_267;
      }
      if (v55 == 3) {
        goto LABEL_268;
      }
      if (v55 < 5) {
        goto LABEL_269;
      }
      uint64_t v56 = a1[5];
      uint64_t v57 = a1[6];
      uint64_t v58 = a1[7];
      uint64_t v59 = a1[8];
      uint64_t v60 = *(void *)(a3 + 32);
      uint64_t v61 = *(void *)(a3 + 40);
      uint64_t v62 = *(void *)(a3 + 48);
      uint64_t v165 = a1[4];
      uint64_t v166 = v56;
      *(void *)&long long v167 = v57;
      *((void *)&v167 + 1) = v58;
      *(void *)&long long v168 = v59;
      *((void *)&v168 + 1) = v60;
      *(void *)&long long v169 = v61;
      *((void *)&v169 + 1) = v62;
      long long v170 = *(_OWORD *)(a3 + 56);
      _s10Accelerate4BNNSO5ShapeOWOi12_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 0xC:
      unint64_t v63 = a1[2];
      if (!v63) {
        goto LABEL_270;
      }
      if (v63 == 1) {
        goto LABEL_271;
      }
      if (v63 < 3) {
        goto LABEL_272;
      }
      if (v63 == 3) {
        goto LABEL_273;
      }
      if (v63 < 5) {
        goto LABEL_274;
      }
      unint64_t v64 = *(void *)(a3 + 16);
      if (!v64) {
        goto LABEL_275;
      }
      if (v64 == 1) {
        goto LABEL_276;
      }
      if (v64 < 3) {
        goto LABEL_277;
      }
      if (v64 == 3) {
        goto LABEL_278;
      }
      if (v64 < 5) {
        goto LABEL_279;
      }
      uint64_t v65 = a1[5];
      uint64_t v66 = a1[6];
      uint64_t v67 = a1[7];
      uint64_t v68 = a1[8];
      uint64_t v69 = *(void *)(a3 + 32);
      uint64_t v70 = *(void *)(a3 + 40);
      uint64_t v71 = *(void *)(a3 + 48);
      uint64_t v165 = a1[4];
      uint64_t v166 = v65;
      *(void *)&long long v167 = v66;
      *((void *)&v167 + 1) = v67;
      *(void *)&long long v168 = v68;
      *((void *)&v168 + 1) = v69;
      *(void *)&long long v169 = v70;
      *((void *)&v169 + 1) = v71;
      long long v170 = *(_OWORD *)(a3 + 56);
      _s10Accelerate4BNNSO5ShapeOWOi13_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 0xD:
      unint64_t v72 = a1[2];
      if (!v72) {
        goto LABEL_280;
      }
      if (v72 == 1) {
        goto LABEL_281;
      }
      if (v72 < 3) {
        goto LABEL_282;
      }
      if (v72 == 3) {
        goto LABEL_283;
      }
      if (v72 < 5) {
        goto LABEL_284;
      }
      if (v72 == 5) {
        goto LABEL_285;
      }
      unint64_t v73 = *(void *)(a3 + 16);
      if (!v73) {
        goto LABEL_286;
      }
      if (v73 == 1) {
        goto LABEL_287;
      }
      if (v73 < 3) {
        goto LABEL_288;
      }
      if (v73 == 3) {
        goto LABEL_289;
      }
      if (v73 < 5) {
        goto LABEL_290;
      }
      if (v73 == 5) {
        goto LABEL_291;
      }
      uint64_t v74 = a1[5];
      uint64_t v75 = a1[6];
      uint64_t v76 = a1[7];
      uint64_t v77 = a1[8];
      uint64_t v78 = a1[9];
      uint64_t v79 = *(void *)(a3 + 32);
      uint64_t v80 = *(void *)(a3 + 40);
      uint64_t v81 = *(void *)(a3 + 48);
      uint64_t v82 = *(void *)(a3 + 56);
      uint64_t v165 = a1[4];
      uint64_t v166 = v74;
      *(void *)&long long v167 = v75;
      *((void *)&v167 + 1) = v76;
      *(void *)&long long v168 = v77;
      *((void *)&v168 + 1) = v78;
      *(void *)&long long v169 = v79;
      *((void *)&v169 + 1) = v80;
      *(void *)&long long v170 = v81;
      *((void *)&v170 + 1) = v82;
      long long v171 = *(_OWORD *)(a3 + 64);
      _s10Accelerate4BNNSO5ShapeOWOi14_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 0xE:
      unint64_t v83 = a1[2];
      if (!v83) {
        goto LABEL_292;
      }
      if (v83 == 1) {
        goto LABEL_293;
      }
      if (v83 < 3) {
        goto LABEL_294;
      }
      if (v83 == 3) {
        goto LABEL_295;
      }
      if (v83 < 5) {
        goto LABEL_296;
      }
      if (v83 == 5) {
        goto LABEL_297;
      }
      unint64_t v84 = *(void *)(a3 + 16);
      if (!v84) {
        goto LABEL_298;
      }
      if (v84 == 1) {
        goto LABEL_299;
      }
      if (v84 < 3) {
        goto LABEL_300;
      }
      if (v84 == 3) {
        goto LABEL_301;
      }
      if (v84 < 5) {
        goto LABEL_302;
      }
      if (v84 == 5) {
        goto LABEL_303;
      }
      uint64_t v85 = a1[5];
      uint64_t v86 = a1[6];
      uint64_t v87 = a1[7];
      uint64_t v88 = a1[8];
      uint64_t v89 = a1[9];
      uint64_t v90 = *(void *)(a3 + 32);
      uint64_t v91 = *(void *)(a3 + 40);
      uint64_t v92 = *(void *)(a3 + 48);
      uint64_t v93 = *(void *)(a3 + 56);
      uint64_t v165 = a1[4];
      uint64_t v166 = v85;
      *(void *)&long long v167 = v86;
      *((void *)&v167 + 1) = v87;
      *(void *)&long long v168 = v88;
      *((void *)&v168 + 1) = v89;
      *(void *)&long long v169 = v90;
      *((void *)&v169 + 1) = v91;
      *(void *)&long long v170 = v92;
      *((void *)&v170 + 1) = v93;
      long long v171 = *(_OWORD *)(a3 + 64);
      _s10Accelerate4BNNSO5ShapeOWOi15_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 0xF:
      unint64_t v94 = a1[2];
      if (!v94) {
        goto LABEL_304;
      }
      if (v94 == 1) {
        goto LABEL_305;
      }
      if (v94 < 3) {
        goto LABEL_306;
      }
      if (v94 == 3) {
        goto LABEL_307;
      }
      if (v94 < 5) {
        goto LABEL_308;
      }
      if (v94 == 5) {
        goto LABEL_309;
      }
      if (v94 < 7) {
        goto LABEL_310;
      }
      unint64_t v95 = *(void *)(a3 + 16);
      if (!v95) {
        goto LABEL_311;
      }
      if (v95 == 1) {
        goto LABEL_312;
      }
      if (v95 < 3) {
        goto LABEL_313;
      }
      if (v95 == 3) {
        goto LABEL_314;
      }
      if (v95 < 5) {
        goto LABEL_315;
      }
      if (v95 == 5) {
        goto LABEL_316;
      }
      if (v95 < 7) {
        goto LABEL_317;
      }
      uint64_t v96 = a1[4];
      uint64_t v97 = a1[5];
      uint64_t v98 = a1[6];
      uint64_t v99 = a1[7];
      uint64_t v100 = a1[8];
      uint64_t v101 = a1[9];
      uint64_t v102 = a1[10];
      uint64_t v103 = *(void *)(a3 + 32);
      uint64_t v104 = *(void *)(a3 + 40);
      uint64_t v106 = *(void *)(a3 + 48);
      uint64_t v105 = *(void *)(a3 + 56);
      uint64_t v107 = *(void *)(a3 + 64);
      uint64_t v165 = v96;
      uint64_t v166 = v97;
      *(void *)&long long v167 = v98;
      *((void *)&v167 + 1) = v99;
      *(void *)&long long v168 = v100;
      *((void *)&v168 + 1) = v101;
      *(void *)&long long v169 = v102;
      *((void *)&v169 + 1) = v103;
      *(void *)&long long v170 = v104;
      *((void *)&v170 + 1) = v106;
      *(void *)&long long v171 = v105;
      *((void *)&v171 + 1) = v107;
      long long v172 = *(_OWORD *)(a3 + 72);
      _s10Accelerate4BNNSO5ShapeOWOi16_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 0x10:
      unint64_t v108 = a1[2];
      if (!v108) {
        goto LABEL_318;
      }
      if (v108 == 1) {
        goto LABEL_319;
      }
      if (v108 < 3) {
        goto LABEL_320;
      }
      if (v108 == 3) {
        goto LABEL_321;
      }
      if (v108 < 5) {
        goto LABEL_322;
      }
      if (v108 == 5) {
        goto LABEL_323;
      }
      if (v108 < 7) {
        goto LABEL_324;
      }
      unint64_t v109 = *(void *)(a3 + 16);
      if (!v109) {
        goto LABEL_325;
      }
      if (v109 == 1) {
        goto LABEL_326;
      }
      if (v109 < 3) {
        goto LABEL_327;
      }
      if (v109 == 3) {
        goto LABEL_328;
      }
      if (v109 < 5) {
        goto LABEL_329;
      }
      if (v109 == 5) {
        goto LABEL_330;
      }
      if (v109 < 7) {
        goto LABEL_331;
      }
      uint64_t v110 = a1[4];
      uint64_t v111 = a1[5];
      uint64_t v112 = a1[6];
      uint64_t v113 = a1[7];
      uint64_t v114 = a1[8];
      uint64_t v115 = a1[9];
      uint64_t v116 = a1[10];
      uint64_t v117 = *(void *)(a3 + 32);
      uint64_t v118 = *(void *)(a3 + 40);
      uint64_t v120 = *(void *)(a3 + 48);
      uint64_t v119 = *(void *)(a3 + 56);
      uint64_t v121 = *(void *)(a3 + 64);
      uint64_t v165 = v110;
      uint64_t v166 = v111;
      *(void *)&long long v167 = v112;
      *((void *)&v167 + 1) = v113;
      *(void *)&long long v168 = v114;
      *((void *)&v168 + 1) = v115;
      *(void *)&long long v169 = v116;
      *((void *)&v169 + 1) = v117;
      *(void *)&long long v170 = v118;
      *((void *)&v170 + 1) = v120;
      *(void *)&long long v171 = v119;
      *((void *)&v171 + 1) = v121;
      long long v172 = *(_OWORD *)(a3 + 72);
      _s10Accelerate4BNNSO5ShapeOWOi17_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 0x11:
      unint64_t v122 = a1[2];
      if (!v122) {
        goto LABEL_332;
      }
      if (v122 == 1) {
        goto LABEL_333;
      }
      if (v122 < 3) {
        goto LABEL_334;
      }
      if (v122 == 3) {
        goto LABEL_335;
      }
      if (v122 < 5) {
        goto LABEL_336;
      }
      if (v122 == 5) {
        goto LABEL_337;
      }
      if (v122 < 7) {
        goto LABEL_338;
      }
      if (v122 == 7) {
        goto LABEL_339;
      }
      unint64_t v123 = *(void *)(a3 + 16);
      if (!v123) {
        goto LABEL_340;
      }
      if (v123 == 1) {
        goto LABEL_341;
      }
      if (v123 < 3) {
        goto LABEL_342;
      }
      if (v123 == 3) {
        goto LABEL_343;
      }
      if (v123 < 5) {
        goto LABEL_344;
      }
      if (v123 == 5) {
        goto LABEL_345;
      }
      if (v123 < 7) {
        goto LABEL_346;
      }
      if (v123 == 7) {
        goto LABEL_347;
      }
      uint64_t v124 = a1[4];
      uint64_t v125 = a1[5];
      uint64_t v126 = a1[6];
      uint64_t v127 = a1[7];
      uint64_t v128 = a1[8];
      uint64_t v129 = a1[9];
      uint64_t v130 = a1[10];
      uint64_t v131 = a1[11];
      uint64_t v132 = *(void *)(a3 + 32);
      uint64_t v133 = *(void *)(a3 + 40);
      uint64_t v134 = *(void *)(a3 + 48);
      uint64_t v135 = *(void *)(a3 + 56);
      uint64_t v136 = *(void *)(a3 + 64);
      uint64_t v137 = *(void *)(a3 + 72);
      uint64_t v165 = v124;
      uint64_t v166 = v125;
      *(void *)&long long v167 = v126;
      *((void *)&v167 + 1) = v127;
      *(void *)&long long v168 = v128;
      *((void *)&v168 + 1) = v129;
      *(void *)&long long v169 = v130;
      *((void *)&v169 + 1) = v131;
      *(void *)&long long v170 = v132;
      *((void *)&v170 + 1) = v133;
      *(void *)&long long v171 = v134;
      *((void *)&v171 + 1) = v135;
      *(void *)&long long v172 = v136;
      *((void *)&v172 + 1) = v137;
      long long v173 = *(_OWORD *)(a3 + 80);
      _s10Accelerate4BNNSO5ShapeOWOi18_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 0x12:
      unint64_t v138 = a1[2];
      if (!v138) {
        goto LABEL_348;
      }
      if (v138 == 1) {
        goto LABEL_349;
      }
      if (v138 < 3) {
        goto LABEL_350;
      }
      if (v138 == 3) {
        goto LABEL_351;
      }
      if (v138 < 5) {
        goto LABEL_352;
      }
      if (v138 == 5) {
        goto LABEL_353;
      }
      if (v138 < 7) {
        goto LABEL_354;
      }
      if (v138 == 7) {
        goto LABEL_355;
      }
      unint64_t v139 = *(void *)(a3 + 16);
      if (!v139) {
        goto LABEL_356;
      }
      if (v139 == 1) {
        goto LABEL_357;
      }
      if (v139 < 3) {
        goto LABEL_358;
      }
      if (v139 == 3) {
        goto LABEL_359;
      }
      if (v139 < 5) {
        goto LABEL_360;
      }
      if (v139 == 5) {
        goto LABEL_361;
      }
      if (v139 < 7) {
        goto LABEL_362;
      }
      if (v139 == 7) {
        goto LABEL_363;
      }
      uint64_t v140 = a1[4];
      uint64_t v141 = a1[5];
      uint64_t v142 = a1[6];
      uint64_t v143 = a1[7];
      uint64_t v144 = a1[8];
      uint64_t v145 = a1[9];
      uint64_t v146 = a1[10];
      uint64_t v147 = a1[11];
      uint64_t v148 = *(void *)(a3 + 32);
      uint64_t v149 = *(void *)(a3 + 40);
      uint64_t v150 = *(void *)(a3 + 48);
      uint64_t v151 = *(void *)(a3 + 56);
      uint64_t v152 = *(void *)(a3 + 64);
      uint64_t v153 = *(void *)(a3 + 72);
      uint64_t v165 = v140;
      uint64_t v166 = v141;
      *(void *)&long long v167 = v142;
      *((void *)&v167 + 1) = v143;
      *(void *)&long long v168 = v144;
      *((void *)&v168 + 1) = v145;
      *(void *)&long long v169 = v146;
      *((void *)&v169 + 1) = v147;
      *(void *)&long long v170 = v148;
      *((void *)&v170 + 1) = v149;
      *(void *)&long long v171 = v150;
      *((void *)&v171 + 1) = v151;
      *(void *)&long long v172 = v152;
      *((void *)&v172 + 1) = v153;
      long long v173 = *(_OWORD *)(a3 + 80);
      _s10Accelerate4BNNSO5ShapeOWOi19_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 0x13:
      unint64_t v154 = a1[2];
      if (!v154) {
        goto LABEL_364;
      }
      if (v154 == 1) {
        goto LABEL_365;
      }
      if (v154 < 3) {
        goto LABEL_366;
      }
      unint64_t v155 = *(void *)(a3 + 16);
      if (!v155) {
        goto LABEL_367;
      }
      if (v155 == 1) {
        goto LABEL_368;
      }
      if (v155 < 3) {
        goto LABEL_369;
      }
      uint64_t v156 = a1[5];
      uint64_t v157 = a1[6];
      uint64_t v158 = *(void *)(a3 + 32);
      uint64_t v165 = a1[4];
      uint64_t v166 = v156;
      *(void *)&long long v167 = v157;
      *((void *)&v167 + 1) = v158;
      long long v168 = *(_OWORD *)(a3 + 40);
      _s10Accelerate4BNNSO5ShapeOWOi7_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    case 0x14:
      unint64_t v159 = a1[2];
      if (!v159) {
        goto LABEL_370;
      }
      if (v159 == 1) {
        goto LABEL_371;
      }
      if (v159 < 3) {
        goto LABEL_372;
      }
      unint64_t v160 = *(void *)(a3 + 16);
      if (!v160) {
        goto LABEL_373;
      }
      if (v160 == 1) {
        goto LABEL_374;
      }
      if (v160 < 3) {
        goto LABEL_375;
      }
      uint64_t v161 = a1[5];
      uint64_t v162 = a1[6];
      uint64_t v163 = *(void *)(a3 + 32);
      uint64_t v165 = a1[4];
      uint64_t v166 = v161;
      *(void *)&long long v167 = v162;
      *((void *)&v167 + 1) = v163;
      long long v168 = *(_OWORD *)(a3 + 40);
      _s10Accelerate4BNNSO5ShapeOWOi8_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
    default:
      if (!a1[2])
      {
        __break(1u);
LABEL_201:
        __break(1u);
LABEL_202:
        __break(1u);
LABEL_203:
        __break(1u);
LABEL_204:
        __break(1u);
LABEL_205:
        __break(1u);
LABEL_206:
        __break(1u);
LABEL_207:
        __break(1u);
LABEL_208:
        __break(1u);
LABEL_209:
        __break(1u);
LABEL_210:
        __break(1u);
LABEL_211:
        __break(1u);
LABEL_212:
        __break(1u);
LABEL_213:
        __break(1u);
LABEL_214:
        __break(1u);
LABEL_215:
        __break(1u);
LABEL_216:
        __break(1u);
LABEL_217:
        __break(1u);
LABEL_218:
        __break(1u);
LABEL_219:
        __break(1u);
LABEL_220:
        __break(1u);
LABEL_221:
        __break(1u);
LABEL_222:
        __break(1u);
LABEL_223:
        __break(1u);
LABEL_224:
        __break(1u);
LABEL_225:
        __break(1u);
LABEL_226:
        __break(1u);
LABEL_227:
        __break(1u);
LABEL_228:
        __break(1u);
LABEL_229:
        __break(1u);
LABEL_230:
        __break(1u);
LABEL_231:
        __break(1u);
LABEL_232:
        __break(1u);
LABEL_233:
        __break(1u);
LABEL_234:
        __break(1u);
LABEL_235:
        __break(1u);
LABEL_236:
        __break(1u);
LABEL_237:
        __break(1u);
LABEL_238:
        __break(1u);
LABEL_239:
        __break(1u);
LABEL_240:
        __break(1u);
LABEL_241:
        __break(1u);
LABEL_242:
        __break(1u);
LABEL_243:
        __break(1u);
LABEL_244:
        __break(1u);
LABEL_245:
        __break(1u);
LABEL_246:
        __break(1u);
LABEL_247:
        __break(1u);
LABEL_248:
        __break(1u);
LABEL_249:
        __break(1u);
LABEL_250:
        __break(1u);
LABEL_251:
        __break(1u);
LABEL_252:
        __break(1u);
LABEL_253:
        __break(1u);
LABEL_254:
        __break(1u);
LABEL_255:
        __break(1u);
LABEL_256:
        __break(1u);
LABEL_257:
        __break(1u);
LABEL_258:
        __break(1u);
LABEL_259:
        __break(1u);
LABEL_260:
        __break(1u);
LABEL_261:
        __break(1u);
LABEL_262:
        __break(1u);
LABEL_263:
        __break(1u);
LABEL_264:
        __break(1u);
LABEL_265:
        __break(1u);
LABEL_266:
        __break(1u);
LABEL_267:
        __break(1u);
LABEL_268:
        __break(1u);
LABEL_269:
        __break(1u);
LABEL_270:
        __break(1u);
LABEL_271:
        __break(1u);
LABEL_272:
        __break(1u);
LABEL_273:
        __break(1u);
LABEL_274:
        __break(1u);
LABEL_275:
        __break(1u);
LABEL_276:
        __break(1u);
LABEL_277:
        __break(1u);
LABEL_278:
        __break(1u);
LABEL_279:
        __break(1u);
LABEL_280:
        __break(1u);
LABEL_281:
        __break(1u);
LABEL_282:
        __break(1u);
LABEL_283:
        __break(1u);
LABEL_284:
        __break(1u);
LABEL_285:
        __break(1u);
LABEL_286:
        __break(1u);
LABEL_287:
        __break(1u);
LABEL_288:
        __break(1u);
LABEL_289:
        __break(1u);
LABEL_290:
        __break(1u);
LABEL_291:
        __break(1u);
LABEL_292:
        __break(1u);
LABEL_293:
        __break(1u);
LABEL_294:
        __break(1u);
LABEL_295:
        __break(1u);
LABEL_296:
        __break(1u);
LABEL_297:
        __break(1u);
LABEL_298:
        __break(1u);
LABEL_299:
        __break(1u);
LABEL_300:
        __break(1u);
LABEL_301:
        __break(1u);
LABEL_302:
        __break(1u);
LABEL_303:
        __break(1u);
LABEL_304:
        __break(1u);
LABEL_305:
        __break(1u);
LABEL_306:
        __break(1u);
LABEL_307:
        __break(1u);
LABEL_308:
        __break(1u);
LABEL_309:
        __break(1u);
LABEL_310:
        __break(1u);
LABEL_311:
        __break(1u);
LABEL_312:
        __break(1u);
LABEL_313:
        __break(1u);
LABEL_314:
        __break(1u);
LABEL_315:
        __break(1u);
LABEL_316:
        __break(1u);
LABEL_317:
        __break(1u);
LABEL_318:
        __break(1u);
LABEL_319:
        __break(1u);
LABEL_320:
        __break(1u);
LABEL_321:
        __break(1u);
LABEL_322:
        __break(1u);
LABEL_323:
        __break(1u);
LABEL_324:
        __break(1u);
LABEL_325:
        __break(1u);
LABEL_326:
        __break(1u);
LABEL_327:
        __break(1u);
LABEL_328:
        __break(1u);
LABEL_329:
        __break(1u);
LABEL_330:
        __break(1u);
LABEL_331:
        __break(1u);
LABEL_332:
        __break(1u);
LABEL_333:
        __break(1u);
LABEL_334:
        __break(1u);
LABEL_335:
        __break(1u);
LABEL_336:
        __break(1u);
LABEL_337:
        __break(1u);
LABEL_338:
        __break(1u);
LABEL_339:
        __break(1u);
LABEL_340:
        __break(1u);
LABEL_341:
        __break(1u);
LABEL_342:
        __break(1u);
LABEL_343:
        __break(1u);
LABEL_344:
        __break(1u);
LABEL_345:
        __break(1u);
LABEL_346:
        __break(1u);
LABEL_347:
        __break(1u);
LABEL_348:
        __break(1u);
LABEL_349:
        __break(1u);
LABEL_350:
        __break(1u);
LABEL_351:
        __break(1u);
LABEL_352:
        __break(1u);
LABEL_353:
        __break(1u);
LABEL_354:
        __break(1u);
LABEL_355:
        __break(1u);
LABEL_356:
        __break(1u);
LABEL_357:
        __break(1u);
LABEL_358:
        __break(1u);
LABEL_359:
        __break(1u);
LABEL_360:
        __break(1u);
LABEL_361:
        __break(1u);
LABEL_362:
        __break(1u);
LABEL_363:
        __break(1u);
LABEL_364:
        __break(1u);
LABEL_365:
        __break(1u);
LABEL_366:
        __break(1u);
LABEL_367:
        __break(1u);
LABEL_368:
        __break(1u);
LABEL_369:
        __break(1u);
LABEL_370:
        __break(1u);
LABEL_371:
        __break(1u);
LABEL_372:
        __break(1u);
LABEL_373:
        __break(1u);
LABEL_374:
        __break(1u);
LABEL_375:
        __break(1u);
        JUMPOUT(0x1D20ACF2CLL);
      }
      if (!*(void *)(a3 + 16)) {
        goto LABEL_201;
      }
      uint64_t v5 = *(void *)(a3 + 32);
      uint64_t v165 = a1[4];
      uint64_t v166 = v5;
      _s10Accelerate4BNNSO5ShapeOWOi_((uint64_t)&v165);
      return outlined init with take of BNNS.Shape((uint64_t)&v165, a4);
  }
}

uint64_t BNNS.Shape.rank.getter()
{
  outlined init with take of BNNS.Shape(v0, (uint64_t)v3);
  unsigned int v1 = _s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v3);
  if (v1 > 0x12) {
    return 8;
  }
  else {
    return qword_1D2136CD8[v1];
  }
}

uint64_t BNNS.Shape.init(arrayLiteral:)@<X0>(void *a1@<X0>, uint64_t a2@<X8>)
{
  v5[0] = 21;
  BNNS.Shape.init(_:dataLayout:stride:)(a1, v5, 0, (uint64_t)v4);
  outlined init with take of BNNS.Shape((uint64_t)v4, (uint64_t)v5);
  return outlined init with take of BNNS.Shape((uint64_t)v5, a2);
}

uint64_t protocol witness for ExpressibleByArrayLiteral.init(arrayLiteral:) in conformance BNNS.Shape@<X0>(void *a1@<X0>, uint64_t a2@<X8>)
{
  v5[0] = 21;
  BNNS.Shape.init(_:dataLayout:stride:)(a1, v5, 0, (uint64_t)v4);
  outlined init with take of BNNS.Shape((uint64_t)v4, (uint64_t)v5);
  return outlined init with take of BNNS.Shape((uint64_t)v5, a2);
}

uint64_t BNNS.Shape.denseTensorSize.getter()
{
  outlined init with take of BNNS.Shape(v0, (uint64_t)v9);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  uint64_t v1 = swift_allocObject();
  *(_OWORD *)(v1 + 16) = xmmword_1D2136B90;
  outlined init with take of BNNS.Shape((uint64_t)v9, (uint64_t)v8);
  BNNS.Shape.size.getter((uint64_t)v7);
  *(void *)(v1 + 32) = v7[0];
  outlined init with take of BNNS.Shape((uint64_t)v9, (uint64_t)v8);
  BNNS.Shape.size.getter((uint64_t)v7);
  *(void *)(v1 + 40) = v7[1];
  outlined init with take of BNNS.Shape((uint64_t)v9, (uint64_t)v8);
  BNNS.Shape.size.getter((uint64_t)v7);
  *(void *)(v1 + 48) = v7[2];
  outlined init with take of BNNS.Shape((uint64_t)v9, (uint64_t)v8);
  BNNS.Shape.size.getter((uint64_t)v7);
  *(void *)(v1 + 56) = v7[3];
  outlined init with take of BNNS.Shape((uint64_t)v9, (uint64_t)v8);
  BNNS.Shape.size.getter((uint64_t)v7);
  *(void *)(v1 + 64) = v7[4];
  outlined init with take of BNNS.Shape((uint64_t)v9, (uint64_t)v8);
  BNNS.Shape.size.getter((uint64_t)v7);
  *(void *)(v1 + 72) = v7[5];
  outlined init with take of BNNS.Shape((uint64_t)v9, (uint64_t)v8);
  BNNS.Shape.size.getter((uint64_t)v7);
  *(void *)(v1 + 80) = v7[6];
  outlined init with take of BNNS.Shape((uint64_t)v9, (uint64_t)v8);
  BNNS.Shape.size.getter((uint64_t)v7);
  *(void *)(v1 + 88) = v7[7];
  uint64_t result = _s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v9);
  if (result > 0x12) {
    uint64_t v3 = 8;
  }
  else {
    uint64_t v3 = qword_1D2136CD8[(int)result];
  }
  uint64_t v4 = 0;
  uint64_t v5 = 1;
  while (1)
  {
    uint64_t v6 = *(void *)(v1 + 8 * v4 + 32);
    if ((unsigned __int128)(v5 * (__int128)v6) >> 64 != (v5 * v6) >> 63) {
      break;
    }
    v5 *= v6;
    if (v3 == ++v4)
    {
      swift_bridgeObjectRelease();
      return v5;
    }
  }
  __break(1u);
  return result;
}

__n128 __swift_memcpy129_8(uint64_t a1, uint64_t a2)
{
  *(_OWORD *)a1 = *(_OWORD *)a2;
  long long v2 = *(_OWORD *)(a2 + 16);
  long long v3 = *(_OWORD *)(a2 + 32);
  long long v4 = *(_OWORD *)(a2 + 64);
  *(_OWORD *)(a1 + 48) = *(_OWORD *)(a2 + 48);
  *(_OWORD *)(a1 + 64) = v4;
  *(_OWORD *)(a1 + 16) = v2;
  *(_OWORD *)(a1 + 32) = v3;
  __n128 result = *(__n128 *)(a2 + 80);
  long long v6 = *(_OWORD *)(a2 + 96);
  long long v7 = *(_OWORD *)(a2 + 112);
  *(unsigned char *)(a1 + 128) = *(unsigned char *)(a2 + 128);
  *(_OWORD *)(a1 + 96) = v6;
  *(_OWORD *)(a1 + 112) = v7;
  *(__n128 *)(a1 + 80) = result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.Shape(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xEC && *(unsigned char *)(a1 + 129)) {
    return (*(_DWORD *)a1 + 236);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 128);
  if (v3 <= 0x14) {
    int v4 = -1;
  }
  else {
    int v4 = v3 ^ 0xFF;
  }
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.Shape(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xEB)
  {
    *(void *)(result + 120) = 0;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(unsigned char *)(result + 128) = 0;
    *(void *)__n128 result = a2 - 236;
    if (a3 >= 0xEC) {
      *(unsigned char *)(result + 129) = 1;
    }
  }
  else
  {
    if (a3 >= 0xEC) {
      *(unsigned char *)(result + 129) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 128) = -(char)a2;
    }
  }
  return result;
}

uint64_t destructiveInjectEnumTag for BNNS.Shape(uint64_t result, char a2)
{
  *(unsigned char *)(result + 128) = a2;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.Shape()
{
  return &type metadata for BNNS.Shape;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi19_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 20;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi18_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 19;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi17_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 18;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi16_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 17;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi15_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 16;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi14_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 15;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi13_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 14;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi12_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 13;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi11_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 12;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi10_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 11;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi9_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 10;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi8_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 9;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi7_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 8;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi6_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 7;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi5_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 6;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi4_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 5;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi3_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 4;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi2_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 3;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi0_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 1;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi1_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 2;
  return result;
}

uint64_t _s10Accelerate4BNNSO5ShapeOWOi_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 0;
  return result;
}

unint64_t specialized static BNNS.arrayToTuple<A>(_:fillValue:)(void *a1, void *a2, void *a3, void *a4, void *a5, unint64_t *a6, void *a7, char *a8, unint64_t *a9)
{
  swift_bridgeObjectRetain();
  char isUniquelyReferenced_nonNull_native = (char *)swift_isUniquelyReferenced_nonNull_native();
  if (!isUniquelyReferenced_nonNull_native || *((void *)a8 + 3) <= 0xFuLL)
  {
    if (*((void *)a8 + 2) <= 8uLL) {
      int64_t v18 = 8;
    }
    else {
      int64_t v18 = *((void *)a8 + 2);
    }
    a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(isUniquelyReferenced_nonNull_native, v18, 0, a8);
  }
  unint64_t v19 = *((void *)a8 + 2);
  if (v19 > 7) {
    goto LABEL_33;
  }
  uint64_t v39 = a6;
  a6 = a9;
  unint64_t v20 = *((void *)a8 + 3);
  unint64_t v21 = v19 + 1;
  if (v19 >= v20 >> 1) {
    goto LABEL_46;
  }
  while (1)
  {
    *((void *)a8 + 2) = v21;
    *(void *)&a8[8 * v19 + 32] = a6;
    if (v19 <= 6)
    {
      uint64_t v38 = a7;
      unint64_t v22 = *((void *)a8 + 3);
      unint64_t v23 = v19 + 2;
      if (v21 >= v22 >> 1) {
        a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v22 > 1), v19 + 2, 1, a8);
      }
      *((void *)a8 + 2) = v23;
      *(void *)&a8[8 * v21 + 32] = a6;
      if (v19 <= 5)
      {
        unint64_t v24 = *((void *)a8 + 3);
        unint64_t v25 = v19 + 3;
        if (v23 >= v24 >> 1) {
          a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v24 > 1), v19 + 3, 1, a8);
        }
        *((void *)a8 + 2) = v25;
        *(void *)&a8[8 * v23 + 32] = a6;
        if (v19 <= 4)
        {
          unint64_t v26 = *((void *)a8 + 3);
          unint64_t v27 = v19 + 4;
          if (v25 >= v26 >> 1) {
            a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v26 > 1), v19 + 4, 1, a8);
          }
          *((void *)a8 + 2) = v27;
          *(void *)&a8[8 * v25 + 32] = a6;
          if (v19 <= 3)
          {
            unint64_t v28 = *((void *)a8 + 3);
            unint64_t v29 = v19 + 5;
            if (v27 >= v28 >> 1) {
              a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v28 > 1), v19 + 5, 1, a8);
            }
            *((void *)a8 + 2) = v29;
            *(void *)&a8[8 * v27 + 32] = a6;
            if (v19 <= 2)
            {
              unint64_t v30 = *((void *)a8 + 3);
              unint64_t v31 = v19 + 6;
              if (v29 >= v30 >> 1) {
                a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v30 > 1), v19 + 6, 1, a8);
              }
              *((void *)a8 + 2) = v31;
              *(void *)&a8[8 * v29 + 32] = a6;
              if (v19 <= 1)
              {
                unint64_t v32 = *((void *)a8 + 3);
                unint64_t v33 = v19 + 7;
                if (v31 >= v32 >> 1) {
                  a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v32 > 1), v19 + 7, 1, a8);
                }
                *((void *)a8 + 2) = v33;
                *(void *)&a8[8 * v31 + 32] = a6;
                if (!v19)
                {
                  unint64_t v34 = *((void *)a8 + 3);
                  if (v33 >= v34 >> 1) {
                    a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v34 > 1), 8, 1, a8);
                  }
                  *((void *)a8 + 2) = 8;
                  *(void *)&a8[8 * v33 + 32] = a6;
                }
              }
            }
          }
        }
      }
      a7 = v38;
    }
    a6 = v39;
LABEL_33:
    unint64_t v21 = *((void *)a8 + 4);
    *a1 = *((void *)a8 + 5);
    unint64_t v20 = *((void *)a8 + 2);
    if (v20 < 3)
    {
      __break(1u);
LABEL_41:
      __break(1u);
LABEL_42:
      __break(1u);
LABEL_43:
      __break(1u);
LABEL_44:
      __break(1u);
      goto LABEL_45;
    }
    *a2 = *((void *)a8 + 6);
    if (v20 == 3) {
      goto LABEL_41;
    }
    *a3 = *((void *)a8 + 7);
    if (v20 < 5) {
      goto LABEL_42;
    }
    *a4 = *((void *)a8 + 8);
    if (v20 == 5) {
      goto LABEL_43;
    }
    *a5 = *((void *)a8 + 9);
    BOOL v35 = v20 == 7;
    if (v20 < 7) {
      goto LABEL_44;
    }
    unint64_t v20 = *((void *)a8 + 10);
    *a6 = v20;
    if (!v35) {
      break;
    }
LABEL_45:
    __break(1u);
LABEL_46:
    a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v20 > 1), v21, 1, a8);
  }
  uint64_t v36 = *((void *)a8 + 11);
  swift_bridgeObjectRelease();
  *a7 = v36;
  return v21;
}

unint64_t specialized static BNNS.arrayToTuple<A>(_:fillValue:)(_OWORD *a1, _OWORD *a2, _OWORD *a3, _OWORD *a4, _OWORD *a5, _OWORD *a6, void *a7, char *a8, _OWORD *a9, _OWORD *a10)
{
  swift_bridgeObjectRetain();
  char isUniquelyReferenced_nonNull_native = (char *)swift_isUniquelyReferenced_nonNull_native();
  uint64_t v52 = a1;
  if (!isUniquelyReferenced_nonNull_native || *((void *)a8 + 3) <= 0xFuLL)
  {
    if (*((void *)a8 + 2) <= 8uLL) {
      int64_t v19 = 8;
    }
    else {
      int64_t v19 = *((void *)a8 + 2);
    }
    a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(isUniquelyReferenced_nonNull_native, v19, 0, a8);
  }
  unint64_t v20 = *((void *)a8 + 2);
  if (v20 > 7)
  {
    unint64_t v23 = a3;
    unint64_t v24 = a4;
    goto LABEL_30;
  }
  uint64_t v50 = a5;
  uint64_t v51 = a6;
  a5 = a9;
  a6 = a10;
  unint64_t v21 = *((void *)a8 + 3);
  unint64_t v22 = v20 + 1;
  unint64_t v23 = a3;
  if (v20 >= v21 >> 1) {
    goto LABEL_47;
  }
  while (1)
  {
    unint64_t v24 = a4;
    *((void *)a8 + 2) = v22;
    unint64_t v25 = &a8[16 * v20];
    *((void *)v25 + 4) = a5;
    *((void *)v25 + 5) = a6;
    if (v20 > 6) {
      goto LABEL_28;
    }
    unint64_t v26 = *((void *)a8 + 3);
    unint64_t v27 = v20 + 2;
    if (v22 >= v26 >> 1) {
      a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v26 > 1), v20 + 2, 1, a8);
    }
    *((void *)a8 + 2) = v27;
    unint64_t v28 = &a8[16 * v22];
    *((void *)v28 + 4) = a5;
    *((void *)v28 + 5) = a6;
    if (v20 > 5) {
      goto LABEL_28;
    }
    unint64_t v29 = *((void *)a8 + 3);
    unint64_t v30 = v20 + 3;
    if (v27 >= v29 >> 1) {
      a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v29 > 1), v20 + 3, 1, a8);
    }
    *((void *)a8 + 2) = v30;
    unint64_t v31 = &a8[16 * v27];
    *((void *)v31 + 4) = a5;
    *((void *)v31 + 5) = a6;
    if (v20 > 4) {
      goto LABEL_28;
    }
    unint64_t v32 = *((void *)a8 + 3);
    unint64_t v33 = v20 + 4;
    if (v30 >= v32 >> 1) {
      a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v32 > 1), v20 + 4, 1, a8);
    }
    *((void *)a8 + 2) = v33;
    unint64_t v34 = &a8[16 * v30];
    *((void *)v34 + 4) = a5;
    *((void *)v34 + 5) = a6;
    if (v20 > 3) {
      goto LABEL_28;
    }
    unint64_t v35 = *((void *)a8 + 3);
    unint64_t v36 = v20 + 5;
    if (v33 >= v35 >> 1) {
      a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v35 > 1), v20 + 5, 1, a8);
    }
    *((void *)a8 + 2) = v36;
    uint64_t v37 = &a8[16 * v33];
    *((void *)v37 + 4) = a5;
    *((void *)v37 + 5) = a6;
    if (v20 > 2) {
      goto LABEL_28;
    }
    unint64_t v38 = *((void *)a8 + 3);
    unint64_t v39 = v20 + 6;
    if (v36 >= v38 >> 1) {
      a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v38 > 1), v20 + 6, 1, a8);
    }
    *((void *)a8 + 2) = v39;
    unint64_t v40 = &a8[16 * v36];
    *((void *)v40 + 4) = a5;
    *((void *)v40 + 5) = a6;
    if (v20 > 1) {
      goto LABEL_28;
    }
    unint64_t v41 = *((void *)a8 + 3);
    unint64_t v42 = v20 + 7;
    if (v39 >= v41 >> 1) {
      a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v41 > 1), v20 + 7, 1, a8);
    }
    *((void *)a8 + 2) = v42;
    uint64_t v43 = &a8[16 * v39];
    *((void *)v43 + 4) = a5;
    *((void *)v43 + 5) = a6;
    if (v20)
    {
LABEL_28:
      a5 = v50;
      a6 = v51;
LABEL_30:
      uint64_t v44 = v52;
    }
    else
    {
      unint64_t v48 = *((void *)a8 + 3);
      if (v42 >= v48 >> 1) {
        a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v48 > 1), 8, 1, a8);
      }
      *((void *)a8 + 2) = 8;
      uint64_t v49 = &a8[16 * v42];
      *((void *)v49 + 4) = a5;
      *((void *)v49 + 5) = a6;
      a6 = v51;
      uint64_t v44 = v52;
      a5 = v50;
    }
    unint64_t v22 = *((void *)a8 + 4);
    a4 = (_OWORD *)*((void *)a8 + 5);
    _OWORD *v44 = *((_OWORD *)a8 + 3);
    unint64_t v21 = *((void *)a8 + 2);
    if (v21 < 3)
    {
      __break(1u);
LABEL_42:
      __break(1u);
LABEL_43:
      __break(1u);
LABEL_44:
      __break(1u);
LABEL_45:
      __break(1u);
      goto LABEL_46;
    }
    *a2 = *((_OWORD *)a8 + 4);
    if (v21 == 3) {
      goto LABEL_42;
    }
    *unint64_t v23 = *((_OWORD *)a8 + 5);
    if (v21 < 5) {
      goto LABEL_43;
    }
    *unint64_t v24 = *((_OWORD *)a8 + 6);
    if (v21 == 5) {
      goto LABEL_44;
    }
    *a5 = *((_OWORD *)a8 + 7);
    if (v21 < 7) {
      goto LABEL_45;
    }
    *a6 = *((_OWORD *)a8 + 8);
    if (v21 != 7) {
      break;
    }
LABEL_46:
    __break(1u);
LABEL_47:
    a8 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v21 > 1), v22, 1, a8);
  }
  uint64_t v45 = *((void *)a8 + 18);
  uint64_t v46 = *((void *)a8 + 19);
  swift_bridgeObjectRelease();
  *a7 = v45;
  a7[1] = v46;
  return v22;
}

uint64_t BNNS.PermuteLayer.__allocating_init(input:output:permutation:filterParameters:)(_OWORD *a1, long long *a2, char *a3, int a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v40 = *MEMORY[0x1E4F143B8];
  long long v10 = a2[8];
  long long v11 = a2[9];
  long long v12 = a2[6];
  __src[18] = a2[7];
  __src[19] = v10;
  long long v13 = a2[10];
  __src[20] = v11;
  __src[21] = v13;
  long long v14 = a2[4];
  long long v15 = a2[5];
  long long v16 = a2[2];
  __src[14] = a2[3];
  __src[15] = v14;
  __src[16] = v15;
  __src[17] = v12;
  long long v17 = *a2;
  __src[12] = a2[1];
  __src[13] = v16;
  long long v18 = a1[9];
  __src[8] = a1[8];
  __src[9] = v18;
  __src[10] = a1[10];
  __src[11] = v17;
  long long v19 = a1[5];
  __src[4] = a1[4];
  __src[5] = v19;
  long long v20 = a1[7];
  __src[6] = a1[6];
  __src[7] = v20;
  long long v21 = a1[1];
  __src[0] = *a1;
  __src[1] = v21;
  long long v22 = a1[3];
  __src[2] = a1[2];
  __src[3] = v22;
  unint64_t v23 = specialized static BNNS.arrayToTuple<A>(_:fillValue:)(v38, v35, &v34, &v33, &v32, &v31, &v30, a3, 0);
  swift_bridgeObjectRelease();
  memcpy(__dst, __src, sizeof(__dst));
  unint64_t v37 = v23;
  v38[1] = v35[0];
  v38[2] = v34;
  v38[3] = v33;
  float v38[4] = v32;
  v38[5] = v31;
  v38[6] = v30;
  if (a6 == 1)
  {
    unint64_t v24 = 0;
  }
  else
  {
    LODWORD(v35[0]) = a4;
    v35[1] = a5;
    v35[2] = a6;
    v35[3] = a7;
    unint64_t v24 = v35;
  }
  uint64_t v25 = MEMORY[0x1D2600050](__dst, v24);
  type metadata accessor for BNNS.PermuteLayer();
  uint64_t v26 = swift_allocObject();
  uint64_t v27 = v26;
  if (v25)
  {
    *(void *)(v26 + 16) = v25;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v27;
}

uint64_t type metadata accessor for BNNS.PermuteLayer()
{
  return self;
}

uint64_t BNNS.PermuteLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.PermuteLayer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

uint64_t BNNS.Layer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.UnaryLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.UnaryLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:)(size_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, _OWORD *a5)
{
  uint64_t v20 = *MEMORY[0x1E4F143B8];
  long long v6 = a4[9];
  *(_OWORD *)&v19.stride[7] = a4[8];
  *(_OWORD *)&v19.data_type = v6;
  *(_OWORD *)&v19.table_data_type = a4[10];
  long long v7 = a4[5];
  *(_OWORD *)&v19.size[7] = a4[4];
  *(_OWORD *)&v19.stride[1] = v7;
  long long v8 = a4[7];
  *(_OWORD *)&v19.stride[3] = a4[6];
  *(_OWORD *)&v19.stride[5] = v8;
  long long v9 = a4[1];
  *(_OWORD *)&v19.flags = *a4;
  *(_OWORD *)&v19.size[1] = v9;
  long long v10 = a4[3];
  *(_OWORD *)&v19.size[3] = a4[2];
  *(_OWORD *)&v19.size[5] = v10;
  long long v11 = a5[9];
  *(_OWORD *)&v18.stride[7] = a5[8];
  *(_OWORD *)&v18.data_type = v11;
  *(_OWORD *)&v18.table_data_type = a5[10];
  long long v12 = a5[5];
  *(_OWORD *)&v18.size[7] = a5[4];
  *(_OWORD *)&v18.stride[1] = v12;
  long long v13 = a5[7];
  *(_OWORD *)&v18.stride[3] = a5[6];
  *(_OWORD *)&v18.stride[5] = v13;
  long long v14 = a5[1];
  *(_OWORD *)&v18.flags = *a5;
  *(_OWORD *)&v18.size[1] = v14;
  long long v15 = a5[3];
  *(_OWORD *)&v18.size[3] = a5[2];
  *(_OWORD *)&v18.size[5] = v15;
  uint64_t result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(0, v5, a1, a2, &v18, (uint64_t)a5, a3, &v19, (uint64_t)a4, 0);
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v17 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t BNNS.Layer.__allocating_init(bnnsFilter:)(uint64_t a1)
{
  uint64_t result = swift_allocObject();
  if (a1)
  {
    *(void *)(result + 16) = a1;
  }
  else
  {
    swift_deallocPartialClassInstance();
    return 0;
  }
  return result;
}

uint64_t BNNS.Layer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

uint64_t BNNS.Layer.bnnsFilter.getter()
{
  return *(void *)(v0 + 16);
}

uint64_t BNNS.BinaryLayer.apply(batchSize:inputA:inputB:output:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return specialized static BNNS.layerApply(_:batchSize:inputA:inputB:output:)(v4, a1, a2, a3, a4);
}

uint64_t BNNS.BinaryLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, _OWORD *a5, _OWORD *a6, _OWORD *a7)
{
  uint64_t v28 = *MEMORY[0x1E4F143B8];
  long long v8 = a5[9];
  *(_OWORD *)&v27.stride[7] = a5[8];
  *(_OWORD *)&v27.data_type = v8;
  *(_OWORD *)&v27.table_data_type = a5[10];
  long long v9 = a5[5];
  *(_OWORD *)&v27.size[7] = a5[4];
  *(_OWORD *)&v27.stride[1] = v9;
  long long v10 = a5[7];
  *(_OWORD *)&v27.stride[3] = a5[6];
  *(_OWORD *)&v27.stride[5] = v10;
  long long v11 = a5[1];
  *(_OWORD *)&v27.flags = *a5;
  *(_OWORD *)&v27.size[1] = v11;
  long long v12 = a5[3];
  *(_OWORD *)&v27.size[3] = a5[2];
  *(_OWORD *)&v27.size[5] = v12;
  long long v13 = a6[9];
  *(_OWORD *)&v26.stride[7] = a6[8];
  *(_OWORD *)&v26.data_type = v13;
  *(_OWORD *)&v26.table_data_type = a6[10];
  long long v14 = a6[5];
  *(_OWORD *)&v26.size[7] = a6[4];
  *(_OWORD *)&v26.stride[1] = v14;
  long long v15 = a6[7];
  *(_OWORD *)&v26.stride[3] = a6[6];
  *(_OWORD *)&v26.stride[5] = v15;
  long long v16 = a6[1];
  *(_OWORD *)&v26.flags = *a6;
  *(_OWORD *)&v26.size[1] = v16;
  long long v17 = a6[3];
  *(_OWORD *)&v26.size[3] = a6[2];
  *(_OWORD *)&v26.size[5] = v17;
  long long v18 = a7[9];
  *(_OWORD *)&v25.stride[7] = a7[8];
  *(_OWORD *)&v25.data_type = v18;
  *(_OWORD *)&v25.table_data_type = a7[10];
  long long v19 = a7[5];
  *(_OWORD *)&v25.size[7] = a7[4];
  *(_OWORD *)&v25.stride[1] = v19;
  long long v20 = a7[7];
  *(_OWORD *)&v25.stride[3] = a7[6];
  *(_OWORD *)&v25.stride[5] = v20;
  long long v21 = a7[1];
  *(_OWORD *)&v25.flags = *a7;
  *(_OWORD *)&v25.size[1] = v21;
  long long v22 = a7[3];
  *(_OWORD *)&v25.size[3] = a7[2];
  *(_OWORD *)&v25.size[5] = v22;
  uint64_t result = closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingWeightsGradient:generatingBiasGradient:)(0, v7, a1, a2, &v26, (uint64_t)a6, a3, &v25, (uint64_t)a7, a4, &v27, (uint64_t)a5, 0);
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *unint64_t v24 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t BNNS.UnaryLayer.__allocating_init(bnnsFilter:)(uint64_t a1)
{
  uint64_t v2 = swift_allocObject();
  uint64_t v3 = v2;
  if (a1)
  {
    *(void *)(v2 + 16) = a1;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v3;
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(BNNSNDArrayDescriptor *a1, uint64_t a2, size_t a3, uint64_t a4, BNNSNDArrayDescriptor *a5, uint64_t a6, uint64_t a7, const BNNSNDArrayDescriptor *a8, uint64_t a9, BNNSNDArrayDescriptor *weights_delta)
{
  filter = *(void **)(a2 + 16);
  outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v88, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v88, (uint64_t)&v89, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  uint64_t v46 = v89;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v84);
  outlined init with take of BNNS.Shape((uint64_t)v84, (uint64_t)v85);
  outlined init with take of BNNS.Shape((uint64_t)v85, (uint64_t)v86);
  BNNS.Shape.size.getter((uint64_t)&v76);
  unint64_t v10 = v76;
  unint64_t v11 = v77;
  unint64_t v12 = v78;
  unint64_t v13 = v79;
  unint64_t v14 = v80;
  unint64_t v15 = v81;
  unint64_t v16 = v82;
  unint64_t v17 = v83;
  outlined init with take of BNNS.Shape((uint64_t)v85, (uint64_t)v86);
  BNNS.Shape.stride.getter((uint64_t)&v76);
  unint64_t in_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v10, v11, v12, v13, v14, v15, v16, v17, v76, v77, v78, v79, v80, v81, v82, v83);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v76);
  outlined init with take of BNNS.Shape((uint64_t)&v76, (uint64_t)v86);
  outlined init with take of BNNS.Shape((uint64_t)v86, (uint64_t)v75);
  BNNS.Shape.size.getter((uint64_t)&v67);
  unint64_t v18 = v67;
  unint64_t v19 = v68;
  unint64_t v20 = v69;
  unint64_t v21 = v70;
  unint64_t v22 = v71;
  unint64_t v23 = v72;
  unint64_t v24 = v73;
  unint64_t v25 = v74;
  outlined init with take of BNNS.Shape((uint64_t)v86, (uint64_t)v75);
  BNNS.Shape.stride.getter((uint64_t)&v67);
  size_t v43 = specialized static BNNS.calculateBatchStride(size:stride:)(v18, v19, v20, v21, v22, v23, v24, v25, v67, v68, v69, v70, v71, v72, v73, v74);
  outlined init with take of BNNSNDArrayDescriptor?(a7 + 136, (uint64_t)v87, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v87, (uint64_t)&v90, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  unint64_t v42 = v90;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v66);
  outlined init with take of BNNS.Shape((uint64_t)v66, (uint64_t)&v67);
  outlined init with take of BNNS.Shape((uint64_t)&v67, (uint64_t)v75);
  BNNS.Shape.size.getter((uint64_t)&v58);
  unint64_t v26 = v58;
  unint64_t v27 = v59;
  unint64_t v28 = v60;
  unint64_t v29 = v61;
  unint64_t v30 = v62;
  unint64_t v31 = v63;
  unint64_t v33 = v64;
  unint64_t v32 = v65;
  outlined init with take of BNNS.Shape((uint64_t)&v67, (uint64_t)v75);
  BNNS.Shape.stride.getter((uint64_t)&v58);
  size_t v34 = specialized static BNNS.calculateBatchStride(size:stride:)(v26, v27, v28, v29, v30, v31, v33, v32, v58, v59, v60, v61, v62, v63, v64, v65);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v58);
  outlined init with take of BNNS.Shape((uint64_t)&v58, (uint64_t)v75);
  outlined init with take of BNNS.Shape((uint64_t)v75, (uint64_t)v57);
  BNNS.Shape.size.getter((uint64_t)&v52);
  long long v35 = v52;
  long long v36 = v53;
  long long v37 = v54;
  unint64_t v39 = v55;
  unint64_t v38 = v56;
  outlined init with take of BNNS.Shape((uint64_t)v75, (uint64_t)v57);
  BNNS.Shape.stride.getter((uint64_t)&v52);
  size_t v40 = specialized static BNNS.calculateBatchStride(size:stride:)(v35, *((unint64_t *)&v35 + 1), v36, *((unint64_t *)&v36 + 1), v37, *((unint64_t *)&v37 + 1), v39, v38, v52, *((unint64_t *)&v52 + 1), v53, *((unint64_t *)&v53 + 1), v54, *((unint64_t *)&v54 + 1), v55, v56);
  return BNNSFilterApplyBackwardBatch(filter, a3, v46, in_stride, a5, v43, v42, v34, a8, v40, weights_delta, a1);
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingWeightsGradient:generatingBiasGradient:)(BNNSNDArrayDescriptor *a1, uint64_t a2, size_t a3, uint64_t a4, BNNSNDArrayDescriptor *a5, uint64_t a6, uint64_t a7, BNNSNDArrayDescriptor *a8, uint64_t a9, uint64_t a10, const BNNSNDArrayDescriptor *out_delta, uint64_t a12, BNNSNDArrayDescriptor *weights_delta)
{
  filter = *(void **)(a2 + 16);
  outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v129, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v129, (uint64_t)&v130, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  unint64_t v68 = v130;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v124);
  outlined init with take of BNNS.Shape((uint64_t)v124, (uint64_t)v125);
  outlined init with take of BNNS.Shape((uint64_t)v125, (uint64_t)v126);
  BNNS.Shape.size.getter((uint64_t)&v116);
  unint64_t v13 = v116;
  unint64_t v14 = v117;
  unint64_t v15 = v118;
  unint64_t v16 = v119;
  unint64_t v17 = v120;
  unint64_t v18 = v121;
  unint64_t v19 = v122;
  unint64_t v20 = v123;
  outlined init with take of BNNS.Shape((uint64_t)v125, (uint64_t)v126);
  BNNS.Shape.stride.getter((uint64_t)&v116);
  size_t v67 = specialized static BNNS.calculateBatchStride(size:stride:)(v13, v14, v15, v16, v17, v18, v19, v20, v116, v117, v118, v119, v120, v121, v122, v123);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v116);
  outlined init with take of BNNS.Shape((uint64_t)&v116, (uint64_t)v126);
  outlined init with take of BNNS.Shape((uint64_t)v126, (uint64_t)v115);
  BNNS.Shape.size.getter((uint64_t)&v107);
  unint64_t v21 = v107;
  unint64_t v22 = v108;
  unint64_t v23 = v109;
  unint64_t v24 = v110;
  unint64_t v25 = v111;
  unint64_t v26 = v112;
  unint64_t v27 = v113;
  unint64_t v28 = v114;
  outlined init with take of BNNS.Shape((uint64_t)v126, (uint64_t)v115);
  BNNS.Shape.stride.getter((uint64_t)&v107);
  size_t v66 = specialized static BNNS.calculateBatchStride(size:stride:)(v21, v22, v23, v24, v25, v26, v27, v28, v107, v108, v109, v110, v111, v112, v113, v114);
  outlined init with take of BNNSNDArrayDescriptor?(a7 + 136, (uint64_t)v128, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v128, (uint64_t)&v131, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  unint64_t v65 = v131;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v106);
  outlined init with take of BNNS.Shape((uint64_t)v106, (uint64_t)&v107);
  outlined init with take of BNNS.Shape((uint64_t)&v107, (uint64_t)v115);
  BNNS.Shape.size.getter((uint64_t)&v98);
  unint64_t v29 = v98;
  unint64_t v30 = v99;
  unint64_t v31 = v100;
  unint64_t v32 = v101;
  unint64_t v33 = v102;
  unint64_t v34 = v103;
  unint64_t v35 = v104;
  unint64_t v36 = v105;
  outlined init with take of BNNS.Shape((uint64_t)&v107, (uint64_t)v115);
  BNNS.Shape.stride.getter((uint64_t)&v98);
  size_t v63 = specialized static BNNS.calculateBatchStride(size:stride:)(v29, v30, v31, v32, v33, v34, v35, v36, v98, v99, v100, v101, v102, v103, v104, v105);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v98);
  outlined init with take of BNNS.Shape((uint64_t)&v98, (uint64_t)v115);
  outlined init with take of BNNS.Shape((uint64_t)v115, (uint64_t)v97);
  BNNS.Shape.size.getter((uint64_t)&v89);
  unint64_t v37 = v89;
  unint64_t v38 = v90;
  unint64_t v39 = v91;
  unint64_t v40 = v92;
  unint64_t v41 = v93;
  unint64_t v42 = v94;
  unint64_t v43 = v95;
  unint64_t v44 = v96;
  outlined init with take of BNNS.Shape((uint64_t)v115, (uint64_t)v97);
  BNNS.Shape.stride.getter((uint64_t)&v89);
  size_t v64 = specialized static BNNS.calculateBatchStride(size:stride:)(v37, v38, v39, v40, v41, v42, v43, v44, v89, v90, v91, v92, v93, v94, v95, v96);
  outlined init with take of BNNSNDArrayDescriptor?(a10 + 136, (uint64_t)v127, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v127, (uint64_t)&v132, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  out = v132;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v88);
  outlined init with take of BNNS.Shape((uint64_t)v88, (uint64_t)&v89);
  outlined init with take of BNNS.Shape((uint64_t)&v89, (uint64_t)v97);
  BNNS.Shape.size.getter((uint64_t)&v80);
  unint64_t v45 = v80;
  unint64_t v46 = v81;
  unint64_t v47 = v82;
  unint64_t v48 = v83;
  unint64_t v49 = v84;
  unint64_t v50 = v85;
  unint64_t v52 = v86;
  unint64_t v51 = v87;
  outlined init with take of BNNS.Shape((uint64_t)&v89, (uint64_t)v97);
  BNNS.Shape.stride.getter((uint64_t)&v80);
  size_t v53 = specialized static BNNS.calculateBatchStride(size:stride:)(v45, v46, v47, v48, v49, v50, v52, v51, v80, v81, v82, v83, v84, v85, v86, v87);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v80);
  outlined init with take of BNNS.Shape((uint64_t)&v80, (uint64_t)v97);
  outlined init with take of BNNS.Shape((uint64_t)v97, (uint64_t)v79);
  BNNS.Shape.size.getter((uint64_t)&v74);
  long long v54 = v74;
  long long v55 = v75;
  long long v56 = v76;
  unint64_t v58 = v77;
  unint64_t v57 = v78;
  outlined init with take of BNNS.Shape((uint64_t)v97, (uint64_t)v79);
  BNNS.Shape.stride.getter((uint64_t)&v74);
  size_t v59 = specialized static BNNS.calculateBatchStride(size:stride:)(v54, *((unint64_t *)&v54 + 1), v55, *((unint64_t *)&v55 + 1), v56, *((unint64_t *)&v56 + 1), v58, v57, v74, *((unint64_t *)&v74 + 1), v75, *((unint64_t *)&v75 + 1), v76, *((unint64_t *)&v76 + 1), v77, v78);
  return BNNSFilterApplyBackwardTwoInputBatch(filter, a3, v68, v67, a5, v66, v65, v63, a8, v64, out, v53, out_delta, v59, weights_delta, a1);
}

uint64_t closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingWeightsGradient:)(BNNSNDArrayDescriptor *a1, uint64_t a2, size_t a3, uint64_t a4, uint64_t a5, const BNNSNDArrayDescriptor *a6)
{
  unint64_t v34 = *(void **)(a2 + 16);
  outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v65, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v65, (uint64_t)&v66, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  unint64_t v33 = v66;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v62);
  outlined init with take of BNNS.Shape((uint64_t)v62, (uint64_t)v63);
  outlined init with take of BNNS.Shape((uint64_t)v63, (uint64_t)v61);
  BNNS.Shape.size.getter((uint64_t)&v53);
  unint64_t v7 = v53;
  unint64_t v8 = v54;
  unint64_t v9 = v55;
  unint64_t v10 = v56;
  unint64_t v11 = v57;
  unint64_t v12 = v58;
  unint64_t v13 = v59;
  unint64_t v14 = v60;
  outlined init with take of BNNS.Shape((uint64_t)v63, (uint64_t)v61);
  BNNS.Shape.stride.getter((uint64_t)&v53);
  size_t v32 = specialized static BNNS.calculateBatchStride(size:stride:)(v7, v8, v9, v10, v11, v12, v13, v14, v53, v54, v55, v56, v57, v58, v59, v60);
  outlined init with take of BNNSNDArrayDescriptor?(a5 + 136, (uint64_t)v64, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v64, (uint64_t)&v67, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  unint64_t v31 = v67;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v52);
  outlined init with take of BNNS.Shape((uint64_t)v52, (uint64_t)&v53);
  outlined init with take of BNNS.Shape((uint64_t)&v53, (uint64_t)v61);
  BNNS.Shape.size.getter((uint64_t)&v44);
  unint64_t v15 = v44;
  unint64_t v16 = v45;
  unint64_t v17 = v46;
  unint64_t v18 = v47;
  unint64_t v19 = v48;
  unint64_t v20 = v49;
  unint64_t v22 = v50;
  unint64_t v21 = v51;
  outlined init with take of BNNS.Shape((uint64_t)&v53, (uint64_t)v61);
  BNNS.Shape.stride.getter((uint64_t)&v44);
  size_t v23 = specialized static BNNS.calculateBatchStride(size:stride:)(v15, v16, v17, v18, v19, v20, v22, v21, v44, v45, v46, v47, v48, v49, v50, v51);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v44);
  outlined init with take of BNNS.Shape((uint64_t)&v44, (uint64_t)v61);
  outlined init with take of BNNS.Shape((uint64_t)v61, (uint64_t)v43);
  BNNS.Shape.size.getter((uint64_t)&v38);
  long long v24 = v38;
  long long v25 = v39;
  long long v26 = v40;
  unint64_t v27 = v41;
  unint64_t v28 = v42;
  outlined init with take of BNNS.Shape((uint64_t)v61, (uint64_t)v43);
  BNNS.Shape.stride.getter((uint64_t)&v38);
  size_t v29 = specialized static BNNS.calculateBatchStride(size:stride:)(v24, *((unint64_t *)&v24 + 1), v25, *((unint64_t *)&v25 + 1), v26, *((unint64_t *)&v26 + 1), v27, v28, v38, *((unint64_t *)&v38 + 1), v39, *((unint64_t *)&v39 + 1), v40, *((unint64_t *)&v40 + 1), v41, v42);
  return BNNSFilterApplyBackwardBatch(v34, a3, v33, v32, 0, 0, v31, v23, a6, v29, a1, 0);
}

uint64_t specialized static BNNS.layerApply(_:batchSize:input:output:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4)
{
  outlined init with take of BNNSNDArrayDescriptor?(a3 + 136, (uint64_t)v40, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v40, (uint64_t)&v41, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  unint64_t v7 = v41;
  if (v41
    && (outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v39, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v39, (uint64_t)&v42, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), v42))
  {
    size_t v23 = v42;
    long long v24 = *(void **)(a1 + 16);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v36);
    outlined init with take of BNNS.Shape((uint64_t)v36, (uint64_t)v37);
    outlined init with take of BNNS.Shape((uint64_t)v37, (uint64_t)v35);
    BNNS.Shape.size.getter((uint64_t)&v30);
    long long v8 = v30;
    long long v9 = v31;
    long long v10 = v32;
    unint64_t v19 = v34;
    size_t in_stride = v33;
    outlined init with take of BNNS.Shape((uint64_t)v37, (uint64_t)v35);
    BNNS.Shape.stride.getter((uint64_t)&v30);
    unint64_t in_stridea = specialized static BNNS.calculateBatchStride(size:stride:)(v8, *((unint64_t *)&v8 + 1), v9, *((unint64_t *)&v9 + 1), v10, *((unint64_t *)&v10 + 1), in_stride, v19, v30, *((unint64_t *)&v30 + 1), v31, *((unint64_t *)&v31 + 1), v32, *((unint64_t *)&v32 + 1), v33, v34);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v35);
    outlined init with take of BNNS.Shape((uint64_t)v35, (uint64_t)v38);
    outlined init with take of BNNS.Shape((uint64_t)v38, (uint64_t)&v30);
    BNNS.Shape.size.getter((uint64_t)&v25);
    long long v11 = v25;
    long long v12 = v26;
    long long v13 = v27;
    unint64_t v14 = v28;
    unint64_t v20 = v29;
    outlined init with take of BNNS.Shape((uint64_t)v38, (uint64_t)&v30);
    BNNS.Shape.stride.getter((uint64_t)&v25);
    size_t v15 = specialized static BNNS.calculateBatchStride(size:stride:)(v11, *((unint64_t *)&v11 + 1), v12, *((unint64_t *)&v12 + 1), v13, *((unint64_t *)&v13 + 1), v14, v20, v25, *((unint64_t *)&v25 + 1), v26, *((unint64_t *)&v26 + 1), v27, *((unint64_t *)&v27 + 1), v28, v29);
    uint64_t result = BNNSFilterApplyBatch(v24, a2, v7, in_stridea, v23, v15);
    if (!result) {
      return result;
    }
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *unint64_t v17 = 0;
  }
  else
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *unint64_t v18 = 2;
  }
  return swift_willThrow();
}

uint64_t specialized static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4, _OWORD *a5, _OWORD *a6, uint64_t a7, uint64_t a8)
{
  uint64_t v35 = *MEMORY[0x1E4F143B8];
  long long v15 = a5[9];
  *(_OWORD *)&v31.stride[7] = a5[8];
  *(_OWORD *)&v31.data_type = v15;
  *(_OWORD *)&v31.table_data_type = a5[10];
  long long v16 = a5[5];
  *(_OWORD *)&v31.size[7] = a5[4];
  *(_OWORD *)&v31.stride[1] = v16;
  long long v17 = a5[7];
  *(_OWORD *)&v31.stride[3] = a5[6];
  *(_OWORD *)&v31.stride[5] = v17;
  long long v18 = a5[1];
  *(_OWORD *)&v31.flags = *a5;
  *(_OWORD *)&v31.size[1] = v18;
  long long v19 = a5[3];
  *(_OWORD *)&v31.size[3] = a5[2];
  *(_OWORD *)&v31.size[5] = v19;
  long long v20 = a6[9];
  *(_OWORD *)&v30.stride[7] = a6[8];
  *(_OWORD *)&v30.data_type = v20;
  *(_OWORD *)&v30.table_data_type = a6[10];
  long long v21 = a6[5];
  *(_OWORD *)&v30.size[7] = a6[4];
  *(_OWORD *)&v30.stride[1] = v21;
  long long v22 = a6[7];
  *(_OWORD *)&v30.stride[3] = a6[6];
  *(_OWORD *)&v30.stride[5] = v22;
  long long v23 = a6[1];
  *(_OWORD *)&v30.flags = *a6;
  *(_OWORD *)&v30.size[1] = v23;
  long long v24 = a6[3];
  *(_OWORD *)&v30.size[3] = a6[2];
  *(_OWORD *)&v30.size[5] = v24;
  outlined init with take of BNNSNDArrayDescriptor?(a7, (uint64_t)&v34, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)&v34) != 1)
  {
    BNNSNDArrayDescriptor v29 = v34;
    outlined init with take of BNNSNDArrayDescriptor?(a8, (uint64_t)&v32, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)&v32) != 1)
    {
      BNNSNDArrayDescriptor v28 = v32;
      uint64_t result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(&v28, a1, a2, a3, &v30, (uint64_t)a6, a4, &v31, (uint64_t)a5, &v29);
      goto LABEL_10;
    }
    long long v27 = &v29;
    goto LABEL_6;
  }
  outlined init with take of BNNSNDArrayDescriptor?(a8, (uint64_t)&v33, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)&v33) == 1)
  {
    long long v27 = 0;
LABEL_6:
    uint64_t result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(0, a1, a2, a3, &v30, (uint64_t)a6, a4, &v31, (uint64_t)a5, v27);
    goto LABEL_10;
  }
  BNNSNDArrayDescriptor v29 = v33;
  uint64_t result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(&v29, a1, a2, a3, &v30, (uint64_t)a6, a4, &v31, (uint64_t)a5, 0);
LABEL_10:
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v26 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t specialized static BNNS.layerApply(_:batchSize:inputA:inputB:output:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  outlined init with take of BNNSNDArrayDescriptor?(a3 + 136, (uint64_t)v64, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v64, (uint64_t)&v65, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  long long v9 = v65;
  if (v65
    && (outlined init with take of BNNSNDArrayDescriptor?(a4 + 136, (uint64_t)v63, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v63, (uint64_t)&v66, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), (long long v10 = v66) != 0)&& (outlined init with take of BNNSNDArrayDescriptor?(a5 + 136, (uint64_t)v62, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v62, (uint64_t)&v67, &demangling cache variable for type metadata for UnsafeMutableRawPointer?), v67))
  {
    long long v38 = *(void **)(a1 + 16);
    unint64_t v68 = v67;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v58);
    outlined init with take of BNNS.Shape((uint64_t)v58, (uint64_t)v59);
    outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v57);
    BNNS.Shape.size.getter((uint64_t)&v49);
    size_t v36 = a2;
    unint64_t v37 = v10;
    unint64_t v12 = v49;
    unint64_t v11 = v50;
    unint64_t v14 = v51;
    unint64_t v13 = v52;
    uint64_t v35 = v9;
    unint64_t v16 = v53;
    unint64_t v15 = v54;
    size_t inA_stride = v55;
    size_t inB_stride = v56;
    outlined init with take of BNNS.Shape((uint64_t)v59, (uint64_t)v57);
    BNNS.Shape.stride.getter((uint64_t)&v49);
    unint64_t inA_stridea = specialized static BNNS.calculateBatchStride(size:stride:)(v12, v11, v14, v13, v16, v15, inA_stride, inB_stride, v49, v50, v51, v52, v53, v54, v55, v56);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v57);
    outlined init with take of BNNS.Shape((uint64_t)v57, (uint64_t)v60);
    outlined init with take of BNNS.Shape((uint64_t)v60, (uint64_t)&v49);
    BNNS.Shape.size.getter((uint64_t)&v44);
    long long v17 = v44;
    long long v18 = v45;
    long long v19 = v46;
    unint64_t v20 = v47;
    size_t inB_stridea = v48;
    outlined init with take of BNNS.Shape((uint64_t)v60, (uint64_t)&v49);
    BNNS.Shape.stride.getter((uint64_t)&v44);
    unint64_t inB_strideb = specialized static BNNS.calculateBatchStride(size:stride:)(v17, *((unint64_t *)&v17 + 1), v18, *((unint64_t *)&v18 + 1), v19, *((unint64_t *)&v19 + 1), v20, inB_stridea, v44, *((unint64_t *)&v44 + 1), v45, *((unint64_t *)&v45 + 1), v46, *((unint64_t *)&v46 + 1), v47, v48);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v49);
    outlined init with take of BNNS.Shape((uint64_t)&v49, (uint64_t)v61);
    outlined init with take of BNNS.Shape((uint64_t)v61, (uint64_t)&v44);
    BNNS.Shape.size.getter((uint64_t)&v39);
    long long v21 = v39;
    long long v22 = v40;
    long long v23 = v41;
    unint64_t v24 = v42;
    unint64_t v32 = v43;
    outlined init with take of BNNS.Shape((uint64_t)v61, (uint64_t)&v44);
    BNNS.Shape.stride.getter((uint64_t)&v39);
    size_t v25 = specialized static BNNS.calculateBatchStride(size:stride:)(v21, *((unint64_t *)&v21 + 1), v22, *((unint64_t *)&v22 + 1), v23, *((unint64_t *)&v23 + 1), v24, v32, v39, *((unint64_t *)&v39 + 1), v40, *((unint64_t *)&v40 + 1), v41, *((unint64_t *)&v41 + 1), v42, v43);
    uint64_t result = BNNSFilterApplyTwoInputBatch(v38, v36, v35, inA_stridea, v37, inB_strideb, v68, v25);
    if (!result) {
      return result;
    }
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v27 = 0;
  }
  else
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    unsigned char *v28 = 2;
  }
  return swift_willThrow();
}

uint64_t type metadata accessor for BNNS.Layer()
{
  return self;
}

uint64_t method lookup function for BNNS.Layer(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.Layer);
}

uint64_t type metadata accessor for BNNS.UnaryLayer()
{
  return self;
}

uint64_t method lookup function for BNNS.UnaryLayer(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.UnaryLayer);
}

uint64_t dispatch thunk of BNNS.UnaryLayer.apply(batchSize:input:output:)(uint64_t a1, uint64_t *a2, uint64_t *a3)
{
  uint64_t v4 = a2[17];
  int v5 = *((_DWORD *)a2 + 36);
  uint64_t v6 = a2[19];
  int v7 = *((_DWORD *)a2 + 40);
  uint64_t v8 = a3[17];
  int v9 = *((_DWORD *)a3 + 36);
  uint64_t v10 = a3[19];
  int v11 = *((_DWORD *)a3 + 40);
  unint64_t v12 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *))(*(void *)v3 + 96);
  uint64_t v28 = *a2;
  long long v29 = *(_OWORD *)(a2 + 1);
  long long v30 = *(_OWORD *)(a2 + 3);
  long long v31 = *(_OWORD *)(a2 + 5);
  long long v32 = *(_OWORD *)(a2 + 7);
  long long v33 = *(_OWORD *)(a2 + 9);
  long long v34 = *(_OWORD *)(a2 + 11);
  long long v35 = *(_OWORD *)(a2 + 13);
  long long v36 = *(_OWORD *)(a2 + 15);
  uint64_t v37 = v4;
  int v38 = v5;
  uint64_t v39 = v6;
  int v40 = v7;
  uint64_t v41 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v14 = *a3;
  long long v15 = *(_OWORD *)(a3 + 1);
  long long v16 = *(_OWORD *)(a3 + 3);
  long long v17 = *(_OWORD *)(a3 + 5);
  long long v18 = *(_OWORD *)(a3 + 7);
  long long v19 = *(_OWORD *)(a3 + 9);
  long long v20 = *(_OWORD *)(a3 + 11);
  long long v21 = *(_OWORD *)(a3 + 13);
  long long v22 = *(_OWORD *)(a3 + 15);
  uint64_t v23 = v8;
  int v24 = v9;
  uint64_t v25 = v10;
  int v26 = v11;
  uint64_t v27 = *(uint64_t *)((char *)a3 + 164);
  return v12(a1, &v28, &v14);
}

uint64_t dispatch thunk of BNNS.UnaryLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5)
{
  uint64_t v6 = a2[17];
  int v7 = *((_DWORD *)a2 + 36);
  uint64_t v8 = a2[19];
  int v9 = *((_DWORD *)a2 + 40);
  uint64_t v10 = a3[17];
  int v11 = *((_DWORD *)a3 + 36);
  uint64_t v12 = a3[19];
  int v13 = *((_DWORD *)a3 + 40);
  uint64_t v14 = a4[17];
  int v15 = *((_DWORD *)a4 + 36);
  uint64_t v16 = a4[19];
  int v17 = *((_DWORD *)a4 + 40);
  uint64_t v18 = a5[17];
  int v19 = *((_DWORD *)a5 + 36);
  uint64_t v20 = a5[19];
  int v21 = *((_DWORD *)a5 + 40);
  unint64_t v98 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(void *)v5 + 104);
  uint64_t v84 = *a2;
  long long v85 = *(_OWORD *)(a2 + 1);
  long long v86 = *(_OWORD *)(a2 + 3);
  long long v87 = *(_OWORD *)(a2 + 5);
  long long v88 = *(_OWORD *)(a2 + 7);
  long long v89 = *(_OWORD *)(a2 + 9);
  long long v90 = *(_OWORD *)(a2 + 11);
  long long v91 = *(_OWORD *)(a2 + 13);
  long long v92 = *(_OWORD *)(a2 + 15);
  uint64_t v93 = v6;
  int v94 = v7;
  uint64_t v95 = v8;
  int v96 = v9;
  uint64_t v97 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v70 = *a3;
  long long v71 = *(_OWORD *)(a3 + 1);
  long long v72 = *(_OWORD *)(a3 + 3);
  long long v73 = *(_OWORD *)(a3 + 5);
  long long v74 = *(_OWORD *)(a3 + 7);
  long long v75 = *(_OWORD *)(a3 + 9);
  long long v76 = *(_OWORD *)(a3 + 11);
  long long v22 = *(_OWORD *)(a3 + 15);
  long long v77 = *(_OWORD *)(a3 + 13);
  long long v23 = *(_OWORD *)(a4 + 1);
  long long v24 = *(_OWORD *)(a4 + 3);
  long long v25 = *(_OWORD *)(a4 + 5);
  long long v26 = *(_OWORD *)(a4 + 7);
  long long v27 = *(_OWORD *)(a4 + 9);
  long long v28 = *(_OWORD *)(a4 + 11);
  long long v29 = *(_OWORD *)(a4 + 13);
  uint64_t v30 = *a4;
  long long v31 = *(_OWORD *)(a4 + 15);
  long long v32 = *(_OWORD *)(a5 + 1);
  long long v33 = *(_OWORD *)(a5 + 3);
  long long v34 = *(_OWORD *)(a5 + 5);
  long long v35 = *(_OWORD *)(a5 + 7);
  long long v36 = *(_OWORD *)(a5 + 9);
  long long v37 = *(_OWORD *)(a5 + 11);
  long long v38 = *(_OWORD *)(a5 + 13);
  uint64_t v39 = *a5;
  long long v40 = *(_OWORD *)(a5 + 15);
  long long v78 = v22;
  uint64_t v79 = v10;
  int v80 = v11;
  uint64_t v81 = v12;
  int v82 = v13;
  uint64_t v83 = *(uint64_t *)((char *)a3 + 164);
  *(void *)&long long v22 = *(uint64_t *)((char *)a5 + 164);
  uint64_t v56 = v30;
  long long v57 = v23;
  *(void *)&long long v23 = *(uint64_t *)((char *)a4 + 164);
  long long v58 = v24;
  long long v59 = v25;
  long long v60 = v26;
  long long v61 = v27;
  long long v62 = v28;
  long long v63 = v29;
  long long v64 = v31;
  uint64_t v65 = v14;
  int v66 = v15;
  uint64_t v67 = v16;
  int v68 = v17;
  uint64_t v69 = v23;
  uint64_t v42 = v39;
  long long v43 = v32;
  long long v44 = v33;
  long long v45 = v34;
  long long v46 = v35;
  long long v47 = v36;
  long long v48 = v37;
  long long v49 = v38;
  long long v50 = v40;
  uint64_t v51 = v18;
  int v52 = v19;
  uint64_t v53 = v20;
  int v54 = v21;
  uint64_t v55 = v22;
  return v98(a1, &v84, &v70, &v56, &v42);
}

uint64_t type metadata accessor for BNNS.BinaryLayer()
{
  return self;
}

uint64_t method lookup function for BNNS.BinaryLayer(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.BinaryLayer);
}

uint64_t dispatch thunk of BNNS.BinaryLayer.apply(batchSize:inputA:inputB:output:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4)
{
  uint64_t v5 = a2[17];
  int v6 = *((_DWORD *)a2 + 36);
  uint64_t v7 = a2[19];
  int v8 = *((_DWORD *)a2 + 40);
  uint64_t v9 = a3[17];
  int v10 = *((_DWORD *)a3 + 36);
  uint64_t v11 = a3[19];
  int v12 = *((_DWORD *)a3 + 40);
  uint64_t v13 = a4[17];
  int v14 = *((_DWORD *)a4 + 36);
  uint64_t v15 = a4[19];
  int v16 = *((_DWORD *)a4 + 40);
  int v17 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *))(*(void *)v4 + 96);
  uint64_t v52 = *a2;
  long long v53 = *(_OWORD *)(a2 + 1);
  long long v54 = *(_OWORD *)(a2 + 3);
  long long v55 = *(_OWORD *)(a2 + 5);
  long long v56 = *(_OWORD *)(a2 + 7);
  long long v57 = *(_OWORD *)(a2 + 9);
  long long v58 = *(_OWORD *)(a2 + 11);
  long long v59 = *(_OWORD *)(a2 + 13);
  long long v60 = *(_OWORD *)(a2 + 15);
  uint64_t v61 = v5;
  int v62 = v6;
  uint64_t v63 = v7;
  int v64 = v8;
  uint64_t v65 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v38 = *a3;
  long long v39 = *(_OWORD *)(a3 + 1);
  long long v40 = *(_OWORD *)(a3 + 3);
  long long v41 = *(_OWORD *)(a3 + 5);
  long long v42 = *(_OWORD *)(a3 + 7);
  long long v43 = *(_OWORD *)(a3 + 9);
  long long v44 = *(_OWORD *)(a3 + 11);
  long long v45 = *(_OWORD *)(a3 + 13);
  long long v46 = *(_OWORD *)(a3 + 15);
  uint64_t v47 = v9;
  int v48 = v10;
  uint64_t v49 = v11;
  int v50 = v12;
  uint64_t v51 = *(uint64_t *)((char *)a3 + 164);
  uint64_t v24 = *a4;
  long long v25 = *(_OWORD *)(a4 + 1);
  long long v26 = *(_OWORD *)(a4 + 3);
  long long v18 = *(_OWORD *)(a4 + 7);
  long long v19 = *(_OWORD *)(a4 + 9);
  long long v20 = *(_OWORD *)(a4 + 11);
  long long v21 = *(_OWORD *)(a4 + 13);
  long long v22 = *(_OWORD *)(a4 + 15);
  long long v27 = *(_OWORD *)(a4 + 5);
  long long v28 = v18;
  long long v29 = v19;
  long long v30 = v20;
  long long v31 = v21;
  long long v32 = v22;
  uint64_t v33 = v13;
  int v34 = v14;
  uint64_t v35 = v15;
  int v36 = v16;
  uint64_t v37 = *(uint64_t *)((char *)a4 + 164);
  return v17(a1, &v52, &v38, &v24);
}

uint64_t dispatch thunk of BNNS.BinaryLayer.applyBackward(batchSize:inputA:inputB:output:outputGradient:generatingInputAGradient:generatingInputBGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t a4, uint64_t a5, uint64_t *a6, uint64_t *a7)
{
  uint64_t v76 = a2[17];
  int v75 = *((_DWORD *)a2 + 36);
  uint64_t v74 = a2[19];
  int v73 = *((_DWORD *)a2 + 40);
  uint64_t v72 = a3[17];
  int v71 = *((_DWORD *)a3 + 36);
  uint64_t v8 = a3[19];
  int v9 = *((_DWORD *)a3 + 40);
  uint64_t v10 = *(void *)(a4 + 136);
  int v11 = *(_DWORD *)(a4 + 144);
  uint64_t v12 = *(void *)(a4 + 152);
  int v13 = *(_DWORD *)(a4 + 160);
  uint64_t v14 = *(void *)(a5 + 136);
  int v15 = *(_DWORD *)(a5 + 144);
  uint64_t v16 = *(void *)(a5 + 152);
  int v17 = *(_DWORD *)(a5 + 160);
  uint64_t v18 = a6[17];
  int v19 = *((_DWORD *)a6 + 36);
  uint64_t v20 = a6[19];
  int v21 = *((_DWORD *)a6 + 40);
  uint64_t v22 = a7[17];
  int v23 = *((_DWORD *)a7 + 36);
  uint64_t v24 = a7[19];
  int v25 = *((_DWORD *)a7 + 40);
  long long v26 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(void *)v7 + 104);
  uint64_t v27 = *a2;
  long long v28 = *(_OWORD *)(a2 + 1);
  long long v29 = *(_OWORD *)(a2 + 3);
  long long v30 = *(_OWORD *)(a2 + 5);
  long long v31 = *(_OWORD *)(a2 + 7);
  long long v32 = *(_OWORD *)(a2 + 9);
  long long v33 = *(_OWORD *)(a2 + 13);
  long long v34 = *(_OWORD *)(a2 + 15);
  uint64_t v35 = *(uint64_t *)((char *)a2 + 164);
  long long v36 = *(_OWORD *)(a3 + 1);
  long long v37 = *(_OWORD *)(a3 + 3);
  long long v38 = *(_OWORD *)(a3 + 5);
  long long v39 = *(_OWORD *)(a3 + 7);
  long long v40 = *(_OWORD *)(a3 + 9);
  long long v41 = *(_OWORD *)(a3 + 11);
  long long v42 = *(_OWORD *)(a3 + 13);
  uint64_t v43 = *a3;
  long long v44 = *(_OWORD *)(a3 + 15);
  uint64_t v45 = *(uint64_t *)((char *)a3 + 164);
  long long v154 = *(_OWORD *)(a2 + 11);
  long long v155 = v33;
  long long v156 = v34;
  uint64_t v161 = v35;
  long long v149 = v28;
  long long v150 = v29;
  long long v151 = v30;
  long long v152 = v31;
  long long v153 = v32;
  long long v140 = v41;
  long long v141 = v42;
  long long v142 = v44;
  uint64_t v147 = v45;
  long long v46 = *(_OWORD *)(a4 + 8);
  long long v47 = *(_OWORD *)(a4 + 24);
  long long v48 = *(_OWORD *)(a4 + 40);
  long long v49 = *(_OWORD *)(a4 + 56);
  long long v50 = *(_OWORD *)(a4 + 72);
  long long v51 = *(_OWORD *)(a4 + 88);
  long long v52 = *(_OWORD *)(a4 + 104);
  *(void *)&long long v34 = *(void *)a4;
  long long v53 = *(_OWORD *)(a4 + 120);
  *(void *)&long long v41 = *(void *)(a4 + 164);
  long long v135 = v36;
  long long v136 = v37;
  long long v137 = v38;
  long long v138 = v39;
  long long v139 = v40;
  long long v126 = v51;
  long long v127 = v52;
  long long v128 = v53;
  uint64_t v133 = v41;
  long long v54 = *(_OWORD *)(a5 + 88);
  long long v121 = v46;
  long long v55 = *(_OWORD *)(a5 + 104);
  long long v122 = v47;
  long long v56 = *(_OWORD *)(a5 + 120);
  long long v123 = v48;
  *(void *)&long long v48 = *(void *)(a5 + 164);
  long long v124 = v49;
  long long v125 = v50;
  long long v112 = v54;
  long long v113 = v55;
  long long v114 = v56;
  uint64_t v119 = v48;
  long long v57 = *(_OWORD *)(a5 + 24);
  long long v58 = *(_OWORD *)(a5 + 40);
  long long v59 = *(_OWORD *)(a5 + 56);
  long long v60 = *(_OWORD *)(a5 + 72);
  *(void *)&long long v54 = *(void *)a5;
  long long v107 = *(_OWORD *)(a5 + 8);
  long long v108 = v57;
  long long v109 = v58;
  long long v110 = v59;
  long long v111 = v60;
  long long v98 = *(_OWORD *)(a6 + 11);
  long long v99 = *(_OWORD *)(a6 + 13);
  long long v100 = *(_OWORD *)(a6 + 15);
  uint64_t v105 = *(uint64_t *)((char *)a6 + 164);
  uint64_t v148 = v27;
  long long v61 = *(_OWORD *)(a6 + 1);
  long long v62 = *(_OWORD *)(a6 + 3);
  uint64_t v134 = v43;
  long long v63 = *(_OWORD *)(a6 + 5);
  uint64_t v120 = v34;
  long long v64 = *(_OWORD *)(a6 + 7);
  uint64_t v106 = v54;
  uint64_t v92 = *a6;
  long long v65 = *(_OWORD *)(a6 + 9);
  long long v93 = v61;
  long long v94 = v62;
  long long v95 = v63;
  long long v96 = v64;
  long long v97 = v65;
  uint64_t v78 = *a7;
  long long v66 = *(_OWORD *)(a7 + 3);
  long long v79 = *(_OWORD *)(a7 + 1);
  long long v80 = v66;
  long long v67 = *(_OWORD *)(a7 + 7);
  long long v81 = *(_OWORD *)(a7 + 5);
  long long v82 = v67;
  long long v68 = *(_OWORD *)(a7 + 11);
  long long v83 = *(_OWORD *)(a7 + 9);
  long long v84 = v68;
  long long v69 = *(_OWORD *)(a7 + 15);
  long long v85 = *(_OWORD *)(a7 + 13);
  long long v86 = v69;
  uint64_t v91 = *(uint64_t *)((char *)a7 + 164);
  uint64_t v157 = v76;
  int v158 = v75;
  uint64_t v159 = v74;
  int v160 = v73;
  uint64_t v143 = v72;
  int v144 = v71;
  uint64_t v145 = v8;
  int v146 = v9;
  uint64_t v129 = v10;
  int v130 = v11;
  uint64_t v131 = v12;
  int v132 = v13;
  uint64_t v115 = v14;
  int v116 = v15;
  uint64_t v117 = v16;
  int v118 = v17;
  uint64_t v101 = v18;
  int v102 = v19;
  uint64_t v103 = v20;
  int v104 = v21;
  uint64_t v87 = v22;
  int v88 = v23;
  uint64_t v89 = v24;
  int v90 = v25;
  return v26(a1, &v148, &v134, &v120, &v106, &v92, &v78);
}

uint64_t vImage.PixelBuffer<>.convert(to:)(uint64_t a1)
{
  uint64_t v1 = (uint64_t (*)(void *, void *, void))MEMORY[0x1E4F16FB8];

  return vImage.PixelBuffer<>.convert(to:)(a1, v1);
}

{
  uint64_t (*v1)(void *, void *, void);
  uint64_t vars8;

  uint64_t v1 = (uint64_t (*)(void *, void *, void))MEMORY[0x1E4F16FB0];

  return vImage.PixelBuffer<>.convert(to:)(a1, v1);
}

{
  uint64_t (*v1)(void *, void *, void);
  uint64_t vars8;

  uint64_t v1 = (uint64_t (*)(void *, void *, void))MEMORY[0x1E4F16FC0];

  return vImage.PixelBuffer<>.convert(to:)(a1, v1);
}

{
  uint64_t (*v1)(void *, void *, void);
  uint64_t vars8;

  uint64_t v1 = (uint64_t (*)(void *, void *, void))MEMORY[0x1E4F16FB0];

  return vImage.PixelBuffer<>.convert(to:)(a1, v1);
}

{
  uint64_t (*v1)(void *, void *, void);
  uint64_t vars8;

  uint64_t v1 = (uint64_t (*)(void *, void *, void))MEMORY[0x1E4F16FB8];

  return vImage.PixelBuffer<>.convert(to:)(a1, v1);
}

{
  uint64_t (*v1)(void *, void *, void);
  uint64_t vars8;

  uint64_t v1 = (uint64_t (*)(void *, void *, void))MEMORY[0x1E4F16FC0];

  return vImage.PixelBuffer<>.convert(to:)(a1, v1);
}

{
  uint64_t (*v1)(void *, void *, void);
  uint64_t vars8;

  uint64_t v1 = (uint64_t (*)(void *, void *, void))MEMORY[0x1E4F16FC0];

  return vImage.PixelBuffer<>.convert(to:)(a1, v1);
}

{
  uint64_t (*v1)(void *, void *, void);
  uint64_t vars8;

  uint64_t v1 = (uint64_t (*)(void *, void *, void))MEMORY[0x1E4F16FB0];

  return vImage.PixelBuffer<>.convert(to:)(a1, v1);
}

{
  uint64_t (*v1)(void *, void *, void);
  uint64_t vars8;

  uint64_t v1 = (uint64_t (*)(void *, void *, void))MEMORY[0x1E4F16FB8];

  return vImage.PixelBuffer<>.convert(to:)(a1, v1);
}

{
  uint64_t v1;
  void *v2;
  uint64_t v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  vImagePixelCount v9;
  size_t v10;
  uint64_t inited;
  void *v12;
  uint64_t v13;
  vImagePixelCount v14;
  vImagePixelCount v15;
  size_t v16;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v20;
  vImagePixelCount v21;
  vImagePixelCount v22;
  size_t v23;
  uint64_t v24;
  uint64_t v25;

  int v25 = *MEMORY[0x1E4F143B8];
  uint64_t v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v3 = v2[6];
  if (v3 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  uint64_t v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v3)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v4)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  int v6 = v5[6];
  if (v6 < 0)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (!v6)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v7)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (v3 != v6)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (v4 != v7)
  {
LABEL_34:
    __break(1u);
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  uint64_t v8 = (void *)v2[4];
  if (!v8)
  {
LABEL_42:
    __break(1u);
LABEL_43:
    __break(1u);
  }
  if ((unint64_t)(v3 - 0x2000000000000000) >> 62 != 3) {
    goto LABEL_35;
  }
  int v9 = 4 * v3;
  uint64_t v10 = v2[7];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135280;
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  *(void *)(inited + 32) = v8;
  *(void *)(inited + 40) = v4;
  *(void *)(inited + 48) = v9;
  *(void *)(inited + 56) = v10;
  *(void *)(inited + 64) = 0;
  if (!v5[2])
  {
LABEL_37:
    __break(1u);
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  uint64_t v12 = (void *)v5[4];
  if (!v12) {
    goto LABEL_43;
  }
  int v13 = v5[6];
  if (v13 < 0) {
    goto LABEL_38;
  }
  if ((unint64_t)(v13 - 0x2000000000000000) >> 62 != 3)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  uint64_t v14 = v5[5];
  if ((v14 & 0x8000000000000000) != 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  int v15 = 4 * v13;
  if ((v15 & 0x8000000000000000) != 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  uint64_t v16 = v5[7];
  uint64_t v20 = v5[4];
  int v21 = v14;
  uint64_t v22 = v15;
  int v23 = v16;
  uint64_t v24 = 0;
  src.data = v8;
  src.height = v4;
  src.width = v9;
  src.rowBytes = v10;
  dest.data = v12;
  dest.height = v14;
  dest.width = v15;
  dest.rowBytes = v16;
  vImageConvert_Planar8toPlanarF(&src, &dest, 1.0, 0.0, 0);
  swift_bridgeObjectRelease();
  return swift_arrayDestroy();
}

{
  uint64_t v1;
  void *v2;
  uint64_t v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  vImagePixelCount v9;
  size_t v10;
  uint64_t inited;
  void *v12;
  uint64_t v13;
  uint64_t v14;
  int64_t v15;
  vImagePixelCount v16;
  size_t v17;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v21;
  vImagePixelCount v22;
  int64_t v23;
  size_t v24;
  uint64_t v25;
  uint64_t v26;

  long long v26 = *MEMORY[0x1E4F143B8];
  uint64_t v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v3 = v2[6];
  if (v3 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  uint64_t v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v3)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v4)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  uint64_t v5 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  int v6 = v5[6];
  if (v6 < 0)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (!v6)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v7)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (v3 != v6)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (v4 != v7)
  {
LABEL_34:
    __break(1u);
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  uint64_t v8 = (void *)v2[4];
  if (!v8)
  {
LABEL_42:
    __break(1u);
LABEL_43:
    __break(1u);
  }
  int v9 = 3 * v3;
  if ((unsigned __int128)(v3 * (__int128)3) >> 64 != (3 * v3) >> 63) {
    goto LABEL_35;
  }
  uint64_t v10 = v2[7];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135280;
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  *(void *)(inited + 32) = v8;
  *(void *)(inited + 40) = v4;
  *(void *)(inited + 48) = v9;
  *(void *)(inited + 56) = v10;
  *(void *)(inited + 64) = 0;
  if (!v5[2])
  {
LABEL_37:
    __break(1u);
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  uint64_t v12 = (void *)v5[4];
  if (!v12) {
    goto LABEL_43;
  }
  int v13 = v5[6];
  if (v13 < 0) {
    goto LABEL_38;
  }
  uint64_t v14 = (unsigned __int128)(v13 * (__int128)3) >> 64;
  int v15 = 3 * v13;
  if (v14 != v15 >> 63)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  uint64_t v16 = v5[5];
  if ((v16 & 0x8000000000000000) != 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (v15 < 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  int v17 = v5[7];
  int v21 = v5[4];
  uint64_t v22 = v16;
  int v23 = v15;
  uint64_t v24 = v17;
  int v25 = 0;
  src.data = v8;
  src.height = v4;
  src.width = v9;
  src.rowBytes = v10;
  dest.data = v12;
  dest.height = v16;
  dest.width = v15;
  dest.rowBytes = v17;
  vImageConvert_Planar8toPlanarF(&src, &dest, 1.0, 0.0, 0);
  swift_bridgeObjectRelease();
  return swift_arrayDestroy();
}

uint64_t vImage.PixelBuffer<>.convert(to:)(uint64_t a1, uint64_t (*a2)(void *, void *, void))
{
  void v15[4] = *MEMORY[0x1E4F143B8];
  uint64_t v3 = *(void **)v2;
  if (!*(void *)(*(void *)v2 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v4 = v3[6];
  if (v4 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v5 = v3[5];
  if (v5 < 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v4)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v5)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  int v6 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v7 = v6[6];
  if (v7 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v8 = v6[5];
  if (v8 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v7)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v8)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v4 != v7)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v5 != v8) {
    goto LABEL_25;
  }
  uint64_t v9 = v3[4];
  uint64_t v10 = v6[4];
  uint64_t v11 = v3[7];
  uint64_t v12 = v6[7];
  v15[0] = v9;
  v15[1] = v5;
  v15[2] = 2 * v4;
  v15[3] = v11;
  v14[0] = v10;
  v14[1] = v5;
  v14[2] = 2 * v4;
  v14[3] = v12;
  return a2(v15, v14, 0);
}

{
  uint64_t v2;
  void *v3;
  unint64_t v4;
  uint64_t v5;
  void *v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  void v14[4];
  void v15[5];

  void v15[4] = *MEMORY[0x1E4F143B8];
  uint64_t v3 = *(void **)v2;
  if (!*(void *)(*(void *)v2 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v4 = v3[6];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v5 = v3[5];
  if (v5 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v4)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v5)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  int v6 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v7 = v6[6];
  if (v7 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v8 = v6[5];
  if (v8 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v7)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v8)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (v4 != v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v5 != v8)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v4 >> 62) {
    goto LABEL_27;
  }
  uint64_t v9 = v3[4];
  uint64_t v10 = v6[4];
  uint64_t v11 = v3[7];
  uint64_t v12 = v6[7];
  v15[0] = v9;
  v15[1] = v5;
  v15[2] = 4 * v4;
  v15[3] = v11;
  v14[0] = v10;
  v14[1] = v5;
  v14[2] = 4 * v4;
  v14[3] = v12;
  return a2(v15, v14, 0);
}

{
  uint64_t v2;
  void *v3;
  uint64_t v4;
  uint64_t v5;
  void *v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  void v14[4];
  void v15[5];

  void v15[4] = *MEMORY[0x1E4F143B8];
  uint64_t v3 = *(void **)v2;
  if (!*(void *)(*(void *)v2 + 16))
  {
    __break(1u);
    goto LABEL_15;
  }
  uint64_t v4 = v3[6];
  if (v4 < 0)
  {
LABEL_15:
    __break(1u);
    goto LABEL_16;
  }
  uint64_t v5 = v3[5];
  if (v5 < 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (!v4)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!v5)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  int v6 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  uint64_t v7 = v6[6];
  if (v7 < 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v8 = v6[5];
  if (v8 < 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (!v7)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  if (!v8)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (v4 != v7)
  {
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
  }
  if (v5 != v8) {
    goto LABEL_25;
  }
  uint64_t v9 = v3[4];
  uint64_t v10 = v3[7];
  v15[0] = v9;
  v15[1] = v5;
  v15[2] = v4;
  v15[3] = v10;
  uint64_t v11 = v6[4];
  uint64_t v12 = v6[7];
  v14[0] = v11;
  v14[1] = v5;
  v14[2] = v4;
  v14[3] = v12;
  return a2(v15, v14, 0);
}

uint64_t vImage.FloodFillConnectivity.init(rawValue:)@<X0>(uint64_t result@<X0>, char *a2@<X8>)
{
  if (result == 8) {
    char v2 = 1;
  }
  else {
    char v2 = 2;
  }
  if (result == 4) {
    char v2 = 0;
  }
  *a2 = v2;
  return result;
}

uint64_t vImage.FloodFillConnectivity.rawValue.getter()
{
  if (*v0) {
    return 8;
  }
  else {
    return 4;
  }
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance vImage.FloodFillConnectivity()
{
  int v1 = *v0;
  Hasher.init(_seed:)();
  if (v1) {
    Swift::UInt32 v2 = 8;
  }
  else {
    Swift::UInt32 v2 = 4;
  }
  Hasher._combine(_:)(v2);
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance vImage.FloodFillConnectivity()
{
  if (*v0) {
    Swift::UInt32 v1 = 8;
  }
  else {
    Swift::UInt32 v1 = 4;
  }
  Hasher._combine(_:)(v1);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance vImage.FloodFillConnectivity()
{
  int v1 = *v0;
  Hasher.init(_seed:)();
  if (v1) {
    Swift::UInt32 v2 = 8;
  }
  else {
    Swift::UInt32 v2 = 4;
  }
  Hasher._combine(_:)(v2);
  return Hasher._finalize()();
}

_DWORD *protocol witness for RawRepresentable.init(rawValue:) in conformance vImage.FloodFillConnectivity@<X0>(_DWORD *result@<X0>, char *a2@<X8>)
{
  if (*result == 8) {
    char v2 = 1;
  }
  else {
    char v2 = 2;
  }
  if (*result == 4) {
    char v3 = 0;
  }
  else {
    char v3 = v2;
  }
  *a2 = v3;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance vImage.FloodFillConnectivity(int *a1@<X8>)
{
  if (*v1) {
    int v2 = 8;
  }
  else {
    int v2 = 4;
  }
  *a1 = v2;
}

vImage_Error vImage.PixelBuffer<>.floodFill(from:newColor:connectivity:)(Pixel_8 a1, unsigned char *a2, double a3, double a4)
{
  uint64_t v15 = *MEMORY[0x1E4F143B8];
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&seedY, a3) || LOBYTE(seedY.height) == 1) {
    goto LABEL_11;
  }
  data = seedY.data;
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&seedY, a4) || LOBYTE(seedY.height) == 1) {
LABEL_12:
  }
    __break(1u);
  uint64_t v9 = *v4;
  if (!*(void *)(*v4 + 16))
  {
    __break(1u);
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  uint64_t v10 = seedY.data;
  long long v11 = *(_OWORD *)(v9 + 48);
  *(_OWORD *)&seedY.data = *(_OWORD *)(v9 + 32);
  *(_OWORD *)&seedY.width = v11;
  if (*a2) {
    int v12 = 8;
  }
  else {
    int v12 = 4;
  }
  return vImageFloodFill_Planar8(&seedY, 0, (vImagePixelCount)data, (vImagePixelCount)v10, a1, v12, 0);
}

vImage_Error vImage.PixelBuffer<>.floodFill(from:newColor:connectivity:)(Pixel_16U a1, unsigned char *a2, double a3, double a4)
{
  uint64_t v15 = *MEMORY[0x1E4F143B8];
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&seedY, a3) || LOBYTE(seedY.height) == 1) {
    goto LABEL_11;
  }
  data = seedY.data;
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&seedY, a4) || LOBYTE(seedY.height) == 1) {
LABEL_12:
  }
    __break(1u);
  uint64_t v9 = *v4;
  if (!*(void *)(*v4 + 16))
  {
    __break(1u);
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  uint64_t v10 = seedY.data;
  long long v11 = *(_OWORD *)(v9 + 48);
  *(_OWORD *)&seedY.data = *(_OWORD *)(v9 + 32);
  *(_OWORD *)&seedY.width = v11;
  if (*a2) {
    int v12 = 8;
  }
  else {
    int v12 = 4;
  }
  return vImageFloodFill_Planar16U(&seedY, 0, (vImagePixelCount)data, (vImagePixelCount)v10, a1, v12, 0);
}

vImage_Error vImage.PixelBuffer<>.floodFill(from:newColor:connectivity:)(uint8_t a1, uint8_t a2, uint8_t a3, uint8_t a4, unsigned char *a5, double a6, double a7)
{
  uint64_t v22 = *MEMORY[0x1E4F143B8];
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&srcDest, a6) || LOBYTE(srcDest.height) == 1) {
    goto LABEL_11;
  }
  data = srcDest.data;
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&srcDest, a7) || LOBYTE(srcDest.height) == 1) {
LABEL_12:
  }
    __break(1u);
  uint64_t v15 = *v7;
  if (!*(void *)(*v7 + 16))
  {
    __break(1u);
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  uint64_t v16 = srcDest.data;
  long long v17 = *(_OWORD *)(v15 + 48);
  *(_OWORD *)&srcDest.data = *(_OWORD *)(v15 + 32);
  *(_OWORD *)&srcDest.width = v17;
  v20[0] = a1;
  v20[1] = a2;
  v20[2] = a3;
  v20[3] = a4;
  if (*a5) {
    int v18 = 8;
  }
  else {
    int v18 = 4;
  }
  return vImageFloodFill_ARGB8888(&srcDest, 0, (vImagePixelCount)data, (vImagePixelCount)v16, v20, v18, 0);
}

vImage_Error vImage.PixelBuffer<>.floodFill(from:newColor:connectivity:)(uint16_t a1, uint16_t a2, uint16_t a3, uint16_t a4, unsigned char *a5, double a6, double a7)
{
  uint64_t v22 = *MEMORY[0x1E4F143B8];
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&srcDest, a6) || LOBYTE(srcDest.height) == 1) {
    goto LABEL_11;
  }
  data = srcDest.data;
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&srcDest, a7) || LOBYTE(srcDest.height) == 1) {
LABEL_12:
  }
    __break(1u);
  uint64_t v15 = *v7;
  if (!*(void *)(*v7 + 16))
  {
    __break(1u);
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  uint64_t v16 = srcDest.data;
  long long v17 = *(_OWORD *)(v15 + 48);
  *(_OWORD *)&srcDest.data = *(_OWORD *)(v15 + 32);
  *(_OWORD *)&srcDest.width = v17;
  v20[0] = a1;
  v20[1] = a2;
  v20[2] = a3;
  v20[3] = a4;
  if (*a5) {
    int v18 = 8;
  }
  else {
    int v18 = 4;
  }
  return vImageFloodFill_ARGB16U(&srcDest, 0, (vImagePixelCount)data, (vImagePixelCount)v16, v20, v18, 0);
}

BOOL specialized static FixedWidthInteger._convert<A>(from:)(uint64_t a1, double a2)
{
  uint64_t v3 = (*(void *)&a2 >> 52) & 0x7FFLL;
  unint64_t v4 = *(void *)&a2 & 0xFFFFFFFFFFFFFLL;
  uint64_t v5 = v3 | *(void *)&a2 & 0xFFFFFFFFFFFFFLL;
  if (v5)
  {
    BOOL result = 0;
    uint64_t v5 = 0;
    char v7 = 1;
    if (v3 != 2047 && a2 > -1.0)
    {
      uint64_t v8 = Double.exponent.getter();
      if (v8 <= 63)
      {
        uint64_t v9 = v8;
        uint64_t v10 = Double.significandWidth.getter();
        uint64_t v11 = v10 + __clz(__rbit64(v4));
        uint64_t v12 = v9 - v11;
        if (__OFSUB__(v9, v11))
        {
          __break(1u);
        }
        else
        {
          if (v11 > 63)
          {
            if (v12 < -64 || v12 > 64) {
              goto LABEL_12;
            }
          }
          else if (v12 < -64 || v12 > 64)
          {
            goto LABEL_12;
          }
          if ((v12 & 0x8000000000000000) == 0)
          {
            if ((unint64_t)v12 < 0x40)
            {
              unint64_t v13 = v4 << v12;
              if (v9 < 0)
              {
LABEL_17:
                uint64_t v14 = 0;
                goto LABEL_24;
              }
LABEL_23:
              uint64_t v14 = 1 << v9;
              goto LABEL_24;
            }
            goto LABEL_12;
          }
        }
        if ((unint64_t)v12 > 0xFFFFFFFFFFFFFFC0)
        {
          unint64_t v13 = v4 >> (v11 - v9);
          if (v9 < 0) {
            goto LABEL_17;
          }
          goto LABEL_23;
        }
LABEL_12:
        unint64_t v13 = 0;
        uint64_t v14 = 0;
        if (v9 < 0)
        {
LABEL_24:
          char v7 = 0;
          uint64_t v5 = v13 | v14;
          BOOL result = v9 >= v10;
          goto LABEL_25;
        }
        goto LABEL_23;
      }
      uint64_t v5 = 0;
      BOOL result = 0;
      char v7 = 1;
    }
  }
  else
  {
    char v7 = 0;
    BOOL result = 1;
  }
LABEL_25:
  *(void *)a1 = v5;
  *(unsigned char *)(a1 + 8) = v7;
  return result;
}

{
  uint64_t v4;
  unint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  BOOL result;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  unint64_t v14;

  unint64_t v4 = (*(void *)&a2 >> 52) & 0x7FFLL;
  uint64_t v5 = *(void *)&a2 & 0xFFFFFFFFFFFFFLL;
  int v6 = v4 | *(void *)&a2 & 0xFFFFFFFFFFFFFLL;
  if (!v6)
  {
    uint64_t v12 = 0;
    BOOL result = 1;
    goto LABEL_25;
  }
  if (v4 == 2047) {
    goto LABEL_28;
  }
  char v7 = Double.exponent.getter();
  if (v7 > 63) {
    goto LABEL_28;
  }
  uint64_t v8 = v7;
  uint64_t v9 = Double.significandWidth.getter();
  BOOL result = v8 >= v9;
  uint64_t v11 = v9 + __clz(__rbit64(v5));
  uint64_t v12 = v8 - v11;
  if (__OFSUB__(v8, v11))
  {
    __break(1u);
LABEL_27:
    if (a2 < 0.0)
    {
LABEL_36:
      LOBYTE(v6) = 0;
      uint64_t v12 = 0x8000000000000000;
      goto LABEL_25;
    }
LABEL_28:
    uint64_t v12 = 0;
    BOOL result = 0;
    LOBYTE(v6) = 1;
    goto LABEL_25;
  }
  if (v11 > 63)
  {
    if (v12 < -64 || v12 > 64) {
      goto LABEL_11;
    }
  }
  else if (v12 < -64 || v12 > 64)
  {
    goto LABEL_11;
  }
  if (v12 < 0) {
    goto LABEL_32;
  }
  if ((unint64_t)v12 < 0x40)
  {
    uint64_t v14 = v5 << v12;
    if (v8 != 63) {
      goto LABEL_17;
    }
LABEL_34:
    if (a2 < 0.0 && !v14) {
      goto LABEL_36;
    }
    goto LABEL_28;
  }
LABEL_11:
  if (v8 == 63) {
    goto LABEL_27;
  }
  unint64_t v13 = 0;
  uint64_t v14 = 0;
  if (v8 < 0) {
    goto LABEL_20;
  }
LABEL_19:
  unint64_t v13 = 1 << v8;
LABEL_20:
  while (1)
  {
    uint64_t v12 = v14 | v13;
    if (a2 >= 0.0) {
      break;
    }
    if ((v12 & 0x8000000000000000) == 0)
    {
      LOBYTE(v6) = 0;
      uint64_t v12 = -v12;
      goto LABEL_25;
    }
    __break(1u);
LABEL_31:
    __break(1u);
LABEL_32:
    if ((unint64_t)v12 <= 0xFFFFFFFFFFFFFFC0) {
      goto LABEL_11;
    }
    uint64_t v14 = v5 >> -(char)v12;
    if (v8 == 63) {
      goto LABEL_34;
    }
LABEL_17:
    if ((v8 & 0x8000000000000000) == 0) {
      goto LABEL_19;
    }
    unint64_t v13 = 0;
  }
  if (v12 < 0) {
    goto LABEL_31;
  }
  LOBYTE(v6) = 0;
LABEL_25:
  *(void *)a1 = v12;
  *(unsigned char *)(a1 + 8) = v6;
  return result;
}

unint64_t lazy protocol witness table accessor for type vImage.FloodFillConnectivity and conformance vImage.FloodFillConnectivity()
{
  unint64_t result = lazy protocol witness table cache variable for type vImage.FloodFillConnectivity and conformance vImage.FloodFillConnectivity;
  if (!lazy protocol witness table cache variable for type vImage.FloodFillConnectivity and conformance vImage.FloodFillConnectivity)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.FloodFillConnectivity and conformance vImage.FloodFillConnectivity);
  }
  return result;
}

unsigned char *storeEnumTagSinglePayload for vImage.FloodFillConnectivity(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 1 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFF) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFE)
  {
    unsigned int v6 = ((a2 - 255) >> 8) + 1;
    *unint64_t result = a2 + 1;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D20B0898);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 1;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vImage.FloodFillConnectivity()
{
  return &type metadata for vImage.FloodFillConnectivity;
}

uint64_t protocol witness for BNNSGraph.PointerArgument.baseAddress.getter in conformance <A> UnsafeMutableBufferPointer<A>()
{
  return UnsafeBufferPointer.baseAddress.getter();
}

uint64_t protocol witness for BNNSGraph.PointerArgument.count.getter in conformance <A> UnsafeMutableBufferPointer<A>()
{
  return *(void *)(v0 + 8);
}

uint64_t BNNSGraph.CompileOptions.CompileOptionsRef.__deallocating_deinit()
{
  BNNSGraphCompileOptionsDestroy();

  return swift_deallocClassInstance();
}

uint64_t BNNSGraph.CompileOptions.init()@<X0>(uint64_t *a1@<X8>)
{
  type metadata accessor for BNNSGraph.CompileOptions.CompileOptionsRef();
  uint64_t v2 = swift_allocObject();
  uint64_t result = BNNSGraphCompileOptionsMakeDefault();
  *(void *)(v2 + 16) = result;
  *(void *)(v2 + 24) = v4;
  *a1 = v2;
  return result;
}

uint64_t type metadata accessor for BNNSGraph.CompileOptions.CompileOptionsRef()
{
  return self;
}

void static BNNSGraph.CompileOptions.OptimizationPreference.performance.getter(_DWORD *a1@<X8>)
{
  *a1 = 0;
}

uint64_t BNNSGraph.CompileOptions.init(useSingleThread:generateDebugInfo:optimizationPreference:)@<X0>(unsigned int *a1@<X2>, uint64_t *a2@<X8>)
{
  uint64_t v3 = *a1;
  type metadata accessor for BNNSGraph.CompileOptions.CompileOptionsRef();
  uint64_t v4 = swift_allocObject();
  uint64_t Default = BNNSGraphCompileOptionsMakeDefault();
  uint64_t v7 = v6;
  *(void *)(v4 + 16) = Default;
  *(void *)(v4 + 24) = v6;
  *a2 = v4;
  BNNSGraphCompileOptionsSetTargetSingleThread();
  BNNSGraphCompileOptionsSetGenerateDebugInfo();

  return MEMORY[0x1F40D1200](Default, v7, v3);
}

uint64_t BNNSGraph.CompileOptions.useSingleThread.setter(char a1)
{
  return BNNSGraph.CompileOptions.useSingleThread.setter(a1, MEMORY[0x1E4F167A8]);
}

uint64_t BNNSGraph.CompileOptions.generateDebugInfo.setter(char a1)
{
  return BNNSGraph.CompileOptions.useSingleThread.setter(a1, MEMORY[0x1E4F167A0]);
}

uint64_t BNNSGraph.CompileOptions.useSingleThread.setter(char a1, uint64_t (*a2)(void, void, void))
{
  return a2(*(void *)(*(void *)v2 + 16), *(void *)(*(void *)v2 + 24), a1 & 1);
}

uint64_t BNNSGraph.CompileOptions.optimizationPreference.setter(unsigned int *a1)
{
  return MEMORY[0x1F40D1200](*(void *)(*(void *)v1 + 16), *(void *)(*(void *)v1 + 24), *a1);
}

uint64_t BNNSGraph.CompileOptions.useSingleThread.getter()
{
  return BNNSGraphCompileOptionsGetTargetSingleThread();
}

uint64_t (*BNNSGraph.CompileOptions.useSingleThread.modify(uint64_t a1))(unsigned __int8 *a1, uint64_t a2)
{
  *(void *)a1 = v1;
  *(unsigned char *)(a1 + 8) = BNNSGraphCompileOptionsGetTargetSingleThread();
  return BNNSGraph.CompileOptions.useSingleThread.modify;
}

uint64_t BNNSGraph.CompileOptions.useSingleThread.modify(unsigned __int8 *a1, uint64_t a2)
{
  return BNNSGraph.CompileOptions.useSingleThread.modify(a1, a2, MEMORY[0x1E4F167A8]);
}

uint64_t BNNSGraph.CompileOptions.generateDebugInfo.getter()
{
  return BNNSGraphCompileOptionsGetGenerateDebugInfo();
}

uint64_t (*BNNSGraph.CompileOptions.generateDebugInfo.modify(uint64_t a1))(unsigned __int8 *a1, uint64_t a2)
{
  *(void *)a1 = v1;
  *(unsigned char *)(a1 + 8) = BNNSGraphCompileOptionsGetGenerateDebugInfo();
  return BNNSGraph.CompileOptions.generateDebugInfo.modify;
}

uint64_t BNNSGraph.CompileOptions.generateDebugInfo.modify(unsigned __int8 *a1, uint64_t a2)
{
  return BNNSGraph.CompileOptions.useSingleThread.modify(a1, a2, MEMORY[0x1E4F167A0]);
}

uint64_t BNNSGraph.CompileOptions.useSingleThread.modify(unsigned __int8 *a1, uint64_t a2, uint64_t (*a3)(void, void, void))
{
  return a3(*(void *)(**(void **)a1 + 16), *(void *)(**(void **)a1 + 24), a1[8]);
}

uint64_t BNNSGraph.CompileOptions.OptimizationPreference.value.getter()
{
  return *v0;
}

uint64_t BNNSGraph.CompileOptions.OptimizationPreference.value.setter(uint64_t result)
{
  _DWORD *v1 = result;
  return result;
}

uint64_t (*BNNSGraph.CompileOptions.OptimizationPreference.value.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNSGraph.CompileOptions.OptimizationPreference.init(_:)@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = result;
  return result;
}

void static BNNSGraph.CompileOptions.OptimizationPreference.internalRepresentationSize.getter(_DWORD *a1@<X8>)
{
  *a1 = 1;
}

BOOL static BNNSGraph.CompileOptions.OptimizationPreference.== infix(_:_:)(_DWORD *a1, _DWORD *a2)
{
  return *a1 == *a2;
}

uint64_t BNNSGraph.CompileOptions.optimizationPreference.getter@<X0>(_DWORD *a1@<X8>)
{
  uint64_t result = BNNSGraphCompileOptionsGetOptimizationPreference();
  *a1 = result;
  return result;
}

uint64_t (*BNNSGraph.CompileOptions.optimizationPreference.modify(uint64_t a1))(unsigned int *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = BNNSGraphCompileOptionsGetOptimizationPreference();
  return BNNSGraph.CompileOptions.optimizationPreference.modify;
}

uint64_t BNNSGraph.CompileOptions.optimizationPreference.modify(unsigned int *a1)
{
  return MEMORY[0x1F40D1200](*(void *)(**(void **)a1 + 16), *(void *)(**(void **)a1 + 24), a1[2]);
}

uint64_t BNNSGraph.Context.deinit()
{
  BNNSGraphContextDestroy_v2();
  return v0;
}

uint64_t BNNSGraph.Context.__deallocating_deinit()
{
  BNNSGraphContextDestroy_v2();

  return swift_deallocClassInstance();
}

uint64_t BNNSGraph.Context.__allocating_init(compileFromPath:functionName:options:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, void *a5)
{
  uint64_t v11 = swift_allocObject();
  uint64_t v12 = (void *)swift_task_alloc();
  *(void *)(v5 + 16) = v12;
  *uint64_t v12 = v5;
  v12[5] = a4;
  void v12[6] = v11;
  v12[3] = a2;
  v12[4] = a3;
  v12[1] = BNNSGraph.Context.__allocating_init(compileFromPath:functionName:options:);
  _OWORD v12[2] = a1;
  v12[7] = *a5;
  return MEMORY[0x1F4188298](BNNSGraph.Context.init(compileFromPath:functionName:options:), 0, 0);
}

uint64_t BNNSGraph.Context.__allocating_init(compileFromPath:functionName:options:)(uint64_t a1)
{
  uint64_t v7 = *v2;
  uint64_t v4 = swift_task_dealloc();
  uint64_t v5 = *(uint64_t (**)(uint64_t))(v7 + 8);
  if (!v1) {
    uint64_t v4 = a1;
  }
  return v5(v4);
}

uint64_t BNNSGraph.Context.init(compileFromPath:functionName:options:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t *a5)
{
  v6[4] = a3;
  v6[5] = a4;
  v6[2] = a1;
  void v6[3] = a2;
  uint64_t v7 = *a5;
  v6[6] = v5;
  v6[7] = v7;
  return MEMORY[0x1F4188298](BNNSGraph.Context.init(compileFromPath:functionName:options:), 0, 0);
}

uint64_t BNNSGraph.Context.init(compileFromPath:functionName:options:)()
{
  uint64_t v1 = v0[5];
  *(unsigned char *)(v0[6] + 48) = 0;
  swift_release();
  String.utf8CString.getter();
  swift_bridgeObjectRelease();
  if (v1)
  {
    String.utf8CString.getter();
    swift_bridgeObjectRelease();
  }
  uint64_t v2 = (void *)v0[6];
  uint64_t v3 = BNNSGraphCompileFromFile_v2();
  uint64_t v5 = v4;
  swift_unknownObjectRelease();
  swift_release();
  v2[2] = v3;
  v2[3] = v5;
  uint64_t v6 = BNNSGraphContextMake();
  v2[4] = v6;
  v2[5] = v7;
  if (v5) {
    BOOL v8 = v3 == 0;
  }
  else {
    BOOL v8 = 1;
  }
  if (v8)
  {
    char v9 = 0;
  }
  else
  {
    if (v7) {
      BOOL v10 = v6 == 0;
    }
    else {
      BOOL v10 = 1;
    }
    if (!v10)
    {
      BNNSGraphContextSetArgumentType();
      uint64_t v14 = (uint64_t (*)(uint64_t))v0[1];
      uint64_t v15 = v0[6];
      return v14(v15);
    }
    char v9 = 1;
  }
  swift_release();
  lazy protocol witness table accessor for type BNNSGraph.Error and conformance BNNSGraph.Error();
  swift_allocError();
  *uint64_t v11 = v9;
  swift_willThrow();
  uint64_t v12 = (uint64_t (*)(void))v0[1];
  return v12();
}

unint64_t lazy protocol witness table accessor for type BNNSGraph.Error and conformance BNNSGraph.Error()
{
  unint64_t result = lazy protocol witness table cache variable for type BNNSGraph.Error and conformance BNNSGraph.Error;
  if (!lazy protocol witness table cache variable for type BNNSGraph.Error and conformance BNNSGraph.Error)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNSGraph.Error and conformance BNNSGraph.Error);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type BNNSGraph.Error and conformance BNNSGraph.Error;
  if (!lazy protocol witness table cache variable for type BNNSGraph.Error and conformance BNNSGraph.Error)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNSGraph.Error and conformance BNNSGraph.Error);
  }
  return result;
}

uint64_t BNNSGraph.Context.setDynamicShapes(_:forFunction:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  v4[4] = a3;
  v4[5] = v3;
  v4[2] = a1;
  v4[3] = a2;
  return MEMORY[0x1F4188298](BNNSGraph.Context.setDynamicShapes(_:forFunction:), 0, 0);
}

uint64_t BNNSGraph.Context.setDynamicShapes(_:forFunction:)()
{
  int64_t v1 = *(void *)(v0 + 16);
  int64_t v2 = *(void *)(v1 + 16);
  unint64_t v3 = MEMORY[0x1E4FBC860];
  if (!v2)
  {
    int64_t v21 = *(void *)(MEMORY[0x1E4FBC860] + 16);
    if (v21)
    {
      uint64_t v19 = MEMORY[0x1E4FBC860];
      goto LABEL_29;
    }
    unint64_t v7 = MEMORY[0x1E4FBC860];
    swift_bridgeObjectRelease();
LABEL_34:
    int64_t v1 = (int64_t)v52;
    if (swift_isUniquelyReferenced_nonNull_native()) {
      goto LABEL_35;
    }
    goto LABEL_80;
  }
  uint64_t v57 = *(void *)(v1 + 16);
  uint64_t v58 = MEMORY[0x1E4FBC860];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v2, 0);
  uint64_t v4 = 0;
  BNNSLayerParametersConvolution __dst = (void *)(v1 + 32);
  while (1)
  {
    uint64_t v5 = &__dst[2 * v4];
    uint64_t v6 = *v5;
    if (*v5)
    {
      int64_t v1 = v5[1];
      uint64_t v60 = MEMORY[0x1E4FBC860];
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v6 & ~(v6 >> 63), 0);
      unint64_t v7 = v6 - 1;
      if (v6 < 1) {
        goto LABEL_78;
      }
      uint64_t v8 = *(void *)v1;
      char v9 = (void *)MEMORY[0x1E4FBC860];
      if ((*(void *)v1 & 0x8000000000000000) != 0)
      {
LABEL_72:
        __break(1u);
        goto LABEL_73;
      }
      uint64_t v10 = v60;
      unint64_t v11 = *(void *)(v60 + 16);
      uint64_t v12 = (uint64_t *)(v1 + 8);
      while (1)
      {
        unint64_t v13 = *(void *)(v60 + 24);
        int64_t v1 = v11 + 1;
        if (v11 >= v13 >> 1)
        {
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v13 > 1), v11 + 1, 1);
          char v9 = (void *)MEMORY[0x1E4FBC860];
        }
        *(void *)(v60 + 16) = v1;
        *(void *)(v60 + 8 * v11 + 32) = v8;
        if (!v7) {
          break;
        }
        uint64_t v14 = *v12++;
        uint64_t v8 = v14;
        --v7;
        ++v11;
        if (v14 < 0) {
          goto LABEL_72;
        }
      }
      unint64_t v7 = v11 + 1;
      if ((v11 + 1) >> 60) {
        goto LABEL_76;
      }
    }
    else
    {
      char v9 = (void *)MEMORY[0x1E4FBC860];
      unint64_t v7 = *(void *)(MEMORY[0x1E4FBC860] + 16);
      uint64_t v10 = MEMORY[0x1E4FBC860];
      if (v7 >> 60) {
        goto LABEL_76;
      }
    }
    uint64_t v15 = (void *)swift_slowAlloc();
    if (v7) {
      break;
    }
LABEL_21:
    swift_bridgeObjectRelease();
    memcpy(v15, v9 + 4, 8 * v7);
    swift_bridgeObjectRelease();
    uint64_t v19 = v58;
    if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(void *)(v58 + 16) + 1, 1);
      uint64_t v19 = v58;
    }
    int64_t v1 = *(void *)(v19 + 16);
    unint64_t v20 = *(void *)(v19 + 24);
    int64_t v21 = v1 + 1;
    if (v1 >= v20 >> 1)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v20 > 1), v1 + 1, 1);
      uint64_t v19 = v58;
    }
    ++v4;
    *(void *)(v19 + 16) = v21;
    uint64_t v22 = v19 + 16 * v1;
    *(void *)(v22 + 32) = v7;
    *(void *)(v22 + 40) = v15;
    if (v4 == v57)
    {
      unint64_t v3 = MEMORY[0x1E4FBC860];
LABEL_29:
      unint64_t v61 = v3;
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v21, 0);
      uint64_t v23 = 0;
      unint64_t v7 = v61;
      unint64_t v24 = *(void *)(v61 + 16);
      do
      {
        long long v25 = *(_OWORD *)(v19 + 16 * v23 + 32);
        unint64_t v26 = *(void *)(v61 + 24);
        if (v24 >= v26 >> 1)
        {
          *(_OWORD *)__dstb = *(_OWORD *)(v19 + 16 * v23 + 32);
          specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v26 > 1), v24 + 1, 1);
          long long v25 = *(_OWORD *)__dstb;
        }
        ++v23;
        *(void *)(v61 + 16) = v24 + 1;
        *(_OWORD *)(v61 + 16 * v24++ + 32) = v25;
      }
      while (v21 != v23);
      swift_bridgeObjectRelease();
      goto LABEL_34;
    }
  }
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v7, 0);
  int64_t v1 = 0;
  while (1)
  {
    uint64_t v16 = *(void *)(v10 + 8 * v1 + 32);
    if (v16 < 0) {
      break;
    }
    unint64_t v18 = v9[2];
    unint64_t v17 = v9[3];
    if (v18 >= v17 >> 1) {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v17 > 1), v18 + 1, 1);
    }
    ++v1;
    void v9[2] = v18 + 1;
    v9[v18 + 4] = v16;
    if (v7 == v1) {
      goto LABEL_21;
    }
  }
LABEL_73:
  __break(1u);
LABEL_74:
  __break(1u);
  while (1)
  {
    __break(1u);
LABEL_76:
    __break(1u);
LABEL_77:
    __break(1u);
LABEL_78:
    __break(1u);
LABEL_79:
    __break(1u);
LABEL_80:
    unint64_t v7 = (unint64_t)specialized _ArrayBuffer._consumeAndCreateNew()(v7);
LABEL_35:
    if (*(void *)(v1 + 32)) {
      String.utf8CString.getter();
    }
    BNNSGraphContextSetDynamicShapes_v2();
    swift_unknownObjectRelease();
    int64_t v27 = *(void *)(v7 + 16);
    if (!v27) {
      break;
    }
    uint64_t v59 = MEMORY[0x1E4FBC860];
    swift_bridgeObjectRetain();
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v27, 0);
    int64_t v1 = 0;
    long long v53 = (void *)v27;
    while (1)
    {
      long long v28 = (uint64_t *)(v7 + 32 + 16 * v1);
      uint64_t v29 = *v28;
      long long v30 = (uint64_t *)v28[1];
      if (*v28)
      {
        uint64_t v62 = MEMORY[0x1E4FBC860];
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v29 & ~(v29 >> 63), 0);
        uint64_t v31 = v29 - 1;
        if (v29 < 1) {
          goto LABEL_79;
        }
        uint64_t v32 = *v30;
        long long v33 = (void *)MEMORY[0x1E4FBC860];
        if (*v30 < 0) {
          goto LABEL_74;
        }
        uint64_t v34 = v62;
        unint64_t v35 = *(void *)(v62 + 16);
        long long v36 = v30 + 1;
        while (1)
        {
          unint64_t v37 = *(void *)(v62 + 24);
          if (v35 >= v37 >> 1)
          {
            specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v37 > 1), v35 + 1, 1);
            long long v33 = (void *)MEMORY[0x1E4FBC860];
          }
          *(void *)(v62 + 16) = v35 + 1;
          *(void *)(v62 + 8 * v35 + 32) = v32;
          if (!v31) {
            break;
          }
          uint64_t v38 = *v36++;
          uint64_t v32 = v38;
          --v31;
          ++v35;
          if (v38 < 0) {
            goto LABEL_74;
          }
        }
        unint64_t v39 = v35 + 1;
        if ((v35 + 1) >> 60) {
          goto LABEL_77;
        }
      }
      else
      {
        long long v33 = (void *)MEMORY[0x1E4FBC860];
        unint64_t v39 = *(void *)(MEMORY[0x1E4FBC860] + 16);
        uint64_t v34 = MEMORY[0x1E4FBC860];
        if (v39 >> 60) {
          goto LABEL_77;
        }
      }
      uint64_t v40 = swift_slowAlloc();
      long long v41 = (void *)v40;
      if (v39) {
        break;
      }
      swift_bridgeObjectRelease();
LABEL_59:
      memcpy(v41, v33 + 4, 8 * v39);
      swift_bridgeObjectRelease();
      if (v30) {
        MEMORY[0x1D26009C0](v30, -1, -1);
      }
      uint64_t v46 = v59;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(void *)(v59 + 16) + 1, 1);
        uint64_t v46 = v59;
      }
      unint64_t v48 = *(void *)(v46 + 16);
      unint64_t v47 = *(void *)(v46 + 24);
      if (v48 >= v47 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v47 > 1), v48 + 1, 1);
        uint64_t v46 = v59;
      }
      ++v1;
      *(void *)(v46 + 16) = v48 + 1;
      uint64_t v49 = v46 + 16 * v48;
      *(void *)(v49 + 32) = v39;
      *(void *)(v49 + 40) = v41;
      if ((void *)v1 == v53)
      {
        swift_bridgeObjectRelease_n();
        int64_t v1 = (int64_t)v52;
        goto LABEL_68;
      }
    }
    __dsta = (void *)v40;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v39, 0);
    uint64_t v42 = 0;
    while (1)
    {
      uint64_t v43 = *(void *)(v34 + 8 * v42 + 32);
      if (v43 < 0) {
        break;
      }
      unint64_t v45 = v33[2];
      unint64_t v44 = v33[3];
      if (v45 >= v44 >> 1) {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v44 > 1), v45 + 1, 1);
      }
      ++v42;
      v33[2] = v45 + 1;
      v33[v45 + 4] = v43;
      if (v39 == v42)
      {
        swift_bridgeObjectRelease();
        long long v41 = __dsta;
        goto LABEL_59;
      }
    }
  }
  swift_bridgeObjectRelease();
  uint64_t v46 = MEMORY[0x1E4FBC860];
LABEL_68:
  long long v50 = *(uint64_t (**)(uint64_t))(v1 + 8);
  return v50(v46);
}

char *BNNSGraph.Shape.dimensions.getter()
{
  uint64_t v1 = *v0;
  unint64_t result = (char *)MEMORY[0x1E4FBC860];
  if (!*v0) {
    return result;
  }
  unint64_t v3 = (uint64_t *)v0[1];
  uint64_t v11 = MEMORY[0x1E4FBC860];
  unint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v1 & ~(v1 >> 63), 0);
  BOOL v4 = v1 < 1;
  uint64_t v5 = v1 - 1;
  if (v4)
  {
LABEL_12:
    __break(1u);
    return result;
  }
  uint64_t v6 = *v3;
  if (*v3 < 0)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  unint64_t result = (char *)v11;
  unint64_t v7 = *(void *)(v11 + 16);
  uint64_t v8 = v3 + 1;
  while (1)
  {
    uint64_t v12 = result;
    unint64_t v9 = *((void *)result + 3);
    if (v7 >= v9 >> 1)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v9 > 1), v7 + 1, 1);
      unint64_t result = v12;
    }
    *((void *)result + 2) = v7 + 1;
    *(void *)&result[8 * v7 + 32] = v6;
    if (!v5) {
      return result;
    }
    uint64_t v10 = *v8++;
    uint64_t v6 = v10;
    --v5;
    ++v7;
    if (v10 < 0) {
      goto LABEL_11;
    }
  }
}

uint64_t BNNSGraph.Shape.init(_:)@<X0>(uint64_t result@<X0>, unint64_t *a2@<X8>)
{
  unint64_t v3 = *(void *)(result + 16);
  if (v3 >> 60)
  {
LABEL_12:
    __break(1u);
    return result;
  }
  uint64_t v4 = result;
  uint64_t v5 = (void *)swift_slowAlloc();
  if (v3)
  {
    uint64_t v11 = MEMORY[0x1E4FBC860];
    unint64_t result = (uint64_t)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v3, 0);
    uint64_t v6 = 0;
    uint64_t v7 = v11;
    while (1)
    {
      uint64_t v8 = *(void *)(v4 + 8 * v6 + 32);
      if (v8 < 0) {
        break;
      }
      unint64_t v10 = *(void *)(v11 + 16);
      unint64_t v9 = *(void *)(v11 + 24);
      if (v10 >= v9 >> 1) {
        unint64_t result = (uint64_t)specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v9 > 1), v10 + 1, 1);
      }
      ++v6;
      *(void *)(v11 + 16) = v10 + 1;
      *(void *)(v11 + 8 * v10 + 32) = v8;
      if (v3 == v6)
      {
        swift_bridgeObjectRelease();
        goto LABEL_10;
      }
    }
    __break(1u);
    goto LABEL_12;
  }
  swift_bridgeObjectRelease();
  uint64_t v7 = MEMORY[0x1E4FBC860];
LABEL_10:
  memcpy(v5, (const void *)(v7 + 32), 8 * v3);
  unint64_t result = swift_bridgeObjectRelease();
  *a2 = v3;
  a2[1] = (unint64_t)v5;
  return result;
}

uint64_t BNNSGraph.Context.setBatchSize(_:forFunction:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  v4[4] = a3;
  v4[5] = v3;
  v4[2] = a1;
  v4[3] = a2;
  return MEMORY[0x1F4188298](BNNSGraph.Context.setBatchSize(_:forFunction:), 0, 0);
}

uint64_t BNNSGraph.Context.setBatchSize(_:forFunction:)()
{
  if ((v0[2] & 0x8000000000000000) == 0)
  {
    if (v0[4]) {
      String.utf8CString.getter();
    }
    BNNSGraphContextSetBatchSize_v2();
    swift_unknownObjectRelease();
  }
  uint64_t v1 = (uint64_t (*)(void))v0[1];
  return v1();
}

uint64_t BNNSGraph.Context.checkForNaNsAndInfinities.setter(char a1)
{
  uint64_t result = BNNSGraphContextEnableNanAndInfChecks();
  *(unsigned char *)(v1 + 48) = a1;
  return result;
}

uint64_t BNNSGraph.Context.checkForNaNsAndInfinities.getter()
{
  return *(unsigned __int8 *)(v0 + 48);
}

uint64_t (*BNNSGraph.Context.checkForNaNsAndInfinities.modify(uint64_t a1))(uint64_t *a1)
{
  *(void *)a1 = v1;
  *(unsigned char *)(a1 + 8) = *(unsigned char *)(v1 + 48);
  return BNNSGraph.Context.checkForNaNsAndInfinities.modify;
}

uint64_t BNNSGraph.Context.checkForNaNsAndInfinities.modify(uint64_t *a1)
{
  uint64_t v1 = *a1;
  char v2 = *((unsigned char *)a1 + 8);
  uint64_t result = BNNSGraphContextEnableNanAndInfChecks();
  *(unsigned char *)(v1 + 48) = v2;
  return result;
}

uint64_t BNNSGraph.Context.executeFunction(_:arguments:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  v4[4] = a3;
  v4[5] = v3;
  v4[2] = a1;
  v4[3] = a2;
  return MEMORY[0x1F4188298](BNNSGraph.Context.executeFunction(_:arguments:), 0, 0);
}

uint64_t BNNSGraph.Context.executeFunction(_:arguments:)()
{
  uint64_t v1 = (char **)v0[4];
  char v2 = *v1;
  unint64_t v3 = *((void *)*v1 + 2);
  BNNSGraphContextSetArgumentType();
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
  {
    char v2 = specialized _ArrayBuffer._consumeAndCreateNew()((uint64_t)v2);
    if (v3) {
      goto LABEL_3;
    }
LABEL_11:
    uint64_t v5 = MEMORY[0x1E4FBC860];
    goto LABEL_12;
  }
  if (!v3) {
    goto LABEL_11;
  }
LABEL_3:
  type metadata accessor for bnns_graph_argument_t(0);
  uint64_t v4 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  uint64_t v5 = v4;
  uint64_t v6 = 0;
  *(void *)(v4 + 16) = v3;
  if (v3 < 4) {
    goto LABEL_7;
  }
  uint64_t v6 = v3 & 0x7FFFFFFFFFFFFFFCLL;
  int64x2_t v7 = (int64x2_t)xmmword_1D2136F20;
  uint64_t v8 = (double *)(v4 + 64);
  int64x2_t v9 = vdupq_n_s64((unint64_t)(v2 + 32));
  long long v10 = 0uLL;
  int64x2_t v11 = vdupq_n_s64(0x140uLL);
  int64x2_t v12 = vdupq_n_s64(4uLL);
  uint64_t v13 = v3 & 0x7FFFFFFFFFFFFFFCLL;
  do
  {
    v14.i64[0] = 160 * v7.i64[0];
    v14.i64[1] = 160 * v7.i64[1];
    uint64_t v15 = v8 - 4;
    int64x2_t v16 = vaddq_s64(v9, v14);
    vst2q_f64(v15, *(float64x2x2_t *)(&v10 - 1));
    v27.val[0] = (float64x2_t)vaddq_s64(v16, v11);
    v27.val[1] = 0uLL;
    vst2q_f64(v8, v27);
    int64x2_t v7 = vaddq_s64(v7, v12);
    v8 += 8;
    v13 -= 4;
  }
  while (v13);
  if (v3 != v6)
  {
LABEL_7:
    uint64_t v17 = (uint64_t)&v2[160 * v6 + 32];
    unint64_t v18 = (void *)(v4 + 16 * v6 + 40);
    unint64_t v19 = v3 - v6;
    do
    {
      *(v18 - 1) = v17;
      *unint64_t v18 = 0;
      v17 += 160;
      v18 += 2;
      --v19;
    }
    while (v19);
  }
LABEL_12:
  uint64_t v20 = v0[3];
  *(void *)(v5 + 16) = v3;
  if (v20) {
    String.utf8CString.getter();
  }
  int v21 = BNNSGraphContextExecute_v2();
  swift_bridgeObjectRelease();
  swift_unknownObjectRelease();
  uint64_t v22 = (char **)v0[4];
  if (v21)
  {
    lazy protocol witness table accessor for type BNNSGraph.Error and conformance BNNSGraph.Error();
    swift_allocError();
    *uint64_t v23 = 2;
    swift_willThrow();
  }
  *uint64_t v22 = v2;
  unint64_t v24 = (uint64_t (*)(void))v0[1];
  return v24();
}

uint64_t BNNSGraph.Context.executeFunction<A>(_:arguments:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v12 = MEMORY[0x1D25FF9B0](a3, a4);
  if (v12 < 0)
  {
    __break(1u);
  }
  else
  {
    uint64_t v5 = v12;
    if (v12)
    {
      type metadata accessor for bnns_graph_argument_t(0);
      a1 = static Array._allocateBufferUninitialized(minimumCapacity:)();
      *(void *)(a1 + 16) = v5;
    }
    else
    {
      a1 = MEMORY[0x1E4FBC860];
    }
    uint64_t v18 = 0;
    v17[0] = a1 + 32;
    v17[1] = v5;
    closure #1 in BNNSGraph.Context.executeFunction<A>(_:arguments:)(v17, (uint64_t)&v18, v5, a3, a4, a5);
    if (v6) {
      goto LABEL_14;
    }
    if (v5 >= v18)
    {
      *(void *)(a1 + 16) = v18;
      BNNSGraphContextSetArgumentType();
      if (a2) {
        String.utf8CString.getter();
      }
      int v13 = BNNSGraphContextExecute_v2();
      swift_unknownObjectRelease();
      uint64_t result = swift_bridgeObjectRelease();
      if (v13)
      {
        lazy protocol witness table accessor for type BNNSGraph.Error and conformance BNNSGraph.Error();
        swift_allocError();
        *uint64_t v15 = 2;
        return swift_willThrow();
      }
      return result;
    }
  }
  __break(1u);
LABEL_14:
  uint64_t v16 = v18;
  if (v5 < v18) {
    __break(1u);
  }
  *(void *)(a1 + 16) = v16;
  uint64_t result = swift_bridgeObjectRelease();
  __break(1u);
  return result;
}

uint64_t closure #1 in BNNSGraph.Context.executeFunction<A>(_:arguments:)(void *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v40 = a4;
  uint64_t v8 = a1;
  uint64_t v38 = *(void *)(a5 - 8);
  uint64_t v9 = MEMORY[0x1F4188790](a1);
  long long v36 = (char *)&v29 - ((v10 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t result = MEMORY[0x1F4188790](v9);
  unint64_t v37 = (char *)&v29 - v14;
  if ((v13 & 0x8000000000000000) == 0)
  {
    long long v30 = v12;
    if (!v13)
    {
LABEL_8:
      *long long v30 = v13;
      return result;
    }
    uint64_t v15 = 0;
    uint64_t v16 = 0;
    uint64_t v34 = *(uint64_t (**)(uint64_t, uint64_t))(a6 + 16);
    uint64_t v35 = a6 + 16;
    uint64_t v17 = (uint64_t (**)(char *, uint64_t))(v38 + 8);
    uint64_t v32 = v13;
    uint64_t v33 = a6 + 24;
    uint64_t v31 = v8;
    while (v13 != v16)
    {
      uint64_t v18 = *v8;
      uint64_t v39 = v15;
      unint64_t v19 = (uint64_t *)(v18 + v15);
      *unint64_t v19 = 0;
      v19[1] = 0;
      uint64_t v20 = v37;
      Array.subscript.getter();
      uint64_t v21 = v34(a5, a6);
      uint64_t v22 = a6;
      uint64_t v23 = *v17;
      uint64_t result = (*v17)(v20, a5);
      if (!v21) {
        goto LABEL_12;
      }
      *unint64_t v19 = v21;
      uint64_t v24 = *v8;
      long long v25 = v36;
      Array.subscript.getter();
      uint64_t v26 = (*(uint64_t (**)(uint64_t, uint64_t))(v22 + 24))(a5, v22);
      uint64_t result = v23(v25, a5);
      uint64_t v27 = *(void *)(v38 + 72);
      if ((unsigned __int128)(v26 * (__int128)v27) >> 64 != (v26 * v27) >> 63) {
        goto LABEL_10;
      }
      ++v16;
      uint64_t v28 = v39;
      *(void *)(v24 + v39 + 8) = v26 * v27;
      uint64_t v15 = v28 + 16;
      uint64_t v13 = v32;
      a6 = v22;
      uint64_t v8 = v31;
      if (v32 == v16) {
        goto LABEL_8;
      }
    }
    __break(1u);
LABEL_10:
    __break(1u);
  }
  __break(1u);
LABEL_12:
  __break(1u);
  return result;
}

Swift::Int __swiftcall BNNSGraph.Context.argumentPosition(forFunction:argument:)(Swift::String_optional forFunction, Swift::String argument)
{
  if (forFunction.value._object) {
    String.utf8CString.getter();
  }
  String.utf8CString.getter();
  Swift::Int ArgumentPosition = BNNSGraphGetArgumentPosition();
  swift_unknownObjectRelease();
  swift_release();
  return ArgumentPosition;
}

uint64_t BNNSGraph.Context.tensor(forFunction:argument:fillKnownDynamicShapes:)@<X0>(uint64_t a1@<X1>, uint64_t a2@<X8>)
{
  uint64_t v8 = *MEMORY[0x1E4F143B8];
  *(_DWORD *)int64x2_t v7 = 0;
  v7[4] = 0;
  memset(&v7[8], 0, 152);
  if (a1) {
    String.utf8CString.getter();
  }
  String.utf8CString.getter();
  int Tensor = BNNSGraphContextGetTensor();
  swift_unknownObjectRelease();
  swift_release();
  if (Tensor)
  {
    _sSo10BNNSTensoraSgWOi0_((uint64_t)v5);
  }
  else
  {
    v5[6] = *(_OWORD *)&v7[96];
    v5[7] = *(_OWORD *)&v7[112];
    v5[8] = *(_OWORD *)&v7[128];
    v5[9] = *(_OWORD *)&v7[144];
    v5[2] = *(_OWORD *)&v7[32];
    v5[3] = *(_OWORD *)&v7[48];
    v5[4] = *(_OWORD *)&v7[64];
    v5[5] = *(_OWORD *)&v7[80];
    v5[0] = *(_OWORD *)v7;
    v5[1] = *(_OWORD *)&v7[16];
    _sSo10BNNSTensoraSgWOi_((uint64_t)v5);
  }
  outlined init with take of BNNSTensor?((uint64_t)v5, (uint64_t)v6);
  return outlined init with take of BNNSTensor?((uint64_t)v6, a2);
}

Swift::Int __swiftcall BNNSGraph.Context.argumentCount(forFunction:)(Swift::String_optional forFunction)
{
  if (forFunction.value._object) {
    String.utf8CString.getter();
  }
  Swift::Int ArgumentCount = BNNSGraphGetArgumentCount();
  swift_unknownObjectRelease();
  return ArgumentCount;
}

uint64_t BNNSGraph.Context.functionCount.getter()
{
  return BNNSGraphGetFunctionCount();
}

uint64_t BNNSGraph.Context.argumentNames(forFunction:)(uint64_t a1, uint64_t a2)
{
  if (a2) {
    String.utf8CString.getter();
  }
  uint64_t ArgumentCount = BNNSGraphGetArgumentCount();
  uint64_t result = swift_unknownObjectRelease();
  if (ArgumentCount < 0)
  {
    __break(1u);
LABEL_18:
    __break(1u);
    return result;
  }
  uint64_t v5 = MEMORY[0x1E4FBC860];
  if (ArgumentCount)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafePointer<Int8>?);
    uint64_t v6 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v6 + 16) = ArgumentCount;
    bzero((void *)(v6 + 32), 8 * ArgumentCount);
    if (!a2) {
      goto LABEL_8;
    }
    goto LABEL_6;
  }
  uint64_t v6 = MEMORY[0x1E4FBC860];
  if (a2) {
LABEL_6:
  }
    String.utf8CString.getter();
LABEL_8:
  BNNSGraphGetArgumentNames();
  swift_unknownObjectRelease();
  int64_t v7 = *(void *)(v6 + 16);
  if (v7)
  {
    swift_bridgeObjectRetain();
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v7, 0);
    uint64_t v8 = 0;
    while (1)
    {
      uint64_t result = *(void *)(v6 + 8 * v8 + 32);
      if (!result) {
        break;
      }
      uint64_t v9 = String.init(cString:)();
      uint64_t v11 = v10;
      unint64_t v13 = *(void *)(v5 + 16);
      unint64_t v12 = *(void *)(v5 + 24);
      if (v13 >= v12 >> 1) {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v12 > 1), v13 + 1, 1);
      }
      ++v8;
      *(void *)(v5 + 16) = v13 + 1;
      uint64_t v14 = v5 + 16 * v13;
      *(void *)(v14 + 32) = v9;
      *(void *)(v14 + 40) = v11;
      if (v7 == v8)
      {
        swift_bridgeObjectRelease_n();
        return v5;
      }
    }
    goto LABEL_18;
  }
  swift_bridgeObjectRelease();
  return v5;
}

uint64_t BNNSGraph.Context.functionNames.getter()
{
  uint64_t result = BNNSGraphGetFunctionCount();
  if (result < 0)
  {
    __break(1u);
LABEL_15:
    __break(1u);
    return result;
  }
  uint64_t v1 = result;
  uint64_t v2 = MEMORY[0x1E4FBC860];
  if (result)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafePointer<Int8>?);
    uint64_t v3 = static Array._allocateBufferUninitialized(minimumCapacity:)();
    *(void *)(v3 + 16) = v1;
    bzero((void *)(v3 + 32), 8 * v1);
  }
  else
  {
    uint64_t v3 = MEMORY[0x1E4FBC860];
  }
  BNNSGraphGetFunctionNames();
  int64_t v4 = *(void *)(v3 + 16);
  if (v4)
  {
    swift_bridgeObjectRetain();
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v4, 0);
    uint64_t v5 = 0;
    while (1)
    {
      uint64_t result = *(void *)(v3 + 8 * v5 + 32);
      if (!result) {
        break;
      }
      uint64_t v6 = String.init(cString:)();
      uint64_t v8 = v7;
      unint64_t v10 = *(void *)(v2 + 16);
      unint64_t v9 = *(void *)(v2 + 24);
      if (v10 >= v9 >> 1) {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v9 > 1), v10 + 1, 1);
      }
      ++v5;
      *(void *)(v2 + 16) = v10 + 1;
      uint64_t v11 = v2 + 16 * v10;
      *(void *)(v11 + 32) = v6;
      *(void *)(v11 + 40) = v8;
      if (v4 == v5)
      {
        swift_bridgeObjectRelease_n();
        return v2;
      }
    }
    goto LABEL_15;
  }
  swift_bridgeObjectRelease();
  return v2;
}

double BNNSGraph.Shape.init(arrayLiteral:)@<D0>(uint64_t a1@<X0>, _OWORD *a2@<X8>)
{
  BNNSGraph.Shape.init(_:)(a1, (unint64_t *)&v4);
  double result = *(double *)&v4;
  *a2 = v4;
  return result;
}

double protocol witness for ExpressibleByArrayLiteral.init(arrayLiteral:) in conformance BNNSGraph.Shape@<D0>(uint64_t a1@<X0>, _OWORD *a2@<X8>)
{
  BNNSGraph.Shape.init(_:)(a1, (unint64_t *)&v4);
  double result = *(double *)&v4;
  *a2 = v4;
  return result;
}

BOOL static BNNSGraph.Error.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void BNNSGraph.Error.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int BNNSGraph.Error.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

char *specialized _ArrayBuffer._consumeAndCreateNew()(uint64_t a1)
{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(void *)(a1 + 16), 0, (char *)a1);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(void *)(a1 + 16), 0, (char *)a1);
}

{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(void *)(a1 + 16), 0, (char *)a1);
}

double _sSo10BNNSTensoraSgWOi0_(uint64_t a1)
{
  double result = 0.0;
  *(_OWORD *)(a1 + 128) = 0u;
  *(_OWORD *)(a1 + 144) = 0u;
  *(_OWORD *)(a1 + 96) = 0u;
  *(_OWORD *)(a1 + 112) = 0u;
  *(_OWORD *)(a1 + 64) = 0u;
  *(_OWORD *)(a1 + 80) = 0u;
  *(_OWORD *)(a1 + 32) = 0u;
  *(_OWORD *)(a1 + 48) = 0u;
  *(_OWORD *)a1 = 0u;
  *(_OWORD *)(a1 + 16) = 0u;
  *(unsigned char *)(a1 + 160) = 1;
  return result;
}

uint64_t outlined init with take of BNNSTensor?(uint64_t a1, uint64_t a2)
{
  uint64_t v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSTensor?);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(void *)(v4 - 8) + 32))(a2, a1, v4);
  return a2;
}

uint64_t _sSo10BNNSTensoraSgWOi_(uint64_t result)
{
  *(unsigned char *)(result + 160) = 0;
  return result;
}

uint64_t sub_1D20B2AC8@<X0>(uint64_t a1@<X0>, unsigned char *a2@<X8>)
{
  return keypath_getTm(a1, MEMORY[0x1E4F16798], a2);
}

uint64_t sub_1D20B2AE0(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return keypath_setTm(a1, a2, a3, a4, MEMORY[0x1E4F167A8]);
}

uint64_t sub_1D20B2AF8@<X0>(uint64_t a1@<X0>, unsigned char *a2@<X8>)
{
  return keypath_getTm(a1, MEMORY[0x1E4F16790], a2);
}

uint64_t keypath_getTm@<X0>(uint64_t a1@<X0>, uint64_t (*a2)(void, void)@<X3>, unsigned char *a3@<X8>)
{
  uint64_t result = a2(*(void *)(*(void *)a1 + 16), *(void *)(*(void *)a1 + 24));
  *a3 = result;
  return result;
}

uint64_t sub_1D20B2B44(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return keypath_setTm(a1, a2, a3, a4, MEMORY[0x1E4F167A0]);
}

uint64_t keypath_setTm(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(void, void, void))
{
  return a5(*(void *)(*(void *)a2 + 16), *(void *)(*(void *)a2 + 24), *a1);
}

uint64_t sub_1D20B2B70@<X0>(_DWORD *a1@<X8>)
{
  uint64_t result = BNNSGraphCompileOptionsGetOptimizationPreference();
  *a1 = result;
  return result;
}

uint64_t sub_1D20B2BA0(unsigned int *a1, uint64_t a2)
{
  return MEMORY[0x1F40D1200](*(void *)(*(void *)a2 + 16), *(void *)(*(void *)a2 + 24), *a1);
}

uint64_t sub_1D20B2BB0@<X0>(uint64_t result@<X0>, unsigned char *a2@<X8>)
{
  *a2 = *(unsigned char *)(*(void *)result + 48);
  return result;
}

uint64_t sub_1D20B2BC0(char *a1, uint64_t *a2)
{
  char v2 = *a1;
  uint64_t v3 = *a2;
  uint64_t result = BNNSGraphContextEnableNanAndInfChecks();
  *(unsigned char *)(v3 + 48) = v2;
  return result;
}

ValueMetadata *type metadata accessor for BNNSGraph()
{
  return &type metadata for BNNSGraph;
}

uint64_t dispatch thunk of BNNSGraph.PointerArgument.baseAddress.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

uint64_t dispatch thunk of BNNSGraph.PointerArgument.count.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 24))();
}

ValueMetadata *type metadata accessor for BNNSGraph.CompileOptions()
{
  return &type metadata for BNNSGraph.CompileOptions;
}

ValueMetadata *type metadata accessor for BNNSGraph.CompileOptions.OptimizationPreference()
{
  return &type metadata for BNNSGraph.CompileOptions.OptimizationPreference;
}

uint64_t type metadata accessor for BNNSGraph.Context()
{
  return self;
}

uint64_t method lookup function for BNNSGraph.Context(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNSGraph.Context);
}

uint64_t dispatch thunk of BNNSGraph.Context.__allocating_init(compileFromPath:functionName:options:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v14 = (uint64_t (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))(*(void *)(v5 + 104)
                                                                            + **(int **)(v5 + 104));
  unint64_t v12 = (void *)swift_task_alloc();
  *(void *)(v6 + 16) = v12;
  *unint64_t v12 = v6;
  v12[1] = dispatch thunk of BNNSGraph.Context.__allocating_init(compileFromPath:functionName:options:);
  return v14(a1, a2, a3, a4, a5);
}

uint64_t dispatch thunk of BNNSGraph.Context.__allocating_init(compileFromPath:functionName:options:)(uint64_t a1)
{
  uint64_t v5 = *v1;
  swift_task_dealloc();
  uint64_t v3 = *(uint64_t (**)(uint64_t))(v5 + 8);
  return v3(a1);
}

uint64_t dispatch thunk of BNNSGraph.Context.setDynamicShapes(_:forFunction:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  unint64_t v10 = (uint64_t (*)(uint64_t, uint64_t, uint64_t))(*(void *)(*(void *)v3 + 112)
                                                          + **(int **)(*(void *)v3 + 112));
  uint64_t v8 = (void *)swift_task_alloc();
  *(void *)(v4 + 16) = v8;
  *uint64_t v8 = v4;
  v8[1] = dispatch thunk of BNNSGraph.Context.setDynamicShapes(_:forFunction:);
  return v10(a1, a2, a3);
}

uint64_t dispatch thunk of BNNSGraph.Context.setBatchSize(_:forFunction:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  unint64_t v10 = (uint64_t (*)(uint64_t, uint64_t, uint64_t))(*(void *)(*(void *)v3 + 120)
                                                          + **(int **)(*(void *)v3 + 120));
  uint64_t v8 = (void *)swift_task_alloc();
  *(void *)(v4 + 16) = v8;
  *uint64_t v8 = v4;
  v8[1] = dispatch thunk of BNNSGraph.Context.setBatchSize(_:forFunction:);
  return v10(a1, a2, a3);
}

uint64_t dispatch thunk of BNNSGraph.Context.setBatchSize(_:forFunction:)()
{
  uint64_t v3 = *v0;
  swift_task_dealloc();
  Swift::UInt v1 = *(uint64_t (**)(void))(v3 + 8);
  return v1();
}

uint64_t dispatch thunk of BNNSGraph.Context.checkForNaNsAndInfinities.getter()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 128))();
}

uint64_t dispatch thunk of BNNSGraph.Context.checkForNaNsAndInfinities.setter()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 136))();
}

uint64_t dispatch thunk of BNNSGraph.Context.checkForNaNsAndInfinities.modify()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 144))();
}

uint64_t dispatch thunk of BNNSGraph.Context.executeFunction(_:arguments:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  unint64_t v10 = (uint64_t (*)(uint64_t, uint64_t, uint64_t))(*(void *)(*(void *)v3 + 176)
                                                          + **(int **)(*(void *)v3 + 176));
  uint64_t v8 = (void *)swift_task_alloc();
  *(void *)(v4 + 16) = v8;
  *uint64_t v8 = v4;
  v8[1] = dispatch thunk of BNNSGraph.Context.setBatchSize(_:forFunction:);
  return v10(a1, a2, a3);
}

uint64_t dispatch thunk of BNNSGraph.Context.executeFunction<A>(_:arguments:)()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 184))();
}

uint64_t dispatch thunk of BNNSGraph.Context.argumentPosition(forFunction:argument:)()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 192))();
}

uint64_t dispatch thunk of BNNSGraph.Context.tensor(forFunction:argument:fillKnownDynamicShapes:)()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 200))();
}

uint64_t dispatch thunk of BNNSGraph.Context.argumentCount(forFunction:)()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 208))();
}

uint64_t dispatch thunk of BNNSGraph.Context.functionCount.getter()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 216))();
}

uint64_t dispatch thunk of BNNSGraph.Context.argumentNames(forFunction:)()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 224))();
}

uint64_t dispatch thunk of BNNSGraph.Context.functionNames.getter()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 232))();
}

ValueMetadata *type metadata accessor for BNNSGraph.Shape()
{
  return &type metadata for BNNSGraph.Shape;
}

unsigned char *storeEnumTagSinglePayload for BNNSGraph.Error(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 3 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 3) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFD) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFC)
  {
    unsigned int v6 = ((a2 - 253) >> 8) + 1;
    *uint64_t result = a2 + 3;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D20B3540);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 3;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNSGraph.Error()
{
  return &type metadata for BNNSGraph.Error;
}

void *specialized _ArrayBuffer._consumeAndCreateNew()(void *a1)
{
  return specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, a1[2], 0, a1);
}

uint64_t static vDSP.powerToDecibels<A>(_:zeroReference:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.powerToDecibels<A>(_:zeroReference:)(a1, a2, a3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:));
}

{
  return static vDSP.powerToDecibels<A>(_:zeroReference:)(a1, a2, a3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:));
}

uint64_t partial apply for closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:)(a1, a2, *(void *)(v2 + 32), *(void *)(v2 + 16), *(void *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float))static vDSP.convert<A, B>(power:toDecibels:zeroReference:), *(float *)(v2 + 40));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:)(a1, a2, *(void *)(v2 + 32), *(void *)(v2 + 16), *(void *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double))static vDSP.convert<A, B>(power:toDecibels:zeroReference:), *(double *)(v2 + 40));
}

uint64_t static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  return static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:));
}

uint64_t static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  return static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:));
}

uint64_t static vDSP.amplitudeToDecibels<A>(_:zeroReference:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.powerToDecibels<A>(_:zeroReference:)(a1, a2, a3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.amplitudeToDecibels<A>(_:zeroReference:));
}

{
  return static vDSP.powerToDecibels<A>(_:zeroReference:)(a1, a2, a3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.amplitudeToDecibels<A>(_:zeroReference:));
}

uint64_t static vDSP.powerToDecibels<A>(_:zeroReference:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(void *, uint64_t *))
{
  uint64_t v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, a4);
}

{
  uint64_t v5;

  uint64_t v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, a4);
}

uint64_t closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, void (*a6)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float), float a7)
{
  uint64_t v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  a6(a3, a1, a4, v14, a5, v15, a7);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  return static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:));
}

uint64_t static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(uint64_t a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  v22[0] = a8;
  uint64_t v14 = *(void *)(a4 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v16 = (char *)v22 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v19 = (*(uint64_t (**)(uint64_t))(*(void *)(v17 + 8) + 16))(v18);
  (*(void (**)(char *, uint64_t, uint64_t))(v14 + 16))(v16, a1, a4);
  uint64_t v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v14 + 8))(v16, a4);
  if (v20 == v19)
  {
    MEMORY[0x1F4188790](result);
    v22[-8] = a4;
    v22[-7] = a5;
    v22[-6] = a6;
    v22[-5] = a7;
    v22[-4] = a1;
    *(float *)&v22[-3] = a2;
    v22[-2] = v19;
    return (*(uint64_t (**)(void))(a7 + 16))(v22[0]);
  }
  else
  {
    __break(1u);
  }
  return result;
}

void closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(const float *a1, int a2, float **a3, vDSP_Length __N, unsigned int __F, float a6)
{
  uint64_t v7 = *MEMORY[0x1E4F143B8];
  float __B = a6;
  if (!a1) {
    goto LABEL_6;
  }
  if (!*a3) {
    goto LABEL_7;
  }
  if ((__N & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  vDSP_vdbcon(a1, 1, &__B, *a3, 1, __N, __F);
}

uint64_t closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, void (*a6)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double), double a7)
{
  uint64_t v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  a6(a3, a1, a4, v14, a5, v15, a7);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  return static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:));
}

uint64_t static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(uint64_t a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  v22[0] = a8;
  uint64_t v14 = *(void *)(a4 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v16 = (char *)v22 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v19 = (*(uint64_t (**)(uint64_t))(*(void *)(v17 + 8) + 16))(v18);
  (*(void (**)(char *, uint64_t, uint64_t))(v14 + 16))(v16, a1, a4);
  uint64_t v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v14 + 8))(v16, a4);
  if (v20 == v19)
  {
    MEMORY[0x1F4188790](result);
    v22[-8] = a4;
    v22[-7] = a5;
    v22[-6] = a6;
    v22[-5] = a7;
    v22[-4] = a1;
    *(double *)&v22[-3] = a2;
    v22[-2] = v19;
    return (*(uint64_t (**)(void))(a7 + 16))(v22[0]);
  }
  else
  {
    __break(1u);
  }
  return result;
}

void closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(const double *a1, int a2, double **a3, vDSP_Length __N, unsigned int __F, double a6)
{
  v6[1] = *(double *)MEMORY[0x1E4F143B8];
  v6[0] = a6;
  if (!a1) {
    goto LABEL_6;
  }
  if (!*a3) {
    goto LABEL_7;
  }
  if ((__N & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  vDSP_vdbconD(a1, 1, v6, *a3, 1, __N, __F);
}

uint64_t partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:));
}

{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:));
}

uint64_t partial apply for closure #1 in static vDSP.amplitudeToDecibels<A>(_:zeroReference:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:)(a1, a2, *(void *)(v2 + 32), *(void *)(v2 + 16), *(void *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float))static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:), *(float *)(v2 + 40));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.powerToDecibels<A>(_:zeroReference:)(a1, a2, *(void *)(v2 + 32), *(void *)(v2 + 16), *(void *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double))static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:), *(double *)(v2 + 40));
}

uint64_t partial apply for closure #1 in static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:));
}

{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:));
}

uint64_t partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(v2 + 16);
  uint64_t v4 = *(void *)(v2 + 32);
  uint64_t v5 = *(void *)(v2 + 64);
  v7[4] = *(_DWORD *)(v2 + 56);
  uint64_t v8 = a1;
  uint64_t v9 = v5;
  return (*(uint64_t (**)(uint64_t, _DWORD *, uint64_t, uint64_t))(v4 + 24))(a2, v7, MEMORY[0x1E4FBC848] + 8, v3);
}

{
  void *v2;
  uint64_t v3;
  uint64_t v4;
  uint64_t v5;
  void v7[6];

  uint64_t v3 = v2[2];
  uint64_t v4 = v2[4];
  uint64_t v5 = v2[8];
  void v7[2] = v2[7];
  void v7[3] = a1;
  v7[4] = v5;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v4 + 24))(a2, v7, MEMORY[0x1E4FBC848] + 8, v3);
}

void partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:)(const double *a1, int a2)
{
  closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, a2, *(double ***)(v2 + 24), *(void *)(v2 + 32), 1u, *(double *)(v2 + 16));
}

void partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(amplitude:toDecibels:zeroReference:)(const float *a1, int a2)
{
  closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, a2, *(float ***)(v2 + 24), *(void *)(v2 + 32), 1u, *(float *)(v2 + 16));
}

void partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(const double *a1, int a2)
{
  closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, a2, *(double ***)(v2 + 24), *(void *)(v2 + 32), 0, *(double *)(v2 + 16));
}

void partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(const float *a1, int a2)
{
  closure #1 in closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, a2, *(float ***)(v2 + 24), *(void *)(v2 + 32), 0, *(float *)(v2 + 16));
}

uint64_t static vDSP.DFTSinglePrecisionSplitComplexFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, unsigned char *a3, char *a4)
{
  return specialized static vDSP.DFTSinglePrecisionSplitComplexFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(a1, a2, a3, a4, MEMORY[0x1E4F16810], MEMORY[0x1E4F16820]);
}

uint64_t protocol witness for static vDSP_DiscreteTransformLifecycleFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:) in conformance vDSP.DFTSinglePrecisionSplitComplexFunctions(uint64_t a1, uint64_t a2, unsigned char *a3, char *a4)
{
  return specialized static vDSP.DFTSinglePrecisionSplitComplexFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(a1, a2, a3, a4, MEMORY[0x1E4F16810], MEMORY[0x1E4F16820]);
}

uint64_t static vDSP.DFTDoublePrecisionSplitComplexFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, unsigned char *a3, char *a4)
{
  return specialized static vDSP.DFTSinglePrecisionSplitComplexFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(a1, a2, a3, a4, MEMORY[0x1E4F16818], MEMORY[0x1E4F16828]);
}

uint64_t protocol witness for static vDSP_DiscreteTransformLifecycleFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:) in conformance vDSP.DFTDoublePrecisionSplitComplexFunctions(uint64_t a1, uint64_t a2, unsigned char *a3, char *a4)
{
  return specialized static vDSP.DFTSinglePrecisionSplitComplexFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(a1, a2, a3, a4, MEMORY[0x1E4F16818], MEMORY[0x1E4F16828]);
}

void static vDSP.DFTSinglePrecisionInterleavedFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
}

void protocol witness for static vDSP_DiscreteTransformLifecycleFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:) in conformance vDSP.DFTSinglePrecisionInterleavedFunctions(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
}

void static vDSP.DFTDoublePrecisionInterleavedFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
}

void protocol witness for static vDSP_DiscreteTransformLifecycleFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:) in conformance vDSP.DFTDoublePrecisionInterleavedFunctions(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
}

uint64_t vDSP.DFTError.errorDescription.getter()
{
  int v1 = *(char *)(v0 + 8);
  if (v1 < 0)
  {
    _StringGuts.grow(_:)(126);
    v6._countAndFlagsBits = 0x2064696C61766E49;
    v6._object = (void *)0xEF2820746E756F63;
    String.append(_:)(v6);
    v7._countAndFlagsBits = dispatch thunk of CustomStringConvertible.description.getter();
    String.append(_:)(v7);
    swift_bridgeObjectRelease();
    unint64_t v8 = 0x80000001D213C550;
    uint64_t v4 = 0x100000000000006DLL;
  }
  else
  {
    if (v1)
    {
      _StringGuts.grow(_:)(136);
      v9._countAndFlagsBits = 0x2064696C61766E49;
      v9._object = (void *)0xEF2820746E756F63;
      String.append(_:)(v9);
      v10._countAndFlagsBits = dispatch thunk of CustomStringConvertible.description.getter();
      String.append(_:)(v10);
      swift_bridgeObjectRelease();
      uint64_t v4 = 0x1000000000000077;
    }
    else
    {
      _StringGuts.grow(_:)(140);
      v2._countAndFlagsBits = 0x2064696C61766E49;
      v2._object = (void *)0xEF2820746E756F63;
      String.append(_:)(v2);
      v3._countAndFlagsBits = dispatch thunk of CustomStringConvertible.description.getter();
      String.append(_:)(v3);
      swift_bridgeObjectRelease();
      uint64_t v4 = 0x100000000000007BLL;
    }
    unint64_t v8 = (unint64_t)(v5 - 32) | 0x8000000000000000;
  }
  String.append(_:)(*(Swift::String *)&v4);
  return 0;
}

uint64_t vDSP.DiscreteFourierTransform.__allocating_init(previous:count:direction:transformType:ofType:)(uint64_t a1, uint64_t a2, char *a3, char *a4)
{
  uint64_t v8 = swift_allocObject();
  vDSP.DiscreteFourierTransform.init(previous:count:direction:transformType:ofType:)(a1, a2, a3, a4);
  return v8;
}

uint64_t vDSP.DiscreteFourierTransform.init(previous:count:direction:transformType:ofType:)(uint64_t a1, uint64_t a2, char *a3, char *a4)
{
  uint64_t v5 = v4;
  char v6 = *a3;
  char v7 = *a4;
  if (a1) {
    uint64_t v8 = *(void *)(a1 + 16);
  }
  else {
    uint64_t v8 = 0;
  }
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  char v17 = v6;
  char v16 = v7;
  uint64_t AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  uint64_t v11 = (*(uint64_t (**)(uint64_t, uint64_t, char *, char *, uint64_t, uint64_t))(AssociatedConformanceWitness + 8))(v8, a2, &v17, &v16, AssociatedTypeWitness, AssociatedConformanceWitness);
  if (v15)
  {
    swift_release();
    type metadata accessor for vDSP.DiscreteFourierTransform();
    swift_deallocPartialClassInstance();
  }
  else
  {
    uint64_t v12 = v11;
    swift_release();
    *(void *)(v5 + 16) = v12;
  }
  return v5;
}

uint64_t vDSP.DiscreteFourierTransform.deinit()
{
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  uint64_t v2 = *(void *)(v0 + 16);
  uint64_t AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  (*(void (**)(uint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness + 16))(v2, AssociatedTypeWitness, AssociatedConformanceWitness);
  return v0;
}

uint64_t vDSP.DiscreteFourierTransform.__deallocating_deinit()
{
  vDSP.DiscreteFourierTransform.deinit();

  return swift_deallocClassInstance();
}

uint64_t vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(a1, a2, a3, a4, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  return vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, a3, a4, a5, a6, a7, a8, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

{
  return vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, a3, a4, a5, a6, a7, a8, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t, void *))
{
  uint64_t v9 = v6;
  uint64_t v17 = MEMORY[0x1E4FBC860];
  uint64_t v14 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  v16[2] = a3;
  v16[3] = a4;
  v16[4] = &v17;
  void v16[5] = a2;
  v16[6] = v9;
  v16[7] = a1;
  return a6(v14, a5, v16);
}

uint64_t closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(uint64_t a1, uint64_t *a2, void *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t (*a10)(uint64_t, uint64_t, char *))
{
  uint64_t v27 = a2;
  char v16 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  uint64_t v17 = v16(a7, a8);
  uint64_t v21 = a7;
  uint64_t v22 = a8;
  uint64_t v23 = a5;
  uint64_t v24 = a6;
  uint64_t v25 = a4;
  uint64_t v26 = a1;
  *a3 = a10(v17, a9, v20);
  swift_bridgeObjectRelease();
  uint64_t result = v16(a7, a8);
  *uint64_t v27 = result;
  return result;
}

uint64_t closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, unint64_t *a10, uint64_t a11)
{
  uint64_t v20 = a7;
  uint64_t v21 = __swift_instantiateConcreteTypeFromMangledName(a9);
  uint64_t v22 = a8;
  uint64_t v23 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a10, a9);
  uint64_t v24 = a5;
  uint64_t v25 = a6;
  uint64_t v26 = a1;
  uint64_t v27 = a3;
  (*(void (**)(uint64_t, unsigned char *, uint64_t, uint64_t, uint64_t))(a8 + 24))(a11, v19, MEMORY[0x1E4FBC848] + 8, v20, a8);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a7, a8);
  *a2 = result;
  return result;
}

uint64_t vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  _OWORD v10[2] = a5;
  v10[3] = a6;
  void v10[4] = a7;
  v10[5] = a8;
  v10[6] = a2;
  v10[7] = a3;
  v10[8] = a4;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t, uint64_t))(a7 + 24))(a9, v10, MEMORY[0x1E4FBC848] + 8, a5, a7);
}

void *closure #1 in closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(void *result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, void *a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t (*a12)(void, uint64_t, uint64_t))
{
  if (!a3)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (!a5)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (!*a7)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (*result) {
    return (void *)a12(*(void *)(a2 + 16), a3, a5);
  }
LABEL_9:
  __break(1u);
  return result;
}

uint64_t vDSP.DiscreteFourierTransform<>.transform<A>(input:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return vDSP.DiscreteFourierTransform<>.transform<A>(input:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(input:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return vDSP.DiscreteFourierTransform<>.transform<A>(input:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(input:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:));
}

{
  return vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:));
}

uint64_t vDSP.DiscreteFourierTransform<>.transform<A>(input:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(uint64_t, uint64_t, void *))
{
  uint64_t v8 = v5;
  uint64_t v12 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  v14[2] = a2;
  v14[3] = a3;
  void v14[4] = v8;
  v14[5] = a1;
  return a5(v12, a4, v14);
}

uint64_t closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(input:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t *a7, unint64_t *a8, uint64_t a9)
{
  uint64_t v18 = a5;
  uint64_t v19 = __swift_instantiateConcreteTypeFromMangledName(a7);
  uint64_t v20 = a6;
  uint64_t v21 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a8, a7);
  uint64_t v22 = a1;
  uint64_t v23 = a3;
  (*(void (**)(uint64_t, unsigned char *, uint64_t, uint64_t, uint64_t))(a6 + 24))(a9, v17, MEMORY[0x1E4FBC848] + 8, v18, a6);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  _OWORD v8[2] = a3;
  void v8[3] = a4;
  void v8[4] = a5;
  void v8[5] = a6;
  void v8[6] = a2;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(a5 + 24))(a7, v8, MEMORY[0x1E4FBC848] + 8, a3);
}

uint64_t specialized static vDSP.DFTSinglePrecisionSplitComplexFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(uint64_t result, uint64_t a2, unsigned char *a3, char *a4, uint64_t (*a5)(uint64_t, uint64_t, uint64_t), uint64_t (*a6)(uint64_t, uint64_t, uint64_t))
{
  char v7 = *a4;
  if (*a4)
  {
    if ((a2 & 0x8000000000000000) == 0)
    {
      if (*a3) {
        uint64_t v8 = 0xFFFFFFFFLL;
      }
      else {
        uint64_t v8 = 1;
      }
      uint64_t v9 = a6(result, a2, v8);
      if (v9) {
        return v9;
      }
      goto LABEL_13;
    }
    __break(1u);
  }
  else if ((a2 & 0x8000000000000000) == 0)
  {
    if (*a3) {
      uint64_t v10 = 0xFFFFFFFFLL;
    }
    else {
      uint64_t v10 = 1;
    }
    uint64_t v9 = a5(result, a2, v10);
    if (v9) {
      return v9;
    }
LABEL_13:
    lazy protocol witness table accessor for type vDSP.DFTError and conformance vDSP.DFTError();
    swift_allocError();
    *(void *)uint64_t v11 = a2;
    *(unsigned char *)(v11 + 8) = v7;
    swift_willThrow();
    return v9;
  }
  __break(1u);
  return result;
}

void specialized static vDSP.DFTSinglePrecisionInterleavedFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(void))
{
  if (a2 < 0)
  {
    __break(1u);
  }
  else if (!a5())
  {
    lazy protocol witness table accessor for type vDSP.DFTError and conformance vDSP.DFTError();
    swift_allocError();
    *(void *)uint64_t v6 = a2;
    *(unsigned char *)(v6 + 8) = 0x80;
    swift_willThrow();
  }
}

uint64_t type metadata accessor for vDSP.DiscreteFourierTransform()
{
  return __swift_instantiateGenericMetadata();
}

uint64_t partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

{
  return partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t (*a4)(uint64_t, uint64_t, char *))
{
  return closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(a1, a2, *(void **)(v4 + 32), *(void *)(v4 + 40), *(void *)(v4 + 48), *(void *)(v4 + 56), *(void *)(v4 + 16), *(void *)(v4 + 24), a3, a4);
}

uint64_t partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 72);
  long long v5 = *(_OWORD *)(v3 + 32);
  long long v8 = *(_OWORD *)(v3 + 16);
  long long v9 = v5;
  long long v10 = *(_OWORD *)(v3 + 56);
  uint64_t v11 = v4;
  uint64_t v12 = a1;
  uint64_t v13 = a2;
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t, void))(v5 + 24))(a3, &v7, MEMORY[0x1E4FBC848] + 8, v8);
}

uint64_t partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(input:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(input:)(a1, a2, v2[4], v2[5], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<DSPComplex>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<DSPComplex> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:));
}

{
  uint64_t *v2;

  return closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(input:)(a1, a2, v2[4], v2[5], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<DSPDoubleComplex>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<DSPDoubleComplex> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:));
}

uint64_t partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:));
}

{
  return partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:));
}

uint64_t partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 40);
  uint64_t v5 = *(void *)(v3 + 56);
  void v7[2] = *(void *)(v3 + 16);
  long long v8 = *(_OWORD *)(v3 + 24);
  uint64_t v9 = v4;
  uint64_t v10 = v5;
  uint64_t v11 = a1;
  uint64_t v12 = a2;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, void))(v4 + 16))(a3, v7, MEMORY[0x1E4FBC848] + 8, v8);
}

_UNKNOWN **associated type witness table accessor for vDSP_DiscreteFourierTransformable.DiscreteFourierTransformFunctions : vDSP_DiscreteTransformLifecycleFunctions in Float()
{
  return &protocol witness table for vDSP.DFTSinglePrecisionSplitComplexFunctions;
}

_UNKNOWN **associated type witness table accessor for vDSP_DiscreteFourierTransformable.DiscreteFourierTransformFunctions : vDSP_DiscreteTransformLifecycleFunctions in Double()
{
  return &protocol witness table for vDSP.DFTDoublePrecisionSplitComplexFunctions;
}

_UNKNOWN **associated type witness table accessor for vDSP_DiscreteFourierTransformable.DiscreteFourierTransformFunctions : vDSP_DiscreteTransformLifecycleFunctions in DSPComplex()
{
  return &protocol witness table for vDSP.DFTSinglePrecisionInterleavedFunctions;
}

_UNKNOWN **associated type witness table accessor for vDSP_DiscreteFourierTransformable.DiscreteFourierTransformFunctions : vDSP_DiscreteTransformLifecycleFunctions in DSPDoubleComplex()
{
  return &protocol witness table for vDSP.DFTDoublePrecisionInterleavedFunctions;
}

uint64_t dispatch thunk of static vDSP_DiscreteTransformLifecycleFunctions.makeDiscreteFourierTransform(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return (*(uint64_t (**)(void))(a6 + 8))();
}

uint64_t dispatch thunk of static vDSP_DiscreteTransformLifecycleFunctions.destroySetup(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 16))();
}

ValueMetadata *type metadata accessor for vDSP.DFTSinglePrecisionSplitComplexFunctions()
{
  return &type metadata for vDSP.DFTSinglePrecisionSplitComplexFunctions;
}

ValueMetadata *type metadata accessor for vDSP.DFTDoublePrecisionSplitComplexFunctions()
{
  return &type metadata for vDSP.DFTDoublePrecisionSplitComplexFunctions;
}

ValueMetadata *type metadata accessor for vDSP.DFTSinglePrecisionInterleavedFunctions()
{
  return &type metadata for vDSP.DFTSinglePrecisionInterleavedFunctions;
}

ValueMetadata *type metadata accessor for vDSP.DFTDoublePrecisionInterleavedFunctions()
{
  return &type metadata for vDSP.DFTDoublePrecisionInterleavedFunctions;
}

uint64_t __swift_memcpy9_8(uint64_t result, uint64_t *a2)
{
  uint64_t v2 = *a2;
  *(unsigned char *)(result + 8) = *((unsigned char *)a2 + 8);
  *(void *)uint64_t result = v2;
  return result;
}

uint64_t getEnumTagSinglePayload for vDSP.DFTError(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0x7F && *(unsigned char *)(a1 + 9)) {
    return (*(_DWORD *)a1 + 127);
  }
  unsigned int v3 = (*(unsigned char *)(a1 + 8) & 0x7E | (*(unsigned __int8 *)(a1 + 8) >> 7)) ^ 0x7F;
  if (v3 >= 0x7E) {
    unsigned int v3 = -1;
  }
  return v3 + 1;
}

uint64_t storeEnumTagSinglePayload for vDSP.DFTError(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0x7E)
  {
    *(unsigned char *)(result + 8) = 0;
    *(void *)uint64_t result = a2 - 127;
    if (a3 >= 0x7F) {
      *(unsigned char *)(result + 9) = 1;
    }
  }
  else
  {
    if (a3 >= 0x7F) {
      *(unsigned char *)(result + 9) = 0;
    }
    if (a2)
    {
      *(void *)uint64_t result = 0;
      *(unsigned char *)(result + 8) = 2 * (((-a2 >> 1) & 0x3F) - ((_BYTE)a2 << 6));
    }
  }
  return result;
}

uint64_t getEnumTag for vDSP.DFTError(uint64_t a1)
{
  return *(unsigned __int8 *)(a1 + 8) >> 7;
}

uint64_t destructiveProjectEnumData for vDSP.DFTError(uint64_t result)
{
  *(unsigned char *)(result + 8) &= ~0x80u;
  return result;
}

uint64_t destructiveInjectEnumTag for vDSP.DFTError(uint64_t result, char a2)
{
  *(unsigned char *)(result + 8) = *(unsigned char *)(result + 8) & 1 | (a2 << 7);
  return result;
}

ValueMetadata *type metadata accessor for vDSP.DFTError()
{
  return &type metadata for vDSP.DFTError;
}

uint64_t type metadata instantiation function for vDSP.DiscreteFourierTransform()
{
  return MEMORY[0x1F41863F0]();
}

uint64_t type metadata completion function for vDSP.DiscreteFourierTransform()
{
  return swift_initClassMetadata2();
}

uint64_t method lookup function for vDSP.DiscreteFourierTransform(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for vDSP.DiscreteFourierTransform);
}

uint64_t dispatch thunk of vDSP.DiscreteFourierTransform.__allocating_init(previous:count:direction:transformType:ofType:)()
{
  return (*(uint64_t (**)(void))(v0 + 128))();
}

void *partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(void *a1)
{
  return partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(a1, MEMORY[0x1E4F16808]);
}

{
  return partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(a1, MEMORY[0x1E4F16800]);
}

void *partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(input:output:)(void *result, uint64_t (*a2)(void))
{
  if (*(void *)(v2 + 56))
  {
    if (*result) {
      return (void *)a2(*(void *)(*(void *)(v2 + 48) + 16));
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

{
  return partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

void *partial apply for closure #1 in closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(void *a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, MEMORY[0x1E4F167E8]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, MEMORY[0x1E4F167E0]);
}

uint64_t partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

{
  return partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v13 = a2;
  uint64_t v4 = *(void *)(v3 + 40);
  void v7[2] = *(void *)(v3 + 16);
  long long v8 = *(_OWORD *)(v3 + 24);
  uint64_t v9 = v4;
  long long v5 = *(_OWORD *)(v3 + 72);
  long long v10 = *(_OWORD *)(v3 + 56);
  long long v11 = v5;
  uint64_t v12 = a1;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, void))(v4 + 16))(a3, v7, MEMORY[0x1E4FBC848] + 8, v8);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2)
{
  uint64_t v13 = a1;
  uint64_t v3 = *(void *)(v2 + 40);
  uint64_t v4 = *(void *)(v2 + 56);
  void v7[2] = *(void *)(v2 + 16);
  long long v5 = *(_OWORD *)(v2 + 80);
  long long v11 = *(_OWORD *)(v2 + 64);
  long long v8 = *(_OWORD *)(v2 + 24);
  uint64_t v9 = v3;
  uint64_t v10 = v4;
  long long v12 = v5;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, void))(v3 + 16))(a2, v7, MEMORY[0x1E4FBC848] + 8, v8);
}

void *partial apply for closure #1 in closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(void *a1, uint64_t (*a2)(void, uint64_t, uint64_t))
{
  return closure #1 in closure #1 in closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, *(void *)(v2 + 48), *(void *)(v2 + 56), *(void *)(v2 + 64), *(void *)(v2 + 72), *(void *)(v2 + 80), *(void **)(v2 + 88), *(void *)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), a2);
}

uint64_t partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, uint64_t a5)
{
  return closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A>(real:imaginary:)(a1, a2, v5[4], v5[5], v5[6], v5[7], v5[2], v5[3], a3, a4, a5);
}

unint64_t lazy protocol witness table accessor for type vDSP.DFTError and conformance vDSP.DFTError()
{
  unint64_t result = lazy protocol witness table cache variable for type vDSP.DFTError and conformance vDSP.DFTError;
  if (!lazy protocol witness table cache variable for type vDSP.DFTError and conformance vDSP.DFTError)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.DFTError and conformance vDSP.DFTError);
  }
  return result;
}

uint64_t __swift_instantiateGenericMetadata()
{
  return swift_getGenericMetadata();
}

uint64_t Array.init(unsafeUninitializedCapacity:initializingWith:)()
{
  return Array.init(_unsafeUninitializedCapacity:initializingWith:)();
}

void vImage.PixelBuffer<>.makeArray<A>(of:channelCount:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *v2;
  if (!*(void *)(*v2 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  uint64_t v4 = *(void *)(v3 + 48);
  if (v4 < 0)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v5 = *(void *)(v3 + 40);
  if (v5 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  uint64_t v6 = v4 * v5;
  if ((unsigned __int128)(v4 * (__int128)v5) >> 64 != (v4 * v5) >> 63)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if ((unsigned __int128)(v6 * (__int128)a2) >> 64 == (v6 * a2) >> 63)
  {
    MEMORY[0x1F4188790](v6 * a2);
    Array.init(_unsafeUninitializedCapacity:initializingWith:)();
    return;
  }
LABEL_11:
  __break(1u);
}

vImage_Error closure #1 in vImage.PixelBuffer<>.makeArray<A>(of:channelCount:)(uint64_t a1, void *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v23 = *MEMORY[0x1E4F143B8];
  long long v12 = (void *)UnsafeBufferPointer.baseAddress.getter();
  uint64_t v13 = *(void **)a3;
  if (!*(void *)(*(void *)a3 + 16))
  {
    __break(1u);
    goto LABEL_10;
  }
  vImagePixelCount v14 = v13[5];
  if ((v14 & 0x8000000000000000) != 0)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  int64_t v15 = v13[6];
  if (v15 < 0)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  uint64_t v16 = *(void *)(*(void *)(a6 - 8) + 72);
  int64_t v17 = v15 * v16;
  if ((unsigned __int128)(v15 * (__int128)v16) >> 64 != (v15 * v16) >> 63)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  if ((unsigned __int128)(v17 * (__int128)a4) >> 64 != (v17 * a4) >> 63)
  {
LABEL_13:
    __break(1u);
LABEL_14:
    __break(1u);
  }
  dest.data = v12;
  dest.height = v14;
  dest.width = v15;
  dest.rowBytes = v17 * a4;
  if ((unsigned __int128)(v16 * (__int128)a4) >> 64 != (v16 * a4) >> 63) {
    goto LABEL_14;
  }
  size_t v18 = v13[7];
  uint64_t v19 = (void *)v13[4];
  vImage_Flags v21 = 0;
  vImage_Error result = vImage_Buffer.copy(destinationBuffer:pixelSize:flags:)(&dest, v16 * a4, &v21, v19, v14, v15, v18);
  if (v6)
  {
    vImage_Error result = swift_unexpectedError();
    __break(1u);
  }
  else
  {
    *a2 = a5;
  }
  return result;
}

vImage_Error partial apply for closure #1 in vImage.PixelBuffer<>.makeArray<A>(of:channelCount:)(uint64_t a1, void *a2)
{
  return closure #1 in vImage.PixelBuffer<>.makeArray<A>(of:channelCount:)(a1, a2, v2[3], v2[4], v2[5], v2[2]);
}

uint64_t BNNS.ArithmeticTernaryFunction.bnnsArithmeticFunction.getter()
{
  return 28;
}

uint64_t static BNNS.ArithmeticTernaryFunction.== infix(_:_:)()
{
  return 1;
}

void BNNS.ArithmeticTernaryFunction.hash(into:)()
{
}

Swift::Int BNNS.ArithmeticTernaryFunction.hashValue.getter()
{
  return Hasher._finalize()();
}

uint64_t protocol witness for static Equatable.== infix(_:_:) in conformance BNNS.ArithmeticTernaryFunction()
{
  return 1;
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance BNNS.ArithmeticTernaryFunction()
{
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance BNNS.ArithmeticTernaryFunction()
{
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance BNNS.ArithmeticTernaryFunction()
{
  return Hasher._finalize()();
}

uint64_t BNNS.TernaryArithmeticLayer.__allocating_init(inputA:inputADescriptorType:inputB:inputBDescriptorType:inputC:inputCDescriptorType:output:outputDescriptorType:function:activation:filterParameters:)(_OWORD *a1, unsigned __int8 *a2, _OWORD *a3, unsigned __int8 *a4, _OWORD *a5, unsigned __int8 *a6, _OWORD *a7, unsigned __int8 *a8, uint64_t a9, uint64_t *a10, int a11, uint64_t a12, uint64_t a13, uint64_t a14)
{
  uint64_t v106 = *MEMORY[0x1E4F143B8];
  long long v14 = a1[9];
  v56[8] = a1[8];
  v56[9] = v14;
  v56[10] = a1[10];
  long long v15 = a1[5];
  v56[4] = a1[4];
  v56[5] = v15;
  long long v16 = a1[7];
  v56[6] = a1[6];
  v56[7] = v16;
  long long v17 = a1[1];
  v56[0] = *a1;
  v56[1] = v17;
  long long v18 = a1[3];
  v56[2] = a1[2];
  v56[3] = v18;
  long long v19 = a3[6];
  *(_OWORD *)&v99[116] = a3[7];
  long long v20 = a3[9];
  *(_OWORD *)&v99[132] = a3[8];
  *(_OWORD *)&v99[148] = v20;
  *(_OWORD *)&v99[164] = a3[10];
  long long v21 = a3[2];
  *(_OWORD *)&v99[52] = a3[3];
  long long v22 = a3[5];
  *(_OWORD *)&v99[68] = a3[4];
  *(_OWORD *)&v99[84] = v22;
  *(_OWORD *)&v99[100] = v19;
  long long v23 = a3[1];
  *(_OWORD *)&v99[4] = *a3;
  *(_OWORD *)&v99[20] = v23;
  *(_OWORD *)&v99[36] = v21;
  long long v24 = a5[6];
  *(_OWORD *)&v98[116] = a5[7];
  long long v25 = a5[9];
  *(_OWORD *)&v98[132] = a5[8];
  *(_OWORD *)&v98[148] = v25;
  *(_OWORD *)&v98[164] = a5[10];
  long long v26 = a5[2];
  *(_OWORD *)&v98[52] = a5[3];
  long long v27 = a5[5];
  *(_OWORD *)&v98[68] = a5[4];
  *(_OWORD *)&v98[84] = v27;
  *(_OWORD *)&v98[100] = v24;
  long long v28 = a5[1];
  *(_OWORD *)&v98[4] = *a5;
  *(_OWORD *)&v98[20] = v28;
  *(_OWORD *)&v98[36] = v26;
  long long v29 = a7[6];
  *(_OWORD *)&v97[116] = a7[7];
  long long v30 = a7[9];
  *(_OWORD *)&v97[132] = a7[8];
  *(_OWORD *)&v97[148] = v30;
  *(_OWORD *)&v97[164] = a7[10];
  long long v31 = a7[2];
  *(_OWORD *)&v97[52] = a7[3];
  long long v32 = a7[5];
  *(_OWORD *)&v97[68] = a7[4];
  *(_OWORD *)&v97[84] = v32;
  *(_OWORD *)&v97[100] = v29;
  long long v33 = a7[1];
  *(_OWORD *)&v97[4] = *a7;
  *(_OWORD *)&v97[20] = v33;
  *(_OWORD *)&v97[36] = v31;
  long long v67 = *(_OWORD *)&v99[144];
  long long v68 = *(_OWORD *)&v99[160];
  long long v63 = *(_OWORD *)&v99[80];
  long long v64 = *(_OWORD *)&v99[96];
  int v34 = *a2;
  int v35 = *a4;
  int v36 = *a6;
  int v37 = *a8;
  uint64_t v38 = *a10;
  long long v65 = *(_OWORD *)&v99[112];
  long long v66 = *(_OWORD *)&v99[128];
  int v57 = v34;
  int v69 = *(_DWORD *)&v99[176];
  long long v62 = *(_OWORD *)&v99[64];
  long long v58 = *(_OWORD *)v99;
  long long v59 = *(_OWORD *)&v99[16];
  long long v60 = *(_OWORD *)&v99[32];
  long long v61 = *(_OWORD *)&v99[48];
  int v70 = v35;
  long long v79 = *(_OWORD *)&v98[128];
  long long v80 = *(_OWORD *)&v98[144];
  long long v81 = *(_OWORD *)&v98[160];
  long long v75 = *(_OWORD *)&v98[64];
  long long v76 = *(_OWORD *)&v98[80];
  long long v77 = *(_OWORD *)&v98[96];
  long long v78 = *(_OWORD *)&v98[112];
  long long v71 = *(_OWORD *)v98;
  long long v72 = *(_OWORD *)&v98[16];
  long long v73 = *(_OWORD *)&v98[32];
  long long v74 = *(_OWORD *)&v98[48];
  int v82 = *(_DWORD *)&v98[176];
  int v83 = v36;
  long long v92 = *(_OWORD *)&v97[128];
  long long v93 = *(_OWORD *)&v97[144];
  long long v94 = *(_OWORD *)&v97[160];
  long long v88 = *(_OWORD *)&v97[64];
  long long v89 = *(_OWORD *)&v97[80];
  long long v90 = *(_OWORD *)&v97[96];
  long long v91 = *(_OWORD *)&v97[112];
  long long v84 = *(_OWORD *)v97;
  long long v85 = *(_OWORD *)&v97[16];
  long long v86 = *(_OWORD *)&v97[32];
  long long v87 = *(_OWORD *)&v97[48];
  int v95 = *(_DWORD *)&v97[176];
  int v96 = v37;
  v48[1] = HIDWORD(v38);
  BNNS.ActivationFunction.bnnsActivation.getter((uint64_t)&v100);
  uint64_t v51 = v101;
  v48[0] = 28;
  uint64_t v49 = v56;
  int v50 = v100;
  uint64_t v52 = v102;
  int v53 = v103;
  long long v54 = v104;
  uint64_t v55 = v105;
  if (a13 == 1)
  {
    uint64_t v39 = 0;
  }
  else
  {
    int v44 = a11;
    uint64_t v45 = a12;
    uint64_t v46 = a13;
    uint64_t v47 = a14;
    uint64_t v39 = &v44;
  }
  uint64_t v40 = MEMORY[0x1D25FFFB0](v48, v39);
  type metadata accessor for BNNS.TernaryArithmeticLayer();
  uint64_t v41 = swift_allocObject();
  uint64_t v42 = v41;
  if (v40)
  {
    *(void *)(v41 + 16) = v40;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v42;
}

uint64_t BNNS.TernaryArithmeticLayer.apply(batchSize:inputA:inputB:inputC:output:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, unint64_t a5)
{
  unint64_t v82 = a5;
  outlined init with take of UnsafeMutableRawPointer?(a2 + 136, (uint64_t)v77);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v77, (uint64_t)&v78);
  uint64_t v9 = v78;
  if (!v78) {
    goto LABEL_7;
  }
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v76);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v76, (uint64_t)&v79);
  uint64_t v10 = v79;
  if (!v79) {
    goto LABEL_7;
  }
  outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v75);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v75, (uint64_t)&v80);
  uint64_t v11 = v80;
  if (v80
    && (outlined init with take of UnsafeMutableRawPointer?(v82 + 136, (uint64_t)v74),
        outlined init with take of UnsafeMutableRawPointer?((uint64_t)v74, (uint64_t)&v81),
        v81))
  {
    out = v81;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
    uint64_t v12 = swift_allocObject();
    *(_OWORD *)(v12 + 16) = xmmword_1D2135DC0;
    *(void *)(v12 + 32) = v9;
    uint64_t v46 = (const void **)(v12 + 32);
    *(void *)(v12 + 40) = v10;
    *(void *)(v12 + 48) = v11;
    uint64_t v45 = *(void **)(v5 + 16);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
    uint64_t v48 = swift_allocObject();
    *(_OWORD *)(v48 + 16) = xmmword_1D2135DC0;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v69);
    outlined init with take of BNNS.Shape((uint64_t)v69, (uint64_t)v70);
    outlined init with take of BNNS.Shape((uint64_t)v70, (uint64_t)v68);
    BNNS.Shape.size.getter((uint64_t)&v60);
    unint64_t v13 = v60;
    unint64_t v14 = v61;
    size_t v44 = a1;
    unint64_t v15 = v62;
    unint64_t v16 = v63;
    unint64_t v17 = v64;
    unint64_t v40 = v66;
    unint64_t v43 = v65;
    unint64_t v37 = v67;
    outlined init with take of BNNS.Shape((uint64_t)v70, (uint64_t)v68);
    BNNS.Shape.stride.getter((uint64_t)&v60);
    *(void *)(v48 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v13, v14, v15, v16, v17, v43, v40, v37, v60, v61, v62, v63, v64, v65, v66, v67);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v68);
    outlined init with take of BNNS.Shape((uint64_t)v68, (uint64_t)v71);
    outlined init with take of BNNS.Shape((uint64_t)v71, (uint64_t)&v60);
    BNNS.Shape.size.getter((uint64_t)&v55);
    long long v18 = v55;
    long long v19 = v56;
    long long v20 = v57;
    unint64_t v38 = v59;
    unint64_t v41 = v58;
    outlined init with take of BNNS.Shape((uint64_t)v71, (uint64_t)&v60);
    BNNS.Shape.stride.getter((uint64_t)&v55);
    *(void *)(v48 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v18, *((unint64_t *)&v18 + 1), v19, *((unint64_t *)&v19 + 1), v20, *((unint64_t *)&v20 + 1), v41, v38, v55, *((unint64_t *)&v55 + 1), v56, *((unint64_t *)&v56 + 1), v57, *((unint64_t *)&v57 + 1), v58, v59);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v60);
    outlined init with take of BNNS.Shape((uint64_t)&v60, (uint64_t)v72);
    outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)&v55);
    BNNS.Shape.size.getter((uint64_t)&v49);
    unint64_t v21 = v50;
    long long v22 = v51;
    long long v23 = v52;
    unint64_t v24 = v53;
    unint64_t v39 = v54;
    unint64_t v42 = v49;
    outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)&v55);
    BNNS.Shape.stride.getter((uint64_t)&v49);
    *(void *)(v48 + 48) = specialized static BNNS.calculateBatchStride(size:stride:)(v42, v21, v22, *((unint64_t *)&v22 + 1), v23, *((unint64_t *)&v23 + 1), v24, v39, v49, v50, v51, *((unint64_t *)&v51 + 1), v52, *((unint64_t *)&v52 + 1), v53, v54);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v69);
    outlined init with take of BNNS.Shape((uint64_t)v69, (uint64_t)v73);
    outlined init with take of BNNS.Shape((uint64_t)v73, (uint64_t)v68);
    BNNS.Shape.size.getter((uint64_t)&v60);
    unint64_t v25 = v60;
    unint64_t v26 = v61;
    unint64_t v27 = v62;
    unint64_t v28 = v63;
    unint64_t v29 = v64;
    unint64_t v30 = v65;
    unint64_t v31 = v66;
    unint64_t v82 = v67;
    outlined init with take of BNNS.Shape((uint64_t)v73, (uint64_t)v68);
    BNNS.Shape.stride.getter((uint64_t)&v60);
    size_t v32 = specialized static BNNS.calculateBatchStride(size:stride:)(v25, v26, v27, v28, v29, v30, v31, v82, v60, v61, v62, v63, v64, v65, v66, v67);
    int v33 = BNNSArithmeticFilterApplyBatch(v45, v44, 3uLL, v46, (const size_t *)(v48 + 32), out, v32);
    swift_bridgeObjectRelease();
    uint64_t result = swift_bridgeObjectRelease();
    if (!v33) {
      return result;
    }
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    unsigned char *v35 = 0;
  }
  else
  {
LABEL_7:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *int v36 = 2;
  }
  return swift_willThrow();
}

uint64_t BNNS.TernaryArithmeticLayer.applyBackward(batchSize:inputA:inputB:inputC:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingInputCGradient:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, _OWORD *a6, long long *a7, _OWORD *a8, long long *a9)
{
  v99[1] = *MEMORY[0x1E4F143B8];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer?>);
  uint64_t v14 = swift_allocObject();
  *(_OWORD *)(v14 + 16) = xmmword_1D2135DC0;
  outlined init with take of UnsafeMutableRawPointer?(a2 + 136, (uint64_t)v96);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v96, (uint64_t)&v97);
  *(void *)(v14 + 32) = v97;
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v95);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v95, (uint64_t)&v98);
  *(void *)(v14 + 40) = v98;
  outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v94);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v94, (uint64_t)v99);
  *(void *)(v14 + 48) = v99[0];
  long long v76 = (char *)v14;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  uint64_t v69 = swift_allocObject();
  *(_OWORD *)(v69 + 16) = xmmword_1D2135DC0;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v90);
  outlined init with take of BNNS.Shape((uint64_t)v90, (uint64_t)v91);
  outlined init with take of BNNS.Shape((uint64_t)v91, (uint64_t)v89);
  BNNS.Shape.size.getter((uint64_t)&v78);
  long long v15 = v78;
  long long v16 = v79;
  unint64_t v17 = v80;
  unint64_t v57 = v81;
  unint64_t v60 = *((void *)&v80 + 1);
  unint64_t v54 = *((void *)&v81 + 1);
  outlined init with take of BNNS.Shape((uint64_t)v91, (uint64_t)v89);
  BNNS.Shape.stride.getter((uint64_t)&v78);
  *(void *)(v69 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v15, *((unint64_t *)&v15 + 1), v16, *((unint64_t *)&v16 + 1), v17, v60, v57, v54, v78, *((unint64_t *)&v78 + 1), v79, *((unint64_t *)&v79 + 1), v80, *((unint64_t *)&v80 + 1), v81, *((unint64_t *)&v81 + 1));
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v89);
  outlined init with take of BNNS.Shape((uint64_t)v89, (uint64_t)v92);
  outlined init with take of BNNS.Shape((uint64_t)v92, (uint64_t)&v78);
  BNNS.Shape.size.getter((uint64_t)&v77);
  unint64_t v18 = v77.size[1];
  unint64_t v58 = v77.size[0];
  unint64_t v61 = *(void *)&v77.flags;
  unint64_t v19 = v77.size[2];
  long long v20 = *(_OWORD *)&v77.size[3];
  unint64_t v53 = v77.size[6];
  unint64_t v55 = v77.size[5];
  outlined init with take of BNNS.Shape((uint64_t)v92, (uint64_t)&v78);
  BNNS.Shape.stride.getter((uint64_t)&v77);
  *(void *)(v69 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v61, v58, v18, v19, v20, *((unint64_t *)&v20 + 1), v55, v53, *(unint64_t *)&v77.flags, v77.size[0], v77.size[1], v77.size[2], v77.size[3], v77.size[4], v77.size[5], v77.size[6]);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v78);
  outlined init with take of BNNS.Shape((uint64_t)&v78, (uint64_t)v93);
  outlined init with take of BNNS.Shape((uint64_t)v93, (uint64_t)&v77);
  BNNS.Shape.size.getter((uint64_t)&v70);
  unint64_t v21 = v71;
  long long v22 = v72;
  long long v23 = v73;
  unint64_t v59 = v74;
  unint64_t v62 = v70;
  unint64_t v56 = v75;
  outlined init with take of BNNS.Shape((uint64_t)v93, (uint64_t)&v77);
  BNNS.Shape.stride.getter((uint64_t)&v70);
  unint64_t v24 = specialized static BNNS.calculateBatchStride(size:stride:)(v62, v21, v22, *((unint64_t *)&v22 + 1), v23, *((unint64_t *)&v23 + 1), v59, v56, v70, v71, v72, *((unint64_t *)&v72 + 1), v73, *((unint64_t *)&v73 + 1), v74, v75);
  long long v25 = a7[8];
  long long v26 = a7[9];
  long long v27 = a7[6];
  v90[7] = a7[7];
  v90[8] = v25;
  long long v28 = a7[10];
  v90[9] = v26;
  v90[10] = v28;
  long long v29 = a7[4];
  long long v30 = a7[5];
  long long v31 = a7[2];
  v90[3] = a7[3];
  v90[4] = v29;
  v90[5] = v30;
  v90[6] = v27;
  long long v32 = *a7;
  v90[1] = a7[1];
  v90[2] = v31;
  long long v33 = a8[9];
  v89[8] = a8[8];
  v89[9] = v33;
  v89[10] = a8[10];
  v90[0] = v32;
  long long v34 = a8[5];
  v89[4] = a8[4];
  v89[5] = v34;
  long long v35 = a8[7];
  v89[6] = a8[6];
  v89[7] = v35;
  long long v36 = a8[1];
  v89[0] = *a8;
  v89[1] = v36;
  long long v37 = a8[3];
  v89[2] = a8[2];
  v89[3] = v37;
  long long v38 = a9[8];
  long long v39 = a9[9];
  long long v40 = a9[6];
  long long v85 = a9[7];
  long long v86 = v38;
  long long v41 = a9[10];
  long long v87 = v39;
  long long v88 = v41;
  long long v42 = a9[4];
  long long v43 = a9[5];
  long long v44 = a9[2];
  long long v81 = a9[3];
  long long v82 = v42;
  *(void *)(v69 + 48) = v24;
  long long v83 = v43;
  long long v84 = v40;
  long long v45 = *a9;
  long long v79 = a9[1];
  long long v80 = v44;
  long long v46 = a6[9];
  *(_OWORD *)&v77.stride[7] = a6[8];
  *(_OWORD *)&v77.data_type = v46;
  *(_OWORD *)&v77.table_data_type = a6[10];
  long long v78 = v45;
  long long v47 = a6[5];
  *(_OWORD *)&v77.size[7] = a6[4];
  *(_OWORD *)&v77.stride[1] = v47;
  long long v48 = a6[7];
  *(_OWORD *)&v77.stride[3] = a6[6];
  *(_OWORD *)&v77.stride[5] = v48;
  long long v49 = a6[1];
  *(_OWORD *)&v77.flags = *a6;
  *(_OWORD *)&v77.size[1] = v49;
  long long v50 = a6[3];
  *(_OWORD *)&v77.size[3] = a6[2];
  *(_OWORD *)&v77.size[5] = v50;
  closure #1 in closure #1 in closure #1 in closure #1 in BNNS.TernaryArithmeticLayer.applyBackward(batchSize:inputA:inputB:inputC:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingInputCGradient:)(&v77, (uint64_t)v90, (uint64_t)v89, (uint64_t)&v78, v68, a1, &v76, (int *)&v70, a3, a4, v69, a5);
  swift_setDeallocating();
  swift_deallocClassInstance();
  if (v70)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v51 = 0;
    swift_willThrow();
  }
  return swift_bridgeObjectRelease();
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in BNNS.TernaryArithmeticLayer.applyBackward(batchSize:inputA:inputB:inputC:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingInputCGradient:)@<X0>(BNNSNDArrayDescriptor *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, size_t a6@<X5>, char **a7@<X6>, int *a8@<X8>, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>>);
  uint64_t v16 = swift_allocObject();
  *(_OWORD *)(v16 + 16) = xmmword_1D2135DC0;
  *(void *)(v16 + 32) = a2;
  uint64_t v68 = (BNNSNDArrayDescriptor **)(v16 + 32);
  *(void *)(v16 + 40) = a3;
  *(void *)(v16 + 48) = a4;
  unint64_t v67 = *(void **)(a5 + 16);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  uint64_t v115 = swift_allocObject();
  *(_OWORD *)(v115 + 16) = xmmword_1D2135DC0;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v102);
  outlined init with take of BNNS.Shape((uint64_t)v102, (uint64_t)v105);
  outlined init with take of BNNS.Shape((uint64_t)v105, (uint64_t)v114);
  BNNS.Shape.size.getter((uint64_t)&v106);
  unint64_t v17 = v106;
  unint64_t v18 = v107;
  unint64_t v19 = v108;
  unint64_t v20 = v109;
  unint64_t v21 = v110;
  unint64_t v22 = v111;
  unint64_t v23 = v112;
  unint64_t v24 = v113;
  outlined init with take of BNNS.Shape((uint64_t)v105, (uint64_t)v114);
  BNNS.Shape.stride.getter((uint64_t)&v106);
  unint64_t v25 = specialized static BNNS.calculateBatchStride(size:stride:)(v17, v18, v19, v20, v21, v22, v23, v24, v106, v107, v108, v109, v110, v111, v112, v113);
  uint64_t v26 = v115;
  uint64_t v27 = v115;
  *(void *)(v115 + 32) = v25;
  unint64_t v65 = (const size_t *)(v27 + 32);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v101);
  outlined init with take of BNNS.Shape((uint64_t)v101, (uint64_t)&v106);
  outlined init with take of BNNS.Shape((uint64_t)&v106, (uint64_t)v114);
  BNNS.Shape.size.getter((uint64_t)&v93);
  unint64_t v62 = v93;
  unint64_t v28 = v94;
  unint64_t v29 = v95;
  unint64_t v30 = v96;
  unint64_t v31 = v97;
  unint64_t v32 = v98;
  unint64_t v33 = v99;
  unint64_t v34 = v100;
  outlined init with take of BNNS.Shape((uint64_t)&v106, (uint64_t)v114);
  BNNS.Shape.stride.getter((uint64_t)&v93);
  *(void *)(v26 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v62, v28, v29, v30, v31, v32, v33, v34, v93, v94, v95, v96, v97, v98, v99, v100);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v93);
  outlined init with take of BNNS.Shape((uint64_t)&v93, (uint64_t)v114);
  outlined init with take of BNNS.Shape((uint64_t)v114, (uint64_t)&v85);
  BNNS.Shape.size.getter((uint64_t)&v77);
  unint64_t v35 = v77;
  unint64_t v36 = v78;
  unint64_t v37 = v79;
  unint64_t v38 = v80;
  unint64_t v39 = v81;
  unint64_t v40 = v82;
  unint64_t v41 = v83;
  unint64_t v42 = v84;
  outlined init with take of BNNS.Shape((uint64_t)v114, (uint64_t)&v85);
  BNNS.Shape.stride.getter((uint64_t)&v77);
  unint64_t v43 = specialized static BNNS.calculateBatchStride(size:stride:)(v35, v36, v37, v38, v39, v40, v41, v42, v77, v78, v79, v80, v81, v82, v83, v84);
  *(void *)(v115 + 48) = v43;
  outlined init with take of UnsafeMutableRawPointer?(a12 + 136, (uint64_t)v103);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v103, (uint64_t)&v104);
  out = v104;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v93);
  outlined init with take of BNNS.Shape((uint64_t)&v93, (uint64_t)v101);
  outlined init with take of BNNS.Shape((uint64_t)v101, (uint64_t)v102);
  BNNS.Shape.size.getter((uint64_t)&v85);
  unint64_t v44 = v85;
  unint64_t v45 = v86;
  unint64_t v46 = v87;
  unint64_t v47 = v88;
  unint64_t v48 = v89;
  unint64_t v49 = v90;
  unint64_t v51 = v91;
  unint64_t v50 = v92;
  outlined init with take of BNNS.Shape((uint64_t)v101, (uint64_t)v102);
  BNNS.Shape.stride.getter((uint64_t)&v85);
  size_t out_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v44, v45, v46, v47, v48, v49, v51, v50, v85, v86, v87, v88, v89, v90, v91, v92);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v85);
  outlined init with take of BNNS.Shape((uint64_t)&v85, (uint64_t)v102);
  outlined init with take of BNNS.Shape((uint64_t)v102, (uint64_t)&v77);
  BNNS.Shape.size.getter((uint64_t)&v72);
  long long v52 = v72;
  long long v53 = v73;
  long long v54 = v74;
  unint64_t v56 = v75;
  unint64_t v55 = v76;
  outlined init with take of BNNS.Shape((uint64_t)v102, (uint64_t)&v77);
  BNNS.Shape.stride.getter((uint64_t)&v72);
  size_t out_delta_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v52, *((unint64_t *)&v52 + 1), v53, *((unint64_t *)&v53 + 1), v54, *((unint64_t *)&v54 + 1), v56, v55, v72, *((unint64_t *)&v72 + 1), v73, *((unint64_t *)&v73 + 1), v74, *((unint64_t *)&v74 + 1), v75, v76);
  unint64_t v58 = *a7;
  char isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
  *a7 = v58;
  if ((isUniquelyReferenced_nonNull_native & 1) == 0) {
    unint64_t v58 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v58 + 2), 0, v58);
  }
  *a7 = v58;
  swift_bridgeObjectRetain();
  int v60 = BNNSArithmeticFilterApplyBackwardBatch(v67, a6, 3uLL, (const void **)v58 + 4, v65, v68, (const size_t *)(a11 + 32), out, out_stride, a1, out_delta_stride);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  uint64_t result = swift_bridgeObjectRelease();
  *a8 = v60;
  return result;
}

uint64_t type metadata accessor for BNNS.TernaryArithmeticLayer()
{
  return self;
}

uint64_t BNNS.TernaryArithmeticLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.TernaryArithmeticLayer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

unint64_t lazy protocol witness table accessor for type BNNS.ArithmeticTernaryFunction and conformance BNNS.ArithmeticTernaryFunction()
{
  unint64_t result = lazy protocol witness table cache variable for type BNNS.ArithmeticTernaryFunction and conformance BNNS.ArithmeticTernaryFunction;
  if (!lazy protocol witness table cache variable for type BNNS.ArithmeticTernaryFunction and conformance BNNS.ArithmeticTernaryFunction)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type BNNS.ArithmeticTernaryFunction and conformance BNNS.ArithmeticTernaryFunction);
  }
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.ArithmeticTernaryFunction(unsigned int *a1, int a2)
{
  if (!a2) {
    return 0;
  }
  if ((a2 + 1) >= 0x10000) {
    int v2 = 4;
  }
  else {
    int v2 = 2;
  }
  if ((a2 + 1) < 0x100) {
    int v3 = 1;
  }
  else {
    int v3 = v2;
  }
  if (v3 == 4) {
    return *a1;
  }
  if (v3 == 2) {
    return *(unsigned __int16 *)a1;
  }
  return *(unsigned __int8 *)a1;
}

unsigned char *storeEnumTagSinglePayload for BNNS.ArithmeticTernaryFunction(unsigned char *result, int a2, int a3)
{
  if ((a3 + 1) >= 0x10000) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) < 0x100) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2)
  {
    switch(v5)
    {
      case 1:
        *unint64_t result = a2;
        return result;
      case 2:
        *(_WORD *)unint64_t result = a2;
        return result;
      case 3:
        goto LABEL_19;
      case 4:
        *(_DWORD *)unint64_t result = a2;
        return result;
      default:
        return result;
    }
  }
  switch(v5)
  {
    case 1:
      *unint64_t result = 0;
      break;
    case 2:
      *(_WORD *)unint64_t result = 0;
      break;
    case 3:
LABEL_19:
      __break(1u);
      JUMPOUT(0x1D20B6F04);
    case 4:
      *(_DWORD *)unint64_t result = 0;
      break;
    default:
      return result;
  }
  return result;
}

uint64_t getEnumTag for BNNS.ArithmeticTernaryFunction()
{
  return 0;
}

ValueMetadata *type metadata accessor for BNNS.ArithmeticTernaryFunction()
{
  return &type metadata for BNNS.ArithmeticTernaryFunction;
}

uint64_t method lookup function for BNNS.TernaryArithmeticLayer(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.TernaryArithmeticLayer);
}

uint64_t dispatch thunk of BNNS.TernaryArithmeticLayer.apply(batchSize:inputA:inputB:inputC:output:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5)
{
  uint64_t v6 = a2[17];
  int v7 = *((_DWORD *)a2 + 36);
  uint64_t v8 = a2[19];
  int v9 = *((_DWORD *)a2 + 40);
  uint64_t v10 = a3[17];
  int v11 = *((_DWORD *)a3 + 36);
  uint64_t v12 = a3[19];
  int v13 = *((_DWORD *)a3 + 40);
  uint64_t v14 = a4[17];
  int v15 = *((_DWORD *)a4 + 36);
  uint64_t v16 = a4[19];
  int v17 = *((_DWORD *)a4 + 40);
  uint64_t v18 = a5[17];
  int v19 = *((_DWORD *)a5 + 36);
  uint64_t v20 = a5[19];
  int v21 = *((_DWORD *)a5 + 40);
  unint64_t v98 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(void *)v5 + 96);
  uint64_t v84 = *a2;
  long long v85 = *(_OWORD *)(a2 + 1);
  long long v86 = *(_OWORD *)(a2 + 3);
  long long v87 = *(_OWORD *)(a2 + 5);
  long long v88 = *(_OWORD *)(a2 + 7);
  long long v89 = *(_OWORD *)(a2 + 9);
  long long v90 = *(_OWORD *)(a2 + 11);
  long long v91 = *(_OWORD *)(a2 + 13);
  long long v92 = *(_OWORD *)(a2 + 15);
  uint64_t v93 = v6;
  int v94 = v7;
  uint64_t v95 = v8;
  int v96 = v9;
  uint64_t v97 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v70 = *a3;
  long long v71 = *(_OWORD *)(a3 + 1);
  long long v72 = *(_OWORD *)(a3 + 3);
  long long v73 = *(_OWORD *)(a3 + 5);
  long long v74 = *(_OWORD *)(a3 + 7);
  long long v75 = *(_OWORD *)(a3 + 9);
  long long v76 = *(_OWORD *)(a3 + 11);
  long long v22 = *(_OWORD *)(a3 + 15);
  long long v77 = *(_OWORD *)(a3 + 13);
  long long v23 = *(_OWORD *)(a4 + 1);
  long long v24 = *(_OWORD *)(a4 + 3);
  long long v25 = *(_OWORD *)(a4 + 5);
  long long v26 = *(_OWORD *)(a4 + 7);
  long long v27 = *(_OWORD *)(a4 + 9);
  long long v28 = *(_OWORD *)(a4 + 11);
  long long v29 = *(_OWORD *)(a4 + 13);
  uint64_t v30 = *a4;
  long long v31 = *(_OWORD *)(a4 + 15);
  long long v32 = *(_OWORD *)(a5 + 1);
  long long v33 = *(_OWORD *)(a5 + 3);
  long long v34 = *(_OWORD *)(a5 + 5);
  long long v35 = *(_OWORD *)(a5 + 7);
  long long v36 = *(_OWORD *)(a5 + 9);
  long long v37 = *(_OWORD *)(a5 + 11);
  long long v38 = *(_OWORD *)(a5 + 13);
  uint64_t v39 = *a5;
  long long v40 = *(_OWORD *)(a5 + 15);
  long long v78 = v22;
  uint64_t v79 = v10;
  int v80 = v11;
  uint64_t v81 = v12;
  int v82 = v13;
  uint64_t v83 = *(uint64_t *)((char *)a3 + 164);
  *(void *)&long long v22 = *(uint64_t *)((char *)a5 + 164);
  uint64_t v56 = v30;
  long long v57 = v23;
  *(void *)&long long v23 = *(uint64_t *)((char *)a4 + 164);
  long long v58 = v24;
  long long v59 = v25;
  long long v60 = v26;
  long long v61 = v27;
  long long v62 = v28;
  long long v63 = v29;
  long long v64 = v31;
  uint64_t v65 = v14;
  int v66 = v15;
  uint64_t v67 = v16;
  int v68 = v17;
  uint64_t v69 = v23;
  uint64_t v42 = v39;
  long long v43 = v32;
  long long v44 = v33;
  long long v45 = v34;
  long long v46 = v35;
  long long v47 = v36;
  long long v48 = v37;
  long long v49 = v38;
  long long v50 = v40;
  uint64_t v51 = v18;
  int v52 = v19;
  uint64_t v53 = v20;
  int v54 = v21;
  uint64_t v55 = v22;
  return v98(a1, &v84, &v70, &v56, &v42);
}

uint64_t dispatch thunk of BNNS.TernaryArithmeticLayer.applyBackward(batchSize:inputA:inputB:inputC:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingInputCGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5, uint64_t *a6, uint64_t *a7, uint64_t *a8, uint64_t a9)
{
  uint64_t v108 = a2[17];
  int v107 = *((_DWORD *)a2 + 36);
  uint64_t v10 = a2[19];
  int v11 = *((_DWORD *)a2 + 40);
  uint64_t v12 = a3[17];
  int v13 = *((_DWORD *)a3 + 36);
  uint64_t v14 = a3[19];
  int v120 = *((_DWORD *)a3 + 40);
  uint64_t v119 = a4[17];
  int v118 = *((_DWORD *)a4 + 36);
  uint64_t v117 = a5[17];
  int v116 = *((_DWORD *)a5 + 36);
  uint64_t v115 = a5[19];
  int v114 = *((_DWORD *)a5 + 40);
  uint64_t v113 = a6[17];
  int v112 = *((_DWORD *)a6 + 36);
  uint64_t v111 = a6[19];
  int v110 = *((_DWORD *)a6 + 40);
  uint64_t v109 = a7[17];
  long long v104 = *(_OWORD *)(a6 + 3);
  long long v105 = *(_OWORD *)(a6 + 1);
  long long v102 = *(_OWORD *)(a6 + 7);
  long long v103 = *(_OWORD *)(a6 + 5);
  long long v101 = *(_OWORD *)(a6 + 9);
  long long v97 = *(_OWORD *)(a6 + 13);
  long long v98 = *(_OWORD *)(a6 + 11);
  uint64_t v106 = *a6;
  long long v96 = *(_OWORD *)(a6 + 15);
  uint64_t v78 = *(uint64_t *)((char *)a6 + 164);
  int v15 = *((_DWORD *)a7 + 36);
  long long v16 = *(_OWORD *)(a5 + 1);
  long long v17 = *(_OWORD *)(a5 + 3);
  long long v18 = *(_OWORD *)(a5 + 5);
  long long v19 = *(_OWORD *)(a5 + 7);
  long long v20 = *(_OWORD *)(a5 + 9);
  long long v70 = *(_OWORD *)(a5 + 11);
  long long v71 = *(_OWORD *)(a5 + 13);
  uint64_t v100 = *a5;
  long long v72 = *(_OWORD *)(a5 + 15);
  uint64_t v73 = *(uint64_t *)((char *)a5 + 164);
  uint64_t v21 = a7[19];
  long long v94 = *(_OWORD *)(a7 + 3);
  long long v95 = *(_OWORD *)(a7 + 1);
  long long v92 = *(_OWORD *)(a7 + 7);
  long long v93 = *(_OWORD *)(a7 + 5);
  long long v91 = *(_OWORD *)(a7 + 9);
  long long v81 = *(_OWORD *)(a7 + 13);
  long long v82 = *(_OWORD *)(a7 + 11);
  uint64_t v99 = *a7;
  long long v80 = *(_OWORD *)(a7 + 15);
  uint64_t v84 = *(uint64_t *)((char *)a7 + 164);
  int v22 = *((_DWORD *)a7 + 40);
  long long v23 = *(_OWORD *)(a4 + 1);
  long long v24 = *(_OWORD *)(a4 + 3);
  uint64_t v25 = a4[19];
  int v26 = *((_DWORD *)a4 + 40);
  uint64_t v27 = a8[17];
  int v28 = *((_DWORD *)a8 + 36);
  uint64_t v29 = a8[19];
  int v30 = *((_DWORD *)a8 + 40);
  uint64_t v31 = *(void *)(a9 + 136);
  int v32 = *(_DWORD *)(a9 + 144);
  uint64_t v33 = *(void *)(a9 + 152);
  int v34 = *(_DWORD *)(a9 + 160);
  long long v35 = *(_OWORD *)(a4 + 5);
  long long v36 = *(_OWORD *)(a4 + 7);
  long long v37 = *(_OWORD *)(a4 + 9);
  long long v38 = *(_OWORD *)(a4 + 11);
  long long v39 = *(_OWORD *)(a4 + 13);
  uint64_t v90 = *a4;
  long long v40 = *(_OWORD *)(a4 + 15);
  uint64_t v41 = *(uint64_t *)((char *)a4 + 164);
  long long v42 = *(_OWORD *)(a2 + 3);
  long long v43 = *(_OWORD *)(a2 + 5);
  long long v44 = *(_OWORD *)(a2 + 7);
  long long v45 = *(_OWORD *)(a2 + 9);
  long long v46 = *(_OWORD *)(a2 + 11);
  long long v47 = *(_OWORD *)(a2 + 13);
  uint64_t v83 = *a2;
  long long v48 = *(_OWORD *)(a2 + 15);
  long long v49 = *(_OWORD *)(a3 + 1);
  long long v50 = *(_OWORD *)(a3 + 3);
  long long v51 = *(_OWORD *)(a3 + 5);
  long long v52 = *(_OWORD *)(a3 + 7);
  long long v53 = *(_OWORD *)(a3 + 9);
  long long v54 = *(_OWORD *)(a3 + 11);
  long long v55 = *(_OWORD *)(a3 + 13);
  uint64_t v77 = *a3;
  long long v56 = *(_OWORD *)(a3 + 15);
  long long v89 = *(_OWORD *)(a8 + 1);
  long long v88 = *(_OWORD *)(a8 + 3);
  long long v87 = *(_OWORD *)(a8 + 5);
  long long v86 = *(_OWORD *)(a8 + 7);
  long long v85 = *(_OWORD *)(a8 + 9);
  long long v74 = *(_OWORD *)(a8 + 11);
  long long v76 = *(_OWORD *)(a8 + 13);
  uint64_t v79 = *a8;
  long long v75 = *(_OWORD *)(a8 + 15);
  uint64_t v57 = *(uint64_t *)((char *)a3 + 164);
  uint64_t v58 = *(uint64_t *)((char *)a2 + 164);
  long long v59 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t *))(*(void *)v9 + 104);
  long long v221 = *(_OWORD *)(a2 + 1);
  long long v222 = v42;
  long long v223 = v43;
  long long v224 = v44;
  long long v225 = v45;
  long long v226 = v46;
  long long v227 = v47;
  long long v228 = v48;
  uint64_t v233 = v58;
  long long v207 = v49;
  long long v208 = v50;
  long long v209 = v51;
  long long v210 = v52;
  long long v211 = v53;
  long long v212 = v54;
  long long v213 = v55;
  long long v214 = v56;
  uint64_t v219 = v57;
  long long v193 = v23;
  long long v194 = v24;
  long long v195 = v35;
  long long v196 = v36;
  long long v197 = v37;
  long long v198 = v38;
  long long v199 = v39;
  long long v200 = v40;
  uint64_t v205 = v41;
  long long v179 = v16;
  long long v180 = v17;
  long long v181 = v18;
  long long v182 = v19;
  long long v183 = v20;
  long long v184 = v70;
  long long v185 = v71;
  long long v186 = v72;
  uint64_t v191 = v73;
  long long v170 = v98;
  long long v171 = v97;
  long long v172 = v96;
  uint64_t v177 = v78;
  uint64_t v229 = v108;
  int v230 = v107;
  uint64_t v231 = v10;
  int v232 = v11;
  uint64_t v215 = v12;
  int v216 = v13;
  uint64_t v217 = v14;
  long long v165 = v105;
  long long v166 = v104;
  long long v167 = v103;
  long long v168 = v102;
  long long v169 = v101;
  long long v156 = v82;
  long long v157 = v81;
  long long v158 = v80;
  uint64_t v163 = v84;
  uint64_t v60 = *(uint64_t *)((char *)a8 + 164);
  long long v151 = v95;
  long long v152 = v94;
  long long v153 = v93;
  long long v154 = v92;
  long long v155 = v91;
  long long v142 = v74;
  long long v143 = v76;
  long long v144 = v75;
  uint64_t v149 = v60;
  uint64_t v220 = v83;
  uint64_t v206 = v77;
  uint64_t v192 = v90;
  uint64_t v178 = v100;
  long long v61 = *(_OWORD *)(a9 + 8);
  long long v62 = *(_OWORD *)(a9 + 24);
  long long v63 = *(_OWORD *)(a9 + 40);
  *(void *)&long long v44 = *(void *)a9;
  long long v64 = *(_OWORD *)(a9 + 56);
  long long v65 = *(_OWORD *)(a9 + 72);
  long long v66 = *(_OWORD *)(a9 + 88);
  uint64_t v164 = v106;
  long long v67 = *(_OWORD *)(a9 + 104);
  uint64_t v150 = v99;
  long long v68 = *(_OWORD *)(a9 + 120);
  uint64_t v136 = v79;
  *(void *)&long long v47 = *(void *)(a9 + 164);
  long long v137 = v89;
  long long v138 = v88;
  long long v139 = v87;
  long long v140 = v86;
  long long v141 = v85;
  uint64_t v122 = v44;
  long long v123 = v61;
  long long v124 = v62;
  long long v125 = v63;
  long long v126 = v64;
  long long v127 = v65;
  long long v128 = v66;
  long long v129 = v67;
  long long v130 = v68;
  uint64_t v135 = v47;
  int v218 = v120;
  uint64_t v201 = v119;
  int v202 = v118;
  uint64_t v203 = v25;
  int v204 = v26;
  uint64_t v187 = v117;
  int v188 = v116;
  uint64_t v189 = v115;
  int v190 = v114;
  uint64_t v173 = v113;
  int v174 = v112;
  uint64_t v175 = v111;
  int v176 = v110;
  uint64_t v159 = v109;
  int v160 = v15;
  uint64_t v161 = v21;
  int v162 = v22;
  uint64_t v145 = v27;
  int v146 = v28;
  uint64_t v147 = v29;
  int v148 = v30;
  uint64_t v131 = v31;
  int v132 = v32;
  uint64_t v133 = v33;
  int v134 = v34;
  return v59(a1, &v220, &v206, &v192, &v178, &v164, &v150, &v136, &v122);
}

uint64_t BNNS.GramLayer.__allocating_init(input:output:alpha:filterParameters:)(_OWORD *a1, _OWORD *a2, uint32_t a3, size_t a4, int (__cdecl *a5)(void **, size_t, size_t), void (__cdecl *a6)(void *), float a7)
{
  uint64_t v27 = *MEMORY[0x1E4F143B8];
  long long v7 = a2[8];
  long long v8 = a2[9];
  long long v9 = a2[6];
  *(_OWORD *)&layer_params.o_desc.stride[5] = a2[7];
  *(_OWORD *)&layer_params.o_desc.stride[7] = v7;
  long long v10 = a2[10];
  *(_OWORD *)&layer_params.o_desc.data_type = v8;
  *(_OWORD *)&layer_params.o_desc.table_data_type = v10;
  long long v11 = a2[4];
  *(_OWORD *)&layer_params.o_desc.stride[1] = a2[5];
  *(_OWORD *)&layer_params.o_desc.stride[3] = v9;
  long long v12 = a2[2];
  *(_OWORD *)&layer_params.o_desc.size[5] = a2[3];
  *(_OWORD *)&layer_params.o_desc.size[7] = v11;
  long long v13 = a2[1];
  *(_OWORD *)&layer_params.o_desc.flags = *a2;
  *(_OWORD *)&layer_params.o_desc.size[1] = v13;
  *(_OWORD *)&layer_params.o_desc.size[3] = v12;
  long long v14 = a1[5];
  *(_OWORD *)&v26[68] = a1[4];
  long long v15 = a1[2];
  *(_OWORD *)&v26[52] = a1[3];
  long long v16 = a1[6];
  *(_OWORD *)&v26[116] = a1[7];
  long long v17 = a1[9];
  *(_OWORD *)&v26[132] = a1[8];
  *(_OWORD *)&v26[148] = v17;
  *(_OWORD *)&v26[164] = a1[10];
  *(_OWORD *)&v26[84] = v14;
  *(_OWORD *)&v26[100] = v16;
  long long v18 = a1[1];
  *(_OWORD *)&v26[4] = *a1;
  *(_OWORD *)&v26[20] = v18;
  *(_OWORD *)&v26[36] = v15;
  layer_params.alpha = a7;
  *(_OWORD *)((char *)&layer_params.i_desc.stride[6] + 4) = *(_OWORD *)&v26[128];
  *(_OWORD *)((char *)&layer_params.i_desc.data + 4) = *(_OWORD *)&v26[144];
  *(_OWORD *)((char *)&layer_params.i_desc.table_data + 4) = *(_OWORD *)&v26[160];
  *((_DWORD *)&layer_params.i_desc.data_bias + 1) = *(_DWORD *)&v26[176];
  *(_OWORD *)((char *)&layer_params.i_desc.size[6] + 4) = *(_OWORD *)&v26[64];
  *(_OWORD *)((char *)layer_params.i_desc.stride + 4) = *(_OWORD *)&v26[80];
  *(_OWORD *)((char *)&layer_params.i_desc.stride[2] + 4) = *(_OWORD *)&v26[96];
  *(_OWORD *)((char *)&layer_params.i_desc.stride[4] + 4) = *(_OWORD *)&v26[112];
  *(_OWORD *)(&layer_params.alpha + 1) = *(_OWORD *)v26;
  *(_OWORD *)((char *)layer_params.i_desc.size + 4) = *(_OWORD *)&v26[16];
  *(_OWORD *)((char *)&layer_params.i_desc.size[2] + 4) = *(_OWORD *)&v26[32];
  *(_OWORD *)((char *)&layer_params.i_desc.size[4] + 4) = *(_OWORD *)&v26[48];
  if (a5 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    p_BNNSFilterParameters filter_params = 0;
  }
  else
  {
    filter_params.flags = a3;
    filter_params.n_threads = a4;
    filter_params.alloc_memory = a5;
    filter_params.free_memory = a6;
    p_BNNSFilterParameters filter_params = &filter_params;
  }
  long long v20 = BNNSFilterCreateLayerGram(&layer_params, p_filter_params);
  type metadata accessor for BNNS.GramLayer();
  uint64_t v21 = swift_allocObject();
  uint64_t v22 = v21;
  if (v20)
  {
    *(void *)(v21 + 16) = v20;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v22;
}

uint64_t type metadata accessor for BNNS.GramLayer()
{
  return self;
}

uint64_t BNNS.GramLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.GramLayer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

uint64_t vImage.PixelBuffer<>.applyGamma(_:intermediateBuffer:destination:)(unsigned int *a1, void *a2, void *a3)
{
  return vImage.PixelBuffer<>.applyGamma(_:intermediateBuffer:destination:)(a1, a2, a3, 4, (uint64_t (*)(unint64_t, void, void, uint64_t, void))specialized vImage.PixelBuffer<>._applyGamma<A, B>(_:intermediateBuffer:destination:widthMultiplier:pixelFormat:));
}

{
  return vImage.PixelBuffer<>.applyGamma(_:intermediateBuffer:destination:)(a1, a2, a3, 3, (uint64_t (*)(unint64_t, void, void, uint64_t, void))specialized vImage.PixelBuffer<>._applyGamma<A, B>(_:intermediateBuffer:destination:widthMultiplier:pixelFormat:));
}

{
  return vImage.PixelBuffer<>.applyGamma(_:intermediateBuffer:destination:)(a1, a2, a3, 2, (uint64_t (*)(unint64_t, void, void, uint64_t, void))specialized vImage.PixelBuffer<>._applyGamma<A, B>(_:intermediateBuffer:destination:widthMultiplier:pixelFormat:));
}

{
  return vImage.PixelBuffer<>.applyGamma(_:intermediateBuffer:destination:)(a1, a2, a3, 1, (uint64_t (*)(unint64_t, void, void, uint64_t, void))specialized vImage.PixelBuffer<>._applyGamma<A, B>(_:intermediateBuffer:destination:widthMultiplier:pixelFormat:));
}

vImage_Error specialized vImage.PixelBuffer<>._applyGamma<A, B>(_:intermediateBuffer:destination:widthMultiplier:pixelFormat:)(uint64_t a1, uint64_t a2, void *a3, uint64_t a4, void *a5)
{
  uint64_t v44 = *MEMORY[0x1E4F143B8];
  if (!a5[2])
  {
    __break(1u);
    goto LABEL_40;
  }
  uint64_t v8 = a5[6];
  if (v8 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  size_t v9 = a5[5];
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v8)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v9)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!a3[2])
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  uint64_t v10 = a3[6];
  if (v10 < 0)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  uint64_t v11 = a3[5];
  if (v11 < 0)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (!v10)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (!v11)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (v8 != v10)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (v9 != v11)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  vImagePixelCount v13 = (vImagePixelCount)vImageRotate90_CbCr16F;
  if (a2)
  {
    uint64_t v14 = a2;
    if (*(void *)(a2 + 16)) {
      goto LABEL_15;
    }
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  size_t v39 = 0;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v14 = swift_allocObject();
  *(_OWORD *)(v14 + 16) = xmmword_1D2135280;
  vImagePixelCount v6 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v8, v9);
  vImagePixelCount v5 = v26;
  uint64_t v28 = v27;
  uint64_t v30 = v29;
  type metadata accessor for vImage.BufferReference();
  uint64_t v31 = (void *)swift_allocObject();
  v31[2] = v6;
  v31[3] = v5;
  v31[4] = v28;
  v31[5] = v30;
  *(void *)(v14 + 32) = v6;
  *(void *)(v14 + 40) = v5;
  *(void *)(v14 + 48) = v28;
  *(void *)(v14 + 56) = v30;
  *(void *)(v14 + 64) = v31;
  vImagePixelCount v13 = 0x1D2135000;
  if (!*(void *)(v14 + 16)) {
    goto LABEL_34;
  }
LABEL_15:
  uint64_t v15 = *(void *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  uint64_t v16 = *(void *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v15)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (!v16)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v8 != v15)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (v9 != v16)
  {
LABEL_56:
    __break(1u);
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  long long v17 = (void *)a5[4];
  if (!v17)
  {
LABEL_64:
    swift_bridgeObjectRetain();
    swift_bridgeObjectRelease();
    __break(1u);
    goto LABEL_65;
  }
  vImagePixelCount v5 = v8 * a4;
  if ((unsigned __int128)(v8 * (__int128)a4) >> 64 != (v8 * a4) >> 63) {
    goto LABEL_57;
  }
  size_t v18 = a5[7];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t inited = swift_initStackObject();
  long long v20 = *(_OWORD *)(v13 + 640);
  *(_OWORD *)(inited + 16) = v20;
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  *(void *)(inited + 32) = v17;
  *(void *)(inited + 40) = v9;
  *(void *)(inited + 48) = v5;
  *(void *)(inited + 56) = v18;
  *(void *)(inited + 64) = 0;
  if (!*(void *)(v14 + 16))
  {
LABEL_59:
    __break(1u);
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  size_t v39 = v18;
  uint64_t v8 = *(void *)(v14 + 32);
  if (!v8)
  {
LABEL_65:
    swift_setDeallocating();
    swift_bridgeObjectRetain();
    swift_arrayDestroy();
    swift_bridgeObjectRelease();
    __break(1u);
LABEL_66:
    swift_setDeallocating();
    swift_arrayDestroy();
    __break(1u);
  }
  uint64_t v21 = *(void *)(v14 + 48);
  if (v21 < 0) {
    goto LABEL_60;
  }
  vImagePixelCount v13 = v21 * a4;
  if ((unsigned __int128)(v21 * (__int128)a4) >> 64 != (v21 * a4) >> 63)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  long long v38 = v17;
  vImagePixelCount v6 = *(void *)(v14 + 40);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  vImagePixelCount v36 = v9;
  size_t v9 = *(void *)(v14 + 56);
  long long v37 = v20;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRelease();
  uint64_t v22 = swift_initStackObject();
  *(_OWORD *)(v22 + 16) = v37;
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  LODWORD(v11) = a1;
  *(void *)(v22 + 32) = v8;
  *(void *)(v22 + 40) = v6;
  *(void *)(v22 + 48) = v13;
  *(void *)(v22 + 56) = v9;
  *(void *)(v22 + 64) = 0;
  if (BYTE4(a1) == 1)
  {
LABEL_35:
    float v25 = *(float *)&v11;
    int v24 = 1;
    long long v23 = a3;
    goto LABEL_37;
  }
  long long v23 = a3;
  if (BYTE4(a1))
  {
    int v24 = a1 + 2;
    float v25 = 0.0;
  }
  else
  {
    int v24 = 0;
    float v25 = *(float *)&a1;
  }
LABEL_37:
  GammaFunction GammaFunction = vImageCreateGammaFunction(v25, v24, 0);
  swift_release();
  src.data = v38;
  src.height = v36;
  src.width = v5;
  src.rowBytes = v39;
  dest.data = (void *)v8;
  dest.height = v6;
  dest.width = v13;
  dest.rowBytes = v9;
  vImageGamma_Planar8toPlanarF(&src, &dest, GammaFunction, 0);
  uint64_t v33 = (void *)v23[4];
  if (!v33) {
    goto LABEL_66;
  }
  size_t v34 = v23[7];
  swift_release();
  src.data = (void *)v8;
  src.height = v6;
  src.width = v13;
  src.rowBytes = v9;
  dest.data = v33;
  dest.height = v36;
  dest.width = v5;
  dest.rowBytes = v34;
  return vImageConvert_PlanarFtoPlanar8(&src, &dest, 1.0, 0.0, 0);
}

{
  vImagePixelCount v5;
  vImagePixelCount v6;
  uint64_t v8;
  size_t v9;
  uint64_t v10;
  uint64_t v11;
  vImagePixelCount v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  uint64_t inited;
  long long v20;
  uint64_t v21;
  uint64_t v22;
  void *v23;
  int v24;
  float v25;
  vImagePixelCount v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  void *v31;
  GammaFunction GammaFunction;
  void *v33;
  size_t v34;
  vImagePixelCount v36;
  long long v37;
  void *v38;
  size_t v39;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v44;

  uint64_t v44 = *MEMORY[0x1E4F143B8];
  if (!a5[2])
  {
    __break(1u);
    goto LABEL_40;
  }
  uint64_t v8 = a5[6];
  if (v8 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  size_t v9 = a5[5];
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v8)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v9)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!a3[2])
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  uint64_t v10 = a3[6];
  if (v10 < 0)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  uint64_t v11 = a3[5];
  if (v11 < 0)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (!v10)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (!v11)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (v8 != v10)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (v9 != v11)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  vImagePixelCount v13 = (vImagePixelCount)vImageRotate90_CbCr16F;
  if (a2)
  {
    uint64_t v14 = a2;
    if (*(void *)(a2 + 16)) {
      goto LABEL_15;
    }
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  size_t v39 = 0;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v14 = swift_allocObject();
  *(_OWORD *)(v14 + 16) = xmmword_1D2135280;
  vImagePixelCount v6 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v8, v9);
  vImagePixelCount v5 = v26;
  uint64_t v28 = v27;
  uint64_t v30 = v29;
  type metadata accessor for vImage.BufferReference();
  uint64_t v31 = (void *)swift_allocObject();
  v31[2] = v6;
  v31[3] = v5;
  v31[4] = v28;
  v31[5] = v30;
  *(void *)(v14 + 32) = v6;
  *(void *)(v14 + 40) = v5;
  *(void *)(v14 + 48) = v28;
  *(void *)(v14 + 56) = v30;
  *(void *)(v14 + 64) = v31;
  vImagePixelCount v13 = 0x1D2135000;
  if (!*(void *)(v14 + 16)) {
    goto LABEL_34;
  }
LABEL_15:
  uint64_t v15 = *(void *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  uint64_t v16 = *(void *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v15)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (!v16)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v8 != v15)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (v9 != v16)
  {
LABEL_56:
    __break(1u);
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  long long v17 = (void *)a5[4];
  if (!v17)
  {
LABEL_64:
    swift_bridgeObjectRetain();
    swift_bridgeObjectRelease();
    __break(1u);
    goto LABEL_65;
  }
  vImagePixelCount v5 = v8 * a4;
  if ((unsigned __int128)(v8 * (__int128)a4) >> 64 != (v8 * a4) >> 63) {
    goto LABEL_57;
  }
  size_t v18 = a5[7];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t inited = swift_initStackObject();
  long long v20 = *(_OWORD *)(v13 + 640);
  *(_OWORD *)(inited + 16) = v20;
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  *(void *)(inited + 32) = v17;
  *(void *)(inited + 40) = v9;
  *(void *)(inited + 48) = v5;
  *(void *)(inited + 56) = v18;
  *(void *)(inited + 64) = 0;
  if (!*(void *)(v14 + 16))
  {
LABEL_59:
    __break(1u);
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  size_t v39 = v18;
  uint64_t v8 = *(void *)(v14 + 32);
  if (!v8)
  {
LABEL_65:
    swift_setDeallocating();
    swift_bridgeObjectRetain();
    swift_arrayDestroy();
    swift_bridgeObjectRelease();
    __break(1u);
LABEL_66:
    swift_setDeallocating();
    swift_arrayDestroy();
    __break(1u);
  }
  uint64_t v21 = *(void *)(v14 + 48);
  if (v21 < 0) {
    goto LABEL_60;
  }
  vImagePixelCount v13 = v21 * a4;
  if ((unsigned __int128)(v21 * (__int128)a4) >> 64 != (v21 * a4) >> 63)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  long long v38 = v17;
  vImagePixelCount v6 = *(void *)(v14 + 40);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  vImagePixelCount v36 = v9;
  size_t v9 = *(void *)(v14 + 56);
  long long v37 = v20;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRelease();
  uint64_t v22 = swift_initStackObject();
  *(_OWORD *)(v22 + 16) = v37;
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  LODWORD(v11) = a1;
  *(void *)(v22 + 32) = v8;
  *(void *)(v22 + 40) = v6;
  *(void *)(v22 + 48) = v13;
  *(void *)(v22 + 56) = v9;
  *(void *)(v22 + 64) = 0;
  if (BYTE4(a1) == 1)
  {
LABEL_35:
    float v25 = *(float *)&v11;
    int v24 = 1;
    long long v23 = a3;
    goto LABEL_37;
  }
  long long v23 = a3;
  if (BYTE4(a1))
  {
    int v24 = a1 + 2;
    float v25 = 0.0;
  }
  else
  {
    int v24 = 0;
    float v25 = *(float *)&a1;
  }
LABEL_37:
  GammaFunction GammaFunction = vImageCreateGammaFunction(v25, v24, 0);
  swift_release();
  src.data = v38;
  src.height = v36;
  src.width = v5;
  src.rowBytes = v39;
  dest.data = (void *)v8;
  dest.height = v6;
  dest.width = v13;
  dest.rowBytes = v9;
  vImageGamma_Planar8toPlanarF(&src, &dest, GammaFunction, 0);
  uint64_t v33 = (void *)v23[4];
  if (!v33) {
    goto LABEL_66;
  }
  size_t v34 = v23[7];
  swift_release();
  src.data = (void *)v8;
  src.height = v6;
  src.width = v13;
  src.rowBytes = v9;
  dest.data = v33;
  dest.height = v36;
  dest.width = v5;
  dest.rowBytes = v34;
  return vImageConvert_PlanarFtoPlanar8(&src, &dest, 1.0, 0.0, 0);
}

{
  vImagePixelCount v5;
  vImagePixelCount v6;
  uint64_t v8;
  size_t v9;
  uint64_t v10;
  uint64_t v11;
  vImagePixelCount v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  uint64_t inited;
  long long v20;
  uint64_t v21;
  uint64_t v22;
  void *v23;
  int v24;
  float v25;
  vImagePixelCount v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  void *v31;
  GammaFunction GammaFunction;
  void *v33;
  size_t v34;
  vImagePixelCount v36;
  long long v37;
  void *v38;
  size_t v39;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v44;

  uint64_t v44 = *MEMORY[0x1E4F143B8];
  if (!a5[2])
  {
    __break(1u);
    goto LABEL_40;
  }
  uint64_t v8 = a5[6];
  if (v8 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  size_t v9 = a5[5];
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v8)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v9)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!a3[2])
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  uint64_t v10 = a3[6];
  if (v10 < 0)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  uint64_t v11 = a3[5];
  if (v11 < 0)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (!v10)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (!v11)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (v8 != v10)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (v9 != v11)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  vImagePixelCount v13 = (vImagePixelCount)vImageRotate90_CbCr16F;
  if (a2)
  {
    uint64_t v14 = a2;
    if (*(void *)(a2 + 16)) {
      goto LABEL_15;
    }
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  size_t v39 = 0;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v14 = swift_allocObject();
  *(_OWORD *)(v14 + 16) = xmmword_1D2135280;
  vImagePixelCount v6 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v8, v9);
  vImagePixelCount v5 = v26;
  uint64_t v28 = v27;
  uint64_t v30 = v29;
  type metadata accessor for vImage.BufferReference();
  uint64_t v31 = (void *)swift_allocObject();
  v31[2] = v6;
  v31[3] = v5;
  v31[4] = v28;
  v31[5] = v30;
  *(void *)(v14 + 32) = v6;
  *(void *)(v14 + 40) = v5;
  *(void *)(v14 + 48) = v28;
  *(void *)(v14 + 56) = v30;
  *(void *)(v14 + 64) = v31;
  vImagePixelCount v13 = 0x1D2135000;
  if (!*(void *)(v14 + 16)) {
    goto LABEL_34;
  }
LABEL_15:
  uint64_t v15 = *(void *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  uint64_t v16 = *(void *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v15)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (!v16)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v8 != v15)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (v9 != v16)
  {
LABEL_56:
    __break(1u);
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  long long v17 = (void *)a5[4];
  if (!v17)
  {
LABEL_64:
    swift_bridgeObjectRetain();
    swift_bridgeObjectRelease();
    __break(1u);
    goto LABEL_65;
  }
  vImagePixelCount v5 = v8 * a4;
  if ((unsigned __int128)(v8 * (__int128)a4) >> 64 != (v8 * a4) >> 63) {
    goto LABEL_57;
  }
  size_t v18 = a5[7];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t inited = swift_initStackObject();
  long long v20 = *(_OWORD *)(v13 + 640);
  *(_OWORD *)(inited + 16) = v20;
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  *(void *)(inited + 32) = v17;
  *(void *)(inited + 40) = v9;
  *(void *)(inited + 48) = v5;
  *(void *)(inited + 56) = v18;
  *(void *)(inited + 64) = 0;
  if (!*(void *)(v14 + 16))
  {
LABEL_59:
    __break(1u);
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  size_t v39 = v18;
  uint64_t v8 = *(void *)(v14 + 32);
  if (!v8)
  {
LABEL_65:
    swift_setDeallocating();
    swift_bridgeObjectRetain();
    swift_arrayDestroy();
    swift_bridgeObjectRelease();
    __break(1u);
LABEL_66:
    swift_setDeallocating();
    swift_arrayDestroy();
    __break(1u);
  }
  uint64_t v21 = *(void *)(v14 + 48);
  if (v21 < 0) {
    goto LABEL_60;
  }
  vImagePixelCount v13 = v21 * a4;
  if ((unsigned __int128)(v21 * (__int128)a4) >> 64 != (v21 * a4) >> 63)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  long long v38 = v17;
  vImagePixelCount v6 = *(void *)(v14 + 40);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  vImagePixelCount v36 = v9;
  size_t v9 = *(void *)(v14 + 56);
  long long v37 = v20;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRelease();
  uint64_t v22 = swift_initStackObject();
  *(_OWORD *)(v22 + 16) = v37;
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  LODWORD(v11) = a1;
  *(void *)(v22 + 32) = v8;
  *(void *)(v22 + 40) = v6;
  *(void *)(v22 + 48) = v13;
  *(void *)(v22 + 56) = v9;
  *(void *)(v22 + 64) = 0;
  if (BYTE4(a1) == 1)
  {
LABEL_35:
    float v25 = *(float *)&v11;
    int v24 = 1;
    long long v23 = a3;
    goto LABEL_37;
  }
  long long v23 = a3;
  if (BYTE4(a1))
  {
    int v24 = a1 + 2;
    float v25 = 0.0;
  }
  else
  {
    int v24 = 0;
    float v25 = *(float *)&a1;
  }
LABEL_37:
  GammaFunction GammaFunction = vImageCreateGammaFunction(v25, v24, 0);
  swift_release();
  src.data = v38;
  src.height = v36;
  src.width = v5;
  src.rowBytes = v39;
  dest.data = (void *)v8;
  dest.height = v6;
  dest.width = v13;
  dest.rowBytes = v9;
  vImageGamma_Planar8toPlanarF(&src, &dest, GammaFunction, 0);
  uint64_t v33 = (void *)v23[4];
  if (!v33) {
    goto LABEL_66;
  }
  size_t v34 = v23[7];
  swift_release();
  src.data = (void *)v8;
  src.height = v6;
  src.width = v13;
  src.rowBytes = v9;
  dest.data = v33;
  dest.height = v36;
  dest.width = v5;
  dest.rowBytes = v34;
  return vImageConvert_PlanarFtoPlanar8(&src, &dest, 1.0, 0.0, 0);
}

{
  vImagePixelCount v5;
  vImagePixelCount v6;
  uint64_t v8;
  size_t v9;
  uint64_t v10;
  uint64_t v11;
  vImagePixelCount v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  uint64_t inited;
  long long v20;
  uint64_t v21;
  uint64_t v22;
  void *v23;
  int v24;
  float v25;
  vImagePixelCount v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  void *v31;
  GammaFunction GammaFunction;
  void *v33;
  size_t v34;
  vImagePixelCount v36;
  long long v37;
  void *v38;
  size_t v39;
  vImage_Buffer dest;
  vImage_Buffer src;
  uint64_t v44;

  uint64_t v44 = *MEMORY[0x1E4F143B8];
  if (!a5[2])
  {
    __break(1u);
    goto LABEL_40;
  }
  uint64_t v8 = a5[6];
  if (v8 < 0)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  size_t v9 = a5[5];
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (!v8)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!v9)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!a3[2])
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  uint64_t v10 = a3[6];
  if (v10 < 0)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  uint64_t v11 = a3[5];
  if (v11 < 0)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (!v10)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (!v11)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (v8 != v10)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (v9 != v11)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  vImagePixelCount v13 = (vImagePixelCount)vImageRotate90_CbCr16F;
  if (a2)
  {
    uint64_t v14 = a2;
    if (*(void *)(a2 + 16)) {
      goto LABEL_15;
    }
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  size_t v39 = 0;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t v14 = swift_allocObject();
  *(_OWORD *)(v14 + 16) = xmmword_1D2135280;
  vImagePixelCount v6 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v8, v9);
  vImagePixelCount v5 = v26;
  uint64_t v28 = v27;
  uint64_t v30 = v29;
  type metadata accessor for vImage.BufferReference();
  uint64_t v31 = (void *)swift_allocObject();
  v31[2] = v6;
  v31[3] = v5;
  v31[4] = v28;
  v31[5] = v30;
  *(void *)(v14 + 32) = v6;
  *(void *)(v14 + 40) = v5;
  *(void *)(v14 + 48) = v28;
  *(void *)(v14 + 56) = v30;
  *(void *)(v14 + 64) = v31;
  vImagePixelCount v13 = 0x1D2135000;
  if (!*(void *)(v14 + 16)) {
    goto LABEL_34;
  }
LABEL_15:
  uint64_t v15 = *(void *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  uint64_t v16 = *(void *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v15)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (!v16)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v8 != v15)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  if (v9 != v16)
  {
LABEL_56:
    __break(1u);
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  long long v17 = (void *)a5[4];
  if (!v17)
  {
LABEL_64:
    swift_bridgeObjectRetain();
    swift_bridgeObjectRelease();
    __break(1u);
    goto LABEL_65;
  }
  vImagePixelCount v5 = v8 * a4;
  if ((unsigned __int128)(v8 * (__int128)a4) >> 64 != (v8 * a4) >> 63) {
    goto LABEL_57;
  }
  size_t v18 = a5[7];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t inited = swift_initStackObject();
  long long v20 = *(_OWORD *)(v13 + 640);
  *(_OWORD *)(inited + 16) = v20;
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  *(void *)(inited + 32) = v17;
  *(void *)(inited + 40) = v9;
  *(void *)(inited + 48) = v5;
  *(void *)(inited + 56) = v18;
  *(void *)(inited + 64) = 0;
  if (!*(void *)(v14 + 16))
  {
LABEL_59:
    __break(1u);
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  size_t v39 = v18;
  uint64_t v8 = *(void *)(v14 + 32);
  if (!v8)
  {
LABEL_65:
    swift_setDeallocating();
    swift_bridgeObjectRetain();
    swift_arrayDestroy();
    swift_bridgeObjectRelease();
    __break(1u);
LABEL_66:
    swift_setDeallocating();
    swift_arrayDestroy();
    __break(1u);
  }
  uint64_t v21 = *(void *)(v14 + 48);
  if (v21 < 0) {
    goto LABEL_60;
  }
  vImagePixelCount v13 = v21 * a4;
  if ((unsigned __int128)(v21 * (__int128)a4) >> 64 != (v21 * a4) >> 63)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  long long v38 = v17;
  vImagePixelCount v6 = *(void *)(v14 + 40);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  vImagePixelCount v36 = v9;
  size_t v9 = *(void *)(v14 + 56);
  long long v37 = v20;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRelease();
  uint64_t v22 = swift_initStackObject();
  *(_OWORD *)(v22 + 16) = v37;
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  LODWORD(v11) = a1;
  *(void *)(v22 + 32) = v8;
  *(void *)(v22 + 40) = v6;
  *(void *)(v22 + 48) = v13;
  *(void *)(v22 + 56) = v9;
  *(void *)(v22 + 64) = 0;
  if (BYTE4(a1) == 1)
  {
LABEL_35:
    float v25 = *(float *)&v11;
    int v24 = 1;
    long long v23 = a3;
    goto LABEL_37;
  }
  long long v23 = a3;
  if (BYTE4(a1))
  {
    int v24 = a1 + 2;
    float v25 = 0.0;
  }
  else
  {
    int v24 = 0;
    float v25 = *(float *)&a1;
  }
LABEL_37:
  GammaFunction GammaFunction = vImageCreateGammaFunction(v25, v24, 0);
  swift_release();
  src.data = v38;
  src.height = v36;
  src.width = v5;
  src.rowBytes = v39;
  dest.data = (void *)v8;
  dest.height = v6;
  dest.width = v13;
  dest.rowBytes = v9;
  vImageGamma_Planar8toPlanarF(&src, &dest, GammaFunction, 0);
  uint64_t v33 = (void *)v23[4];
  if (!v33) {
    goto LABEL_66;
  }
  size_t v34 = v23[7];
  swift_release();
  src.data = (void *)v8;
  src.height = v6;
  src.width = v13;
  src.rowBytes = v9;
  dest.data = v33;
  dest.height = v36;
  dest.width = v5;
  dest.rowBytes = v34;
  return vImageConvert_PlanarFtoPlanar8(&src, &dest, 1.0, 0.0, 0);
}

vImage_Error vImage.PixelBuffer<>._applyGamma<A, B>(_:intermediateBuffer:destination:widthMultiplier:pixelFormat:)(uint64_t a1, void **a2, uint64_t *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  uint64_t v58 = *MEMORY[0x1E4F143B8];
  float v51 = *(float *)a1;
  int v52 = *(unsigned __int8 *)(a1 + 4);
  vImagePixelCount v13 = *a2;
  uint64_t v14 = *a3;
  uint64_t v15 = *v11;
  dest.data = *v11;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&src);
  data = src.data;
  vImagePixelCount height = src.height;
  long long v55 = (void *)v14;
  type metadata accessor for vImage.PixelBuffer();
  vImage.PixelBuffer.size.getter(&dest);
  long long v53 = (void *)v14;
  swift_bridgeObjectRelease();
  if (data != dest.data || height != dest.height)
  {
    __break(1u);
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v13)
  {
    size_t v18 = v13;
  }
  else
  {
    dest.data = v15;
    vImage.PixelBuffer.size.getter(&src);
    *(_OWORD *)&dest.data = *(_OWORD *)&src.data;
    vImage.PixelBuffer<>.init(size:pixelFormat:)((uint64_t)&dest, a8, a11, (uint64_t *)&v55);
    size_t v18 = v55;
  }
  dest.data = v15;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&src);
  long long v20 = src.data;
  vImagePixelCount v19 = src.height;
  long long v55 = v18;
  type metadata accessor for vImage.PixelBuffer();
  vImage.PixelBuffer.size.getter(&dest);
  if (v20 != dest.data || v19 != dest.height) {
    goto LABEL_24;
  }
  src.data = v15;
  uint64_t v21 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v21)
  {
LABEL_31:
    swift_bridgeObjectRelease();
    __break(1u);
    goto LABEL_32;
  }
  uint64_t v22 = (void *)v21;
  src.data = v15;
  uint64_t v23 = vImage.PixelBuffer.width.getter();
  vImagePixelCount v24 = v23 * a4;
  if ((unsigned __int128)(v23 * (__int128)a4) >> 64 != (v23 * a4) >> 63) {
    goto LABEL_25;
  }
  src.data = v15;
  uint64_t v25 = vImage.PixelBuffer.height.getter();
  src.data = v15;
  vImage.PixelBuffer<>.vImageBuffer.getter();
  size_t v27 = v26;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135280;
  if (((v25 | v24) & 0x8000000000000000) != 0)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  *(void *)(inited + 32) = v22;
  *(void *)(inited + 40) = v25;
  size_t v49 = v27;
  vImagePixelCount v50 = v25;
  *(void *)(inited + 48) = v24;
  *(void *)(inited + 56) = v27;
  *(void *)(inited + 64) = 0;
  src.data = v18;
  uint64_t v29 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v29)
  {
LABEL_32:
    swift_setDeallocating();
    swift_arrayDestroy();
    swift_bridgeObjectRelease();
    __break(1u);
    goto LABEL_33;
  }
  uint64_t v30 = (void *)v29;
  src.data = v18;
  uint64_t v31 = vImage.PixelBuffer.width.getter();
  vImagePixelCount v32 = v31 * a4;
  if ((unsigned __int128)(v31 * (__int128)a4) >> 64 != (v31 * a4) >> 63) {
    goto LABEL_27;
  }
  long long v48 = v22;
  src.data = v18;
  vImagePixelCount v33 = vImage.PixelBuffer.height.getter();
  src.data = v18;
  vImage.PixelBuffer<>.vImageBuffer.getter();
  size_t v35 = v34;
  swift_bridgeObjectRelease();
  uint64_t v36 = swift_initStackObject();
  *(_OWORD *)(v36 + 16) = xmmword_1D2135280;
  if (((v33 | v32) & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  *(void *)(v36 + 32) = v30;
  *(void *)(v36 + 40) = v33;
  *(void *)(v36 + 48) = v32;
  *(void *)(v36 + 56) = v35;
  *(void *)(v36 + 64) = 0;
  if (v52 == 1)
  {
    float v38 = v51;
    int v37 = 1;
  }
  else if (v52)
  {
    int v37 = LODWORD(v51) + 2;
    float v38 = 0.0;
  }
  else
  {
    int v37 = 0;
    float v38 = v51;
  }
  GammaFunction GammaFunction = vImageCreateGammaFunction(v38, v37, 0);
  swift_release();
  src.data = v48;
  src.vImagePixelCount height = v50;
  src.width = v24;
  src.rowBytes = v49;
  dest.data = v30;
  dest.vImagePixelCount height = v33;
  dest.width = v32;
  dest.rowBytes = v35;
  vImageGamma_Planar8toPlanarF(&src, &dest, GammaFunction, 0);
  src.data = v53;
  uint64_t v40 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v40)
  {
LABEL_33:
    swift_setDeallocating();
    swift_arrayDestroy();
    __break(1u);
  }
  uint64_t v41 = (void *)v40;
  src.data = v53;
  uint64_t v42 = vImage.PixelBuffer.width.getter();
  vImagePixelCount v43 = v42 * a4;
  if ((unsigned __int128)(v42 * (__int128)a4) >> 64 != (v42 * a4) >> 63) {
    goto LABEL_29;
  }
  src.data = v53;
  vImagePixelCount v44 = vImage.PixelBuffer.height.getter();
  src.data = v53;
  vImage.PixelBuffer<>.vImageBuffer.getter();
  size_t v46 = v45;
  swift_release();
  if (((v44 | v43) & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  src.data = v30;
  src.vImagePixelCount height = v33;
  src.width = v32;
  src.rowBytes = v35;
  dest.data = v41;
  dest.vImagePixelCount height = v44;
  dest.width = v43;
  dest.rowBytes = v46;
  return vImageConvert_PlanarFtoPlanar8(&src, &dest, 1.0, 0.0, 0);
}

uint64_t vImage.PixelBuffer<>.applyGamma(_:intermediateBuffer:destination:)(unsigned int *a1, void *a2, void *a3, uint64_t a4, uint64_t (*a5)(unint64_t, void, void, uint64_t, void))
{
  return a5(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a2, *a3, a4, *v5);
}

vImage_Error vImage.PixelBuffer<>.applyGamma(_:destination:)(uint64_t a1, void **a2, uint64_t a3, uint64_t a4)
{
  uint64_t v6 = (*(uint64_t (**)(void, uint64_t))(a4 + 24))(*(void *)(a3 + 16), a4);

  return vImage.PixelBuffer<>._applyGamma<A>(_:destination:widthMultiplier:pixelFormat:)(a1, a2, v6);
}

vImage_Error vImage.PixelBuffer<>._applyGamma<A>(_:destination:widthMultiplier:pixelFormat:)(uint64_t a1, void **a2, uint64_t a3)
{
  uint64_t v32 = *MEMORY[0x1E4F143B8];
  float v27 = *(float *)a1;
  int v28 = *(unsigned __int8 *)(a1 + 4);
  vImagePixelCount v5 = *a2;
  uint64_t v6 = *v3;
  dest.data = *v3;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&src);
  data = src.data;
  vImagePixelCount height = src.height;
  type metadata accessor for vImage.PixelBuffer();
  vImage.PixelBuffer.size.getter(&dest);
  swift_bridgeObjectRelease();
  if (data != dest.data || height != dest.height)
  {
    __break(1u);
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  src.data = v6;
  uint64_t v29 = (void *)vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v29)
  {
LABEL_20:
    __break(1u);
LABEL_21:
    swift_setDeallocating();
    swift_arrayDestroy();
    __break(1u);
  }
  src.data = v6;
  uint64_t v9 = vImage.PixelBuffer.width.getter();
  vImagePixelCount v10 = v9 * a3;
  if ((unsigned __int128)(v9 * (__int128)a3) >> 64 != (v9 * a3) >> 63) {
    goto LABEL_16;
  }
  src.data = v6;
  vImagePixelCount v11 = vImage.PixelBuffer.height.getter();
  src.data = v6;
  vImage.PixelBuffer<>.vImageBuffer.getter();
  size_t v13 = v12;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135280;
  if (((v11 | v10) & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  *(void *)(inited + 32) = v29;
  *(void *)(inited + 40) = v11;
  *(void *)(inited + 48) = v10;
  *(void *)(inited + 56) = v13;
  size_t v26 = v13;
  *(void *)(inited + 64) = 0;
  src.data = v5;
  uint64_t v15 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v15) {
    goto LABEL_21;
  }
  uint64_t v16 = (void *)v15;
  src.data = v5;
  uint64_t v17 = vImage.PixelBuffer.width.getter();
  vImagePixelCount v18 = v17 * a3;
  if ((unsigned __int128)(v17 * (__int128)a3) >> 64 != (v17 * a3) >> 63) {
    goto LABEL_18;
  }
  src.data = v5;
  vImagePixelCount v19 = vImage.PixelBuffer.height.getter();
  src.data = v5;
  vImage.PixelBuffer<>.vImageBuffer.getter();
  if (((v19 | v18) & 0x8000000000000000) != 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  size_t v21 = v20;
  if (v28 == 1)
  {
    float v23 = v27;
    int v22 = 1;
  }
  else if (v28)
  {
    int v22 = LODWORD(v27) + 2;
    float v23 = 0.0;
  }
  else
  {
    int v22 = 0;
    float v23 = v27;
  }
  GammaFunction GammaFunction = vImageCreateGammaFunction(v23, v22, 0);
  swift_release();
  src.data = v29;
  src.vImagePixelCount height = v11;
  src.width = v10;
  src.rowBytes = v26;
  dest.data = v16;
  dest.vImagePixelCount height = v19;
  dest.width = v18;
  dest.rowBytes = v21;
  return vImageGamma_PlanarF(&src, &dest, GammaFunction, 0);
}

uint64_t vImage.PixelBuffer<>.applyGamma(linearParameters:exponentialParameters:boundary:destination:)(float a1, float a2, float a3, float a4, float a5, float a6, float a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v50 = *MEMORY[0x1E4F143B8];
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&src);
  vImage.PixelBuffer.size.getter(&dest);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  if (src.data != dest.data || src.height != dest.height)
  {
    __break(1u);
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  uint64_t v19 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v19)
  {
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
  }
  uint64_t v20 = v19;
  uint64_t v21 = vImage.PixelBuffer.width.getter();
  uint64_t v22 = *(void *)(a9 + 16);
  size_t v39 = *(uint64_t (**)(uint64_t, uint64_t))(a10 + 24);
  uint64_t v40 = a10;
  uint64_t v23 = v39(v22, a10);
  uint64_t v24 = v21 * v23;
  if ((unsigned __int128)(v21 * (__int128)v23) >> 64 != (v21 * v23) >> 63) {
    goto LABEL_11;
  }
  uint64_t v25 = vImage.PixelBuffer.height.getter();
  vImage.PixelBuffer<>.vImageBuffer.getter();
  uint64_t v27 = v26;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135280;
  if ((v25 | v24) < 0)
  {
LABEL_12:
    __break(1u);
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  uint64_t v29 = inited;
  *(void *)(inited + 32) = v20;
  *(void *)(inited + 40) = v25;
  *(void *)(inited + 48) = v24;
  *(void *)(inited + 56) = v27;
  *(void *)(inited + 64) = 0;
  uint64_t v30 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v30) {
    goto LABEL_16;
  }
  uint64_t v31 = (void *)v30;
  uint64_t v32 = vImage.PixelBuffer.width.getter();
  uint64_t v33 = v39(v22, v40);
  vImagePixelCount v34 = v32 * v33;
  if ((unsigned __int128)(v32 * (__int128)v33) >> 64 != (v32 * v33) >> 63) {
    goto LABEL_13;
  }
  vImagePixelCount v35 = vImage.PixelBuffer.height.getter();
  vImage.PixelBuffer<>.vImageBuffer.getter();
  if (((v35 | v34) & 0x8000000000000000) != 0)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  vImagePixelCount v43 = v31;
  vImagePixelCount v44 = v35;
  vImagePixelCount v45 = v34;
  size_t v46 = v36;
  uint64_t v47 = 0;
  long long v37 = *(_OWORD *)(v29 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v29 + 32);
  *(_OWORD *)&src.width = v37;
  dest.data = v31;
  dest.vImagePixelCount height = v35;
  dest.width = v34;
  dest.rowBytes = v36;
  v49[0] = a3;
  v49[1] = a4;
  v49[2] = a6;
  v48[0] = a1;
  v48[1] = a2;
  vImagePiecewiseGamma_PlanarF(&src, &dest, v49, a5, v48, a7, 0);
  swift_bridgeObjectRelease();
  return swift_arrayDestroy();
}

uint64_t vImage.PixelBuffer<>.applyGamma(linearParameters:exponentialParameters:boundary:destination:)(Pixel_8 a1, float a2, float a3, float a4, float a5, float a6, float a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v50 = *MEMORY[0x1E4F143B8];
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&src);
  vImage.PixelBuffer.size.getter(&dest);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  if (src.data != dest.data || src.height != dest.height)
  {
    __break(1u);
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  Pixel_8 v39 = a1;
  uint64_t v19 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v19)
  {
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
  }
  uint64_t v20 = v19;
  uint64_t v21 = vImage.PixelBuffer.width.getter();
  uint64_t v22 = *(void *)(a9 + 16);
  uint64_t v40 = *(uint64_t (**)(uint64_t))(a10 + 24);
  uint64_t v23 = v40(v22);
  uint64_t v24 = v21 * v23;
  if ((unsigned __int128)(v21 * (__int128)v23) >> 64 != (v21 * v23) >> 63) {
    goto LABEL_11;
  }
  uint64_t v25 = vImage.PixelBuffer.height.getter();
  vImage.PixelBuffer<>.vImageBuffer.getter();
  uint64_t v27 = v26;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135280;
  if ((v25 | v24) < 0)
  {
LABEL_12:
    __break(1u);
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  uint64_t v29 = inited;
  *(void *)(inited + 32) = v20;
  *(void *)(inited + 40) = v25;
  *(void *)(inited + 48) = v24;
  *(void *)(inited + 56) = v27;
  *(void *)(inited + 64) = 0;
  uint64_t v30 = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (!v30) {
    goto LABEL_16;
  }
  uint64_t v31 = (void *)v30;
  uint64_t v32 = vImage.PixelBuffer.width.getter();
  uint64_t v33 = ((uint64_t (*)(uint64_t, uint64_t))v40)(v22, a10);
  vImagePixelCount v34 = v32 * v33;
  if ((unsigned __int128)(v32 * (__int128)v33) >> 64 != (v32 * v33) >> 63) {
    goto LABEL_13;
  }
  vImagePixelCount v35 = vImage.PixelBuffer.height.getter();
  vImage.PixelBuffer<>.vImageBuffer.getter();
  if (((v35 | v34) & 0x8000000000000000) != 0)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  vImagePixelCount v43 = v31;
  vImagePixelCount v44 = v35;
  vImagePixelCount v45 = v34;
  size_t v46 = v36;
  uint64_t v47 = 0;
  long long v37 = *(_OWORD *)(v29 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v29 + 32);
  *(_OWORD *)&src.width = v37;
  dest.data = v31;
  dest.vImagePixelCount height = v35;
  dest.width = v34;
  dest.rowBytes = v36;
  v49[0] = a4;
  v49[1] = a5;
  v49[2] = a7;
  v48[0] = a2;
  v48[1] = a3;
  vImagePiecewiseGamma_Planar8(&src, &dest, v49, a6, v48, v39, 0);
  swift_bridgeObjectRelease();
  return swift_arrayDestroy();
}

uint64_t __swift_memcpy5_4(uint64_t result, int *a2)
{
  int v2 = *a2;
  *(unsigned char *)(result + 4) = *((unsigned char *)a2 + 4);
  *(_DWORD *)unint64_t result = v2;
  return result;
}

uint64_t getEnumTagSinglePayload for vImage.Gamma(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFE && *(unsigned char *)(a1 + 5)) {
    return (*(_DWORD *)a1 + 254);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 4);
  if (v3 <= 2) {
    int v4 = -1;
  }
  else {
    int v4 = v3 ^ 0xFF;
  }
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for vImage.Gamma(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFD)
  {
    *(unsigned char *)(result + 4) = 0;
    *(_DWORD *)unint64_t result = a2 - 254;
    if (a3 >= 0xFE) {
      *(unsigned char *)(result + 5) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFE) {
      *(unsigned char *)(result + 5) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 4) = -(char)a2;
    }
  }
  return result;
}

uint64_t getEnumTag for vImage.Gamma(uint64_t a1)
{
  if (*(unsigned __int8 *)(a1 + 4) <= 1u) {
    return *(unsigned __int8 *)(a1 + 4);
  }
  else {
    return (*(_DWORD *)a1 + 2);
  }
}

uint64_t destructiveInjectEnumTag for vImage.Gamma(uint64_t result, unsigned int a2)
{
  if (a2 >= 2)
  {
    *(_DWORD *)unint64_t result = a2 - 2;
    LOBYTE(a2) = 2;
  }
  *(unsigned char *)(result + 4) = a2;
  return result;
}

ValueMetadata *type metadata accessor for vImage.Gamma()
{
  return &type metadata for vImage.Gamma;
}

void *BNNS.FusedFullyConnectedParameters.init(weights:bias:)@<X0>(_OWORD *a1@<X0>, uint64_t a2@<X1>, void *a3@<X8>)
{
  outlined init with take of BNNSNDArrayDescriptor?(a2, (uint64_t)v11);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v11, (uint64_t)v12);
  long long v5 = a1[9];
  __src[8] = a1[8];
  __src[9] = v5;
  __src[10] = a1[10];
  long long v6 = a1[5];
  __src[4] = a1[4];
  __src[5] = v6;
  long long v7 = a1[7];
  __src[6] = a1[6];
  __src[7] = v7;
  long long v8 = a1[1];
  __src[0] = *a1;
  __src[1] = v8;
  long long v9 = a1[3];
  __src[2] = a1[2];
  __src[3] = v9;
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v12, (uint64_t)&__src[11]);
  return memcpy(a3, __src, 0x161uLL);
}

__n128 BNNS.FusedFullyConnectedParameters.weights.getter@<Q0>(uint64_t a1@<X8>)
{
  long long v2 = *(_OWORD *)(v1 + 144);
  *(_OWORD *)(a1 + 128) = *(_OWORD *)(v1 + 128);
  *(_OWORD *)(a1 + 144) = v2;
  *(_OWORD *)(a1 + 160) = *(_OWORD *)(v1 + 160);
  long long v3 = *(_OWORD *)(v1 + 80);
  *(_OWORD *)(a1 + 64) = *(_OWORD *)(v1 + 64);
  *(_OWORD *)(a1 + 80) = v3;
  long long v4 = *(_OWORD *)(v1 + 112);
  *(_OWORD *)(a1 + 96) = *(_OWORD *)(v1 + 96);
  *(_OWORD *)(a1 + 112) = v4;
  long long v5 = *(_OWORD *)(v1 + 16);
  *(_OWORD *)a1 = *(_OWORD *)v1;
  *(_OWORD *)(a1 + 16) = v5;
  __n128 result = *(__n128 *)(v1 + 48);
  *(_OWORD *)(a1 + 32) = *(_OWORD *)(v1 + 32);
  *(__n128 *)(a1 + 48) = result;
  return result;
}

__n128 BNNS.FusedFullyConnectedParameters.weights.setter(uint64_t a1)
{
  long long v2 = *(_OWORD *)(a1 + 144);
  *(_OWORD *)(v1 + 128) = *(_OWORD *)(a1 + 128);
  *(_OWORD *)(v1 + 144) = v2;
  *(_OWORD *)(v1 + 160) = *(_OWORD *)(a1 + 160);
  long long v3 = *(_OWORD *)(a1 + 80);
  *(_OWORD *)(v1 + 64) = *(_OWORD *)(a1 + 64);
  *(_OWORD *)(v1 + 80) = v3;
  long long v4 = *(_OWORD *)(a1 + 112);
  *(_OWORD *)(v1 + 96) = *(_OWORD *)(a1 + 96);
  *(_OWORD *)(v1 + 112) = v4;
  long long v5 = *(_OWORD *)(a1 + 16);
  *(_OWORD *)uint64_t v1 = *(_OWORD *)a1;
  *(_OWORD *)(v1 + 16) = v5;
  __n128 result = *(__n128 *)(a1 + 48);
  *(_OWORD *)(v1 + 32) = *(_OWORD *)(a1 + 32);
  *(__n128 *)(v1 + 48) = result;
  return result;
}

uint64_t (*BNNS.FusedFullyConnectedParameters.weights.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t BNNS.FusedFullyConnectedParameters.bias.getter@<X0>(uint64_t a1@<X8>)
{
  outlined init with take of BNNSNDArrayDescriptor?(v1 + 176, (uint64_t)v4);
  return outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v4, a1);
}

uint64_t BNNS.FusedFullyConnectedParameters.bias.setter(uint64_t a1)
{
  return outlined init with take of BNNSNDArrayDescriptor?(a1, v1 + 176);
}

uint64_t (*BNNS.FusedFullyConnectedParameters.bias.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

double protocol witness for FusableLayerParametersWrapper.layerParameters(input:output:) in conformance BNNS.FusedFullyConnectedParameters@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, uint64_t *a3@<X8>)
{
  long long v7 = v3[9];
  long long v58 = v3[8];
  long long v59 = v7;
  long long v60 = v3[10];
  long long v8 = v3[5];
  long long v54 = v3[4];
  long long v55 = v8;
  long long v9 = v3[7];
  long long v56 = v3[6];
  long long v57 = v9;
  long long v10 = v3[1];
  long long v50 = *v3;
  long long v51 = v10;
  long long v11 = v3[3];
  long long v52 = v3[2];
  long long v53 = v11;
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)(v3 + 11), (uint64_t)v61);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v61, (uint64_t)v62);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v62) == 1)
  {
    uint64_t v47 = 0;
    uint64_t v48 = 0;
    uint64_t v45 = 0;
    uint64_t v46 = 0;
    uint64_t v43 = 0;
    uint64_t v44 = 0;
    uint64_t v41 = 0;
    uint64_t v42 = 0;
    uint64_t v39 = 0;
    uint64_t v40 = 0;
    uint64_t v37 = 0;
    uint64_t v38 = 0;
    uint64_t v36 = 0;
    uint64_t v12 = 0;
    uint64_t v13 = 0;
    uint64_t v14 = 0;
    uint64_t v15 = 0;
    uint64_t v16 = 0;
    int v17 = 0;
    uint64_t v18 = 0;
    uint64_t v34 = 0;
    uint64_t v35 = 0;
  }
  else
  {
    uint64_t v35 = v62[0];
    uint64_t v47 = v62[2];
    uint64_t v48 = v62[1];
    uint64_t v45 = v62[4];
    uint64_t v46 = v62[3];
    uint64_t v43 = v62[6];
    uint64_t v44 = v62[5];
    uint64_t v41 = v62[8];
    uint64_t v42 = v62[7];
    uint64_t v39 = v62[10];
    uint64_t v40 = v62[9];
    uint64_t v37 = v62[12];
    uint64_t v38 = v62[11];
    uint64_t v36 = v62[13];
    uint64_t v12 = v62[14];
    uint64_t v13 = v62[15];
    uint64_t v14 = v62[16];
    uint64_t v15 = v62[17];
    uint64_t v34 = v62[18];
    uint64_t v16 = v62[19];
    int v17 = v63;
    uint64_t v18 = v64;
    int v33 = v65;
  }
  long long v19 = a1[9];
  __src[8] = a1[8];
  __src[9] = v19;
  long long v20 = a1[5];
  __src[4] = a1[4];
  __src[5] = v20;
  long long v21 = a1[7];
  __src[6] = a1[6];
  __src[7] = v21;
  long long v22 = a1[1];
  __src[0] = *a1;
  __src[1] = v22;
  long long v23 = a1[3];
  __src[2] = a1[2];
  __src[3] = v23;
  __src[18] = v57;
  __src[19] = v58;
  __src[20] = v59;
  __src[21] = v60;
  __src[14] = v53;
  __src[15] = v54;
  long long v24 = a1[10];
  __src[16] = v55;
  __src[17] = v56;
  __src[10] = v24;
  __src[11] = v50;
  __src[12] = v51;
  __src[13] = v52;
  long long v25 = a2[9];
  __src[30] = a2[8];
  __src[31] = v25;
  __src[32] = a2[10];
  long long v26 = a2[5];
  __src[26] = a2[4];
  __src[27] = v26;
  long long v27 = a2[7];
  __src[28] = a2[6];
  __src[29] = v27;
  long long v28 = a2[1];
  _OWORD __src[22] = *a2;
  _OWORD __src[23] = v28;
  long long v29 = a2[3];
  __src[24] = a2[2];
  __src[25] = v29;
  type metadata accessor for BNNSLayerParametersFullyConnected(0);
  a3[3] = v30;
  a3[4] = (uint64_t)&protocol witness table for BNNSLayerParametersFullyConnected;
  uint64_t v31 = swift_allocObject();
  *a3 = v31;
  memcpy((void *)(v31 + 16), __src, 0x210uLL);
  *(void *)(v31 + 544) = v35;
  *(void *)(v31 + 552) = v48;
  *(void *)(v31 + 560) = v47;
  *(void *)(v31 + 568) = v46;
  *(void *)(v31 + 576) = v45;
  *(void *)(v31 + 584) = v44;
  *(void *)(v31 + 592) = v43;
  *(void *)(v31 + 600) = v42;
  *(void *)(v31 + 608) = v41;
  *(void *)(v31 + 616) = v40;
  *(void *)(v31 + 624) = v39;
  *(void *)(v31 + 632) = v38;
  *(void *)(v31 + 640) = v37;
  *(void *)(v31 + 648) = v36;
  *(void *)(v31 + 656) = v12;
  *(void *)(v31 + 664) = v13;
  *(void *)(v31 + 672) = v14;
  *(void *)(v31 + 680) = v15;
  *(void *)(v31 + 688) = v34;
  *(void *)(v31 + 696) = v16;
  *(_DWORD *)(v31 + 704) = v17;
  *(void *)(v31 + 708) = v18;
  *(_DWORD *)(v31 + 716) = v33;
  *(void *)(v31 + 720) = 0x7FC0000000000000;
  *(void *)(v31 + 728) = 0x17FC00000;
  double result = 0.0;
  *(_OWORD *)(v31 + 736) = 0u;
  *(_OWORD *)(v31 + 752) = 0u;
  return result;
}

void *__swift_memcpy353_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x161uLL);
}

uint64_t getEnumTagSinglePayload for BNNS.FusedFullyConnectedParameters(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 353)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for BNNS.FusedFullyConnectedParameters(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(void *)(result + 344) = 0;
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(unsigned char *)(result + 352) = 0;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(void *)double result = (a2 - 1);
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 353) = v3;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.FusedFullyConnectedParameters()
{
  return &type metadata for BNNS.FusedFullyConnectedParameters;
}

uint64_t sub_1D20BA510()
{
  return MEMORY[0x1F4186498](v0, 768, 7);
}

uint64_t vImage.BufferReference.__deallocating_deinit()
{
  uint64_t result = *(void *)(v0 + 16);
  if (result)
  {
    MEMORY[0x1D26009C0](result, -1, -1);
    return swift_deallocClassInstance();
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t type metadata accessor for vImage.BufferReference()
{
  return self;
}

uint64_t destroy for vImage.BufferWrapper()
{
  return swift_release();
}

uint64_t initializeWithCopy for vImage.BufferWrapper(uint64_t a1, uint64_t a2)
{
  long long v3 = *(_OWORD *)(a2 + 16);
  *(_OWORD *)a1 = *(_OWORD *)a2;
  *(_OWORD *)(a1 + 16) = v3;
  *(void *)(a1 + 32) = *(void *)(a2 + 32);
  swift_retain();
  return a1;
}

void *assignWithCopy for vImage.BufferWrapper(void *a1, void *a2)
{
  *a1 = *a2;
  a1[1] = a2[1];
  a1[2] = a2[2];
  a1[3] = a2[3];
  a1[4] = a2[4];
  swift_retain();
  swift_release();
  return a1;
}

uint64_t assignWithTake for vImage.BufferWrapper(uint64_t a1, uint64_t a2)
{
  long long v3 = *(_OWORD *)(a2 + 16);
  *(_OWORD *)a1 = *(_OWORD *)a2;
  *(_OWORD *)(a1 + 16) = v3;
  *(void *)(a1 + 32) = *(void *)(a2 + 32);
  swift_release();
  return a1;
}

uint64_t getEnumTagSinglePayload for vImage.BufferWrapper(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0x7FFFFFFF && *(unsigned char *)(a1 + 40)) {
    return (*(_DWORD *)a1 + 0x7FFFFFFF);
  }
  unint64_t v3 = *(void *)(a1 + 32);
  if (v3 >= 0xFFFFFFFF) {
    LODWORD(v3) = -1;
  }
  int v4 = v3 - 1;
  if (v4 < 0) {
    int v4 = -1;
  }
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for vImage.BufferWrapper(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0x7FFFFFFE)
  {
    *(_OWORD *)(result + 8) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(void *)uint64_t result = a2 - 0x7FFFFFFF;
    if (a3 >= 0x7FFFFFFF) {
      *(unsigned char *)(result + 40) = 1;
    }
  }
  else
  {
    if (a3 >= 0x7FFFFFFF) {
      *(unsigned char *)(result + 40) = 0;
    }
    if (a2) {
      *(void *)(result + 32) = a2;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vImage.BufferWrapper()
{
  return &type metadata for vImage.BufferWrapper;
}

void *specialized vImage.BufferWrapper.init(copying:bitsPerPixel:)@<X0>(void *a1@<X0>, vImagePixelCount a2@<X1>, vImagePixelCount a3@<X2>, size_t a4@<X3>, unint64_t a5@<X4>, void *a6@<X8>)
{
  uint64_t v24 = *MEMORY[0x1E4F143B8];
  dest.data = a1;
  dest.vImagePixelCount height = a2;
  dest.vImagePixelCount width = a3;
  dest.size_t rowBytes = a4;
  MEMORY[0x1D2600EF0](&dest);
  if ((a5 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  if (HIDWORD(a5)) {
    goto LABEL_5;
  }
  dest.data = (void *)specialized vImage_Buffer.init(size:bitsPerPixel:)(v12, v13);
  dest.vImagePixelCount height = v14;
  dest.vImagePixelCount width = v15;
  dest.size_t rowBytes = v16;
  v22[0] = 0;
  vImage_Buffer.copy(destinationBuffer:pixelSize:flags:)(&dest, a5 >> 3, v22, a1, a2, a3, a4);
  data = dest.data;
  vImagePixelCount height = dest.height;
  vImagePixelCount width = dest.width;
  size_t rowBytes = dest.rowBytes;
  type metadata accessor for vImage.BufferReference();
  uint64_t result = (void *)swift_allocObject();
  result[2] = data;
  result[3] = height;
  result[4] = width;
  result[5] = rowBytes;
  *a6 = data;
  a6[1] = height;
  a6[2] = width;
  a6[3] = rowBytes;
  a6[4] = result;
  return result;
}

uint64_t specialized vImage.BufferWrapper.init(cgImage:format:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  uint64_t v15 = *MEMORY[0x1E4F143B8];
  long long v13 = 0u;
  long long v14 = 0u;
  uint64_t v4 = MEMORY[0x1D2600F20](&v13, a2, 0, a1, 0);
  if (v4)
  {
    uint64_t v5 = v4;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    long long v7 = v6;
    vImage.Error.init(rawValue:)(v5, &v12);
    char v8 = v12;
    if (v12 == 20) {
      char v8 = 11;
    }
    *long long v7 = v8;
    return swift_willThrow();
  }
  else
  {
    long long v10 = v13;
    long long v11 = v14;
    type metadata accessor for vImage.BufferReference();
    uint64_t result = swift_allocObject();
    *(_OWORD *)(result + 16) = v10;
    *(_OWORD *)(result + 32) = v11;
    *(_OWORD *)a3 = v10;
    *(_OWORD *)(a3 + 16) = v11;
    *(void *)(a3 + 32) = result;
  }
  return result;
}

uint64_t specialized vImage.BufferWrapper.init(cvPixelBuffer:cvImageFormat:cgImageFormat:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X8>)
{
  uint64_t v16 = *MEMORY[0x1E4F143B8];
  long long v14 = 0u;
  long long v15 = 0u;
  uint64_t v5 = MEMORY[0x1D2600F30](&v14, a3, a1, a2, 0, 0);
  if (v5)
  {
    uint64_t v6 = v5;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    char v8 = v7;
    vImage.Error.init(rawValue:)(v6, &v13);
    char v9 = v13;
    if (v13 == 20) {
      char v9 = 11;
    }
    *char v8 = v9;
    return swift_willThrow();
  }
  else
  {
    long long v11 = v14;
    long long v12 = v15;
    type metadata accessor for vImage.BufferReference();
    uint64_t result = swift_allocObject();
    *(_OWORD *)(result + 16) = v11;
    *(_OWORD *)(result + 32) = v12;
    *(_OWORD *)a4 = v11;
    *(_OWORD *)(a4 + 16) = v12;
    *(void *)(a4 + 32) = result;
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error()
{
  unint64_t result = lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error;
  if (!lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error;
  if (!lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error;
  if (!lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage.Error and conformance vImage.Error);
  }
  return result;
}

uint64_t vImage.PixelBuffer<>.init(interleavedBuffer:)@<X0>(uint64_t *a1@<X0>, uint64_t *a2@<X8>)
{
  uint64_t v4 = *a1;
  if (!*(void *)(*a1 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  uint64_t v5 = *(void *)(v4 + 48);
  if (v5 < 0)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v2 = *(void *)(v4 + 40);
  if (v2 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (!v5)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (v2)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    uint64_t v6 = swift_allocObject();
    *(_OWORD *)(v6 + 16) = xmmword_1D2135280;
    uint64_t v7 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v5, v2);
    uint64_t v9 = v8;
    uint64_t v11 = v10;
    uint64_t v13 = v12;
    type metadata accessor for vImage.BufferReference();
    long long v14 = (void *)swift_allocObject();
    v14[2] = v7;
    v14[3] = v9;
    void v14[4] = v11;
    v14[5] = v13;
    *(void *)(v6 + 32) = v7;
    *(void *)(v6 + 40) = v9;
    *(void *)(v6 + 48) = v11;
    *(void *)(v6 + 56) = v13;
    *(void *)(v6 + 64) = v14;
    v16[0] = v6;
    v16[1] = v4;
    vImage.PixelBuffer<>.convert(to:)((uint64_t)v16);
    uint64_t result = swift_bridgeObjectRelease();
    *a2 = v6;
    return result;
  }
LABEL_11:
  __break(1u);

  uint64_t result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

vImage_Error vImage.PixelBuffer<>.convert(to:channelOrdering:)(uint64_t a1, unsigned char *a2)
{
  uint64_t v16 = *MEMORY[0x1E4F143B8];
  unint64_t v3 = *(void **)v2;
  if (!*(void *)(*(void *)v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  vImagePixelCount v4 = v3[6];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  vImagePixelCount v5 = v3[5];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v6 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v7 = v6[6];
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v8 = v6[5];
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
LABEL_28:
    __break(1u);
  }
  if (v5 != v8) {
    goto LABEL_28;
  }
  uint64_t v9 = (void *)v3[4];
  size_t v10 = v3[7];
  src.data = v9;
  src.vImagePixelCount height = v5;
  src.vImagePixelCount width = v4;
  src.size_t rowBytes = v10;
  uint64_t v11 = (void *)v6[4];
  size_t v12 = v6[7];
  dest.data = v11;
  dest.vImagePixelCount height = v5;
  dest.vImagePixelCount width = v4;
  dest.size_t rowBytes = v12;
  if (*a2) {
    return vImageConvert_RGBAFFFFtoRGBFFF(&src, &dest, 0);
  }
  else {
    return vImageConvert_ARGBFFFFtoRGBFFF(&src, &dest, 0);
  }
}

{
  uint64_t v2;
  void *v3;
  vImagePixelCount v4;
  vImagePixelCount v5;
  void *v6;
  uint64_t v7;
  uint64_t v8;
  void *v9;
  size_t v10;
  void *v11;
  size_t v12;
  vImage_Buffer v14;
  vImage_Buffer v15;
  uint64_t v16;

  uint64_t v16 = *MEMORY[0x1E4F143B8];
  unint64_t v3 = *(void **)v2;
  if (!*(void *)(*(void *)v2 + 16))
  {
    __break(1u);
    goto LABEL_18;
  }
  vImagePixelCount v4 = v3[6];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  vImagePixelCount v5 = v3[5];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v4)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v5)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v6 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v7 = v6[6];
  if (v7 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v8 = v6[5];
  if (v8 < 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v7)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (!v8)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (v4 != v7)
  {
LABEL_27:
    __break(1u);
LABEL_28:
    __break(1u);
  }
  if (v5 != v8) {
    goto LABEL_28;
  }
  uint64_t v9 = (void *)v3[4];
  size_t v10 = v3[7];
  v15.data = v9;
  v15.vImagePixelCount height = v5;
  v15.vImagePixelCount width = v4;
  v15.size_t rowBytes = v10;
  uint64_t v11 = (void *)v6[4];
  size_t v12 = v6[7];
  v14.data = v11;
  v14.vImagePixelCount height = v5;
  v14.vImagePixelCount width = v4;
  v14.size_t rowBytes = v12;
  if (*a2) {
    return vImageConvert_RGBA8888toRGB888(&v15, &v14, 0);
  }
  else {
    return vImageConvert_ARGB8888toRGB888(&v15, &v14, 0);
  }
}

vImage_Error vImage.PixelBuffer<>.interleave(planarSourceBuffers:)(void *a1)
{
  uint64_t v31 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  vImagePixelCount v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v3)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v4)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  vImagePixelCount v5 = (void *)a1[4];
  if (!v5[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v6)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v6)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v7)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  uint64_t v8 = (void *)a1[5];
  if (!v8[2])
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  uint64_t v9 = v8[6];
  if (v9 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  uint64_t v10 = v8[5];
  if (v10 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v9)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v10)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v9)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v4 != v10)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  uint64_t v11 = a1[6];
  if (!*(void *)(v11 + 16))
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  uint64_t v12 = *(void *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  uint64_t v13 = *(void *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v12)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (!v13)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v3 != v12)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  if (v4 != v13)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  uint64_t v14 = a1[7];
  if (!*(void *)(v14 + 16))
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  uint64_t v15 = *(void *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  uint64_t v16 = *(void *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v15)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (!v16)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v3 != v15)
  {
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
  }
  if (v4 != v16) {
    goto LABEL_69;
  }
  int v17 = (void *)v5[4];
  size_t v18 = v5[7];
  srcA.data = v17;
  srcA.vImagePixelCount height = v4;
  srcA.vImagePixelCount width = v3;
  srcA.size_t rowBytes = v18;
  long long v19 = (void *)v8[4];
  size_t v20 = v8[7];
  srcR.data = v19;
  srcR.vImagePixelCount height = v4;
  srcR.vImagePixelCount width = v3;
  srcR.size_t rowBytes = v20;
  size_t v21 = *(void *)(v11 + 56);
  srcG.data = *(void **)(v11 + 32);
  srcG.vImagePixelCount height = v4;
  srcG.vImagePixelCount width = v3;
  srcG.size_t rowBytes = v21;
  size_t v22 = *(void *)(v14 + 56);
  srcB.data = *(void **)(v14 + 32);
  srcB.vImagePixelCount height = v4;
  srcB.vImagePixelCount width = v3;
  srcB.size_t rowBytes = v22;
  long long v23 = (void *)v2[4];
  size_t v24 = v2[7];
  dest.data = v23;
  dest.vImagePixelCount height = v4;
  dest.vImagePixelCount width = v3;
  dest.size_t rowBytes = v24;
  return vImageConvert_PlanarFtoARGBFFFF(&srcA, &srcR, &srcG, &srcB, &dest, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  void *v19;
  size_t v20;
  size_t v21;
  size_t v22;
  void *v23;
  size_t v24;
  vImage_Buffer argbDest;
  vImage_Buffer bSrc;
  vImage_Buffer gSrc;
  vImage_Buffer rSrc;
  vImage_Buffer aSrc;
  uint64_t v31;

  uint64_t v31 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  vImagePixelCount v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v3)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v4)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  vImagePixelCount v5 = (void *)a1[4];
  if (!v5[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v6)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v6)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v7)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  uint64_t v8 = (void *)a1[5];
  if (!v8[2])
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  uint64_t v9 = v8[6];
  if (v9 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  uint64_t v10 = v8[5];
  if (v10 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v9)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v10)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v9)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v4 != v10)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  uint64_t v11 = a1[6];
  if (!*(void *)(v11 + 16))
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  uint64_t v12 = *(void *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  uint64_t v13 = *(void *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v12)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (!v13)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v3 != v12)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  if (v4 != v13)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  uint64_t v14 = a1[7];
  if (!*(void *)(v14 + 16))
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  uint64_t v15 = *(void *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  uint64_t v16 = *(void *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v15)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (!v16)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v3 != v15)
  {
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
  }
  if (v4 != v16) {
    goto LABEL_69;
  }
  int v17 = (void *)v5[4];
  size_t v18 = v5[7];
  aSrc.data = v17;
  aSrc.vImagePixelCount height = v4;
  aSrc.vImagePixelCount width = v3;
  aSrc.size_t rowBytes = v18;
  long long v19 = (void *)v8[4];
  size_t v20 = v8[7];
  rSrc.data = v19;
  rSrc.vImagePixelCount height = v4;
  rSrc.vImagePixelCount width = v3;
  rSrc.size_t rowBytes = v20;
  size_t v21 = *(void *)(v11 + 56);
  gSrc.data = *(void **)(v11 + 32);
  gSrc.vImagePixelCount height = v4;
  gSrc.vImagePixelCount width = v3;
  gSrc.size_t rowBytes = v21;
  size_t v22 = *(void *)(v14 + 56);
  bSrc.data = *(void **)(v14 + 32);
  bSrc.vImagePixelCount height = v4;
  bSrc.vImagePixelCount width = v3;
  bSrc.size_t rowBytes = v22;
  long long v23 = (void *)v2[4];
  size_t v24 = v2[7];
  argbDest.data = v23;
  argbDest.vImagePixelCount height = v4;
  argbDest.vImagePixelCount width = v3;
  argbDest.size_t rowBytes = v24;
  return vImageConvert_Planar16UtoARGB16U(&aSrc, &rSrc, &gSrc, &bSrc, &argbDest, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  void *v19;
  size_t v20;
  size_t v21;
  size_t v22;
  void *v23;
  size_t v24;
  vImage_Buffer dest;
  vImage_Buffer srcB;
  vImage_Buffer srcG;
  vImage_Buffer srcR;
  vImage_Buffer srcA;
  uint64_t v31;

  uint64_t v31 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  vImagePixelCount v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v3)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v4)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  vImagePixelCount v5 = (void *)a1[4];
  if (!v5[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v6)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v6)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v7)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  uint64_t v8 = (void *)a1[5];
  if (!v8[2])
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  uint64_t v9 = v8[6];
  if (v9 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  uint64_t v10 = v8[5];
  if (v10 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v9)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v10)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v9)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v4 != v10)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  uint64_t v11 = a1[6];
  if (!*(void *)(v11 + 16))
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  uint64_t v12 = *(void *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  uint64_t v13 = *(void *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v12)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (!v13)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v3 != v12)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  if (v4 != v13)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  uint64_t v14 = a1[7];
  if (!*(void *)(v14 + 16))
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  uint64_t v15 = *(void *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  uint64_t v16 = *(void *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v15)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (!v16)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v3 != v15)
  {
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
  }
  if (v4 != v16) {
    goto LABEL_69;
  }
  int v17 = (void *)v5[4];
  size_t v18 = v5[7];
  srcA.data = v17;
  srcA.vImagePixelCount height = v4;
  srcA.vImagePixelCount width = v3;
  srcA.size_t rowBytes = v18;
  long long v19 = (void *)v8[4];
  size_t v20 = v8[7];
  srcR.data = v19;
  srcR.vImagePixelCount height = v4;
  srcR.vImagePixelCount width = v3;
  srcR.size_t rowBytes = v20;
  size_t v21 = *(void *)(v11 + 56);
  srcG.data = *(void **)(v11 + 32);
  srcG.vImagePixelCount height = v4;
  srcG.vImagePixelCount width = v3;
  srcG.size_t rowBytes = v21;
  size_t v22 = *(void *)(v14 + 56);
  srcB.data = *(void **)(v14 + 32);
  srcB.vImagePixelCount height = v4;
  srcB.vImagePixelCount width = v3;
  srcB.size_t rowBytes = v22;
  long long v23 = (void *)v2[4];
  size_t v24 = v2[7];
  dest.data = v23;
  dest.vImagePixelCount height = v4;
  dest.vImagePixelCount width = v3;
  dest.size_t rowBytes = v24;
  return vImageConvert_Planar8toARGB8888(&srcA, &srcR, &srcG, &srcB, &dest, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  void *v14;
  size_t v15;
  void *v16;
  size_t v17;
  size_t v18;
  void *v19;
  size_t v20;
  vImage_Buffer rgbDest;
  vImage_Buffer planarBlue;
  vImage_Buffer planarGreen;
  vImage_Buffer planarRed;
  uint64_t v26;

  long long v26 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 3)
  {
    __break(1u);
    goto LABEL_30;
  }
  uint64_t v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  vImagePixelCount v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v3)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v4)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  vImagePixelCount v5 = (void *)a1[4];
  if (!v5[2])
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v7)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v3 != v6)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (v4 != v7)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  uint64_t v8 = (void *)a1[5];
  if (!v8[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  uint64_t v9 = v8[6];
  if (v9 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  uint64_t v10 = v8[5];
  if (v10 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v10)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v9)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v10)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  uint64_t v11 = a1[6];
  if (!*(void *)(v11 + 16))
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  uint64_t v12 = *(void *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  uint64_t v13 = *(void *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v13)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v12)
  {
LABEL_54:
    __break(1u);
LABEL_55:
    __break(1u);
  }
  if (v4 != v13) {
    goto LABEL_55;
  }
  uint64_t v14 = (void *)v5[4];
  uint64_t v15 = v5[7];
  planarRed.data = v14;
  planarRed.vImagePixelCount height = v4;
  planarRed.vImagePixelCount width = v3;
  planarRed.size_t rowBytes = v15;
  uint64_t v16 = (void *)v8[4];
  int v17 = v8[7];
  planarGreen.data = v16;
  planarGreen.vImagePixelCount height = v4;
  planarGreen.vImagePixelCount width = v3;
  planarGreen.size_t rowBytes = v17;
  size_t v18 = *(void *)(v11 + 56);
  planarBlue.data = *(void **)(v11 + 32);
  planarBlue.vImagePixelCount height = v4;
  planarBlue.vImagePixelCount width = v3;
  planarBlue.size_t rowBytes = v18;
  long long v19 = (void *)v2[4];
  size_t v20 = v2[7];
  rgbDest.data = v19;
  rgbDest.vImagePixelCount height = v4;
  rgbDest.vImagePixelCount width = v3;
  rgbDest.size_t rowBytes = v20;
  return vImageConvert_Planar8toRGB888(&planarRed, &planarGreen, &planarBlue, &rgbDest, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  void *v14;
  size_t v15;
  void *v16;
  size_t v17;
  size_t v18;
  void *v19;
  size_t v20;
  vImage_Buffer rgbDest;
  vImage_Buffer planarBlue;
  vImage_Buffer planarGreen;
  vImage_Buffer planarRed;
  uint64_t v26;

  long long v26 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 3)
  {
    __break(1u);
    goto LABEL_30;
  }
  uint64_t v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  vImagePixelCount v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v3)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v4)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  vImagePixelCount v5 = (void *)a1[4];
  if (!v5[2])
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v7)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v3 != v6)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (v4 != v7)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  uint64_t v8 = (void *)a1[5];
  if (!v8[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  uint64_t v9 = v8[6];
  if (v9 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  uint64_t v10 = v8[5];
  if (v10 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v10)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v9)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v10)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  uint64_t v11 = a1[6];
  if (!*(void *)(v11 + 16))
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  uint64_t v12 = *(void *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  uint64_t v13 = *(void *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v13)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v12)
  {
LABEL_54:
    __break(1u);
LABEL_55:
    __break(1u);
  }
  if (v4 != v13) {
    goto LABEL_55;
  }
  uint64_t v14 = (void *)v5[4];
  uint64_t v15 = v5[7];
  planarRed.data = v14;
  planarRed.vImagePixelCount height = v4;
  planarRed.vImagePixelCount width = v3;
  planarRed.size_t rowBytes = v15;
  uint64_t v16 = (void *)v8[4];
  int v17 = v8[7];
  planarGreen.data = v16;
  planarGreen.vImagePixelCount height = v4;
  planarGreen.vImagePixelCount width = v3;
  planarGreen.size_t rowBytes = v17;
  size_t v18 = *(void *)(v11 + 56);
  planarBlue.data = *(void **)(v11 + 32);
  planarBlue.vImagePixelCount height = v4;
  planarBlue.vImagePixelCount width = v3;
  planarBlue.size_t rowBytes = v18;
  long long v19 = (void *)v2[4];
  size_t v20 = v2[7];
  rgbDest.data = v19;
  rgbDest.vImagePixelCount height = v4;
  rgbDest.vImagePixelCount width = v3;
  rgbDest.size_t rowBytes = v20;
  return vImageConvert_PlanarFtoRGBFFF(&planarRed, &planarGreen, &planarBlue, &rgbDest, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  void *v19;
  size_t v20;
  size_t v21;
  size_t v22;
  void *v23;
  size_t v24;
  vImage_Buffer argbDest;
  vImage_Buffer bSrc;
  vImage_Buffer gSrc;
  vImage_Buffer rSrc;
  vImage_Buffer aSrc;
  uint64_t v31;

  uint64_t v31 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  vImagePixelCount v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v3)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v4)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  vImagePixelCount v5 = (void *)a1[4];
  if (!v5[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v6)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v6)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v7)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  uint64_t v8 = (void *)a1[5];
  if (!v8[2])
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  uint64_t v9 = v8[6];
  if (v9 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  uint64_t v10 = v8[5];
  if (v10 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v9)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v10)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v9)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v4 != v10)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  uint64_t v11 = a1[6];
  if (!*(void *)(v11 + 16))
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  uint64_t v12 = *(void *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  uint64_t v13 = *(void *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v12)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (!v13)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v3 != v12)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  if (v4 != v13)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  uint64_t v14 = a1[7];
  if (!*(void *)(v14 + 16))
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  uint64_t v15 = *(void *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  uint64_t v16 = *(void *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v15)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (!v16)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v3 != v15)
  {
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
  }
  if (v4 != v16) {
    goto LABEL_69;
  }
  int v17 = (void *)v5[4];
  size_t v18 = v5[7];
  aSrc.data = v17;
  aSrc.vImagePixelCount height = v4;
  aSrc.vImagePixelCount width = v3;
  aSrc.size_t rowBytes = v18;
  long long v19 = (void *)v8[4];
  size_t v20 = v8[7];
  rSrc.data = v19;
  rSrc.vImagePixelCount height = v4;
  rSrc.vImagePixelCount width = v3;
  rSrc.size_t rowBytes = v20;
  size_t v21 = *(void *)(v11 + 56);
  gSrc.data = *(void **)(v11 + 32);
  gSrc.vImagePixelCount height = v4;
  gSrc.vImagePixelCount width = v3;
  gSrc.size_t rowBytes = v21;
  size_t v22 = *(void *)(v14 + 56);
  bSrc.data = *(void **)(v14 + 32);
  bSrc.vImagePixelCount height = v4;
  bSrc.vImagePixelCount width = v3;
  bSrc.size_t rowBytes = v22;
  long long v23 = (void *)v2[4];
  size_t v24 = v2[7];
  argbDest.data = v23;
  argbDest.vImagePixelCount height = v4;
  argbDest.vImagePixelCount width = v3;
  argbDest.size_t rowBytes = v24;
  return vImageConvert_Planar16UtoARGB16U(&aSrc, &rSrc, &gSrc, &bSrc, &argbDest, 0);
}

vImage_Error vImage.PixelBuffer<>.deinterleave(planarDestinationBuffers:)(void *a1)
{
  uint64_t v31 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  vImagePixelCount v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v3)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v4)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  vImagePixelCount v5 = (void *)a1[4];
  if (!v5[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v6)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v6)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v7)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  uint64_t v8 = (void *)a1[5];
  if (!v8[2])
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  uint64_t v9 = v8[6];
  if (v9 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  uint64_t v10 = v8[5];
  if (v10 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v9)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v10)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v9)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v4 != v10)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  uint64_t v11 = a1[6];
  if (!*(void *)(v11 + 16))
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  uint64_t v12 = *(void *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  uint64_t v13 = *(void *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v12)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (!v13)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v3 != v12)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  if (v4 != v13)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  uint64_t v14 = a1[7];
  if (!*(void *)(v14 + 16))
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  uint64_t v15 = *(void *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  uint64_t v16 = *(void *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v15)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (!v16)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v3 != v15)
  {
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
  }
  if (v4 != v16) {
    goto LABEL_69;
  }
  int v17 = (void *)v5[4];
  size_t v18 = v5[7];
  destA.data = v17;
  destA.vImagePixelCount height = v4;
  destA.vImagePixelCount width = v3;
  destA.size_t rowBytes = v18;
  long long v19 = (void *)v8[4];
  size_t v20 = v8[7];
  destR.data = v19;
  destR.vImagePixelCount height = v4;
  destR.vImagePixelCount width = v3;
  destR.size_t rowBytes = v20;
  size_t v21 = *(void *)(v11 + 56);
  destG.data = *(void **)(v11 + 32);
  destG.vImagePixelCount height = v4;
  destG.vImagePixelCount width = v3;
  destG.size_t rowBytes = v21;
  size_t v22 = *(void *)(v14 + 56);
  destB.data = *(void **)(v14 + 32);
  destB.vImagePixelCount height = v4;
  destB.vImagePixelCount width = v3;
  destB.size_t rowBytes = v22;
  long long v23 = (void *)v2[4];
  size_t v24 = v2[7];
  srcARGB.data = v23;
  srcARGB.vImagePixelCount height = v4;
  srcARGB.vImagePixelCount width = v3;
  srcARGB.size_t rowBytes = v24;
  return vImageConvert_ARGBFFFFtoPlanarF(&srcARGB, &destA, &destR, &destG, &destB, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  void *v19;
  size_t v20;
  size_t v21;
  size_t v22;
  void *v23;
  size_t v24;
  vImage_Buffer argbSrc;
  vImage_Buffer bDest;
  vImage_Buffer gDest;
  vImage_Buffer rDest;
  vImage_Buffer aDest;
  uint64_t v31;

  uint64_t v31 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  vImagePixelCount v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v3)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v4)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  vImagePixelCount v5 = (void *)a1[4];
  if (!v5[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v6)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v6)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v7)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  uint64_t v8 = (void *)a1[5];
  if (!v8[2])
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  uint64_t v9 = v8[6];
  if (v9 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  uint64_t v10 = v8[5];
  if (v10 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v9)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v10)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v9)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v4 != v10)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  uint64_t v11 = a1[6];
  if (!*(void *)(v11 + 16))
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  uint64_t v12 = *(void *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  uint64_t v13 = *(void *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v12)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (!v13)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v3 != v12)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  if (v4 != v13)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  uint64_t v14 = a1[7];
  if (!*(void *)(v14 + 16))
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  uint64_t v15 = *(void *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  uint64_t v16 = *(void *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v15)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (!v16)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v3 != v15)
  {
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
  }
  if (v4 != v16) {
    goto LABEL_69;
  }
  int v17 = (void *)v5[4];
  size_t v18 = v5[7];
  aDest.data = v17;
  aDest.vImagePixelCount height = v4;
  aDest.vImagePixelCount width = v3;
  aDest.size_t rowBytes = v18;
  long long v19 = (void *)v8[4];
  size_t v20 = v8[7];
  rDest.data = v19;
  rDest.vImagePixelCount height = v4;
  rDest.vImagePixelCount width = v3;
  rDest.size_t rowBytes = v20;
  size_t v21 = *(void *)(v11 + 56);
  gDest.data = *(void **)(v11 + 32);
  gDest.vImagePixelCount height = v4;
  gDest.vImagePixelCount width = v3;
  gDest.size_t rowBytes = v21;
  size_t v22 = *(void *)(v14 + 56);
  bDest.data = *(void **)(v14 + 32);
  bDest.vImagePixelCount height = v4;
  bDest.vImagePixelCount width = v3;
  bDest.size_t rowBytes = v22;
  long long v23 = (void *)v2[4];
  size_t v24 = v2[7];
  argbSrc.data = v23;
  argbSrc.vImagePixelCount height = v4;
  argbSrc.vImagePixelCount width = v3;
  argbSrc.size_t rowBytes = v24;
  return vImageConvert_ARGB16UtoPlanar16U(&argbSrc, &aDest, &rDest, &gDest, &bDest, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  void *v19;
  size_t v20;
  size_t v21;
  size_t v22;
  void *v23;
  size_t v24;
  vImage_Buffer srcARGB;
  vImage_Buffer destB;
  vImage_Buffer destG;
  vImage_Buffer destR;
  vImage_Buffer destA;
  uint64_t v31;

  uint64_t v31 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  vImagePixelCount v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v3)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v4)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  vImagePixelCount v5 = (void *)a1[4];
  if (!v5[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v6)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v6)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v7)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  uint64_t v8 = (void *)a1[5];
  if (!v8[2])
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  uint64_t v9 = v8[6];
  if (v9 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  uint64_t v10 = v8[5];
  if (v10 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v9)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v10)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v9)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v4 != v10)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  uint64_t v11 = a1[6];
  if (!*(void *)(v11 + 16))
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  uint64_t v12 = *(void *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  uint64_t v13 = *(void *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v12)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (!v13)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v3 != v12)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  if (v4 != v13)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  uint64_t v14 = a1[7];
  if (!*(void *)(v14 + 16))
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  uint64_t v15 = *(void *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  uint64_t v16 = *(void *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v15)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (!v16)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v3 != v15)
  {
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
  }
  if (v4 != v16) {
    goto LABEL_69;
  }
  int v17 = (void *)v5[4];
  size_t v18 = v5[7];
  destA.data = v17;
  destA.vImagePixelCount height = v4;
  destA.vImagePixelCount width = v3;
  destA.size_t rowBytes = v18;
  long long v19 = (void *)v8[4];
  size_t v20 = v8[7];
  destR.data = v19;
  destR.vImagePixelCount height = v4;
  destR.vImagePixelCount width = v3;
  destR.size_t rowBytes = v20;
  size_t v21 = *(void *)(v11 + 56);
  destG.data = *(void **)(v11 + 32);
  destG.vImagePixelCount height = v4;
  destG.vImagePixelCount width = v3;
  destG.size_t rowBytes = v21;
  size_t v22 = *(void *)(v14 + 56);
  destB.data = *(void **)(v14 + 32);
  destB.vImagePixelCount height = v4;
  destB.vImagePixelCount width = v3;
  destB.size_t rowBytes = v22;
  long long v23 = (void *)v2[4];
  size_t v24 = v2[7];
  srcARGB.data = v23;
  srcARGB.vImagePixelCount height = v4;
  srcARGB.vImagePixelCount width = v3;
  srcARGB.size_t rowBytes = v24;
  return vImageConvert_ARGB8888toPlanar8(&srcARGB, &destA, &destR, &destG, &destB, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  void *v14;
  size_t v15;
  void *v16;
  size_t v17;
  size_t v18;
  void *v19;
  size_t v20;
  vImage_Buffer rgbSrc;
  vImage_Buffer blueDest;
  vImage_Buffer greenDest;
  vImage_Buffer redDest;
  uint64_t v26;

  long long v26 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 3)
  {
    __break(1u);
    goto LABEL_30;
  }
  uint64_t v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  vImagePixelCount v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v3)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v4)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  vImagePixelCount v5 = (void *)a1[4];
  if (!v5[2])
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v7)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v3 != v6)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (v4 != v7)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  uint64_t v8 = (void *)a1[5];
  if (!v8[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  uint64_t v9 = v8[6];
  if (v9 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  uint64_t v10 = v8[5];
  if (v10 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v10)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v9)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v10)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  uint64_t v11 = a1[6];
  if (!*(void *)(v11 + 16))
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  uint64_t v12 = *(void *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  uint64_t v13 = *(void *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v13)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v12)
  {
LABEL_54:
    __break(1u);
LABEL_55:
    __break(1u);
  }
  if (v4 != v13) {
    goto LABEL_55;
  }
  uint64_t v14 = (void *)v5[4];
  uint64_t v15 = v5[7];
  redDest.data = v14;
  redDest.vImagePixelCount height = v4;
  redDest.vImagePixelCount width = v3;
  redDest.size_t rowBytes = v15;
  uint64_t v16 = (void *)v8[4];
  int v17 = v8[7];
  greenDest.data = v16;
  greenDest.vImagePixelCount height = v4;
  greenDest.vImagePixelCount width = v3;
  greenDest.size_t rowBytes = v17;
  size_t v18 = *(void *)(v11 + 56);
  blueDest.data = *(void **)(v11 + 32);
  blueDest.vImagePixelCount height = v4;
  blueDest.vImagePixelCount width = v3;
  blueDest.size_t rowBytes = v18;
  long long v19 = (void *)v2[4];
  size_t v20 = v2[7];
  rgbSrc.data = v19;
  rgbSrc.vImagePixelCount height = v4;
  rgbSrc.vImagePixelCount width = v3;
  rgbSrc.size_t rowBytes = v20;
  return vImageConvert_RGB888toPlanar8(&rgbSrc, &redDest, &greenDest, &blueDest, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  void *v14;
  size_t v15;
  void *v16;
  size_t v17;
  size_t v18;
  void *v19;
  size_t v20;
  vImage_Buffer rgbSrc;
  vImage_Buffer blueDest;
  vImage_Buffer greenDest;
  vImage_Buffer redDest;
  uint64_t v26;

  long long v26 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 3)
  {
    __break(1u);
    goto LABEL_30;
  }
  uint64_t v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  vImagePixelCount v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v3)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v4)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  vImagePixelCount v5 = (void *)a1[4];
  if (!v5[2])
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v6)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (!v7)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v3 != v6)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (v4 != v7)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  uint64_t v8 = (void *)a1[5];
  if (!v8[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  uint64_t v9 = v8[6];
  if (v9 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  uint64_t v10 = v8[5];
  if (v10 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v10)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v9)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v10)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  uint64_t v11 = a1[6];
  if (!*(void *)(v11 + 16))
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  uint64_t v12 = *(void *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  uint64_t v13 = *(void *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v13)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v12)
  {
LABEL_54:
    __break(1u);
LABEL_55:
    __break(1u);
  }
  if (v4 != v13) {
    goto LABEL_55;
  }
  uint64_t v14 = (void *)v5[4];
  uint64_t v15 = v5[7];
  redDest.data = v14;
  redDest.vImagePixelCount height = v4;
  redDest.vImagePixelCount width = v3;
  redDest.size_t rowBytes = v15;
  uint64_t v16 = (void *)v8[4];
  int v17 = v8[7];
  greenDest.data = v16;
  greenDest.vImagePixelCount height = v4;
  greenDest.vImagePixelCount width = v3;
  greenDest.size_t rowBytes = v17;
  size_t v18 = *(void *)(v11 + 56);
  blueDest.data = *(void **)(v11 + 32);
  blueDest.vImagePixelCount height = v4;
  blueDest.vImagePixelCount width = v3;
  blueDest.size_t rowBytes = v18;
  long long v19 = (void *)v2[4];
  size_t v20 = v2[7];
  rgbSrc.data = v19;
  rgbSrc.vImagePixelCount height = v4;
  rgbSrc.vImagePixelCount width = v3;
  rgbSrc.size_t rowBytes = v20;
  return vImageConvert_RGBFFFtoPlanarF(&rgbSrc, &redDest, &greenDest, &blueDest, 0);
}

{
  uint64_t v1;
  void *v2;
  vImagePixelCount v3;
  vImagePixelCount v4;
  void *v5;
  uint64_t v6;
  uint64_t v7;
  void *v8;
  uint64_t v9;
  uint64_t v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void *v17;
  size_t v18;
  void *v19;
  size_t v20;
  size_t v21;
  size_t v22;
  void *v23;
  size_t v24;
  vImage_Buffer argbSrc;
  vImage_Buffer bDest;
  vImage_Buffer gDest;
  vImage_Buffer rDest;
  vImage_Buffer aDest;
  uint64_t v31;

  uint64_t v31 = *MEMORY[0x1E4F143B8];
  if (a1[2] != 4)
  {
    __break(1u);
    goto LABEL_37;
  }
  uint64_t v2 = *(void **)v1;
  if (!*(void *)(*(void *)v1 + 16))
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  vImagePixelCount v3 = v2[6];
  if ((v3 & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  vImagePixelCount v4 = v2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (!v3)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  if (!v4)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  vImagePixelCount v5 = (void *)a1[4];
  if (!v5[2])
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  uint64_t v6 = v5[6];
  if (v6 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  uint64_t v7 = v5[5];
  if (v7 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v6)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v7)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  if (v3 != v6)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  if (v4 != v7)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  uint64_t v8 = (void *)a1[5];
  if (!v8[2])
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  uint64_t v9 = v8[6];
  if (v9 < 0)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  uint64_t v10 = v8[5];
  if (v10 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (!v9)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v10)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (v3 != v9)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  if (v4 != v10)
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  uint64_t v11 = a1[6];
  if (!*(void *)(v11 + 16))
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  uint64_t v12 = *(void *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  uint64_t v13 = *(void *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v12)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (!v13)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v3 != v12)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  if (v4 != v13)
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  uint64_t v14 = a1[7];
  if (!*(void *)(v14 + 16))
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  uint64_t v15 = *(void *)(v14 + 48);
  if (v15 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  uint64_t v16 = *(void *)(v14 + 40);
  if (v16 < 0)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v15)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (!v16)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v3 != v15)
  {
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
  }
  if (v4 != v16) {
    goto LABEL_69;
  }
  int v17 = (void *)v5[4];
  size_t v18 = v5[7];
  aDest.data = v17;
  aDest.vImagePixelCount height = v4;
  aDest.vImagePixelCount width = v3;
  aDest.size_t rowBytes = v18;
  long long v19 = (void *)v8[4];
  size_t v20 = v8[7];
  rDest.data = v19;
  rDest.vImagePixelCount height = v4;
  rDest.vImagePixelCount width = v3;
  rDest.size_t rowBytes = v20;
  size_t v21 = *(void *)(v11 + 56);
  gDest.data = *(void **)(v11 + 32);
  gDest.vImagePixelCount height = v4;
  gDest.vImagePixelCount width = v3;
  gDest.size_t rowBytes = v21;
  size_t v22 = *(void *)(v14 + 56);
  bDest.data = *(void **)(v14 + 32);
  bDest.vImagePixelCount height = v4;
  bDest.vImagePixelCount width = v3;
  bDest.size_t rowBytes = v22;
  long long v23 = (void *)v2[4];
  size_t v24 = v2[7];
  argbSrc.data = v23;
  argbSrc.vImagePixelCount height = v4;
  argbSrc.vImagePixelCount width = v3;
  argbSrc.size_t rowBytes = v24;
  return vImageConvert_ARGB16UtoPlanar16U(&argbSrc, &aDest, &rDest, &gDest, &bDest, 0);
}

uint64_t vImage.PixelBuffer<>.deinterleave(destination:)(uint64_t *a1)
{
  uint64_t v1 = (void *)specialized vImage.PixelBuffer<>.pixelBuffers.getter(*a1);
  vImage.PixelBuffer<>.deinterleave(planarDestinationBuffers:)(v1);

  return swift_bridgeObjectRelease();
}

{
  void *v1;
  uint64_t vars8;

  uint64_t v1 = (void *)specialized vImage.PixelBuffer<>.pixelBuffers.getter(*a1);
  vImage.PixelBuffer<>.deinterleave(planarDestinationBuffers:)(v1);

  return swift_bridgeObjectRelease();
}

{
  void *v1;
  uint64_t vars8;

  uint64_t v1 = (void *)specialized vImage.PixelBuffer<>.pixelBuffers.getter(*a1);
  vImage.PixelBuffer<>.deinterleave(planarDestinationBuffers:)(v1);

  return swift_bridgeObjectRelease();
}

{
  void *v1;
  uint64_t vars8;

  uint64_t v1 = (void *)specialized vImage.PixelBuffer<>.pixelBuffers.getter(*a1);
  vImage.PixelBuffer<>.deinterleave(planarDestinationBuffers:)(v1);

  return swift_bridgeObjectRelease();
}

vImage_Error vImage.PixelBuffer<>.overwriteChannels(withScalar:)(Pixel_8 a1)
{
  uint64_t v6 = *MEMORY[0x1E4F143B8];
  uint64_t v2 = *v1;
  if (!*(void *)(*v1 + 16)) {
    __break(1u);
  }
  long long v3 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&v5.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&v5.vImagePixelCount width = v3;
  return vImageOverwriteChannelsWithScalar_Planar8(a1, &v5, 0);
}

vImage_Error vImage.PixelBuffer<>.overwriteChannels(_:withScalar:destination:)(uint64_t a1, Pixel_8 a2, uint64_t *a3)
{
  uint64_t v43 = *MEMORY[0x1E4F143B8];
  uint64_t v4 = *v3;
  if (!*(void *)(*v3 + 16)) {
    goto LABEL_42;
  }
  uint64_t v5 = *(void *)(v4 + 48);
  if (v5 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  uint64_t v6 = *(void *)(v4 + 40);
  if (v6 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v5)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v6)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  uint64_t v7 = *a3;
  if (!*(void *)(*a3 + 16))
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  uint64_t v8 = *(void *)(v7 + 48);
  if (v8 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  uint64_t v9 = *(void *)(v7 + 40);
  if (v9 < 0)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v8)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (!v9)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v5 != v8)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (v6 != v9)
  {
LABEL_53:
    __break(1u);
LABEL_54:
    __break(1u);
  }
  Pixel_8 v10 = a2;
  int64_t v12 = *(void *)(a1 + 16);
  if (v12)
  {
    src.data = (void *)MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v12, 0);
    data = (int8x16_t *)src.data;
    uint64_t v14 = (unsigned __int8 *)(a1 + 32);
    do
    {
      int v15 = *v14++;
      char v16 = 3 - v15;
      if (((3 - v15) & 0xFFFFFF00) != 0) {
        goto LABEL_40;
      }
      src.data = data;
      unint64_t v18 = data[1].u64[0];
      unint64_t v17 = data[1].u64[1];
      unint64_t v19 = v18 + 1;
      if (v18 >= v17 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v17 > 1), v18 + 1, 1);
        data = (int8x16_t *)src.data;
      }
      data[1].i64[0] = v19;
      data[2].i8[v18] = 1 << (v16 & 7);
      --v12;
    }
    while (v12);
    Pixel_8 v10 = a2;
LABEL_21:
    if (v19 < 8)
    {
      unint64_t v20 = 0;
      uint8_t v21 = 0;
LABEL_32:
      unint64_t v34 = v19 - v20;
      uint64_t v35 = &data[2].i8[v20];
      do
      {
        char v36 = *v35++;
        v21 |= v36;
        --v34;
      }
      while (v34);
LABEL_34:
      swift_bridgeObjectRelease();
      if (v21 > 0xFu)
      {
LABEL_41:
        __break(1u);
LABEL_42:
        __break(1u);
        goto LABEL_43;
      }
      if (*(void *)(v4 + 16)) {
        goto LABEL_36;
      }
LABEL_39:
      __break(1u);
LABEL_40:
      __break(1u);
      goto LABEL_41;
    }
    if (v19 >= 0x20)
    {
      unint64_t v20 = v19 & 0xFFFFFFFFFFFFFFE0;
      size_t v22 = data + 3;
      int8x16_t v23 = 0uLL;
      unint64_t v24 = v19 & 0xFFFFFFFFFFFFFFE0;
      int8x16_t v25 = 0uLL;
      do
      {
        int8x16_t v23 = vorrq_s8(v22[-1], v23);
        int8x16_t v25 = vorrq_s8(*v22, v25);
        v22 += 2;
        v24 -= 32;
      }
      while (v24);
      int8x16_t v26 = vorrq_s8(v25, v23);
      *(int8x8_t *)v26.i8 = vorr_s8(*(int8x8_t *)v26.i8, (int8x8_t)*(_OWORD *)&vextq_s8(v26, v26, 8uLL));
      unint64_t v27 = v26.i64[0] | HIDWORD(v26.i64[0]) | ((unint64_t)(v26.i64[0] | HIDWORD(v26.i64[0])) >> 16);
      uint8_t v21 = v27 | BYTE1(v27);
      if (v19 == v20) {
        goto LABEL_34;
      }
      if ((v19 & 0x18) == 0) {
        goto LABEL_32;
      }
    }
    else
    {
      uint8_t v21 = 0;
      unint64_t v20 = 0;
    }
    unint64_t v28 = v20;
    unint64_t v20 = v19 & 0xFFFFFFFFFFFFFFF8;
    int8x8_t v29 = (int8x8_t)v21;
    uint64_t v30 = (int8x8_t *)&data[2].i8[v28];
    unint64_t v31 = v28 - (v19 & 0xFFFFFFFFFFFFFFF8);
    do
    {
      int8x8_t v32 = *v30++;
      int8x8_t v29 = vorr_s8(v32, v29);
      v31 += 8;
    }
    while (v31);
    uint64_t v33 = *(void *)&v29 | HIDWORD(*(void *)&v29) | ((*(void *)&v29 | HIDWORD(*(void *)&v29)) >> 16);
    uint8_t v21 = v33 | BYTE1(v33);
    if (v19 == v20) {
      goto LABEL_34;
    }
    goto LABEL_32;
  }
  data = (int8x16_t *)MEMORY[0x1E4FBC860];
  unint64_t v19 = *(void *)(MEMORY[0x1E4FBC860] + 16);
  if (v19) {
    goto LABEL_21;
  }
  swift_bridgeObjectRelease();
  uint8_t v21 = 0;
  if (!*(void *)(v4 + 16)) {
    goto LABEL_39;
  }
LABEL_36:
  long long v37 = *(_OWORD *)(v4 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v4 + 32);
  *(_OWORD *)&src.vImagePixelCount width = v37;
  if (!*(void *)(v7 + 16)) {
    goto LABEL_54;
  }
  long long v38 = *(_OWORD *)(v7 + 48);
  *(_OWORD *)&dest.data = *(_OWORD *)(v7 + 32);
  *(_OWORD *)&dest.vImagePixelCount width = v38;
  return vImageOverwriteChannelsWithScalar_ARGB8888(v10, &src, &dest, v21, 0);
}

vImage_Error vImage.PixelBuffer<>.overwriteChannels(_:withPixel:destination:)(uint64_t a1, uint8_t a2, uint8_t a3, uint8_t a4, uint8_t a5, uint64_t *a6)
{
  uint64_t v53 = *MEMORY[0x1E4F143B8];
  uint64_t v7 = *v6;
  if (!*(void *)(*v6 + 16)) {
    goto LABEL_41;
  }
  uint64_t v8 = *(void *)(v7 + 48);
  if (v8 < 0)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  uint64_t v9 = *(void *)(v7 + 40);
  if (v9 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!v8)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  uint64_t v10 = *a6;
  if (!*(void *)(*a6 + 16))
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  uint64_t v11 = *(void *)(v10 + 48);
  if (v11 < 0)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  uint64_t v12 = *(void *)(v10 + 40);
  if (v12 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (!v11)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v12)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (v8 != v11)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v9 != v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  uint8_t v13 = a5;
  uint8_t v14 = a4;
  uint8_t v15 = a3;
  uint8_t v16 = a2;
  int64_t v18 = *(void *)(a1 + 16);
  if (v18)
  {
    src.data = (void *)MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v18, 0);
    data = (int8x16_t *)src.data;
    unint64_t v20 = (unsigned __int8 *)(a1 + 32);
    while (1)
    {
      int v21 = *v20++;
      char v22 = 3 - v21;
      if (((3 - v21) & 0xFFFFFF00) != 0) {
        break;
      }
      src.data = data;
      unint64_t v24 = data[1].u64[0];
      unint64_t v23 = data[1].u64[1];
      unint64_t v25 = v24 + 1;
      if (v24 >= v23 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v23 > 1), v24 + 1, 1);
        data = (int8x16_t *)src.data;
      }
      data[1].i64[0] = v25;
      data[2].i8[v24] = 1 << (v22 & 7);
      if (!--v18)
      {
        uint8_t v14 = a4;
        uint8_t v13 = a5;
        uint8_t v16 = a2;
        uint8_t v15 = a3;
        goto LABEL_21;
      }
    }
    __break(1u);
    goto LABEL_40;
  }
  data = (int8x16_t *)MEMORY[0x1E4FBC860];
  unint64_t v25 = *(void *)(MEMORY[0x1E4FBC860] + 16);
  if (!v25)
  {
    swift_bridgeObjectRelease();
    uint8_t v27 = 0;
    goto LABEL_35;
  }
LABEL_21:
  if (v25 < 8)
  {
    unint64_t v26 = 0;
    uint8_t v27 = 0;
    goto LABEL_32;
  }
  if (v25 >= 0x20)
  {
    unint64_t v26 = v25 & 0xFFFFFFFFFFFFFFE0;
    unint64_t v28 = data + 3;
    int8x16_t v29 = 0uLL;
    unint64_t v30 = v25 & 0xFFFFFFFFFFFFFFE0;
    int8x16_t v31 = 0uLL;
    do
    {
      int8x16_t v29 = vorrq_s8(v28[-1], v29);
      int8x16_t v31 = vorrq_s8(*v28, v31);
      v28 += 2;
      v30 -= 32;
    }
    while (v30);
    int8x16_t v32 = vorrq_s8(v31, v29);
    *(int8x8_t *)v32.i8 = vorr_s8(*(int8x8_t *)v32.i8, (int8x8_t)*(_OWORD *)&vextq_s8(v32, v32, 8uLL));
    unint64_t v33 = v32.i64[0] | HIDWORD(v32.i64[0]) | ((unint64_t)(v32.i64[0] | HIDWORD(v32.i64[0])) >> 16);
    uint8_t v27 = v33 | BYTE1(v33);
    if (v25 == v26) {
      goto LABEL_34;
    }
    if ((v25 & 0x18) == 0)
    {
LABEL_32:
      unint64_t v40 = v25 - v26;
      uint64_t v41 = &data[2].i8[v26];
      do
      {
        char v42 = *v41++;
        v27 |= v42;
        --v40;
      }
      while (v40);
      goto LABEL_34;
    }
  }
  else
  {
    uint8_t v27 = 0;
    unint64_t v26 = 0;
  }
  unint64_t v34 = v26;
  unint64_t v26 = v25 & 0xFFFFFFFFFFFFFFF8;
  int8x8_t v35 = (int8x8_t)v27;
  char v36 = (int8x8_t *)&data[2].i8[v34];
  unint64_t v37 = v34 - (v25 & 0xFFFFFFFFFFFFFFF8);
  do
  {
    int8x8_t v38 = *v36++;
    int8x8_t v35 = vorr_s8(v38, v35);
    v37 += 8;
  }
  while (v37);
  uint64_t v39 = *(void *)&v35 | HIDWORD(*(void *)&v35) | ((*(void *)&v35 | HIDWORD(*(void *)&v35)) >> 16);
  uint8_t v27 = v39 | BYTE1(v39);
  if (v25 != v26) {
    goto LABEL_32;
  }
LABEL_34:
  swift_bridgeObjectRelease();
  if (v27 > 0xFu)
  {
LABEL_40:
    __break(1u);
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
LABEL_35:
  v52[0] = v16;
  v52[1] = v15;
  v52[2] = v14;
  v52[3] = v13;
  if (!*(void *)(v7 + 16))
  {
LABEL_53:
    __break(1u);
LABEL_54:
    __break(1u);
  }
  long long v43 = *(_OWORD *)(v7 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v7 + 32);
  *(_OWORD *)&src.vImagePixelCount width = v43;
  if (!*(void *)(v10 + 16)) {
    goto LABEL_54;
  }
  long long v44 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&dest.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&dest.vImagePixelCount width = v44;
  return vImageOverwriteChannelsWithPixel_ARGB8888(v52, &src, &dest, v27, 0);
}

uint64_t vImage.PixelBuffer<>.overwriteChannels(_:withInterleavedBuffer:destination:)(uint64_t a1, uint64_t *a2, uint64_t *a3)
{
  long long v3 = (uint64_t (*)(_OWORD *, _OWORD *, _OWORD *, void, void))MEMORY[0x1E4F17148];

  return vImage.PixelBuffer<>.overwriteChannels(_:withInterleavedBuffer:destination:)(a1, a2, a3, v3);
}

{
  uint64_t (*v3)(_OWORD *, _OWORD *, _OWORD *, void, void);
  uint64_t vars8;

  long long v3 = (uint64_t (*)(_OWORD *, _OWORD *, _OWORD *, void, void))MEMORY[0x1E4F17150];

  return vImage.PixelBuffer<>.overwriteChannels(_:withInterleavedBuffer:destination:)(a1, a2, a3, v3);
}

uint64_t vImage.PixelBuffer<>.overwriteChannels(_:withPlanarBuffer:destination:)(uint64_t a1, uint64_t *a2, uint64_t *a3)
{
  long long v3 = (uint64_t (*)(_OWORD *, _OWORD *, _OWORD *, void, void))MEMORY[0x1E4F17038];

  return vImage.PixelBuffer<>.overwriteChannels(_:withInterleavedBuffer:destination:)(a1, a2, a3, v3);
}

{
  uint64_t (*v3)(_OWORD *, _OWORD *, _OWORD *, void, void);
  uint64_t vars8;

  long long v3 = (uint64_t (*)(_OWORD *, _OWORD *, _OWORD *, void, void))MEMORY[0x1E4F17040];

  return vImage.PixelBuffer<>.overwriteChannels(_:withInterleavedBuffer:destination:)(a1, a2, a3, v3);
}

vImage_Error vImage.PixelBuffer<>.overwriteChannels(withScalar:)(Pixel_16F a1)
{
  uint64_t v6 = *MEMORY[0x1E4F143B8];
  uint64_t v2 = *v1;
  if (!*(void *)(*v1 + 16)) {
    __break(1u);
  }
  long long v3 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&v5.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&v5.vImagePixelCount width = v3;
  return vImageOverwriteChannelsWithScalar_Planar16F(a1, &v5, 0);
}

vImage_Error vImage.PixelBuffer<>.overwriteChannels(_:withPixel:destination:)(uint64_t a1, uint16_t a2, uint16_t a3, uint16_t a4, uint16_t a5, uint64_t *a6)
{
  uint64_t v53 = *MEMORY[0x1E4F143B8];
  uint64_t v7 = *v6;
  if (!*(void *)(*v6 + 16)) {
    goto LABEL_41;
  }
  uint64_t v8 = *(void *)(v7 + 48);
  if (v8 < 0)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  uint64_t v9 = *(void *)(v7 + 40);
  if (v9 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!v8)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  uint64_t v10 = *a6;
  if (!*(void *)(*a6 + 16))
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  uint64_t v11 = *(void *)(v10 + 48);
  if (v11 < 0)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  uint64_t v12 = *(void *)(v10 + 40);
  if (v12 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (!v11)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v12)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (v8 != v11)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v9 != v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  uint16_t v13 = a5;
  uint16_t v14 = a4;
  uint16_t v15 = a3;
  uint16_t v16 = a2;
  int64_t v18 = *(void *)(a1 + 16);
  if (v18)
  {
    src.data = (void *)MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v18, 0);
    data = (int8x16_t *)src.data;
    unint64_t v20 = (unsigned __int8 *)(a1 + 32);
    while (1)
    {
      int v21 = *v20++;
      char v22 = 3 - v21;
      if (((3 - v21) & 0xFFFFFF00) != 0) {
        break;
      }
      src.data = data;
      unint64_t v24 = data[1].u64[0];
      unint64_t v23 = data[1].u64[1];
      unint64_t v25 = v24 + 1;
      if (v24 >= v23 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v23 > 1), v24 + 1, 1);
        data = (int8x16_t *)src.data;
      }
      data[1].i64[0] = v25;
      data[2].i8[v24] = 1 << (v22 & 7);
      if (!--v18)
      {
        uint16_t v14 = a4;
        uint16_t v13 = a5;
        uint16_t v16 = a2;
        uint16_t v15 = a3;
        goto LABEL_21;
      }
    }
    __break(1u);
    goto LABEL_40;
  }
  data = (int8x16_t *)MEMORY[0x1E4FBC860];
  unint64_t v25 = *(void *)(MEMORY[0x1E4FBC860] + 16);
  if (!v25)
  {
    swift_bridgeObjectRelease();
    uint8_t v27 = 0;
    goto LABEL_35;
  }
LABEL_21:
  if (v25 < 8)
  {
    unint64_t v26 = 0;
    uint8_t v27 = 0;
    goto LABEL_32;
  }
  if (v25 >= 0x20)
  {
    unint64_t v26 = v25 & 0xFFFFFFFFFFFFFFE0;
    unint64_t v28 = data + 3;
    int8x16_t v29 = 0uLL;
    unint64_t v30 = v25 & 0xFFFFFFFFFFFFFFE0;
    int8x16_t v31 = 0uLL;
    do
    {
      int8x16_t v29 = vorrq_s8(v28[-1], v29);
      int8x16_t v31 = vorrq_s8(*v28, v31);
      v28 += 2;
      v30 -= 32;
    }
    while (v30);
    int8x16_t v32 = vorrq_s8(v31, v29);
    *(int8x8_t *)v32.i8 = vorr_s8(*(int8x8_t *)v32.i8, (int8x8_t)*(_OWORD *)&vextq_s8(v32, v32, 8uLL));
    unint64_t v33 = v32.i64[0] | HIDWORD(v32.i64[0]) | ((unint64_t)(v32.i64[0] | HIDWORD(v32.i64[0])) >> 16);
    uint8_t v27 = v33 | BYTE1(v33);
    if (v25 == v26) {
      goto LABEL_34;
    }
    if ((v25 & 0x18) == 0)
    {
LABEL_32:
      unint64_t v40 = v25 - v26;
      uint64_t v41 = &data[2].i8[v26];
      do
      {
        char v42 = *v41++;
        v27 |= v42;
        --v40;
      }
      while (v40);
      goto LABEL_34;
    }
  }
  else
  {
    uint8_t v27 = 0;
    unint64_t v26 = 0;
  }
  unint64_t v34 = v26;
  unint64_t v26 = v25 & 0xFFFFFFFFFFFFFFF8;
  int8x8_t v35 = (int8x8_t)v27;
  char v36 = (int8x8_t *)&data[2].i8[v34];
  unint64_t v37 = v34 - (v25 & 0xFFFFFFFFFFFFFFF8);
  do
  {
    int8x8_t v38 = *v36++;
    int8x8_t v35 = vorr_s8(v38, v35);
    v37 += 8;
  }
  while (v37);
  uint64_t v39 = *(void *)&v35 | HIDWORD(*(void *)&v35) | ((*(void *)&v35 | HIDWORD(*(void *)&v35)) >> 16);
  uint8_t v27 = v39 | BYTE1(v39);
  if (v25 != v26) {
    goto LABEL_32;
  }
LABEL_34:
  swift_bridgeObjectRelease();
  if (v27 > 0xFu)
  {
LABEL_40:
    __break(1u);
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
LABEL_35:
  v52[0] = v16;
  v52[1] = v15;
  v52[2] = v14;
  v52[3] = v13;
  if (!*(void *)(v7 + 16))
  {
LABEL_53:
    __break(1u);
LABEL_54:
    __break(1u);
  }
  long long v43 = *(_OWORD *)(v7 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v7 + 32);
  *(_OWORD *)&src.vImagePixelCount width = v43;
  if (!*(void *)(v10 + 16)) {
    goto LABEL_54;
  }
  long long v44 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&dest.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&dest.vImagePixelCount width = v44;
  return vImageOverwriteChannelsWithPixel_ARGB16U(v52, &src, &dest, v27, 0);
}

vImage_Error vImage.PixelBuffer<>.overwriteChannels(withScalar:)(Pixel_F a1)
{
  uint64_t v6 = *MEMORY[0x1E4F143B8];
  uint64_t v2 = *v1;
  if (!*(void *)(*v1 + 16)) {
    __break(1u);
  }
  long long v3 = *(_OWORD *)(v2 + 48);
  *(_OWORD *)&v5.data = *(_OWORD *)(v2 + 32);
  *(_OWORD *)&v5.vImagePixelCount width = v3;
  return vImageOverwriteChannelsWithScalar_PlanarF(a1, &v5, 0);
}

vImage_Error vImage.PixelBuffer<>.overwriteChannels(_:withScalar:destination:)(uint64_t a1, uint64_t *a2, Pixel_F a3)
{
  uint64_t v42 = *MEMORY[0x1E4F143B8];
  uint64_t v4 = *v3;
  if (!*(void *)(*v3 + 16)) {
    goto LABEL_42;
  }
  uint64_t v5 = *(void *)(v4 + 48);
  if (v5 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  uint64_t v6 = *(void *)(v4 + 40);
  if (v6 < 0)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v5)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  if (!v6)
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  uint64_t v7 = *a2;
  if (!*(void *)(*a2 + 16))
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  uint64_t v8 = *(void *)(v7 + 48);
  if (v8 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  uint64_t v9 = *(void *)(v7 + 40);
  if (v9 < 0)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v8)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (!v9)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v5 != v8)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (v6 != v9)
  {
LABEL_53:
    __break(1u);
LABEL_54:
    __break(1u);
  }
  int64_t v12 = *(void *)(a1 + 16);
  if (v12)
  {
    src.data = (void *)MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v12, 0);
    data = (int8x16_t *)src.data;
    uint16_t v14 = (unsigned __int8 *)(a1 + 32);
    while (1)
    {
      int v15 = *v14++;
      char v16 = 3 - v15;
      if (((3 - v15) & 0xFFFFFF00) != 0) {
        goto LABEL_40;
      }
      src.data = data;
      unint64_t v18 = data[1].u64[0];
      unint64_t v17 = data[1].u64[1];
      unint64_t v19 = v18 + 1;
      if (v18 >= v17 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v17 > 1), v18 + 1, 1);
        data = (int8x16_t *)src.data;
      }
      data[1].i64[0] = v19;
      data[2].i8[v18] = 1 << (v16 & 7);
      if (!--v12) {
        goto LABEL_21;
      }
    }
  }
  data = (int8x16_t *)MEMORY[0x1E4FBC860];
  unint64_t v19 = *(void *)(MEMORY[0x1E4FBC860] + 16);
  if (!v19)
  {
    swift_bridgeObjectRelease();
    uint8_t v21 = 0;
    if (*(void *)(v4 + 16)) {
      goto LABEL_36;
    }
LABEL_39:
    __break(1u);
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
LABEL_21:
  if (v19 < 8)
  {
    unint64_t v20 = 0;
    uint8_t v21 = 0;
    goto LABEL_32;
  }
  if (v19 >= 0x20)
  {
    unint64_t v20 = v19 & 0xFFFFFFFFFFFFFFE0;
    char v22 = data + 3;
    int8x16_t v23 = 0uLL;
    unint64_t v24 = v19 & 0xFFFFFFFFFFFFFFE0;
    int8x16_t v25 = 0uLL;
    do
    {
      int8x16_t v23 = vorrq_s8(v22[-1], v23);
      int8x16_t v25 = vorrq_s8(*v22, v25);
      v22 += 2;
      v24 -= 32;
    }
    while (v24);
    int8x16_t v26 = vorrq_s8(v25, v23);
    *(int8x8_t *)v26.i8 = vorr_s8(*(int8x8_t *)v26.i8, (int8x8_t)*(_OWORD *)&vextq_s8(v26, v26, 8uLL));
    unint64_t v27 = v26.i64[0] | HIDWORD(v26.i64[0]) | ((unint64_t)(v26.i64[0] | HIDWORD(v26.i64[0])) >> 16);
    uint8_t v21 = v27 | BYTE1(v27);
    if (v19 == v20) {
      goto LABEL_34;
    }
    if ((v19 & 0x18) == 0)
    {
LABEL_32:
      unint64_t v34 = v19 - v20;
      int8x8_t v35 = &data[2].i8[v20];
      do
      {
        char v36 = *v35++;
        v21 |= v36;
        --v34;
      }
      while (v34);
      goto LABEL_34;
    }
  }
  else
  {
    uint8_t v21 = 0;
    unint64_t v20 = 0;
  }
  unint64_t v28 = v20;
  unint64_t v20 = v19 & 0xFFFFFFFFFFFFFFF8;
  int8x8_t v29 = (int8x8_t)v21;
  unint64_t v30 = (int8x8_t *)&data[2].i8[v28];
  unint64_t v31 = v28 - (v19 & 0xFFFFFFFFFFFFFFF8);
  do
  {
    int8x8_t v32 = *v30++;
    int8x8_t v29 = vorr_s8(v32, v29);
    v31 += 8;
  }
  while (v31);
  uint64_t v33 = *(void *)&v29 | HIDWORD(*(void *)&v29) | ((*(void *)&v29 | HIDWORD(*(void *)&v29)) >> 16);
  uint8_t v21 = v33 | BYTE1(v33);
  if (v19 != v20) {
    goto LABEL_32;
  }
LABEL_34:
  swift_bridgeObjectRelease();
  if (v21 > 0xFu)
  {
LABEL_41:
    __break(1u);
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  if (!*(void *)(v4 + 16)) {
    goto LABEL_39;
  }
LABEL_36:
  long long v37 = *(_OWORD *)(v4 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v4 + 32);
  *(_OWORD *)&src.vImagePixelCount width = v37;
  if (!*(void *)(v7 + 16)) {
    goto LABEL_54;
  }
  long long v38 = *(_OWORD *)(v7 + 48);
  *(_OWORD *)&v40.data = *(_OWORD *)(v7 + 32);
  *(_OWORD *)&v40.vImagePixelCount width = v38;
  return vImageOverwriteChannelsWithScalar_ARGBFFFF(a3, &src, &v40, v21, 0);
}

vImage_Error vImage.PixelBuffer<>.overwriteChannels(_:withPixel:destination:)(uint64_t a1, uint64_t *a2, float a3, float a4, float a5, float a6)
{
  uint64_t v49 = *MEMORY[0x1E4F143B8];
  uint64_t v7 = *v6;
  if (!*(void *)(*v6 + 16)) {
    goto LABEL_41;
  }
  uint64_t v8 = *(void *)(v7 + 48);
  if (v8 < 0)
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  uint64_t v9 = *(void *)(v7 + 40);
  if (v9 < 0)
  {
LABEL_43:
    __break(1u);
    goto LABEL_44;
  }
  if (!v8)
  {
LABEL_44:
    __break(1u);
    goto LABEL_45;
  }
  if (!v9)
  {
LABEL_45:
    __break(1u);
    goto LABEL_46;
  }
  uint64_t v10 = *a2;
  if (!*(void *)(*a2 + 16))
  {
LABEL_46:
    __break(1u);
    goto LABEL_47;
  }
  uint64_t v11 = *(void *)(v10 + 48);
  if (v11 < 0)
  {
LABEL_47:
    __break(1u);
    goto LABEL_48;
  }
  uint64_t v12 = *(void *)(v10 + 40);
  if (v12 < 0)
  {
LABEL_48:
    __break(1u);
    goto LABEL_49;
  }
  if (!v11)
  {
LABEL_49:
    __break(1u);
    goto LABEL_50;
  }
  if (!v12)
  {
LABEL_50:
    __break(1u);
    goto LABEL_51;
  }
  if (v8 != v11)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  if (v9 != v12)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  int64_t v18 = *(void *)(a1 + 16);
  if (v18)
  {
    src.data = (void *)MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v18, 0);
    data = (int8x16_t *)src.data;
    unint64_t v20 = (unsigned __int8 *)(a1 + 32);
    while (1)
    {
      int v21 = *v20++;
      char v22 = 3 - v21;
      if (((3 - v21) & 0xFFFFFF00) != 0) {
        break;
      }
      src.data = data;
      unint64_t v24 = data[1].u64[0];
      unint64_t v23 = data[1].u64[1];
      unint64_t v25 = v24 + 1;
      if (v24 >= v23 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v23 > 1), v24 + 1, 1);
        data = (int8x16_t *)src.data;
      }
      data[1].i64[0] = v25;
      data[2].i8[v24] = 1 << (v22 & 7);
      if (!--v18) {
        goto LABEL_21;
      }
    }
    __break(1u);
    goto LABEL_40;
  }
  data = (int8x16_t *)MEMORY[0x1E4FBC860];
  unint64_t v25 = *(void *)(MEMORY[0x1E4FBC860] + 16);
  if (!v25)
  {
    swift_bridgeObjectRelease();
    uint8_t v27 = 0;
    goto LABEL_35;
  }
LABEL_21:
  if (v25 < 8)
  {
    unint64_t v26 = 0;
    uint8_t v27 = 0;
    goto LABEL_32;
  }
  if (v25 >= 0x20)
  {
    unint64_t v26 = v25 & 0xFFFFFFFFFFFFFFE0;
    unint64_t v28 = data + 3;
    int8x16_t v29 = 0uLL;
    unint64_t v30 = v25 & 0xFFFFFFFFFFFFFFE0;
    int8x16_t v31 = 0uLL;
    do
    {
      int8x16_t v29 = vorrq_s8(v28[-1], v29);
      int8x16_t v31 = vorrq_s8(*v28, v31);
      v28 += 2;
      v30 -= 32;
    }
    while (v30);
    int8x16_t v32 = vorrq_s8(v31, v29);
    *(int8x8_t *)v32.i8 = vorr_s8(*(int8x8_t *)v32.i8, (int8x8_t)*(_OWORD *)&vextq_s8(v32, v32, 8uLL));
    unint64_t v33 = v32.i64[0] | HIDWORD(v32.i64[0]) | ((unint64_t)(v32.i64[0] | HIDWORD(v32.i64[0])) >> 16);
    uint8_t v27 = v33 | BYTE1(v33);
    if (v25 == v26) {
      goto LABEL_34;
    }
    if ((v25 & 0x18) == 0)
    {
LABEL_32:
      unint64_t v40 = v25 - v26;
      uint64_t v41 = &data[2].i8[v26];
      do
      {
        char v42 = *v41++;
        v27 |= v42;
        --v40;
      }
      while (v40);
      goto LABEL_34;
    }
  }
  else
  {
    uint8_t v27 = 0;
    unint64_t v26 = 0;
  }
  unint64_t v34 = v26;
  unint64_t v26 = v25 & 0xFFFFFFFFFFFFFFF8;
  int8x8_t v35 = (int8x8_t)v27;
  char v36 = (int8x8_t *)&data[2].i8[v34];
  unint64_t v37 = v34 - (v25 & 0xFFFFFFFFFFFFFFF8);
  do
  {
    int8x8_t v38 = *v36++;
    int8x8_t v35 = vorr_s8(v38, v35);
    v37 += 8;
  }
  while (v37);
  uint64_t v39 = *(void *)&v35 | HIDWORD(*(void *)&v35) | ((*(void *)&v35 | HIDWORD(*(void *)&v35)) >> 16);
  uint8_t v27 = v39 | BYTE1(v39);
  if (v25 != v26) {
    goto LABEL_32;
  }
LABEL_34:
  swift_bridgeObjectRelease();
  if (v27 > 0xFu)
  {
LABEL_40:
    __break(1u);
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
LABEL_35:
  v48[0] = a3;
  v48[1] = a4;
  float v48[2] = a5;
  v48[3] = a6;
  if (!*(void *)(v7 + 16))
  {
LABEL_53:
    __break(1u);
LABEL_54:
    __break(1u);
  }
  long long v43 = *(_OWORD *)(v7 + 48);
  *(_OWORD *)&src.data = *(_OWORD *)(v7 + 32);
  *(_OWORD *)&src.vImagePixelCount width = v43;
  if (!*(void *)(v10 + 16)) {
    goto LABEL_54;
  }
  long long v44 = *(_OWORD *)(v10 + 48);
  *(_OWORD *)&v46.data = *(_OWORD *)(v10 + 32);
  *(_OWORD *)&v46.vImagePixelCount width = v44;
  return vImageOverwriteChannelsWithPixel_ARGBFFFF(v48, &src, &v46, v27, 0);
}

uint64_t vImage.PixelBuffer<>.overwriteChannels(_:withInterleavedBuffer:destination:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t (*a4)(_OWORD *, _OWORD *, _OWORD *, void, void))
{
  uint64_t v50 = *MEMORY[0x1E4F143B8];
  uint64_t v5 = *v4;
  if (!*(void *)(*v4 + 16)) {
    goto LABEL_50;
  }
  uint64_t v6 = *(void *)(v5 + 48);
  if (v6 < 0)
  {
LABEL_51:
    __break(1u);
    goto LABEL_52;
  }
  uint64_t v7 = *(void *)(v5 + 40);
  if (v7 < 0)
  {
LABEL_52:
    __break(1u);
    goto LABEL_53;
  }
  if (!v6)
  {
LABEL_53:
    __break(1u);
    goto LABEL_54;
  }
  if (!v7)
  {
LABEL_54:
    __break(1u);
    goto LABEL_55;
  }
  uint64_t v8 = *a3;
  if (!*(void *)(*a3 + 16))
  {
LABEL_55:
    __break(1u);
    goto LABEL_56;
  }
  uint64_t v9 = *(void *)(v8 + 48);
  if (v9 < 0)
  {
LABEL_56:
    __break(1u);
    goto LABEL_57;
  }
  uint64_t v10 = *(void *)(v8 + 40);
  if (v10 < 0)
  {
LABEL_57:
    __break(1u);
    goto LABEL_58;
  }
  if (!v9)
  {
LABEL_58:
    __break(1u);
    goto LABEL_59;
  }
  if (!v10)
  {
LABEL_59:
    __break(1u);
    goto LABEL_60;
  }
  if (v6 != v9)
  {
LABEL_60:
    __break(1u);
    goto LABEL_61;
  }
  if (v7 != v10)
  {
LABEL_61:
    __break(1u);
    goto LABEL_62;
  }
  uint64_t v11 = *a2;
  if (!*(void *)(*a2 + 16))
  {
LABEL_62:
    __break(1u);
    goto LABEL_63;
  }
  uint64_t v12 = *(void *)(v11 + 48);
  if (v12 < 0)
  {
LABEL_63:
    __break(1u);
    goto LABEL_64;
  }
  uint64_t v13 = *(void *)(v11 + 40);
  if (v13 < 0)
  {
LABEL_64:
    __break(1u);
    goto LABEL_65;
  }
  if (!v12)
  {
LABEL_65:
    __break(1u);
    goto LABEL_66;
  }
  if (!v13)
  {
LABEL_66:
    __break(1u);
    goto LABEL_67;
  }
  if (v6 != v12)
  {
LABEL_67:
    __break(1u);
    goto LABEL_68;
  }
  if (v7 != v13)
  {
LABEL_68:
    __break(1u);
    goto LABEL_69;
  }
  uint16_t v14 = a4;
  int64_t v16 = *(void *)(a1 + 16);
  if (v16)
  {
    uint64_t v45 = *v4;
    *(void *)&v49[0] = MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v16, 0);
    unint64_t v17 = *(int8x16_t **)&v49[0];
    int64_t v18 = (unsigned __int8 *)(a1 + 32);
    do
    {
      int v19 = *v18++;
      char v20 = 3 - v19;
      if (((3 - v19) & 0xFFFFFF00) != 0) {
        goto LABEL_48;
      }
      *(void *)&v49[0] = v17;
      unint64_t v22 = v17[1].u64[0];
      unint64_t v21 = v17[1].u64[1];
      unint64_t v23 = v22 + 1;
      if (v22 >= v21 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v21 > 1), v22 + 1, 1);
        unint64_t v17 = *(int8x16_t **)&v49[0];
      }
      v17[1].i64[0] = v23;
      v17[2].i8[v22] = 1 << (v20 & 7);
      --v16;
    }
    while (v16);
    uint64_t v5 = v45;
    uint16_t v14 = a4;
LABEL_28:
    if (v23 < 8)
    {
      unint64_t v24 = 0;
      unsigned __int8 v25 = 0;
LABEL_39:
      unint64_t v38 = v23 - v24;
      uint64_t v39 = &v17[2].i8[v24];
      do
      {
        char v40 = *v39++;
        v25 |= v40;
        --v38;
      }
      while (v38);
LABEL_41:
      swift_bridgeObjectRelease();
      if (v25 > 0xFu)
      {
LABEL_49:
        __break(1u);
LABEL_50:
        __break(1u);
        goto LABEL_51;
      }
      if (*(void *)(v11 + 16)) {
        goto LABEL_43;
      }
LABEL_47:
      __break(1u);
LABEL_48:
      __break(1u);
      goto LABEL_49;
    }
    if (v23 >= 0x20)
    {
      unint64_t v24 = v23 & 0xFFFFFFFFFFFFFFE0;
      unint64_t v26 = v17 + 3;
      int8x16_t v27 = 0uLL;
      unint64_t v28 = v23 & 0xFFFFFFFFFFFFFFE0;
      int8x16_t v29 = 0uLL;
      do
      {
        int8x16_t v27 = vorrq_s8(v26[-1], v27);
        int8x16_t v29 = vorrq_s8(*v26, v29);
        v26 += 2;
        v28 -= 32;
      }
      while (v28);
      int8x16_t v30 = vorrq_s8(v29, v27);
      *(int8x8_t *)v30.i8 = vorr_s8(*(int8x8_t *)v30.i8, (int8x8_t)*(_OWORD *)&vextq_s8(v30, v30, 8uLL));
      unint64_t v31 = v30.i64[0] | HIDWORD(v30.i64[0]) | ((unint64_t)(v30.i64[0] | HIDWORD(v30.i64[0])) >> 16);
      unsigned __int8 v25 = v31 | BYTE1(v31);
      if (v23 == v24) {
        goto LABEL_41;
      }
      if ((v23 & 0x18) == 0) {
        goto LABEL_39;
      }
    }
    else
    {
      unsigned __int8 v25 = 0;
      unint64_t v24 = 0;
    }
    unint64_t v32 = v24;
    unint64_t v24 = v23 & 0xFFFFFFFFFFFFFFF8;
    int8x8_t v33 = (int8x8_t)v25;
    unint64_t v34 = (int8x8_t *)&v17[2].i8[v32];
    unint64_t v35 = v32 - (v23 & 0xFFFFFFFFFFFFFFF8);
    do
    {
      int8x8_t v36 = *v34++;
      int8x8_t v33 = vorr_s8(v36, v33);
      v35 += 8;
    }
    while (v35);
    uint64_t v37 = *(void *)&v33 | HIDWORD(*(void *)&v33) | ((*(void *)&v33 | HIDWORD(*(void *)&v33)) >> 16);
    unsigned __int8 v25 = v37 | BYTE1(v37);
    if (v23 == v24) {
      goto LABEL_41;
    }
    goto LABEL_39;
  }
  unint64_t v17 = (int8x16_t *)MEMORY[0x1E4FBC860];
  unint64_t v23 = *(void *)(MEMORY[0x1E4FBC860] + 16);
  if (v23) {
    goto LABEL_28;
  }
  swift_bridgeObjectRelease();
  unsigned __int8 v25 = 0;
  if (!*(void *)(v11 + 16)) {
    goto LABEL_47;
  }
LABEL_43:
  long long v41 = *(_OWORD *)(v11 + 48);
  v49[0] = *(_OWORD *)(v11 + 32);
  v49[1] = v41;
  if (!*(void *)(v5 + 16))
  {
LABEL_69:
    __break(1u);
LABEL_70:
    __break(1u);
  }
  long long v42 = *(_OWORD *)(v5 + 48);
  v48[0] = *(_OWORD *)(v5 + 32);
  v48[1] = v42;
  if (!*(void *)(v8 + 16)) {
    goto LABEL_70;
  }
  long long v43 = *(_OWORD *)(v8 + 48);
  v47[0] = *(_OWORD *)(v8 + 32);
  v47[1] = v43;
  return v14(v49, v48, v47, v25, 0);
}

Swift::Void __swiftcall __spoils<CF,ZF,NF,VF,X0,X1,X2,X3,X4,X5,X6,X7,X8,X9,X10,X11,X12,X13,X14,X15,X16,X17,X21,Q0,Q1,Q2,Q3,Q4,Q5,Q6,Q7,Q16,Q17,Q18,Q19,Q20,Q21,Q22,Q23,Q24,Q25,Q26,Q27,Q28,Q29,Q30,Q31> BNNSOptimizer.step(parameters:gradients:accumulators:filterParameters:)(Swift::OpaquePointer parameters, Swift::OpaquePointer gradients, Swift::OpaquePointer accumulators, BNNSFilterParameters_optional *filterParameters)
{
}

{
  uint64_t v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t *boxed_opaque_existential_1;
  uint64_t v11[3];
  uint64_t v12;
  uint64_t v13;
  uint64_t v14;

  uint64_t v6 = v4;
  uint64_t v12 = v4;
  uint64_t v13 = v14;
  boxed_opaque_existential_1 = __swift_allocate_boxed_opaque_existential_1(v11);
  (*(void (**)(uint64_t *, uint64_t, uint64_t))(*(void *)(v6 - 8) + 16))(boxed_opaque_existential_1, v5, v6);
  specialized static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)((uint64_t)v11, (uint64_t)parameters._rawValue, (uint64_t)gradients._rawValue, (uint64_t)accumulators._rawValue);
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v11);
}

uint64_t *__swift_allocate_boxed_opaque_existential_1(uint64_t *a1)
{
  uint64_t v1 = a1;
  if ((*(unsigned char *)(*(void *)(a1[3] - 8) + 82) & 2) != 0)
  {
    *a1 = swift_allocBox();
    return (uint64_t *)v2;
  }
  return v1;
}

uint64_t closure #1 in closure #4 in static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)(uint64_t a1, _DWORD *a2, void *a3, uint64_t a4, char **a5, char **a6, char **a7, uint64_t a8)
{
  uint64_t v15 = a3[3];
  uint64_t v16 = a3[4];
  __swift_project_boxed_opaque_existential_1(a3, v15);
  uint64_t v17 = (*(uint64_t (**)(uint64_t, uint64_t))(v16 + 16))(v15, v16);
  int64_t v18 = *a5;
  char isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
  *a5 = v18;
  if ((isUniquelyReferenced_nonNull_native & 1) == 0) {
    int64_t v18 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v18 + 2), 0, v18);
  }
  *a5 = v18;
  char v20 = *a6;
  swift_bridgeObjectRetain();
  char v21 = swift_isUniquelyReferenced_nonNull_native();
  *a6 = v20;
  if ((v21 & 1) == 0) {
    char v20 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v20 + 2), 0, v20);
  }
  *a6 = v20;
  unint64_t v22 = *a7;
  swift_bridgeObjectRetain();
  char v23 = swift_isUniquelyReferenced_nonNull_native();
  *a7 = v22;
  if ((v23 & 1) == 0) {
    unint64_t v22 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v22 + 2), 0, v22);
  }
  *a7 = v22;
  int v24 = MEMORY[0x1D2600310](v17, a1, a4, v18 + 32, v20 + 32, v22 + 32, a8);
  swift_bridgeObjectRelease();
  uint64_t result = swift_bridgeObjectRelease();
  *a2 = v24;
  return result;
}

uint64_t BNNS.AdamOptimizer.accumulatorCountMultiplier.getter()
{
  if (*(unsigned char *)(v0 + 53)) {
    return 3;
  }
  else {
    return 2;
  }
}

float BNNS.AdamOptimizer.learningRate.getter()
{
  return *(float *)v0;
}

void BNNS.AdamOptimizer.learningRate.setter(float a1)
{
  unint64_t v2 = *v1;
  unint64_t v3 = v1[3];
  unint64_t v4 = v1[5];
  if (*((unsigned char *)v1 + 52) == 1)
  {
    int v5 = *((_DWORD *)v1 + 12);
    unint64_t v6 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
  }
  else
  {
    int v5 = 0;
    unint64_t v6 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    v3 &= 0x1FFFFFFFFuLL;
    unint64_t v4 = v1[5];
  }
  unint64_t *v1 = v6;
  v1[3] = v3;
  v1[5] = v4;
  *((_DWORD *)v1 + 12) = v5;
}

uint64_t **(*BNNS.AdamOptimizer.learningRate.modify(uint64_t a1))(uint64_t **a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *v1;
  return BNNS.AdamOptimizer.learningRate.modify;
}

uint64_t **BNNS.AdamOptimizer.learningRate.modify(uint64_t **a1)
{
  uint64_t v1 = (unint64_t *)*a1;
  unsigned int v3 = *((_DWORD *)a1 + 2);
  uint64_t result = a1 + 1;
  unsigned int v4 = v3;
  unint64_t v5 = *v1;
  unint64_t v6 = v1[3];
  unint64_t v7 = v1[5];
  int v8 = *((unsigned __int8 *)v1 + 52);
  if (*((unsigned char *)v1 + 52))
  {
    int v8 = *((_DWORD *)v1 + 12);
    unint64_t v9 = v5 & 0xFFFFFFFF00000000 | v4;
  }
  else
  {
    unint64_t v9 = v5 & 0xFFFFFFFF00000000 | v4;
    v6 &= 0x1FFFFFFFFuLL;
    unint64_t v7 = v1[5];
  }
  unint64_t *v1 = v9;
  v1[3] = v6;
  v1[5] = v7;
  *((_DWORD *)v1 + 12) = v8;
  return result;
}

float BNNS.AdamOptimizer.beta1.getter()
{
  return *(float *)(v0 + 4);
}

void BNNS.AdamOptimizer.beta1.setter(float a1)
{
  uint64_t v2 = *v1;
  uint64_t v3 = *((void *)v1 + 3);
  uint64_t v4 = *((void *)v1 + 5);
  if (*((unsigned char *)v1 + 52) == 1)
  {
    unsigned int v5 = v1[12];
    unint64_t v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
  }
  else
  {
    unsigned int v5 = 0;
    unint64_t v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
    v3 &= 0x1FFFFFFFFuLL;
    uint64_t v4 = *((void *)v1 + 5);
  }
  *(void *)uint64_t v1 = v6;
  *((void *)v1 + 3) = v3;
  *((void *)v1 + 5) = v4;
  v1[12] = v5;
}

unsigned int **(*BNNS.AdamOptimizer.beta1.modify(uint64_t a1))(unsigned int **a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 4);
  return BNNS.AdamOptimizer.beta1.modify;
}

unsigned int **BNNS.AdamOptimizer.beta1.modify(unsigned int **a1)
{
  uint64_t v1 = *a1;
  unsigned int v3 = *((_DWORD *)a1 + 2);
  uint64_t result = a1 + 1;
  unsigned int v4 = v3;
  uint64_t v5 = *v1;
  uint64_t v6 = *((void *)v1 + 3);
  uint64_t v7 = *((void *)v1 + 5);
  unsigned int v8 = *((unsigned __int8 *)v1 + 52);
  if (*((unsigned char *)v1 + 52))
  {
    unsigned int v8 = v1[12];
    unint64_t v9 = v5 | ((unint64_t)v4 << 32);
  }
  else
  {
    unint64_t v9 = v5 | ((unint64_t)v4 << 32);
    v6 &= 0x1FFFFFFFFuLL;
    uint64_t v7 = *((void *)v1 + 5);
  }
  *(void *)uint64_t v1 = v9;
  *((void *)v1 + 3) = v6;
  *((void *)v1 + 5) = v7;
  v1[12] = v8;
  return result;
}

float BNNS.AdamOptimizer.beta2.getter()
{
  return *(float *)(v0 + 8);
}

void BNNS.AdamOptimizer.beta2.setter(float a1)
{
  uint64_t v2 = *(void *)(v1 + 8);
  uint64_t v3 = *(void *)(v1 + 24);
  uint64_t v4 = *(void *)(v1 + 40);
  if (*(unsigned char *)(v1 + 52) == 1)
  {
    int v5 = *(_DWORD *)(v1 + 48);
    unint64_t v6 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
  }
  else
  {
    int v5 = 0;
    unint64_t v6 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    v3 &= 0x1FFFFFFFFuLL;
    uint64_t v4 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 8) = v6;
  *(void *)(v1 + 24) = v3;
  *(void *)(v1 + 40) = v4;
  *(_DWORD *)(v1 + 48) = v5;
}

uint64_t *(*BNNS.AdamOptimizer.beta2.modify(uint64_t a1))(uint64_t *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(void *)(v1 + 8);
  return BNNS.AdamOptimizer.beta2.modify;
}

uint64_t *BNNS.AdamOptimizer.beta2.modify(uint64_t *a1)
{
  uint64_t v1 = *a1;
  unsigned int v3 = *((_DWORD *)a1 + 2);
  uint64_t result = a1 + 1;
  unsigned int v4 = v3;
  uint64_t v5 = *(void *)(v1 + 8);
  uint64_t v6 = *(void *)(v1 + 24);
  uint64_t v7 = *(void *)(v1 + 40);
  int v8 = *(unsigned __int8 *)(v1 + 52);
  if (*(unsigned char *)(v1 + 52))
  {
    int v8 = *(_DWORD *)(v1 + 48);
    unint64_t v9 = v5 & 0xFFFFFFFF00000000 | v4;
  }
  else
  {
    unint64_t v9 = v5 & 0xFFFFFFFF00000000 | v4;
    v6 &= 0x1FFFFFFFFuLL;
    uint64_t v7 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 8) = v9;
  *(void *)(v1 + 24) = v6;
  *(void *)(v1 + 40) = v7;
  *(_DWORD *)(v1 + 48) = v8;
  return result;
}

float BNNS.AdamOptimizer.timeStep.getter()
{
  return *(float *)(v0 + 12);
}

void BNNS.AdamOptimizer.timeStep.setter(float a1)
{
  uint64_t v2 = *(unsigned int *)(v1 + 8);
  uint64_t v3 = *(void *)(v1 + 24);
  uint64_t v4 = *(void *)(v1 + 40);
  if (*(unsigned char *)(v1 + 52) == 1)
  {
    int v5 = *(_DWORD *)(v1 + 48);
    unint64_t v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
  }
  else
  {
    int v5 = 0;
    unint64_t v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
    v3 &= 0x1FFFFFFFFuLL;
    uint64_t v4 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 8) = v6;
  *(void *)(v1 + 24) = v3;
  *(void *)(v1 + 40) = v4;
  *(_DWORD *)(v1 + 48) = v5;
}

uint64_t *(*BNNS.AdamOptimizer.timeStep.modify(uint64_t a1))(uint64_t *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 12);
  return BNNS.AdamOptimizer.timeStep.modify;
}

uint64_t *BNNS.AdamOptimizer.timeStep.modify(uint64_t *a1)
{
  uint64_t v1 = *a1;
  unsigned int v3 = *((_DWORD *)a1 + 2);
  uint64_t result = a1 + 1;
  unsigned int v4 = v3;
  uint64_t v5 = *(unsigned int *)(v1 + 8);
  uint64_t v6 = *(void *)(v1 + 24);
  uint64_t v7 = *(void *)(v1 + 40);
  int v8 = *(unsigned __int8 *)(v1 + 52);
  if (*(unsigned char *)(v1 + 52))
  {
    int v8 = *(_DWORD *)(v1 + 48);
    unint64_t v9 = v5 | ((unint64_t)v4 << 32);
  }
  else
  {
    unint64_t v9 = v5 | ((unint64_t)v4 << 32);
    v6 &= 0x1FFFFFFFFuLL;
    uint64_t v7 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 8) = v9;
  *(void *)(v1 + 24) = v6;
  *(void *)(v1 + 40) = v7;
  *(_DWORD *)(v1 + 48) = v8;
  return result;
}

float BNNS.AdamOptimizer.epsilon.getter()
{
  return *(float *)(v0 + 16);
}

void BNNS.AdamOptimizer.epsilon.setter(float a1)
{
  uint64_t v3 = *(void *)(v1 + 16);
  uint64_t v2 = *(void *)(v1 + 24);
  uint64_t v4 = *(void *)(v1 + 40);
  if (*(unsigned char *)(v1 + 52) == 1)
  {
    int v5 = *(_DWORD *)(v1 + 48);
    unint64_t v6 = v3 & 0xFFFFFFFF00000000 | LODWORD(a1);
  }
  else
  {
    int v5 = 0;
    unint64_t v6 = v3 & 0xFFFFFFFF00000000 | LODWORD(a1);
    v2 &= 0x1FFFFFFFFuLL;
    uint64_t v4 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 16) = v6;
  *(void *)(v1 + 24) = v2;
  *(void *)(v1 + 40) = v4;
  *(_DWORD *)(v1 + 48) = v5;
}

uint64_t *(*BNNS.AdamOptimizer.epsilon.modify(uint64_t a1))(uint64_t *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(void *)(v1 + 16);
  return BNNS.AdamOptimizer.epsilon.modify;
}

uint64_t *BNNS.AdamOptimizer.epsilon.modify(uint64_t *a1)
{
  uint64_t v1 = *a1;
  unsigned int v3 = *((_DWORD *)a1 + 2);
  uint64_t result = a1 + 1;
  unsigned int v4 = v3;
  uint64_t v6 = *(void *)(v1 + 16);
  uint64_t v5 = *(void *)(v1 + 24);
  uint64_t v7 = *(void *)(v1 + 40);
  int v8 = *(unsigned __int8 *)(v1 + 52);
  if (*(unsigned char *)(v1 + 52))
  {
    int v8 = *(_DWORD *)(v1 + 48);
    unint64_t v9 = v6 & 0xFFFFFFFF00000000 | v4;
  }
  else
  {
    unint64_t v9 = v6 & 0xFFFFFFFF00000000 | v4;
    v5 &= 0x1FFFFFFFFuLL;
    uint64_t v7 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 16) = v9;
  *(void *)(v1 + 24) = v5;
  *(void *)(v1 + 40) = v7;
  *(_DWORD *)(v1 + 48) = v8;
  return result;
}

float BNNS.AdamOptimizer.gradientScale.getter()
{
  return *(float *)(v0 + 20);
}

void BNNS.AdamOptimizer.gradientScale.setter(float a1)
{
  uint64_t v2 = *(unsigned int *)(v1 + 16);
  uint64_t v3 = *(void *)(v1 + 24);
  uint64_t v4 = *(void *)(v1 + 40);
  if (*(unsigned char *)(v1 + 52) == 1)
  {
    int v5 = *(_DWORD *)(v1 + 48);
    unint64_t v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
  }
  else
  {
    int v5 = 0;
    unint64_t v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
    v3 &= 0x1FFFFFFFFuLL;
    uint64_t v4 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 16) = v6;
  *(void *)(v1 + 24) = v3;
  *(void *)(v1 + 40) = v4;
  *(_DWORD *)(v1 + 48) = v5;
}

uint64_t *(*BNNS.AdamOptimizer.gradientScale.modify(uint64_t a1))(uint64_t *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 20);
  return BNNS.AdamOptimizer.gradientScale.modify;
}

uint64_t *BNNS.AdamOptimizer.gradientScale.modify(uint64_t *a1)
{
  uint64_t v1 = *a1;
  unsigned int v3 = *((_DWORD *)a1 + 2);
  uint64_t result = a1 + 1;
  unsigned int v4 = v3;
  uint64_t v5 = *(unsigned int *)(v1 + 16);
  uint64_t v6 = *(void *)(v1 + 24);
  uint64_t v7 = *(void *)(v1 + 40);
  int v8 = *(unsigned __int8 *)(v1 + 52);
  if (*(unsigned char *)(v1 + 52))
  {
    int v8 = *(_DWORD *)(v1 + 48);
    unint64_t v9 = v5 | ((unint64_t)v4 << 32);
  }
  else
  {
    unint64_t v9 = v5 | ((unint64_t)v4 << 32);
    v6 &= 0x1FFFFFFFFuLL;
    uint64_t v7 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 16) = v9;
  *(void *)(v1 + 24) = v6;
  *(void *)(v1 + 40) = v7;
  *(_DWORD *)(v1 + 48) = v8;
  return result;
}

float BNNS.AdamOptimizer.regularizationScale.getter()
{
  return *(float *)(v0 + 24);
}

void BNNS.AdamOptimizer.regularizationScale.setter(float a1)
{
  uint64_t v2 = *(void *)(v1 + 24);
  uint64_t v3 = *(void *)(v1 + 40);
  if (*(unsigned char *)(v1 + 52) == 1)
  {
    int v4 = *(_DWORD *)(v1 + 48);
    unint64_t v5 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
  }
  else
  {
    int v4 = 0;
    unint64_t v5 = v2 & 0x100000000 | LODWORD(a1);
    uint64_t v3 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 24) = v5;
  *(void *)(v1 + 40) = v3;
  *(_DWORD *)(v1 + 48) = v4;
}

uint64_t *(*BNNS.AdamOptimizer.regularizationScale.modify(uint64_t a1))(uint64_t *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(void *)(v1 + 24);
  return BNNS.AdamOptimizer.regularizationScale.modify;
}

uint64_t *BNNS.AdamOptimizer.regularizationScale.modify(uint64_t *a1)
{
  uint64_t v1 = *a1;
  unsigned int v3 = *((_DWORD *)a1 + 2);
  uint64_t result = a1 + 1;
  unsigned int v4 = v3;
  uint64_t v5 = *(void *)(v1 + 24);
  uint64_t v6 = *(void *)(v1 + 40);
  int v7 = *(unsigned __int8 *)(v1 + 52);
  if (*(unsigned char *)(v1 + 52))
  {
    int v7 = *(_DWORD *)(v1 + 48);
    unint64_t v8 = v5 & 0xFFFFFFFF00000000 | v4;
  }
  else
  {
    unint64_t v8 = v5 & 0x100000000 | v4;
    uint64_t v6 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 24) = v8;
  *(void *)(v1 + 40) = v6;
  *(_DWORD *)(v1 + 48) = v7;
  return result;
}

uint64_t BNNS.AdamOptimizer.gradientBounds.getter()
{
  uint64_t result = *(void *)(v0 + 32);
  if (*(unsigned char *)(v0 + 52) == 1)
  {
    if (result == 1)
    {
      if (*((float *)&result + 1) <= COERCE_FLOAT(*(void *)(v0 + 40))) {
        return *(long long *)(v0 + 32) >> 32;
      }
      __break(1u);
      goto LABEL_10;
    }
    return 0;
  }
  if ((*(unsigned char *)(v0 + 28) & 1) == 0) {
    return 0;
  }
  if (*(float *)&result > *((float *)&result + 1)) {
LABEL_10:
  }
    __break(1u);
  return result;
}

uint64_t key path getter for BNNS.AdamOptimizer.gradientBounds : BNNS.AdamOptimizer@<X0>(uint64_t a1@<X8>)
{
  uint64_t result = BNNS.AdamOptimizer.gradientBounds.getter();
  *(void *)a1 = result;
  *(unsigned char *)(a1 + 8) = v3 & 1;
  return result;
}

uint64_t key path setter for BNNS.AdamOptimizer.gradientBounds : BNNS.AdamOptimizer(uint64_t result, uint64_t a2)
{
  unint64_t v2 = *(void *)result;
  int v3 = *(unsigned __int8 *)(result + 8);
  unint64_t v4 = *(void *)(a2 + 24);
  if (*(unsigned char *)(a2 + 52) == 1)
  {
    int v5 = *(_DWORD *)(a2 + 48);
    unint64_t v6 = HIDWORD(v2);
    unint64_t v2 = (v2 << 32) | 1;
    BOOL v7 = v3 == 0;
    if (*(unsigned char *)(result + 8)) {
      unint64_t v8 = 0;
    }
    else {
      unint64_t v8 = v6;
    }
    if (!v7) {
      unint64_t v2 = 0;
    }
    unint64_t v9 = *(void *)(a2 + 40) & 0xFFFFFFFF00000000 | v8;
  }
  else
  {
    int v5 = 0;
    BOOL v7 = v3 == 0;
    uint64_t v10 = 0x100000000;
    if (!v7)
    {
      uint64_t v10 = 0;
      unint64_t v2 = 0;
    }
    unint64_t v9 = *(void *)(a2 + 40);
    unint64_t v4 = v10 & 0xFFFFFFFF00000000 | *(void *)(a2 + 24);
  }
  *(void *)(a2 + 24) = v4;
  *(void *)(a2 + 32) = v2;
  *(void *)(a2 + 40) = v9;
  *(_DWORD *)(a2 + 48) = v5;
  return result;
}

unint64_t BNNS.AdamOptimizer.gradientBounds.setter(unint64_t result, char a2)
{
  uint64_t v3 = a2 & 1;
  uint64_t v4 = *(void *)(v2 + 24);
  if (*(unsigned char *)(v2 + 52) == 1)
  {
    int v5 = *(_DWORD *)(v2 + 48);
    BOOL v6 = v3 == 0;
    if (a2) {
      unint64_t v7 = 0;
    }
    else {
      unint64_t v7 = HIDWORD(result);
    }
    if (v6) {
      uint64_t v8 = (result << 32) | 1;
    }
    else {
      uint64_t v8 = 0;
    }
    unint64_t v9 = *(void *)(v2 + 40) & 0xFFFFFFFF00000000 | v7;
  }
  else
  {
    int v5 = 0;
    if (a2) {
      uint64_t v8 = 0;
    }
    else {
      uint64_t v8 = result;
    }
    uint64_t v4 = (*(void *)(v2 + 24) | (unint64_t)(v3 << 32)) ^ 0x100000000;
    unint64_t v9 = *(void *)(v2 + 40);
  }
  *(void *)(v2 + 24) = v4;
  *(void *)(v2 + 32) = v8;
  *(void *)(v2 + 40) = v9;
  *(_DWORD *)(v2 + 48) = v5;
  return result;
}

uint64_t (*BNNS.AdamOptimizer.gradientBounds.modify(uint64_t (*result)(uint64_t result)))(uint64_t result)
{
  *((void *)result + 2) = v1;
  unint64_t v2 = *(void *)(v1 + 32);
  if (*(unsigned char *)(v1 + 52) != 1)
  {
    if (*(unsigned char *)(v1 + 28))
    {
      if (*(float *)&v2 <= *((float *)&v2 + 1))
      {
        char v4 = 0;
        goto LABEL_9;
      }
      goto LABEL_11;
    }
LABEL_6:
    unint64_t v2 = 0;
    char v4 = 1;
    goto LABEL_9;
  }
  if (v2 != 1) {
    goto LABEL_6;
  }
  unint64_t v3 = HIDWORD(v2);
  if (*(float *)&v3 <= COERCE_FLOAT(*(void *)(v1 + 40)))
  {
    char v4 = 0;
    unint64_t v2 = v3 | (*(void *)(v1 + 40) << 32);
LABEL_9:
    *(void *)uint64_t result = v2;
    *((unsigned char *)result + 8) = v4;
    return BNNS.AdamOptimizer.gradientBounds.modify;
  }
  __break(1u);
LABEL_11:
  __break(1u);
  return result;
}

uint64_t BNNS.AdamOptimizer.gradientBounds.modify(uint64_t result)
{
  uint64_t v1 = *(void *)(result + 16);
  unint64_t v2 = *(void *)result;
  int v3 = *(unsigned __int8 *)(result + 8);
  unint64_t v4 = *(void *)(v1 + 24);
  int v5 = *(unsigned __int8 *)(v1 + 52);
  if (*(unsigned char *)(v1 + 52))
  {
    int v5 = *(_DWORD *)(v1 + 48);
    unint64_t v6 = HIDWORD(v2);
    unint64_t v2 = (v2 << 32) | 1;
    BOOL v7 = v3 == 0;
    if (*(unsigned char *)(result + 8)) {
      unint64_t v8 = 0;
    }
    else {
      unint64_t v8 = v6;
    }
    if (!v7) {
      unint64_t v2 = 0;
    }
    unint64_t v9 = *(void *)(v1 + 40) & 0xFFFFFFFF00000000 | v8;
  }
  else
  {
    BOOL v7 = v3 == 0;
    uint64_t v10 = 0x100000000;
    if (!v7)
    {
      uint64_t v10 = 0;
      unint64_t v2 = 0;
    }
    unint64_t v9 = *(void *)(v1 + 40);
    unint64_t v4 = v10 & 0xFFFFFFFF00000000 | *(void *)(v1 + 24);
  }
  *(void *)(v1 + 24) = v4;
  *(void *)(v1 + 32) = v2;
  *(void *)(v1 + 40) = v9;
  *(_DWORD *)(v1 + 48) = v5;
  return result;
}

uint64_t BNNS.AdamOptimizer.regularizationFunction.getter()
{
  if (*(unsigned char *)(v0 + 52) == 1) {
    return *(unsigned int *)(v0 + 28);
  }
  else {
    return *(void *)(v0 + 40);
  }
}

uint64_t BNNS.AdamOptimizer.regularizationFunction.setter(uint64_t result)
{
  if (*(unsigned char *)(v1 + 52) == 1)
  {
    int v2 = *(_DWORD *)(v1 + 48);
    uint64_t v3 = *(void *)(v1 + 40);
    unint64_t v4 = *(void *)(v1 + 24) | ((unint64_t)result << 32);
  }
  else
  {
    int v2 = 0;
    unint64_t v4 = *(void *)(v1 + 24) & 0x1FFFFFFFFLL;
    uint64_t v3 = result;
  }
  *(void *)(v1 + 24) = v4;
  *(void *)(v1 + 40) = v3;
  *(_DWORD *)(v1 + 48) = v2;
  return result;
}

uint64_t *(*BNNS.AdamOptimizer.regularizationFunction.modify(uint64_t a1))(uint64_t *a1)
{
  *(void *)a1 = v1;
  if (*(unsigned char *)(v1 + 52) == 1) {
    LODWORD(v2) = *(_DWORD *)(v1 + 28);
  }
  else {
    uint64_t v2 = *(void *)(v1 + 40);
  }
  *(_DWORD *)(a1 + 8) = v2;
  return BNNS.AdamOptimizer.regularizationFunction.modify;
}

uint64_t *BNNS.AdamOptimizer.regularizationFunction.modify(uint64_t *a1)
{
  uint64_t v1 = *a1;
  unsigned int v4 = *((_DWORD *)a1 + 2);
  uint64_t result = a1 + 1;
  uint64_t v3 = v4;
  int v5 = *(unsigned __int8 *)(v1 + 52);
  if (*(unsigned char *)(v1 + 52))
  {
    int v5 = *(_DWORD *)(v1 + 48);
    unint64_t v6 = *(void *)(v1 + 24) | ((unint64_t)v3 << 32);
    uint64_t v3 = *(void *)(v1 + 40);
  }
  else
  {
    unint64_t v6 = *(void *)(v1 + 24) & 0x1FFFFFFFFLL;
  }
  *(void *)(v1 + 24) = v6;
  *(void *)(v1 + 40) = v3;
  *(_DWORD *)(v1 + 48) = v5;
  return result;
}

unint64_t BNNS.AdamOptimizer.init(learningRate:beta1:beta2:timeStep:epsilon:gradientScale:regularizationScale:clipsGradientsTo:regularizationFunction:)@<X0>(unint64_t result@<X0>, char a2@<W1>, uint64_t a3@<X2>, int8x16_t *a4@<X8>, unsigned int a5@<S0>, int32x2_t a6@<D1>, unsigned int a7@<S2>, __int32 a8@<S3>, unsigned int a9@<S4>, unsigned int a10@<S5>, unsigned int a11@<S6>)
{
  uint64_t v11 = HIDWORD(result);
  a6.i32[1] = a8;
  v12.i64[0] = a5;
  v12.i64[1] = a7;
  *a4 = vorrq_s8((int8x16_t)vshll_n_s32(a6, 0x20uLL), v12);
  a4[1].i64[0] = a9 | ((unint64_t)a10 << 32);
  a4[1].i64[1] = a11 | (unint64_t)(a3 << 32);
  if (a2)
  {
    uint64_t v11 = 0;
    uint64_t v13 = 0;
  }
  else
  {
    uint64_t v13 = (result << 32) | 1;
  }
  a4[2].i64[0] = v13;
  a4[2].i64[1] = v11;
  a4[3].i32[0] = 0;
  a4[3].i16[2] = 1;
  return result;
}

uint64_t BNNS.AdamOptimizer.bnnsOptimizerFunction.getter()
{
  if (*(unsigned char *)(v0 + 53)) {
    unsigned int v1 = 11;
  }
  else {
    unsigned int v1 = 8;
  }
  if (*(unsigned char *)(v0 + 52)) {
    return v1;
  }
  else {
    return 2;
  }
}

uint64_t protocol witness for BNNSOptimizer.step(parameters:gradients:accumulators:filterParameters:) in conformance BNNS.AdamOptimizer(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  long long v20 = v9[1];
  long long v21 = *v9;
  uint64_t v13 = *((void *)v9 + 4);
  uint64_t v14 = *((void *)v9 + 5);
  int v15 = *((_DWORD *)v9 + 12);
  char v16 = *((unsigned char *)v9 + 52);
  char v17 = *((unsigned char *)v9 + 53);
  vImage_Flags v22[3] = a8;
  v22[4] = a9;
  uint64_t v18 = swift_allocObject();
  v22[0] = v18;
  *(_OWORD *)(v18 + 16) = v21;
  *(_OWORD *)(v18 + 32) = v20;
  *(void *)(v18 + 48) = v13;
  *(void *)(v18 + 56) = v14;
  *(_DWORD *)(v18 + 64) = v15;
  *(unsigned char *)(v18 + 68) = v16;
  *(unsigned char *)(v18 + 69) = v17;
  specialized static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)((uint64_t)v22, a1, a2, a3);
  return __swift_destroy_boxed_opaque_existential_1((uint64_t)v22);
}

uint64_t protocol witness for BNNSOptimizer.bnnsOptimizerFunction.getter in conformance BNNS.AdamOptimizer()
{
  if (*(unsigned char *)(v0 + 53)) {
    unsigned int v1 = 11;
  }
  else {
    unsigned int v1 = 8;
  }
  if (*(unsigned char *)(v0 + 52)) {
    return v1;
  }
  else {
    return 2;
  }
}

uint64_t protocol witness for BNNSOptimizer.accumulatorCountMultiplier.getter in conformance BNNS.AdamOptimizer()
{
  if (*(unsigned char *)(v0 + 53)) {
    return 3;
  }
  else {
    return 2;
  }
}

uint64_t protocol witness for WithOptimizerAlgFields.withOptimizerAlgFields(body:) in conformance BNNS.AdamOptimizer(uint64_t (*a1)(_OWORD *))
{
  uint64_t v9 = *MEMORY[0x1E4F143B8];
  int v2 = *(_DWORD *)(v1 + 48);
  char v3 = *(unsigned char *)(v1 + 52);
  long long v4 = *(_OWORD *)(v1 + 16);
  v6[0] = *(_OWORD *)v1;
  v6[1] = v4;
  double v6[2] = *(_OWORD *)(v1 + 32);
  int v7 = v2;
  char v8 = v3;
  return a1(v6);
}

uint64_t BNNS.RMSPropOptimizer.accumulatorCountMultiplier.getter()
{
  uint64_t v1 = 1;
  if ((*(_DWORD *)(v0 + 16) & 0x7FFFFFFF) != 0) {
    uint64_t v1 = 2;
  }
  return v1 + (HIDWORD(*(void *)(v0 + 8)) & 1);
}

float BNNS.RMSPropOptimizer.momentum.getter()
{
  return *(float *)(v0 + 16);
}

uint64_t BNNS.RMSPropOptimizer.centered.getter()
{
  return *(unsigned char *)(v0 + 12) & 1;
}

float BNNS.RMSPropOptimizer.learningRate.getter()
{
  return *(float *)v0;
}

unsigned int *key path setter for BNNS.RMSPropOptimizer.learningRate : BNNS.RMSPropOptimizer(unsigned int *result, unint64_t *a2)
{
  unsigned int v2 = *result;
  unint64_t v3 = *a2;
  unint64_t v4 = a2[1];
  unint64_t v5 = a2[3];
  unint64_t v6 = a2[5];
  if ((v4 & 0x8000000000000000) != 0)
  {
    int v7 = *((_DWORD *)a2 + 12);
    unint64_t v8 = v3 & 0xFFFFFFFF00000000 | v2;
    unint64_t v9 = v4 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v7 = 0;
    unint64_t v8 = v3 & 0xFFFFFFFF00000000 | v2;
    unint64_t v9 = v4 & 0x1FFFFFFFFLL;
    v5 &= 0x1FFFFFFFFuLL;
    unint64_t v6 = a2[5];
  }
  *a2 = v8;
  a2[1] = v9;
  a2[3] = v5;
  a2[5] = v6;
  *((_DWORD *)a2 + 12) = v7;
  return result;
}

void BNNS.RMSPropOptimizer.learningRate.setter(float a1)
{
  unint64_t v2 = *v1;
  unint64_t v3 = v1[1];
  unint64_t v4 = v1[3];
  unint64_t v5 = v1[5];
  if ((v3 & 0x8000000000000000) != 0)
  {
    int v6 = *((_DWORD *)v1 + 12);
    unint64_t v7 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    unint64_t v8 = v3 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v6 = 0;
    unint64_t v7 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    unint64_t v8 = v3 & 0x1FFFFFFFFLL;
    v4 &= 0x1FFFFFFFFuLL;
    unint64_t v5 = v1[5];
  }
  unint64_t *v1 = v7;
  v1[1] = v8;
  v1[3] = v4;
  v1[5] = v5;
  *((_DWORD *)v1 + 12) = v6;
}

uint64_t **(*BNNS.RMSPropOptimizer.learningRate.modify(uint64_t a1))(uint64_t **a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *v1;
  return BNNS.RMSPropOptimizer.learningRate.modify;
}

uint64_t **BNNS.RMSPropOptimizer.learningRate.modify(uint64_t **a1)
{
  uint64_t v1 = (unint64_t *)*a1;
  unsigned int v3 = *((_DWORD *)a1 + 2);
  uint64_t result = a1 + 1;
  unsigned int v4 = v3;
  unint64_t v5 = *v1;
  unint64_t v6 = v1[1];
  unint64_t v7 = v1[3];
  unint64_t v8 = v1[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
    int v9 = *((_DWORD *)v1 + 12);
    unint64_t v10 = v5 & 0xFFFFFFFF00000000 | v4;
    unint64_t v11 = v6 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v9 = 0;
    unint64_t v10 = v5 & 0xFFFFFFFF00000000 | v4;
    unint64_t v11 = v6 & 0x1FFFFFFFFLL;
    v7 &= 0x1FFFFFFFFuLL;
    unint64_t v8 = v1[5];
  }
  unint64_t *v1 = v10;
  v1[1] = v11;
  v1[3] = v7;
  v1[5] = v8;
  *((_DWORD *)v1 + 12) = v9;
  return result;
}

float BNNS.RMSPropOptimizer.alpha.getter()
{
  return *(float *)(v0 + 4);
}

unsigned int *key path setter for BNNS.RMSPropOptimizer.alpha : BNNS.RMSPropOptimizer(unsigned int *result, unsigned int *a2)
{
  unsigned int v2 = *result;
  uint64_t v3 = *a2;
  uint64_t v4 = *((void *)a2 + 1);
  uint64_t v5 = *((void *)a2 + 3);
  uint64_t v6 = *((void *)a2 + 5);
  if (v4 < 0)
  {
    unsigned int v7 = a2[12];
    unint64_t v8 = v3 | ((unint64_t)v2 << 32);
    unint64_t v9 = v4 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    unsigned int v7 = 0;
    unint64_t v8 = v3 | ((unint64_t)v2 << 32);
    unint64_t v9 = v4 & 0x1FFFFFFFFLL;
    v5 &= 0x1FFFFFFFFuLL;
    uint64_t v6 = *((void *)a2 + 5);
  }
  *(void *)a2 = v8;
  *((void *)a2 + 1) = v9;
  *((void *)a2 + 3) = v5;
  *((void *)a2 + 5) = v6;
  a2[12] = v7;
  return result;
}

void BNNS.RMSPropOptimizer.alpha.setter(float a1)
{
  uint64_t v2 = *v1;
  uint64_t v3 = *((void *)v1 + 1);
  uint64_t v4 = *((void *)v1 + 3);
  uint64_t v5 = *((void *)v1 + 5);
  if (v3 < 0)
  {
    unsigned int v6 = v1[12];
    unint64_t v7 = v2 | ((unint64_t)LODWORD(a1) << 32);
    unint64_t v8 = v3 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    unsigned int v6 = 0;
    unint64_t v7 = v2 | ((unint64_t)LODWORD(a1) << 32);
    unint64_t v8 = v3 & 0x1FFFFFFFFLL;
    v4 &= 0x1FFFFFFFFuLL;
    uint64_t v5 = *((void *)v1 + 5);
  }
  *(void *)uint64_t v1 = v7;
  *((void *)v1 + 1) = v8;
  *((void *)v1 + 3) = v4;
  *((void *)v1 + 5) = v5;
  v1[12] = v6;
}

unsigned int **(*BNNS.RMSPropOptimizer.alpha.modify(uint64_t a1))(unsigned int **a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 4);
  return BNNS.RMSPropOptimizer.alpha.modify;
}

unsigned int **BNNS.RMSPropOptimizer.alpha.modify(unsigned int **a1)
{
  uint64_t v1 = *a1;
  unsigned int v3 = *((_DWORD *)a1 + 2);
  uint64_t result = a1 + 1;
  unsigned int v4 = v3;
  uint64_t v5 = *v1;
  uint64_t v6 = *((void *)v1 + 1);
  uint64_t v7 = *((void *)v1 + 3);
  uint64_t v8 = *((void *)v1 + 5);
  if (v6 < 0)
  {
    unsigned int v9 = v1[12];
    unint64_t v10 = v5 | ((unint64_t)v4 << 32);
    unint64_t v11 = v6 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    unsigned int v9 = 0;
    unint64_t v10 = v5 | ((unint64_t)v4 << 32);
    unint64_t v11 = v6 & 0x1FFFFFFFFLL;
    v7 &= 0x1FFFFFFFFuLL;
    uint64_t v8 = *((void *)v1 + 5);
  }
  *(void *)uint64_t v1 = v10;
  *((void *)v1 + 1) = v11;
  *((void *)v1 + 3) = v7;
  *((void *)v1 + 5) = v8;
  v1[12] = v9;
  return result;
}

float BNNS.RMSPropOptimizer.epsilon.getter()
{
  return *(float *)(v0 + 8);
}

void BNNS.RMSPropOptimizer.epsilon.setter(float a1)
{
  uint64_t v2 = *(void *)(v1 + 8);
  uint64_t v3 = *(void *)(v1 + 24);
  uint64_t v4 = *(void *)(v1 + 40);
  if (v2 < 0)
  {
    int v5 = *(_DWORD *)(v1 + 48);
    unint64_t v6 = v2 & 0x100000000 | LODWORD(a1) | 0x8000000000000000;
  }
  else
  {
    int v5 = 0;
    unint64_t v6 = v2 & 0x100000000 | LODWORD(a1);
    v3 &= 0x1FFFFFFFFuLL;
    uint64_t v4 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 8) = v6;
  *(void *)(v1 + 24) = v3;
  *(void *)(v1 + 40) = v4;
  *(_DWORD *)(v1 + 48) = v5;
}

uint64_t *(*BNNS.RMSPropOptimizer.epsilon.modify(uint64_t a1))(uint64_t *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(void *)(v1 + 8);
  return BNNS.RMSPropOptimizer.epsilon.modify;
}

uint64_t *BNNS.RMSPropOptimizer.epsilon.modify(uint64_t *a1)
{
  uint64_t v1 = *a1;
  unsigned int v3 = *((_DWORD *)a1 + 2);
  uint64_t result = a1 + 1;
  unsigned int v4 = v3;
  uint64_t v5 = *(void *)(v1 + 8);
  uint64_t v6 = *(void *)(v1 + 24);
  uint64_t v7 = *(void *)(v1 + 40);
  if (v5 < 0)
  {
    int v8 = *(_DWORD *)(v1 + 48);
    unint64_t v9 = v5 & 0x100000000 | v4 | 0x8000000000000000;
  }
  else
  {
    int v8 = 0;
    unint64_t v9 = v5 & 0x100000000 | v4;
    v6 &= 0x1FFFFFFFFuLL;
    uint64_t v7 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 8) = v9;
  *(void *)(v1 + 24) = v6;
  *(void *)(v1 + 40) = v7;
  *(_DWORD *)(v1 + 48) = v8;
  return result;
}

uint64_t BNNS.RMSPropOptimizer.centered.setter(uint64_t result)
{
  uint64_t v2 = *(void *)(v1 + 24);
  uint64_t v3 = *(void *)(v1 + 40);
  if ((*(void *)(v1 + 8) & 0x8000000000000000) != 0)
  {
    int v4 = *(_DWORD *)(v1 + 48);
    unint64_t v7 = 0x8000000000000000;
    if (result) {
      unint64_t v7 = 0x8000000100000000;
    }
    unint64_t v6 = v7 & 0xFFFFFFFF00000000 | *(void *)(v1 + 8);
  }
  else
  {
    int v4 = 0;
    uint64_t v5 = 0x100000000;
    if ((result & 1) == 0) {
      uint64_t v5 = 0;
    }
    unint64_t v6 = v5 & 0xFFFFFFFF00000000 | *(void *)(v1 + 8);
    v2 &= 0x1FFFFFFFFuLL;
    uint64_t v3 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 8) = v6;
  *(void *)(v1 + 24) = v2;
  *(void *)(v1 + 40) = v3;
  *(_DWORD *)(v1 + 48) = v4;
  return result;
}

unsigned __int8 *(*BNNS.RMSPropOptimizer.centered.modify(uint64_t a1))(unsigned __int8 *result)
{
  *(void *)a1 = v1;
  *(unsigned char *)(a1 + 8) = *(unsigned char *)(v1 + 12) & 1;
  return BNNS.RMSPropOptimizer.centered.modify;
}

unsigned __int8 *BNNS.RMSPropOptimizer.centered.modify(unsigned __int8 *result)
{
  uint64_t v1 = *(void *)result;
  int v2 = result[8];
  uint64_t v3 = *(void *)(*(void *)result + 24);
  uint64_t v4 = *(void *)(*(void *)result + 40);
  if ((*(void *)(*(void *)result + 8) & 0x8000000000000000) != 0)
  {
    int v5 = *(_DWORD *)(v1 + 48);
    BOOL v6 = v2 == 0;
    unint64_t v9 = 0x8000000000000000;
    if (!v6) {
      unint64_t v9 = 0x8000000100000000;
    }
    unint64_t v8 = v9 & 0xFFFFFFFF00000000 | *(void *)(*(void *)result + 8);
  }
  else
  {
    int v5 = 0;
    BOOL v6 = v2 == 0;
    uint64_t v7 = 0x100000000;
    if (v6) {
      uint64_t v7 = 0;
    }
    unint64_t v8 = v7 & 0xFFFFFFFF00000000 | *(void *)(*(void *)result + 8);
    v3 &= 0x1FFFFFFFFuLL;
    uint64_t v4 = *(void *)(*(void *)result + 40);
  }
  *(void *)(v1 + 8) = v8;
  *(void *)(v1 + 24) = v3;
  *(void *)(v1 + 40) = v4;
  *(_DWORD *)(v1 + 48) = v5;
  return result;
}

unsigned int *key path setter for BNNS.RMSPropOptimizer.momentum : BNNS.RMSPropOptimizer(unsigned int *result, uint64_t a2)
{
  unsigned int v2 = *result;
  uint64_t v4 = *(void *)(a2 + 8);
  uint64_t v3 = *(void *)(a2 + 16);
  uint64_t v5 = *(void *)(a2 + 24);
  uint64_t v6 = *(void *)(a2 + 40);
  if (v4 < 0)
  {
    int v7 = *(_DWORD *)(a2 + 48);
    unint64_t v10 = v3 & 0xFFFFFFFF00000000 | v2;
    unint64_t v9 = v4 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v7 = 0;
    unint64_t v8 = v3 & 0xFFFFFFFF00000000;
    unint64_t v9 = v4 & 0x1FFFFFFFFLL;
    unint64_t v10 = v8 | v2;
    v5 &= 0x1FFFFFFFFuLL;
    uint64_t v6 = *(void *)(a2 + 40);
  }
  *(void *)(a2 + 8) = v9;
  *(void *)(a2 + 16) = v10;
  *(void *)(a2 + 24) = v5;
  *(void *)(a2 + 40) = v6;
  *(_DWORD *)(a2 + 48) = v7;
  return result;
}

void BNNS.RMSPropOptimizer.momentum.setter(float a1)
{
  uint64_t v3 = *(void *)(v1 + 8);
  uint64_t v2 = *(void *)(v1 + 16);
  uint64_t v4 = *(void *)(v1 + 24);
  uint64_t v5 = *(void *)(v1 + 40);
  if (v3 < 0)
  {
    int v6 = *(_DWORD *)(v1 + 48);
    unint64_t v9 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    unint64_t v8 = v3 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v6 = 0;
    unint64_t v7 = v2 & 0xFFFFFFFF00000000;
    unint64_t v8 = v3 & 0x1FFFFFFFFLL;
    unint64_t v9 = v7 | LODWORD(a1);
    v4 &= 0x1FFFFFFFFuLL;
    uint64_t v5 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 8) = v8;
  *(void *)(v1 + 16) = v9;
  *(void *)(v1 + 24) = v4;
  *(void *)(v1 + 40) = v5;
  *(_DWORD *)(v1 + 48) = v6;
}

uint64_t *(*BNNS.RMSPropOptimizer.momentum.modify(uint64_t a1))(uint64_t *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(void *)(v1 + 16);
  return BNNS.RMSPropOptimizer.momentum.modify;
}

uint64_t *BNNS.RMSPropOptimizer.momentum.modify(uint64_t *a1)
{
  uint64_t v1 = *a1;
  unsigned int v3 = *((_DWORD *)a1 + 2);
  uint64_t result = a1 + 1;
  unsigned int v4 = v3;
  uint64_t v6 = *(void *)(v1 + 8);
  uint64_t v5 = *(void *)(v1 + 16);
  uint64_t v7 = *(void *)(v1 + 24);
  uint64_t v8 = *(void *)(v1 + 40);
  if (v6 < 0)
  {
    int v9 = *(_DWORD *)(v1 + 48);
    unint64_t v12 = v5 & 0xFFFFFFFF00000000 | v4;
    unint64_t v11 = v6 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v9 = 0;
    unint64_t v10 = v5 & 0xFFFFFFFF00000000;
    unint64_t v11 = v6 & 0x1FFFFFFFFLL;
    unint64_t v12 = v10 | v4;
    v7 &= 0x1FFFFFFFFuLL;
    uint64_t v8 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 8) = v11;
  *(void *)(v1 + 16) = v12;
  *(void *)(v1 + 24) = v7;
  *(void *)(v1 + 40) = v8;
  *(_DWORD *)(v1 + 48) = v9;
  return result;
}

float BNNS.RMSPropOptimizer.gradientScale.getter()
{
  return *(float *)(v0 + 20);
}

unsigned int *key path setter for BNNS.RMSPropOptimizer.gradientScale : BNNS.RMSPropOptimizer(unsigned int *result, uint64_t a2)
{
  unsigned int v2 = *result;
  uint64_t v3 = *(void *)(a2 + 8);
  uint64_t v4 = *(unsigned int *)(a2 + 16);
  uint64_t v5 = *(void *)(a2 + 24);
  uint64_t v6 = *(void *)(a2 + 40);
  if (v3 < 0)
  {
    int v7 = *(_DWORD *)(a2 + 48);
    unint64_t v9 = v4 | ((unint64_t)v2 << 32);
    unint64_t v8 = v3 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v7 = 0;
    unint64_t v8 = v3 & 0x1FFFFFFFFLL;
    unint64_t v9 = v4 | ((unint64_t)v2 << 32);
    v5 &= 0x1FFFFFFFFuLL;
    uint64_t v6 = *(void *)(a2 + 40);
  }
  *(void *)(a2 + 8) = v8;
  *(void *)(a2 + 16) = v9;
  *(void *)(a2 + 24) = v5;
  *(void *)(a2 + 40) = v6;
  *(_DWORD *)(a2 + 48) = v7;
  return result;
}

void BNNS.RMSPropOptimizer.gradientScale.setter(float a1)
{
  uint64_t v2 = *(void *)(v1 + 8);
  uint64_t v3 = *(unsigned int *)(v1 + 16);
  uint64_t v4 = *(void *)(v1 + 24);
  uint64_t v5 = *(void *)(v1 + 40);
  if (v2 < 0)
  {
    int v6 = *(_DWORD *)(v1 + 48);
    unint64_t v8 = v3 | ((unint64_t)LODWORD(a1) << 32);
    unint64_t v7 = v2 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v6 = 0;
    unint64_t v7 = v2 & 0x1FFFFFFFFLL;
    unint64_t v8 = v3 | ((unint64_t)LODWORD(a1) << 32);
    v4 &= 0x1FFFFFFFFuLL;
    uint64_t v5 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 8) = v7;
  *(void *)(v1 + 16) = v8;
  *(void *)(v1 + 24) = v4;
  *(void *)(v1 + 40) = v5;
  *(_DWORD *)(v1 + 48) = v6;
}

uint64_t *(*BNNS.RMSPropOptimizer.gradientScale.modify(uint64_t a1))(uint64_t *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 20);
  return BNNS.RMSPropOptimizer.gradientScale.modify;
}

uint64_t *BNNS.RMSPropOptimizer.gradientScale.modify(uint64_t *a1)
{
  uint64_t v1 = *a1;
  unsigned int v3 = *((_DWORD *)a1 + 2);
  uint64_t result = a1 + 1;
  unsigned int v4 = v3;
  uint64_t v5 = *(void *)(v1 + 8);
  uint64_t v6 = *(unsigned int *)(v1 + 16);
  uint64_t v7 = *(void *)(v1 + 24);
  uint64_t v8 = *(void *)(v1 + 40);
  if (v5 < 0)
  {
    int v9 = *(_DWORD *)(v1 + 48);
    unint64_t v11 = v6 | ((unint64_t)v4 << 32);
    unint64_t v10 = v5 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v9 = 0;
    unint64_t v10 = v5 & 0x1FFFFFFFFLL;
    unint64_t v11 = v6 | ((unint64_t)v4 << 32);
    v7 &= 0x1FFFFFFFFuLL;
    uint64_t v8 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 8) = v10;
  *(void *)(v1 + 16) = v11;
  *(void *)(v1 + 24) = v7;
  *(void *)(v1 + 40) = v8;
  *(_DWORD *)(v1 + 48) = v9;
  return result;
}

float BNNS.RMSPropOptimizer.regularizationScale.getter()
{
  return *(float *)(v0 + 24);
}

void BNNS.RMSPropOptimizer.regularizationScale.setter(float a1)
{
  uint64_t v2 = *(void *)(v1 + 8);
  uint64_t v3 = *(void *)(v1 + 24);
  uint64_t v4 = *(void *)(v1 + 40);
  if (v2 < 0)
  {
    int v5 = *(_DWORD *)(v1 + 48);
    unint64_t v7 = v3 & 0xFFFFFFFF00000000 | LODWORD(a1);
    unint64_t v6 = v2 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v5 = 0;
    unint64_t v6 = v2 & 0x1FFFFFFFFLL;
    unint64_t v7 = v3 & 0x100000000 | LODWORD(a1);
    uint64_t v4 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 8) = v6;
  *(void *)(v1 + 24) = v7;
  *(void *)(v1 + 40) = v4;
  *(_DWORD *)(v1 + 48) = v5;
}

uint64_t *(*BNNS.RMSPropOptimizer.regularizationScale.modify(uint64_t a1))(uint64_t *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(void *)(v1 + 24);
  return BNNS.RMSPropOptimizer.regularizationScale.modify;
}

uint64_t *BNNS.RMSPropOptimizer.regularizationScale.modify(uint64_t *a1)
{
  uint64_t v1 = *a1;
  unsigned int v3 = *((_DWORD *)a1 + 2);
  uint64_t result = a1 + 1;
  unsigned int v4 = v3;
  uint64_t v5 = *(void *)(v1 + 8);
  uint64_t v6 = *(void *)(v1 + 24);
  uint64_t v7 = *(void *)(v1 + 40);
  if (v5 < 0)
  {
    int v8 = *(_DWORD *)(v1 + 48);
    unint64_t v10 = v6 & 0xFFFFFFFF00000000 | v4;
    unint64_t v9 = v5 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v8 = 0;
    unint64_t v9 = v5 & 0x1FFFFFFFFLL;
    unint64_t v10 = v6 & 0x100000000 | v4;
    uint64_t v7 = *(void *)(v1 + 40);
  }
  *(void *)(v1 + 8) = v9;
  *(void *)(v1 + 24) = v10;
  *(void *)(v1 + 40) = v7;
  *(_DWORD *)(v1 + 48) = v8;
  return result;
}

uint64_t BNNS.RMSPropOptimizer.gradientBounds.getter()
{
  uint64_t result = *(void *)(v0 + 32);
  if ((*(void *)(v0 + 8) & 0x8000000000000000) != 0)
  {
    if (result == 1)
    {
      if (*((float *)&result + 1) <= COERCE_FLOAT(*(void *)(v0 + 40))) {
        return *(long long *)(v0 + 32) >> 32;
      }
      goto LABEL_10;
    }
    return 0;
  }
  if ((*(unsigned char *)(v0 + 28) & 1) == 0) {
    return 0;
  }
  if (*(float *)&result > *((float *)&result + 1))
  {
    __break(1u);
LABEL_10:
    __break(1u);
  }
  return result;
}

uint64_t key path getter for BNNS.RMSPropOptimizer.gradientBounds : BNNS.RMSPropOptimizer@<X0>(uint64_t a1@<X8>)
{
  uint64_t result = BNNS.RMSPropOptimizer.gradientBounds.getter();
  *(void *)a1 = result;
  *(unsigned char *)(a1 + 8) = v3 & 1;
  return result;
}

uint64_t key path setter for BNNS.RMSPropOptimizer.gradientBounds : BNNS.RMSPropOptimizer(uint64_t result, uint64_t a2)
{
  unint64_t v2 = *(void *)result;
  uint64_t v3 = *(void *)(a2 + 8);
  unint64_t v4 = *(void *)(a2 + 24);
  if (v3 < 0)
  {
    int v5 = *(_DWORD *)(a2 + 48);
    unint64_t v9 = HIDWORD(v2);
    unint64_t v2 = (v2 << 32) | 1;
    if (*(unsigned char *)(result + 8)) {
      unint64_t v10 = 0;
    }
    else {
      unint64_t v10 = v9;
    }
    if (*(unsigned char *)(result + 8)) {
      unint64_t v2 = 0;
    }
    unint64_t v8 = *(void *)(a2 + 40) & 0xFFFFFFFF00000000 | v10;
    unint64_t v7 = v3 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v5 = 0;
    if (*(unsigned char *)(result + 8)) {
      uint64_t v6 = 0;
    }
    else {
      uint64_t v6 = 0x100000000;
    }
    if (*(unsigned char *)(result + 8)) {
      unint64_t v2 = 0;
    }
    unint64_t v7 = v3 & 0x1FFFFFFFFLL;
    unint64_t v8 = *(void *)(a2 + 40);
    unint64_t v4 = v6 & 0xFFFFFFFF00000000 | *(void *)(a2 + 24);
  }
  *(void *)(a2 + 8) = v7;
  *(void *)(a2 + 24) = v4;
  *(void *)(a2 + 32) = v2;
  *(void *)(a2 + 40) = v8;
  *(_DWORD *)(a2 + 48) = v5;
  return result;
}

unint64_t BNNS.RMSPropOptimizer.gradientBounds.setter(unint64_t result, char a2)
{
  uint64_t v3 = a2 & 1;
  uint64_t v4 = *(void *)(v2 + 8);
  uint64_t v5 = *(void *)(v2 + 24);
  if (v4 < 0)
  {
    int v6 = *(_DWORD *)(v2 + 48);
    BOOL v10 = v3 == 0;
    if (a2) {
      unint64_t v11 = 0;
    }
    else {
      unint64_t v11 = HIDWORD(result);
    }
    if (v10) {
      uint64_t v7 = (result << 32) | 1;
    }
    else {
      uint64_t v7 = 0;
    }
    unint64_t v9 = *(void *)(v2 + 40) & 0xFFFFFFFF00000000 | v11;
    unint64_t v8 = v4 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v6 = 0;
    if (a2) {
      uint64_t v7 = 0;
    }
    else {
      uint64_t v7 = result;
    }
    unint64_t v8 = v4 & 0x1FFFFFFFFLL;
    uint64_t v5 = (*(void *)(v2 + 24) | (unint64_t)(v3 << 32)) ^ 0x100000000;
    unint64_t v9 = *(void *)(v2 + 40);
  }
  *(void *)(v2 + 8) = v8;
  *(void *)(v2 + 24) = v5;
  *(void *)(v2 + 32) = v7;
  *(void *)(v2 + 40) = v9;
  *(_DWORD *)(v2 + 48) = v6;
  return result;
}

uint64_t (*BNNS.RMSPropOptimizer.gradientBounds.modify(uint64_t (*result)(uint64_t result)))(uint64_t result)
{
  *((void *)result + 2) = v1;
  unint64_t v2 = *(void *)(v1 + 32);
  if ((*(void *)(v1 + 8) & 0x8000000000000000) == 0)
  {
    if (*(unsigned char *)(v1 + 28))
    {
      if (*(float *)&v2 <= *((float *)&v2 + 1))
      {
        char v3 = 0;
LABEL_9:
        *(void *)uint64_t result = v2;
        *((unsigned char *)result + 8) = v3;
        return BNNS.RMSPropOptimizer.gradientBounds.modify;
      }
      __break(1u);
      goto LABEL_11;
    }
LABEL_8:
    unint64_t v2 = 0;
    char v3 = 1;
    goto LABEL_9;
  }
  if (v2 != 1) {
    goto LABEL_8;
  }
  unint64_t v4 = HIDWORD(v2);
  if (*(float *)&v4 <= COERCE_FLOAT(*(void *)(v1 + 40)))
  {
    char v3 = 0;
    unint64_t v2 = v4 | (*(void *)(v1 + 40) << 32);
    goto LABEL_9;
  }
LABEL_11:
  __break(1u);
  return result;
}

uint64_t BNNS.RMSPropOptimizer.gradientBounds.modify(uint64_t result)
{
  uint64_t v1 = *(void *)(result + 16);
  unint64_t v2 = *(void *)result;
  uint64_t v3 = *(void *)(v1 + 8);
  unint64_t v4 = *(void *)(v1 + 24);
  if (v3 < 0)
  {
    int v5 = *(_DWORD *)(v1 + 48);
    unint64_t v9 = HIDWORD(v2);
    unint64_t v2 = (v2 << 32) | 1;
    if (*(unsigned char *)(result + 8)) {
      unint64_t v10 = 0;
    }
    else {
      unint64_t v10 = v9;
    }
    if (*(unsigned char *)(result + 8)) {
      unint64_t v2 = 0;
    }
    unint64_t v8 = *(void *)(v1 + 40) & 0xFFFFFFFF00000000 | v10;
    unint64_t v7 = v3 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v5 = 0;
    if (*(unsigned char *)(result + 8)) {
      uint64_t v6 = 0;
    }
    else {
      uint64_t v6 = 0x100000000;
    }
    if (*(unsigned char *)(result + 8)) {
      unint64_t v2 = 0;
    }
    unint64_t v7 = v3 & 0x1FFFFFFFFLL;
    unint64_t v8 = *(void *)(v1 + 40);
    unint64_t v4 = v6 & 0xFFFFFFFF00000000 | *(void *)(v1 + 24);
  }
  *(void *)(v1 + 8) = v7;
  *(void *)(v1 + 24) = v4;
  *(void *)(v1 + 32) = v2;
  *(void *)(v1 + 40) = v8;
  *(_DWORD *)(v1 + 48) = v5;
  return result;
}

uint64_t BNNS.RMSPropOptimizer.regularizationFunction.getter()
{
  if ((*(void *)(v0 + 8) & 0x8000000000000000) != 0) {
    return *(unsigned int *)(v0 + 28);
  }
  else {
    return *(void *)(v0 + 40);
  }
}

uint64_t BNNS.RMSPropOptimizer.regularizationFunction.setter(uint64_t result)
{
  uint64_t v2 = *(void *)(v1 + 8);
  if (v2 < 0)
  {
    int v3 = *(_DWORD *)(v1 + 48);
    uint64_t v6 = *(void *)(v1 + 40);
    unint64_t v5 = *(void *)(v1 + 24) | ((unint64_t)result << 32);
    unint64_t v4 = v2 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v3 = 0;
    unint64_t v4 = v2 & 0x1FFFFFFFFLL;
    unint64_t v5 = *(void *)(v1 + 24) & 0x1FFFFFFFFLL;
    uint64_t v6 = result;
  }
  *(void *)(v1 + 8) = v4;
  *(void *)(v1 + 24) = v5;
  *(void *)(v1 + 40) = v6;
  *(_DWORD *)(v1 + 48) = v3;
  return result;
}

uint64_t *(*BNNS.RMSPropOptimizer.regularizationFunction.modify(uint64_t a1))(uint64_t *a1)
{
  *(void *)a1 = v1;
  if ((*(void *)(v1 + 8) & 0x8000000000000000) != 0) {
    LODWORD(v2) = *(_DWORD *)(v1 + 28);
  }
  else {
    uint64_t v2 = *(void *)(v1 + 40);
  }
  *(_DWORD *)(a1 + 8) = v2;
  return BNNS.RMSPropOptimizer.regularizationFunction.modify;
}

uint64_t *BNNS.RMSPropOptimizer.regularizationFunction.modify(uint64_t *a1)
{
  uint64_t v1 = *a1;
  unsigned int v4 = *((_DWORD *)a1 + 2);
  uint64_t result = a1 + 1;
  uint64_t v3 = v4;
  uint64_t v5 = *(void *)(v1 + 8);
  if (v5 < 0)
  {
    int v6 = *(_DWORD *)(v1 + 48);
    unint64_t v8 = *(void *)(v1 + 24) | ((unint64_t)v3 << 32);
    uint64_t v3 = *(void *)(v1 + 40);
    unint64_t v7 = v5 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v6 = 0;
    unint64_t v7 = v5 & 0x1FFFFFFFFLL;
    unint64_t v8 = *(void *)(v1 + 24) & 0x1FFFFFFFFLL;
  }
  *(void *)(v1 + 8) = v7;
  *(void *)(v1 + 24) = v8;
  *(void *)(v1 + 40) = v3;
  *(_DWORD *)(v1 + 48) = v6;
  return result;
}

uint64_t BNNS.RMSPropOptimizer.init(learningRate:alpha:epsilon:centered:momentum:gradientScale:regularizationScale:clipsGradientsTo:regularizationFunction:)@<X0>(uint64_t result@<X0>, unint64_t a2@<X1>, char a3@<W2>, uint64_t a4@<X3>, uint64_t a5@<X8>, unsigned int a6@<S0>, unsigned int a7@<S1>, unsigned int a8@<S2>, unsigned int a9@<S3>, unsigned int a10@<S4>, unsigned int a11@<S5>)
{
  unint64_t v11 = HIDWORD(a2);
  uint64_t v12 = (a2 << 32) | 1;
  if (a3)
  {
    unint64_t v11 = 0;
    uint64_t v12 = 0;
  }
  uint64_t v13 = 0x100000000;
  if ((result & 1) == 0) {
    uint64_t v13 = 0;
  }
  *(void *)a5 = a6 | ((unint64_t)a7 << 32);
  *(void *)(a5 + 8) = v13 | a8 | 0x8000000000000000;
  *(void *)(a5 + 16) = a9 | ((unint64_t)a10 << 32);
  *(void *)(a5 + 24) = a11 | (unint64_t)(a4 << 32);
  *(void *)(a5 + 32) = v12;
  *(void *)(a5 + 40) = v11;
  *(_DWORD *)(a5 + 48) = 0;
  return result;
}

uint64_t BNNS.RMSPropOptimizer.bnnsOptimizerFunction.getter()
{
  if (*(uint64_t *)(v0 + 8) < 0) {
    return 9;
  }
  else {
    return 3;
  }
}

uint64_t protocol witness for BNNSOptimizer.step(parameters:gradients:accumulators:filterParameters:) in conformance BNNS.RMSPropOptimizer(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  long long v18 = v9[1];
  long long v19 = *v9;
  uint64_t v13 = *((void *)v9 + 4);
  uint64_t v14 = *((void *)v9 + 5);
  int v15 = *((_DWORD *)v9 + 12);
  v20[3] = a8;
  uint16_t v20[4] = a9;
  uint64_t v16 = swift_allocObject();
  v20[0] = v16;
  *(_OWORD *)(v16 + 16) = v19;
  *(_OWORD *)(v16 + 32) = v18;
  *(void *)(v16 + 48) = v13;
  *(void *)(v16 + 56) = v14;
  *(_DWORD *)(v16 + 64) = v15;
  specialized static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)((uint64_t)v20, a1, a2, a3);
  return __swift_destroy_boxed_opaque_existential_1((uint64_t)v20);
}

uint64_t protocol witness for BNNSOptimizer.bnnsOptimizerFunction.getter in conformance BNNS.RMSPropOptimizer()
{
  if (*(uint64_t *)(v0 + 8) < 0) {
    return 9;
  }
  else {
    return 3;
  }
}

uint64_t protocol witness for BNNSOptimizer.accumulatorCountMultiplier.getter in conformance BNNS.RMSPropOptimizer()
{
  uint64_t v1 = 1;
  if ((*(_DWORD *)(v0 + 16) & 0x7FFFFFFF) != 0) {
    uint64_t v1 = 2;
  }
  return v1 + (HIDWORD(*(void *)(v0 + 8)) & 1);
}

uint64_t protocol witness for WithOptimizerAlgFields.withOptimizerAlgFields(body:) in conformance BNNS.RMSPropOptimizer(uint64_t (*a1)(_OWORD *))
{
  uint64_t v7 = *MEMORY[0x1E4F143B8];
  int v2 = *(_DWORD *)(v1 + 48);
  long long v3 = *(_OWORD *)(v1 + 16);
  v5[0] = *(_OWORD *)v1;
  v5[1] = v3;
  v5[2] = *(_OWORD *)(v1 + 32);
  int v6 = v2;
  return a1(v5);
}

BOOL BNNS.SGDMomentumOptimizer.accumulatorCountMultiplier.getter()
{
  return (*(_DWORD *)(v0 + 4) & 0x7FFFFFFF) != 0;
}

float BNNS.SGDMomentumOptimizer.momentum.getter()
{
  return *(float *)(v0 + 4);
}

float BNNS.SGDMomentumOptimizer.learningRate.getter()
{
  return *(float *)v0;
}

unsigned int *key path setter for BNNS.SGDMomentumOptimizer.learningRate : BNNS.SGDMomentumOptimizer(unsigned int *result, unint64_t *a2)
{
  unsigned int v2 = *result;
  unint64_t v3 = *a2;
  unint64_t v5 = a2[2];
  unint64_t v4 = a2[3];
  if ((v5 & 0x80000000) != 0)
  {
    unint64_t v6 = a2[5];
    unint64_t v7 = v3 & 0xFFFFFFFF00000000 | v2;
    unint64_t v8 = v5 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    unint64_t v6 = 0;
    unint64_t v7 = v3 & 0xFFFFFFFF00000000 | v2;
    unint64_t v8 = v5 & 0xFFFFFFFF00000001;
    v4 &= 0x1FFFFFFFFuLL;
  }
  *a2 = v7;
  a2[2] = v8;
  a2[3] = v4;
  a2[5] = v6;
  return result;
}

void BNNS.SGDMomentumOptimizer.learningRate.setter(float a1)
{
  unint64_t v2 = *v1;
  unint64_t v4 = v1[2];
  unint64_t v3 = v1[3];
  if ((v4 & 0x80000000) != 0)
  {
    unint64_t v5 = v1[5];
    unint64_t v6 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    unint64_t v7 = v4 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    unint64_t v5 = 0;
    unint64_t v6 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    unint64_t v7 = v4 & 0xFFFFFFFF00000001;
    v3 &= 0x1FFFFFFFFuLL;
  }
  unint64_t *v1 = v6;
  v1[2] = v7;
  v1[3] = v3;
  v1[5] = v5;
}

uint64_t (*BNNS.SGDMomentumOptimizer.learningRate.modify(uint64_t a1))(uint64_t a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *v1;
  return BNNS.SGDMomentumOptimizer.learningRate.modify;
}

uint64_t BNNS.SGDMomentumOptimizer.learningRate.modify(uint64_t a1)
{
  uint64_t v1 = *(unint64_t **)a1;
  unsigned int v3 = *(_DWORD *)(a1 + 8);
  uint64_t result = a1 + 8;
  unsigned int v4 = v3;
  unint64_t v5 = *v1;
  unint64_t v7 = v1[2];
  unint64_t v6 = v1[3];
  if ((v7 & 0x80000000) != 0)
  {
    unint64_t v8 = v1[5];
    unint64_t v9 = v5 & 0xFFFFFFFF00000000 | v4;
    unint64_t v10 = v7 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    unint64_t v8 = 0;
    unint64_t v9 = v5 & 0xFFFFFFFF00000000 | v4;
    unint64_t v10 = v7 & 0xFFFFFFFF00000001;
    v6 &= 0x1FFFFFFFFuLL;
  }
  unint64_t *v1 = v9;
  v1[2] = v10;
  v1[3] = v6;
  v1[5] = v8;
  return result;
}

unsigned int *key path setter for BNNS.SGDMomentumOptimizer.momentum : BNNS.SGDMomentumOptimizer(unsigned int *result, unint64_t *a2)
{
  unsigned int v2 = *result;
  uint64_t v3 = *(unsigned int *)a2;
  unint64_t v5 = a2[2];
  unint64_t v4 = a2[3];
  if ((v5 & 0x80000000) != 0)
  {
    unint64_t v6 = a2[5];
    unint64_t v7 = v3 | ((unint64_t)v2 << 32);
    unint64_t v8 = v5 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    unint64_t v6 = 0;
    unint64_t v7 = v3 | ((unint64_t)v2 << 32);
    unint64_t v8 = v5 & 0xFFFFFFFF00000001;
    v4 &= 0x1FFFFFFFFuLL;
  }
  *a2 = v7;
  a2[2] = v8;
  a2[3] = v4;
  a2[5] = v6;
  return result;
}

void BNNS.SGDMomentumOptimizer.momentum.setter(float a1)
{
  uint64_t v2 = *(unsigned int *)v1;
  unint64_t v4 = v1[2];
  unint64_t v3 = v1[3];
  if ((v4 & 0x80000000) != 0)
  {
    unint64_t v5 = v1[5];
    unint64_t v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
    unint64_t v7 = v4 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    unint64_t v5 = 0;
    unint64_t v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
    unint64_t v7 = v4 & 0xFFFFFFFF00000001;
    v3 &= 0x1FFFFFFFFuLL;
  }
  unint64_t *v1 = v6;
  v1[2] = v7;
  v1[3] = v3;
  v1[5] = v5;
}

uint64_t (*BNNS.SGDMomentumOptimizer.momentum.modify(uint64_t a1))(uint64_t a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 4);
  return BNNS.SGDMomentumOptimizer.momentum.modify;
}

uint64_t BNNS.SGDMomentumOptimizer.momentum.modify(uint64_t a1)
{
  uint64_t v1 = *(unint64_t **)a1;
  unsigned int v3 = *(_DWORD *)(a1 + 8);
  uint64_t result = a1 + 8;
  unsigned int v4 = v3;
  uint64_t v5 = *(unsigned int *)v1;
  unint64_t v7 = v1[2];
  unint64_t v6 = v1[3];
  if ((v7 & 0x80000000) != 0)
  {
    unint64_t v8 = v1[5];
    unint64_t v9 = v5 | ((unint64_t)v4 << 32);
    unint64_t v10 = v7 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    unint64_t v8 = 0;
    unint64_t v9 = v5 | ((unint64_t)v4 << 32);
    unint64_t v10 = v7 & 0xFFFFFFFF00000001;
    v6 &= 0x1FFFFFFFFuLL;
  }
  unint64_t *v1 = v9;
  v1[2] = v10;
  v1[3] = v6;
  v1[5] = v8;
  return result;
}

float BNNS.SGDMomentumOptimizer.gradientScale.getter()
{
  return *(float *)(v0 + 8);
}

unsigned int *key path setter for BNNS.SGDMomentumOptimizer.gradientScale : BNNS.SGDMomentumOptimizer(unsigned int *result, void *a2)
{
  unsigned int v2 = *result;
  uint64_t v3 = a2[1];
  uint64_t v4 = a2[2];
  uint64_t v5 = a2[3];
  if ((v4 & 0x80000000) != 0)
  {
    uint64_t v6 = a2[5];
    unint64_t v7 = v3 & 0xFFFFFFFF00000000 | v2;
    unint64_t v8 = v4 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    uint64_t v6 = 0;
    unint64_t v7 = v3 & 0xFFFFFFFF00000000 | v2;
    unint64_t v8 = v4 & 0xFFFFFFFF00000001;
    v5 &= 0x1FFFFFFFFuLL;
  }
  a2[1] = v7;
  a2[2] = v8;
  a2[3] = v5;
  a2[5] = v6;
  return result;
}

void BNNS.SGDMomentumOptimizer.gradientScale.setter(float a1)
{
  uint64_t v2 = v1[1];
  uint64_t v3 = v1[2];
  uint64_t v4 = v1[3];
  if ((v3 & 0x80000000) != 0)
  {
    uint64_t v5 = v1[5];
    unint64_t v6 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    unint64_t v7 = v3 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    uint64_t v5 = 0;
    unint64_t v6 = v2 & 0xFFFFFFFF00000000 | LODWORD(a1);
    unint64_t v7 = v3 & 0xFFFFFFFF00000001;
    v4 &= 0x1FFFFFFFFuLL;
  }
  v1[1] = v6;
  v1[2] = v7;
  v1[3] = v4;
  v1[5] = v5;
}

uint64_t (*BNNS.SGDMomentumOptimizer.gradientScale.modify(uint64_t a1))(uint64_t a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(void *)(v1 + 8);
  return BNNS.SGDMomentumOptimizer.gradientScale.modify;
}

uint64_t BNNS.SGDMomentumOptimizer.gradientScale.modify(uint64_t a1)
{
  uint64_t v1 = *(void **)a1;
  unsigned int v3 = *(_DWORD *)(a1 + 8);
  uint64_t result = a1 + 8;
  unsigned int v4 = v3;
  uint64_t v5 = v1[1];
  uint64_t v6 = v1[2];
  uint64_t v7 = v1[3];
  if ((v6 & 0x80000000) != 0)
  {
    uint64_t v8 = v1[5];
    unint64_t v9 = v5 & 0xFFFFFFFF00000000 | v4;
    unint64_t v10 = v6 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    uint64_t v8 = 0;
    unint64_t v9 = v5 & 0xFFFFFFFF00000000 | v4;
    unint64_t v10 = v6 & 0xFFFFFFFF00000001;
    v7 &= 0x1FFFFFFFFuLL;
  }
  v1[1] = v9;
  v1[2] = v10;
  v1[3] = v7;
  v1[5] = v8;
  return result;
}

float BNNS.SGDMomentumOptimizer.regularizationScale.getter()
{
  return *(float *)(v0 + 12);
}

unsigned int *key path setter for BNNS.SGDMomentumOptimizer.regularizationScale : BNNS.SGDMomentumOptimizer(unsigned int *result, uint64_t a2)
{
  unsigned int v2 = *result;
  uint64_t v3 = *(unsigned int *)(a2 + 8);
  uint64_t v5 = *(void *)(a2 + 16);
  uint64_t v4 = *(void *)(a2 + 24);
  if ((v5 & 0x80000000) != 0)
  {
    uint64_t v6 = *(void *)(a2 + 40);
    unint64_t v7 = v3 | ((unint64_t)v2 << 32);
    unint64_t v8 = v5 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    uint64_t v6 = 0;
    unint64_t v7 = v3 | ((unint64_t)v2 << 32);
    unint64_t v8 = v5 & 0xFFFFFFFF00000001;
    v4 &= 0x1FFFFFFFFuLL;
  }
  *(void *)(a2 + 8) = v7;
  *(void *)(a2 + 16) = v8;
  *(void *)(a2 + 24) = v4;
  *(void *)(a2 + 40) = v6;
  return result;
}

void BNNS.SGDMomentumOptimizer.regularizationScale.setter(float a1)
{
  uint64_t v2 = *(unsigned int *)(v1 + 8);
  uint64_t v4 = *(void *)(v1 + 16);
  uint64_t v3 = *(void *)(v1 + 24);
  if ((v4 & 0x80000000) != 0)
  {
    uint64_t v5 = *(void *)(v1 + 40);
    unint64_t v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
    unint64_t v7 = v4 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    uint64_t v5 = 0;
    unint64_t v6 = v2 | ((unint64_t)LODWORD(a1) << 32);
    unint64_t v7 = v4 & 0xFFFFFFFF00000001;
    v3 &= 0x1FFFFFFFFuLL;
  }
  *(void *)(v1 + 8) = v6;
  *(void *)(v1 + 16) = v7;
  *(void *)(v1 + 24) = v3;
  *(void *)(v1 + 40) = v5;
}

uint64_t *(*BNNS.SGDMomentumOptimizer.regularizationScale.modify(uint64_t a1))(uint64_t *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 12);
  return BNNS.SGDMomentumOptimizer.regularizationScale.modify;
}

uint64_t *BNNS.SGDMomentumOptimizer.regularizationScale.modify(uint64_t *a1)
{
  uint64_t v1 = *a1;
  unsigned int v3 = *((_DWORD *)a1 + 2);
  uint64_t result = a1 + 1;
  unsigned int v4 = v3;
  uint64_t v5 = *(unsigned int *)(v1 + 8);
  uint64_t v7 = *(void *)(v1 + 16);
  uint64_t v6 = *(void *)(v1 + 24);
  if ((v7 & 0x80000000) != 0)
  {
    uint64_t v8 = *(void *)(v1 + 40);
    unint64_t v9 = v5 | ((unint64_t)v4 << 32);
    unint64_t v10 = v7 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    uint64_t v8 = 0;
    unint64_t v9 = v5 | ((unint64_t)v4 << 32);
    unint64_t v10 = v7 & 0xFFFFFFFF00000001;
    v6 &= 0x1FFFFFFFFuLL;
  }
  *(void *)(v1 + 8) = v9;
  *(void *)(v1 + 16) = v10;
  *(void *)(v1 + 24) = v6;
  *(void *)(v1 + 40) = v8;
  return result;
}

uint64_t BNNS.SGDMomentumOptimizer.gradientBounds.getter()
{
  unint64_t v2 = v0[2];
  unint64_t v1 = v0[3];
  if ((v2 & 0x80000000) == 0)
  {
    if (v2)
    {
      unint64_t v3 = HIDWORD(v2);
      if (*(float *)&v3 <= *(float *)&v1) {
        return v3 | (v1 << 32);
      }
      __break(1u);
      goto LABEL_10;
    }
    return 0;
  }
  if (HIDWORD(v1) != 1) {
    return 0;
  }
  uint64_t result = v0[4];
  if (*(float *)&result > *((float *)&result + 1)) {
LABEL_10:
  }
    __break(1u);
  return result;
}

unint64_t *key path setter for BNNS.SGDMomentumOptimizer.gradientBounds : BNNS.SGDMomentumOptimizer(unint64_t *result, void *a2)
{
  unint64_t v2 = *result;
  int v3 = *((unsigned __int8 *)result + 8);
  uint64_t v4 = a2[2];
  if ((v4 & 0x80000000) != 0)
  {
    uint64_t v5 = a2[5];
    BOOL v11 = v3 == 0;
    if (*((unsigned char *)result + 8)) {
      uint64_t v12 = 0;
    }
    else {
      uint64_t v12 = 0x100000000;
    }
    if (v11) {
      unint64_t v6 = *result;
    }
    else {
      unint64_t v6 = 0;
    }
    unint64_t v10 = v12 & 0xFFFFFFFF00000000 | a2[3];
    unint64_t v9 = v4 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    uint64_t v5 = 0;
    unint64_t v6 = a2[4];
    unint64_t v7 = HIDWORD(v2);
    uint64_t v8 = v2 << 32;
    if (*((unsigned char *)result + 8))
    {
      uint64_t v8 = 0;
      unint64_t v7 = 0;
    }
    unint64_t v9 = v8 | v3 ^ 1u;
    unint64_t v10 = a2[3] & 0x100000000 | v7;
  }
  a2[2] = v9;
  a2[3] = v10;
  a2[4] = v6;
  a2[5] = v5;
  return result;
}

unint64_t BNNS.SGDMomentumOptimizer.gradientBounds.setter(unint64_t result, char a2)
{
  uint64_t v3 = a2 & 1;
  uint64_t v4 = v2[2];
  if ((v4 & 0x80000000) != 0)
  {
    uint64_t v5 = v2[5];
    if (a2) {
      unint64_t v8 = 0;
    }
    else {
      unint64_t v8 = result;
    }
    uint64_t v12 = (v2[3] | (unint64_t)(v3 << 32)) ^ 0x100000000;
    unint64_t v11 = v4 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    uint64_t v5 = 0;
    BOOL v6 = v3 == 0;
    BOOL v7 = v3 == 0;
    unint64_t v8 = v2[4];
    unint64_t v9 = HIDWORD(result);
    uint64_t v10 = result << 32;
    if (!v6)
    {
      uint64_t v10 = 0;
      unint64_t v9 = 0;
    }
    unint64_t v11 = v10 | v7;
    uint64_t v12 = v2[3] & 0x100000000 | v9;
  }
  v2[2] = v11;
  v2[3] = v12;
  v2[4] = v8;
  v2[5] = v5;
  return result;
}

unint64_t *(*BNNS.SGDMomentumOptimizer.gradientBounds.modify(unint64_t *(*result)(unint64_t *result)))(unint64_t *result)
{
  *((void *)result + 2) = v1;
  unint64_t v3 = v1[2];
  unint64_t v2 = v1[3];
  if ((v3 & 0x80000000) == 0)
  {
    if (v3)
    {
      unint64_t v4 = HIDWORD(v3);
      if (*((float *)&v3 + 1) <= *(float *)&v2)
      {
        char v5 = 0;
        uint64_t v6 = v4 | (v2 << 32);
LABEL_9:
        *(void *)uint64_t result = v6;
        *((unsigned char *)result + 8) = v5;
        return BNNS.SGDMomentumOptimizer.gradientBounds.modify;
      }
      __break(1u);
      goto LABEL_11;
    }
LABEL_8:
    uint64_t v6 = 0;
    char v5 = 1;
    goto LABEL_9;
  }
  if (HIDWORD(v2) != 1) {
    goto LABEL_8;
  }
  uint64_t v6 = v1[4];
  if (*(float *)&v6 <= *((float *)&v6 + 1))
  {
    char v5 = 0;
    goto LABEL_9;
  }
LABEL_11:
  __break(1u);
  return result;
}

unint64_t *BNNS.SGDMomentumOptimizer.gradientBounds.modify(unint64_t *result)
{
  unint64_t v1 = (void *)result[2];
  unint64_t v2 = *result;
  int v3 = *((unsigned __int8 *)result + 8);
  uint64_t v4 = v1[2];
  if ((v4 & 0x80000000) != 0)
  {
    uint64_t v5 = v1[5];
    BOOL v11 = v3 == 0;
    if (*((unsigned char *)result + 8)) {
      uint64_t v12 = 0;
    }
    else {
      uint64_t v12 = 0x100000000;
    }
    if (v11) {
      unint64_t v6 = *result;
    }
    else {
      unint64_t v6 = 0;
    }
    unint64_t v10 = v12 & 0xFFFFFFFF00000000 | v1[3];
    unint64_t v9 = v4 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    uint64_t v5 = 0;
    unint64_t v6 = v1[4];
    unint64_t v7 = HIDWORD(v2);
    uint64_t v8 = v2 << 32;
    if (*((unsigned char *)result + 8))
    {
      uint64_t v8 = 0;
      unint64_t v7 = 0;
    }
    unint64_t v9 = v8 | v3 ^ 1u;
    unint64_t v10 = v1[3] & 0x100000000 | v7;
  }
  v1[2] = v9;
  v1[3] = v10;
  v1[4] = v6;
  v1[5] = v5;
  return result;
}

uint64_t (*BNNS.SGDMomentumOptimizer.usesNestrovMomentum.modify(uint64_t a1))()
{
  *(void *)a1 = v1;
  if ((*(void *)(v1 + 16) & 0x80000000) != 0) {
    uint64_t v2 = *(void *)(v1 + 16) & 1;
  }
  else {
    uint64_t v2 = *(void *)(v1 + 24) & 0x100000000;
  }
  *(unsigned char *)(a1 + 8) = v2 != 0;
  return BNNS.SGDMomentumOptimizer.usesNestrovMomentum.modify;
}

BOOL BNNS.SGDMomentumOptimizer.usesNestrovMomentum.getter()
{
  if ((*(void *)(v0 + 16) & 0x80000000) != 0) {
    uint64_t v1 = *(void *)(v0 + 16) & 1;
  }
  else {
    uint64_t v1 = *(void *)(v0 + 24) & 0x100000000;
  }
  return v1 != 0;
}

uint64_t BNNS.SGDMomentumOptimizer.usesNestrovMomentum.setter(uint64_t result)
{
  uint64_t v3 = v1[2];
  unint64_t v2 = v1[3];
  if ((v3 & 0x80000000) != 0)
  {
    uint64_t v4 = v1[5];
    unint64_t v5 = v3 & 0xFFFFFFFF00000000 | result & 1 | 0x80000000;
  }
  else
  {
    uint64_t v4 = 0;
    unint64_t v5 = v3 & 0xFFFFFFFF00000001;
    uint64_t v6 = 0x100000000;
    if ((result & 1) == 0) {
      uint64_t v6 = 0;
    }
    unint64_t v2 = v6 & 0xFFFFFFFF00000000 | v1[3];
  }
  v1[2] = v5;
  v1[3] = v2;
  v1[5] = v4;
  return result;
}

uint64_t (*BNNS.SGDMomentumOptimizer.usesNesterovMomentum.modify(uint64_t a1))()
{
  *(void *)a1 = v1;
  if ((*(void *)(v1 + 16) & 0x80000000) != 0) {
    uint64_t v2 = *(void *)(v1 + 16) & 1;
  }
  else {
    uint64_t v2 = *(void *)(v1 + 24) & 0x100000000;
  }
  *(unsigned char *)(a1 + 8) = v2 != 0;
  return BNNS.SGDMomentumOptimizer.usesNestrovMomentum.modify;
}

unsigned __int8 *BNNS.SGDMomentumOptimizer.usesNestrovMomentum.modify(unsigned __int8 *result)
{
  uint64_t v1 = *(void **)result;
  uint64_t v2 = result[8];
  uint64_t v4 = *(void *)(*(void *)result + 16);
  unint64_t v3 = *(void *)(*(void *)result + 24);
  if ((v4 & 0x80000000) != 0)
  {
    uint64_t v5 = v1[5];
    unint64_t v6 = v4 & 0xFFFFFFFF00000000 | v2 | 0x80000000;
  }
  else
  {
    uint64_t v5 = 0;
    unint64_t v6 = v4 & 0xFFFFFFFF00000001;
    BOOL v7 = v2 == 0;
    uint64_t v8 = 0x100000000;
    if (v7) {
      uint64_t v8 = 0;
    }
    unint64_t v3 = v8 & 0xFFFFFFFF00000000 | *(void *)(*(void *)result + 24);
  }
  v1[2] = v6;
  v1[3] = v3;
  v1[5] = v5;
  return result;
}

unint64_t BNNS.SGDMomentumOptimizer.regularizationFunction.getter()
{
  unint64_t v1 = *(void *)(v0 + 16);
  if ((v1 & 0x80000000) != 0) {
    return HIDWORD(v1);
  }
  else {
    return *(void *)(v0 + 32);
  }
}

unsigned int *key path setter for BNNS.SGDMomentumOptimizer.regularizationFunction : BNNS.SGDMomentumOptimizer(unsigned int *result, void *a2)
{
  uint64_t v2 = *result;
  uint64_t v4 = a2[2];
  uint64_t v3 = a2[3];
  unint64_t v5 = a2[4];
  if ((v4 & 0x80000000) != 0)
  {
    uint64_t v6 = a2[5];
    unint64_t v7 = a2[2] & 1 | (v2 << 32) | 0x80000000;
  }
  else
  {
    uint64_t v6 = 0;
    unint64_t v7 = v4 & 0xFFFFFFFF00000001;
    v3 &= 0x1FFFFFFFFuLL;
    unint64_t v5 = v5 & 0xFFFFFFFF00000000 | v2;
  }
  a2[2] = v7;
  a2[3] = v3;
  a2[4] = v5;
  a2[5] = v6;
  return result;
}

uint64_t BNNS.SGDMomentumOptimizer.regularizationFunction.setter(uint64_t result)
{
  uint64_t v3 = v1[2];
  uint64_t v2 = v1[3];
  unint64_t v4 = v1[4];
  if ((v3 & 0x80000000) != 0)
  {
    uint64_t v5 = v1[5];
    unint64_t v6 = v1[2] & 1 | (result << 32) | 0x80000000;
  }
  else
  {
    uint64_t v5 = 0;
    unint64_t v6 = v3 & 0xFFFFFFFF00000001;
    v2 &= 0x1FFFFFFFFuLL;
    unint64_t v4 = v4 & 0xFFFFFFFF00000000 | result;
  }
  v1[2] = v6;
  v1[3] = v2;
  v1[4] = v4;
  v1[5] = v5;
  return result;
}

uint64_t (*BNNS.SGDMomentumOptimizer.regularizationFunction.modify(uint64_t a1))(uint64_t a1)
{
  *(void *)a1 = v1;
  unint64_t v2 = *(void *)(v1 + 16);
  if ((v2 & 0x80000000) != 0) {
    unint64_t v3 = HIDWORD(v2);
  }
  else {
    unint64_t v3 = *(void *)(v1 + 32);
  }
  *(_DWORD *)(a1 + 8) = v3;
  return BNNS.SGDMomentumOptimizer.regularizationFunction.modify;
}

uint64_t BNNS.SGDMomentumOptimizer.regularizationFunction.modify(uint64_t a1)
{
  uint64_t v1 = *(void **)a1;
  unsigned int v4 = *(_DWORD *)(a1 + 8);
  uint64_t result = a1 + 8;
  uint64_t v3 = v4;
  uint64_t v6 = v1[2];
  uint64_t v5 = v1[3];
  unint64_t v7 = v1[4];
  if ((v6 & 0x80000000) != 0)
  {
    uint64_t v8 = v1[5];
    unint64_t v9 = v1[2] & 1 | (v3 << 32) | 0x80000000;
  }
  else
  {
    uint64_t v8 = 0;
    unint64_t v9 = v6 & 0xFFFFFFFF00000001;
    v5 &= 0x1FFFFFFFFuLL;
    unint64_t v7 = v7 & 0xFFFFFFFF00000000 | v3;
  }
  v1[2] = v9;
  v1[3] = v5;
  v1[4] = v7;
  v1[5] = v8;
  return result;
}

uint64_t BNNS.SGDMomentumOptimizer.sgdMomentumVariant.getter()
{
  if ((*(unsigned char *)(v0 + 19) & 0x80) != 0) {
    return *(void *)(v0 + 24);
  }
  else {
    return *(unsigned int *)(v0 + 36);
  }
}

unsigned int *key path setter for BNNS.SGDMomentumOptimizer.sgdMomentumVariant : BNNS.SGDMomentumOptimizer(unsigned int *result, int8x16_t *a2)
{
  int8x16_t v2 = a2[1];
  unint64_t v3 = a2[2].u64[0];
  if ((a2[1].i64[0] & 0x80000000) != 0)
  {
    uint64_t v4 = a2[2].i64[1];
    v6.i64[0] = vdupq_n_s64(0x80000000uLL).u64[0];
    v6.i64[1] = *result;
    int8x16_t v5 = vorrq_s8(vandq_s8(v2, (int8x16_t)xmmword_1D2137530), v6);
  }
  else
  {
    uint64_t v4 = 0;
    int8x16_t v5 = vandq_s8(v2, (int8x16_t)xmmword_1D2137540);
    unint64_t v3 = a2[2].i64[0] | ((unint64_t)*result << 32);
  }
  a2[1] = v5;
  a2[2].i64[0] = v3;
  a2[2].i64[1] = v4;
  return result;
}

uint64_t BNNS.SGDMomentumOptimizer.sgdMomentumVariant.setter(uint64_t result)
{
  int8x16_t v2 = v1[1];
  unint64_t v3 = v1[2].u64[0];
  if ((v1[1].i64[0] & 0x80000000) != 0)
  {
    uint64_t v4 = v1[2].i64[1];
    v6.i64[0] = vdupq_n_s64(0x80000000uLL).u64[0];
    v6.i64[1] = result;
    int8x16_t v5 = vorrq_s8(vandq_s8(v2, (int8x16_t)xmmword_1D2137530), v6);
  }
  else
  {
    uint64_t v4 = 0;
    int8x16_t v5 = vandq_s8(v2, (int8x16_t)xmmword_1D2137540);
    unint64_t v3 = v1[2].i64[0] | ((unint64_t)result << 32);
  }
  v1[1] = v5;
  v1[2].i64[0] = v3;
  v1[2].i64[1] = v4;
  return result;
}

uint64_t (*BNNS.SGDMomentumOptimizer.sgdMomentumVariant.modify(uint64_t a1))(uint64_t a1)
{
  *(void *)a1 = v1;
  if ((*(unsigned char *)(v1 + 19) & 0x80) != 0) {
    uint64_t v2 = *(void *)(v1 + 24);
  }
  else {
    LODWORD(v2) = *(_DWORD *)(v1 + 36);
  }
  *(_DWORD *)(a1 + 8) = v2;
  return BNNS.SGDMomentumOptimizer.sgdMomentumVariant.modify;
}

uint64_t BNNS.SGDMomentumOptimizer.sgdMomentumVariant.modify(uint64_t a1)
{
  uint64_t v1 = *(int8x16_t **)a1;
  unsigned int v4 = *(_DWORD *)(a1 + 8);
  uint64_t result = a1 + 8;
  uint64_t v3 = v4;
  int8x16_t v5 = v1[1];
  unint64_t v6 = v1[2].u64[0];
  if ((v1[1].i64[0] & 0x80000000) != 0)
  {
    uint64_t v7 = v1[2].i64[1];
    v9.i64[0] = vdupq_n_s64(0x80000000uLL).u64[0];
    v9.i64[1] = v3;
    int8x16_t v8 = vorrq_s8(vandq_s8(v5, (int8x16_t)xmmword_1D2137530), v9);
  }
  else
  {
    uint64_t v7 = 0;
    int8x16_t v8 = vandq_s8(v5, (int8x16_t)xmmword_1D2137540);
    unint64_t v6 = v1[2].i64[0] | ((unint64_t)v3 << 32);
  }
  v1[1] = v8;
  v1[2].i64[0] = v6;
  v1[2].i64[1] = v7;
  return result;
}

uint64_t BNNS.SGDMomentumOptimizer.init(learningRate:momentum:gradientScale:regularizationScale:clipsGradientsTo:usesNestrovMomentum:regularizationFunction:sgdMomentumVariant:)@<X0>(uint64_t result@<X0>, char a2@<W1>, char a3@<W2>, uint64_t a4@<X3>, unsigned int a5@<W4>, int8x16_t *a6@<X8>, unsigned int a7@<S0>, int32x2_t a8@<D1>, unsigned int a9@<S2>, __int32 a10@<S3>)
{
  a8.i32[1] = a10;
  v10.i64[0] = a7;
  v10.i64[1] = a9;
  *a6 = vorrq_s8((int8x16_t)vshll_n_s32(a8, 0x20uLL), v10);
  a6[1].i64[0] = a3 & 1 | (unint64_t)(a4 << 32) | 0x80000000;
  a6[1].i64[1] = (a5 | ((unint64_t)(a2 & 1) << 32)) ^ 0x100000000;
  if (a2) {
    uint64_t v11 = 0;
  }
  else {
    uint64_t v11 = result;
  }
  a6[2].i64[0] = v11;
  a6[2].i64[1] = 0;
  return result;
}

uint64_t BNNS.SGDMomentumOptimizer.bnnsOptimizerFunction.getter()
{
  if ((*(unsigned char *)(v0 + 19) & 0x80) != 0) {
    return 7;
  }
  else {
    return 1;
  }
}

uint64_t protocol witness for BNNSOptimizer.step(parameters:gradients:accumulators:filterParameters:) in conformance BNNS.SGDMomentumOptimizer(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  long long v17 = v9[1];
  long long v18 = *v9;
  uint64_t v13 = *((void *)v9 + 4);
  uint64_t v14 = *((void *)v9 + 5);
  v19[3] = a8;
  v19[4] = a9;
  uint64_t v15 = swift_allocObject();
  v19[0] = v15;
  *(_OWORD *)(v15 + 16) = v18;
  *(_OWORD *)(v15 + 32) = v17;
  *(void *)(v15 + 48) = v13;
  *(void *)(v15 + 56) = v14;
  specialized static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)((uint64_t)v19, a1, a2, a3);
  return __swift_destroy_boxed_opaque_existential_1((uint64_t)v19);
}

uint64_t protocol witness for BNNSOptimizer.bnnsOptimizerFunction.getter in conformance BNNS.SGDMomentumOptimizer()
{
  if ((*(unsigned char *)(v0 + 19) & 0x80) != 0) {
    return 7;
  }
  else {
    return 1;
  }
}

BOOL protocol witness for BNNSOptimizer.accumulatorCountMultiplier.getter in conformance BNNS.SGDMomentumOptimizer()
{
  return (*(_DWORD *)(v0 + 4) & 0x7FFFFFFF) != 0;
}

uint64_t protocol witness for WithOptimizerAlgFields.withOptimizerAlgFields(body:) in conformance BNNS.SGDMomentumOptimizer(uint64_t (*a1)(_OWORD *))
{
  uint64_t v5 = *MEMORY[0x1E4F143B8];
  long long v2 = v1[1];
  v4[0] = *v1;
  v4[1] = v2;
  v4[2] = v1[2];
  return a1(v4);
}

uint64_t protocol witness for BNNSOptimizer.step(parameters:gradients:accumulators:filterParameters:) in conformance BNNS.AdamWOptimizer(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  int v13 = *(_DWORD *)(v9 + 24);
  char v14 = *(unsigned char *)(v9 + 52);
  v17[3] = a8;
  v17[4] = a9;
  uint64_t v15 = swift_allocObject();
  v17[0] = v15;
  *(_OWORD *)(v15 + 16) = *(_OWORD *)v9;
  *(void *)(v15 + 32) = *(void *)(v9 + 16);
  *(_DWORD *)(v15 + 40) = v13;
  *(void *)(v15 + 44) = *(void *)(v9 + 28);
  *(_OWORD *)(v15 + 52) = *(_OWORD *)(v9 + 36);
  *(unsigned char *)(v15 + 68) = v14;
  specialized static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)((uint64_t)v17, a1, a2, a3);
  return __swift_destroy_boxed_opaque_existential_1((uint64_t)v17);
}

uint64_t specialized static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v97 = *MEMORY[0x1E4F143B8];
  int64_t v6 = *(void *)(a2 + 16);
  uint64_t v7 = *(void *)(a3 + 16);
  outlined init with copy of BNNSOptimizer(a1, (uint64_t)v94);
  if (v6 != v7)
  {
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v94);
LABEL_13:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *int8x16_t v30 = 3;
    return swift_willThrow();
  }
  uint64_t v78 = a1;
  int64_t v8 = *(void *)(a4 + 16);
  uint64_t v9 = v95;
  uint64_t v10 = v96;
  __swift_project_boxed_opaque_existential_1(v94, v95);
  uint64_t v11 = (*(uint64_t (**)(uint64_t, uint64_t))(v10 + 24))(v9, v10);
  int64_t v12 = v6 * v11;
  if ((unsigned __int128)(v6 * (__int128)v11) >> 64 != (v6 * v11) >> 63) {
    __break(1u);
  }
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v94);
  if (v8 != v12) {
    goto LABEL_13;
  }
  uint64_t v13 = MEMORY[0x1E4FBC860];
  int64_t v77 = v6;
  if (v6)
  {
    uint64_t v75 = a4;
    int64_t v76 = v8;
    v82[0] = MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v6, 0);
    int64_t v14 = v6;
    uint64_t v15 = (long long *)(a2 + 32);
    uint64_t v16 = v82[0];
    do
    {
      long long v17 = v15[9];
      long long v91 = v15[8];
      long long v92 = v17;
      long long v93 = v15[10];
      long long v18 = v15[5];
      long long v87 = v15[4];
      long long v88 = v18;
      long long v19 = v15[7];
      long long v89 = v15[6];
      long long v90 = v19;
      long long v20 = v15[1];
      long long v83 = *v15;
      long long v84 = v20;
      long long v21 = v15[3];
      long long v85 = v15[2];
      long long v86 = v21;
      unint64_t v22 = (_OWORD *)swift_slowAlloc();
      long long v23 = v92;
      v22[8] = v91;
      v22[9] = v23;
      v22[10] = v93;
      long long v24 = v88;
      v22[4] = v87;
      void v22[5] = v24;
      long long v25 = v90;
      v22[6] = v89;
      v22[7] = v25;
      long long v26 = v84;
      *unint64_t v22 = v83;
      v22[1] = v26;
      long long v27 = v86;
      void v22[2] = v85;
      vImage_Flags v22[3] = v27;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(void *)(v16 + 16) + 1, 1);
        uint64_t v16 = v82[0];
      }
      unint64_t v29 = *(void *)(v16 + 16);
      unint64_t v28 = *(void *)(v16 + 24);
      if (v29 >= v28 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v28 > 1), v29 + 1, 1);
        uint64_t v16 = v82[0];
      }
      *(void *)(v16 + 16) = v29 + 1;
      *(void *)(v16 + 8 * v29 + 32) = v22;
      v15 += 11;
      --v14;
    }
    while (v14);
    uint64_t v81 = v16;
    v82[0] = v13;
    int64_t v32 = v77;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v77, 0);
    int8x8_t v33 = (long long *)(a3 + 32);
    do
    {
      long long v34 = v33[9];
      long long v91 = v33[8];
      long long v92 = v34;
      long long v93 = v33[10];
      long long v35 = v33[5];
      long long v87 = v33[4];
      long long v88 = v35;
      long long v36 = v33[7];
      long long v89 = v33[6];
      long long v90 = v36;
      long long v37 = v33[1];
      long long v83 = *v33;
      long long v84 = v37;
      long long v38 = v33[3];
      long long v85 = v33[2];
      long long v86 = v38;
      uint64_t v39 = (_OWORD *)swift_slowAlloc();
      long long v40 = v92;
      unsigned char v39[8] = v91;
      v39[9] = v40;
      v39[10] = v93;
      long long v41 = v88;
      v39[4] = v87;
      v39[5] = v41;
      long long v42 = v90;
      v39[6] = v89;
      v39[7] = v42;
      long long v43 = v84;
      *uint64_t v39 = v83;
      v39[1] = v43;
      long long v44 = v86;
      v39[2] = v85;
      v39[3] = v44;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(void *)(v13 + 16) + 1, 1);
        uint64_t v13 = v82[0];
      }
      unint64_t v46 = *(void *)(v13 + 16);
      unint64_t v45 = *(void *)(v13 + 24);
      if (v46 >= v45 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v45 > 1), v46 + 1, 1);
        uint64_t v13 = v82[0];
      }
      *(void *)(v13 + 16) = v46 + 1;
      *(void *)(v13 + 8 * v46 + 32) = v39;
      v33 += 11;
      --v32;
    }
    while (v32);
    a4 = v75;
    int64_t v8 = v76;
  }
  else
  {
    uint64_t v81 = MEMORY[0x1E4FBC860];
  }
  uint64_t v47 = MEMORY[0x1E4FBC860];
  uint64_t v48 = v78;
  if (v8)
  {
    v82[0] = MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v8, 0);
    uint64_t v49 = (long long *)(a4 + 32);
    uint64_t v47 = v82[0];
    do
    {
      long long v50 = v49[9];
      long long v91 = v49[8];
      long long v92 = v50;
      long long v93 = v49[10];
      long long v51 = v49[5];
      long long v87 = v49[4];
      long long v88 = v51;
      long long v52 = v49[7];
      long long v89 = v49[6];
      long long v90 = v52;
      long long v53 = v49[1];
      long long v83 = *v49;
      long long v84 = v53;
      long long v54 = v49[3];
      long long v85 = v49[2];
      long long v86 = v54;
      long long v55 = (_OWORD *)swift_slowAlloc();
      long long v56 = v92;
      v55[8] = v91;
      v55[9] = v56;
      v55[10] = v93;
      long long v57 = v88;
      v55[4] = v87;
      v55[5] = v57;
      long long v58 = v90;
      v55[6] = v89;
      v55[7] = v58;
      long long v59 = v84;
      *long long v55 = v83;
      v55[1] = v59;
      long long v60 = v86;
      v55[2] = v85;
      v55[3] = v60;
      int64_t v61 = v8;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(void *)(v47 + 16) + 1, 1);
        uint64_t v47 = v82[0];
      }
      unint64_t v63 = *(void *)(v47 + 16);
      unint64_t v62 = *(void *)(v47 + 24);
      if (v63 >= v62 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v62 > 1), v63 + 1, 1);
        uint64_t v47 = v82[0];
      }
      *(void *)(v47 + 16) = v63 + 1;
      *(void *)(v47 + 8 * v63 + 32) = v55;
      v49 += 11;
      int64_t v8 = v61 - 1;
    }
    while (v61 != 1);
    uint64_t v48 = v78;
  }
  outlined init with copy of BNNSOptimizer(v48, (uint64_t)&v83);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSOptimizer);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for WithOptimizerAlgFields);
  swift_dynamicCast();
  uint64_t v64 = v82[4];
  int v65 = __swift_project_boxed_opaque_existential_1(v82, v82[3]);
  MEMORY[0x1F4188790](v65);
  (*(void (**)(uint64_t (*)()))(v64 + 8))(partial apply for closure #1 in closure #4 in static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:));
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v82);
  uint64_t v66 = *(void *)(v81 + 16);
  if (v66)
  {
    swift_bridgeObjectRetain();
    for (uint64_t i = 0; i != v66; ++i)
    {
      uint64_t v68 = *(void *)(v81 + 8 * i + 32);
      MEMORY[0x1D26009C0](v68, -1, -1);
    }
    swift_bridgeObjectRelease();
  }
  uint64_t v69 = *(void *)(v13 + 16);
  if (v69)
  {
    swift_bridgeObjectRetain();
    for (uint64_t j = 0; j != v69; ++j)
    {
      uint64_t v71 = *(void *)(v13 + 8 * j + 32);
      MEMORY[0x1D26009C0](v71, -1, -1);
    }
    swift_bridgeObjectRelease();
  }
  uint64_t v72 = *(void *)(v47 + 16);
  if (v72)
  {
    swift_bridgeObjectRetain();
    for (uint64_t k = 0; k != v72; ++k)
    {
      uint64_t v74 = *(void *)(v47 + 8 * k + 32);
      if (v74) {
        MEMORY[0x1D26009C0](v74, -1, -1);
      }
    }
    swift_bridgeObjectRelease();
  }
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

uint64_t __swift_destroy_boxed_opaque_existential_1(uint64_t a1)
{
  uint64_t v1 = *(void *)(*(void *)(a1 + 24) - 8);
  if ((*(unsigned char *)(v1 + 82) & 2) != 0) {
    return swift_release();
  }
  else {
    return (*(uint64_t (**)(void))(v1 + 8))();
  }
}

void *sub_1D20C0BE8@<X0>(void *result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *result;
  return result;
}

unsigned int *sub_1D20C0BF4(unsigned int *result, unint64_t *a2)
{
  unsigned int v2 = *result;
  unint64_t v3 = *a2;
  unint64_t v4 = a2[3];
  unint64_t v5 = a2[5];
  if (*((unsigned char *)a2 + 52) == 1)
  {
    int v6 = *((_DWORD *)a2 + 12);
    unint64_t v7 = v3 & 0xFFFFFFFF00000000 | v2;
  }
  else
  {
    int v6 = 0;
    unint64_t v7 = v3 & 0xFFFFFFFF00000000 | v2;
    v4 &= 0x1FFFFFFFFuLL;
    unint64_t v5 = a2[5];
  }
  *a2 = v7;
  a2[3] = v4;
  a2[5] = v5;
  *((_DWORD *)a2 + 12) = v6;
  return result;
}

uint64_t sub_1D20C0C50@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_DWORD *)(result + 4);
  return result;
}

unsigned int *sub_1D20C0C5C(unsigned int *result, unsigned int *a2)
{
  unsigned int v2 = *result;
  uint64_t v3 = *a2;
  uint64_t v4 = *((void *)a2 + 3);
  uint64_t v5 = *((void *)a2 + 5);
  if (*((unsigned char *)a2 + 52) == 1)
  {
    unsigned int v6 = a2[12];
    unint64_t v7 = v3 | ((unint64_t)v2 << 32);
  }
  else
  {
    unsigned int v6 = 0;
    unint64_t v7 = v3 | ((unint64_t)v2 << 32);
    v4 &= 0x1FFFFFFFFuLL;
    uint64_t v5 = *((void *)a2 + 5);
  }
  *(void *)a2 = v7;
  *((void *)a2 + 3) = v4;
  *((void *)a2 + 5) = v5;
  a2[12] = v6;
  return result;
}

uint64_t sub_1D20C0CB0@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(void *)(result + 8);
  return result;
}

unsigned int *sub_1D20C0CBC(unsigned int *result, uint64_t a2)
{
  unsigned int v2 = *result;
  uint64_t v3 = *(void *)(a2 + 8);
  uint64_t v4 = *(void *)(a2 + 24);
  uint64_t v5 = *(void *)(a2 + 40);
  if (*(unsigned char *)(a2 + 52) == 1)
  {
    int v6 = *(_DWORD *)(a2 + 48);
    unint64_t v7 = v3 & 0xFFFFFFFF00000000 | v2;
  }
  else
  {
    int v6 = 0;
    unint64_t v7 = v3 & 0xFFFFFFFF00000000 | v2;
    v4 &= 0x1FFFFFFFFuLL;
    uint64_t v5 = *(void *)(a2 + 40);
  }
  *(void *)(a2 + 8) = v7;
  *(void *)(a2 + 24) = v4;
  *(void *)(a2 + 40) = v5;
  *(_DWORD *)(a2 + 48) = v6;
  return result;
}

uint64_t sub_1D20C0D18@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_DWORD *)(result + 12);
  return result;
}

unsigned int *sub_1D20C0D24(unsigned int *result, uint64_t a2)
{
  unsigned int v2 = *result;
  uint64_t v3 = *(unsigned int *)(a2 + 8);
  uint64_t v4 = *(void *)(a2 + 24);
  uint64_t v5 = *(void *)(a2 + 40);
  if (*(unsigned char *)(a2 + 52) == 1)
  {
    int v6 = *(_DWORD *)(a2 + 48);
    unint64_t v7 = v3 | ((unint64_t)v2 << 32);
  }
  else
  {
    int v6 = 0;
    unint64_t v7 = v3 | ((unint64_t)v2 << 32);
    v4 &= 0x1FFFFFFFFuLL;
    uint64_t v5 = *(void *)(a2 + 40);
  }
  *(void *)(a2 + 8) = v7;
  *(void *)(a2 + 24) = v4;
  *(void *)(a2 + 40) = v5;
  *(_DWORD *)(a2 + 48) = v6;
  return result;
}

uint64_t sub_1D20C0D78@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(void *)(result + 16);
  return result;
}

unsigned int *sub_1D20C0D84(unsigned int *result, uint64_t a2)
{
  unsigned int v2 = *result;
  uint64_t v4 = *(void *)(a2 + 16);
  uint64_t v3 = *(void *)(a2 + 24);
  uint64_t v5 = *(void *)(a2 + 40);
  if (*(unsigned char *)(a2 + 52) == 1)
  {
    int v6 = *(_DWORD *)(a2 + 48);
    unint64_t v7 = v4 & 0xFFFFFFFF00000000 | v2;
  }
  else
  {
    int v6 = 0;
    unint64_t v7 = v4 & 0xFFFFFFFF00000000 | v2;
    v3 &= 0x1FFFFFFFFuLL;
    uint64_t v5 = *(void *)(a2 + 40);
  }
  *(void *)(a2 + 16) = v7;
  *(void *)(a2 + 24) = v3;
  *(void *)(a2 + 40) = v5;
  *(_DWORD *)(a2 + 48) = v6;
  return result;
}

uint64_t sub_1D20C0DD8@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_DWORD *)(result + 20);
  return result;
}

unsigned int *sub_1D20C0DE4(unsigned int *result, uint64_t a2)
{
  unsigned int v2 = *result;
  uint64_t v3 = *(unsigned int *)(a2 + 16);
  uint64_t v4 = *(void *)(a2 + 24);
  uint64_t v5 = *(void *)(a2 + 40);
  if (*(unsigned char *)(a2 + 52) == 1)
  {
    int v6 = *(_DWORD *)(a2 + 48);
    unint64_t v7 = v3 | ((unint64_t)v2 << 32);
  }
  else
  {
    int v6 = 0;
    unint64_t v7 = v3 | ((unint64_t)v2 << 32);
    v4 &= 0x1FFFFFFFFuLL;
    uint64_t v5 = *(void *)(a2 + 40);
  }
  *(void *)(a2 + 16) = v7;
  *(void *)(a2 + 24) = v4;
  *(void *)(a2 + 40) = v5;
  *(_DWORD *)(a2 + 48) = v6;
  return result;
}

uint64_t sub_1D20C0E34@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(void *)(result + 24);
  return result;
}

unsigned int *sub_1D20C0E40(unsigned int *result, uint64_t a2)
{
  unsigned int v2 = *result;
  uint64_t v3 = *(void *)(a2 + 24);
  uint64_t v4 = *(void *)(a2 + 40);
  if (*(unsigned char *)(a2 + 52) == 1)
  {
    int v5 = *(_DWORD *)(a2 + 48);
    unint64_t v6 = v3 & 0xFFFFFFFF00000000 | v2;
  }
  else
  {
    int v5 = 0;
    unint64_t v6 = v3 & 0x100000000 | v2;
    uint64_t v4 = *(void *)(a2 + 40);
  }
  *(void *)(a2 + 24) = v6;
  *(void *)(a2 + 40) = v4;
  *(_DWORD *)(a2 + 48) = v5;
  return result;
}

uint64_t sub_1D20C0E98@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  if (*(unsigned char *)(result + 52) == 1) {
    *a2 = *(_DWORD *)(result + 28);
  }
  else {
    *a2 = *(void *)(result + 40);
  }
  return result;
}

unsigned int *sub_1D20C0EBC(unsigned int *result, uint64_t a2)
{
  if (*(unsigned char *)(a2 + 52) == 1)
  {
    int v2 = *(_DWORD *)(a2 + 48);
    uint64_t v3 = *(void *)(a2 + 40);
    unint64_t v4 = *(void *)(a2 + 24) | ((unint64_t)*result << 32);
  }
  else
  {
    int v2 = 0;
    unint64_t v4 = *(void *)(a2 + 24) & 0x1FFFFFFFFLL;
    uint64_t v3 = *result;
  }
  *(void *)(a2 + 24) = v4;
  *(void *)(a2 + 40) = v3;
  *(_DWORD *)(a2 + 48) = v2;
  return result;
}

void *sub_1D20C0EFC@<X0>(void *result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *result;
  return result;
}

uint64_t sub_1D20C0F0C@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_DWORD *)(result + 4);
  return result;
}

uint64_t sub_1D20C0F1C@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(void *)(result + 8);
  return result;
}

unsigned int *sub_1D20C0F28(unsigned int *result, uint64_t a2)
{
  unsigned int v2 = *result;
  uint64_t v3 = *(void *)(a2 + 8);
  uint64_t v4 = *(void *)(a2 + 24);
  uint64_t v5 = *(void *)(a2 + 40);
  if (v3 < 0)
  {
    int v6 = *(_DWORD *)(a2 + 48);
    unint64_t v7 = v3 & 0x100000000 | v2 | 0x8000000000000000;
  }
  else
  {
    int v6 = 0;
    unint64_t v7 = v3 & 0x100000000 | v2;
    v4 &= 0x1FFFFFFFFuLL;
    uint64_t v5 = *(void *)(a2 + 40);
  }
  *(void *)(a2 + 8) = v7;
  *(void *)(a2 + 24) = v4;
  *(void *)(a2 + 40) = v5;
  *(_DWORD *)(a2 + 48) = v6;
  return result;
}

uint64_t sub_1D20C0F80@<X0>(uint64_t result@<X0>, unsigned char *a2@<X8>)
{
  *a2 = *(unsigned char *)(result + 12) & 1;
  return result;
}

unsigned __int8 *sub_1D20C0F90(unsigned __int8 *result, uint64_t a2)
{
  int v2 = *result;
  uint64_t v3 = *(void *)(a2 + 24);
  uint64_t v4 = *(void *)(a2 + 40);
  if ((*(void *)(a2 + 8) & 0x8000000000000000) != 0)
  {
    int v5 = *(_DWORD *)(a2 + 48);
    BOOL v6 = v2 == 0;
    unint64_t v9 = 0x8000000000000000;
    if (!v6) {
      unint64_t v9 = 0x8000000100000000;
    }
    unint64_t v8 = v9 & 0xFFFFFFFF00000000 | *(void *)(a2 + 8);
  }
  else
  {
    int v5 = 0;
    BOOL v6 = v2 == 0;
    uint64_t v7 = 0x100000000;
    if (v6) {
      uint64_t v7 = 0;
    }
    unint64_t v8 = v7 & 0xFFFFFFFF00000000 | *(void *)(a2 + 8);
    v3 &= 0x1FFFFFFFFuLL;
    uint64_t v4 = *(void *)(a2 + 40);
  }
  *(void *)(a2 + 8) = v8;
  *(void *)(a2 + 24) = v3;
  *(void *)(a2 + 40) = v4;
  *(_DWORD *)(a2 + 48) = v5;
  return result;
}

uint64_t sub_1D20C0FF4@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(void *)(result + 16);
  return result;
}

uint64_t sub_1D20C1004@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_DWORD *)(result + 20);
  return result;
}

uint64_t sub_1D20C1014@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(void *)(result + 24);
  return result;
}

unsigned int *sub_1D20C1020(unsigned int *result, uint64_t a2)
{
  unsigned int v2 = *result;
  uint64_t v3 = *(void *)(a2 + 8);
  uint64_t v4 = *(void *)(a2 + 24);
  uint64_t v5 = *(void *)(a2 + 40);
  if (v3 < 0)
  {
    int v6 = *(_DWORD *)(a2 + 48);
    unint64_t v8 = v4 & 0xFFFFFFFF00000000 | v2;
    unint64_t v7 = v3 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v6 = 0;
    unint64_t v7 = v3 & 0x1FFFFFFFFLL;
    unint64_t v8 = v4 & 0x100000000 | v2;
    uint64_t v5 = *(void *)(a2 + 40);
  }
  *(void *)(a2 + 8) = v7;
  *(void *)(a2 + 24) = v8;
  *(void *)(a2 + 40) = v5;
  *(_DWORD *)(a2 + 48) = v6;
  return result;
}

uint64_t sub_1D20C1084@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  if ((*(void *)(result + 8) & 0x8000000000000000) != 0) {
    LODWORD(v2) = *(_DWORD *)(result + 28);
  }
  else {
    uint64_t v2 = *(void *)(result + 40);
  }
  *a2 = v2;
  return result;
}

unsigned int *sub_1D20C10A4(unsigned int *result, uint64_t a2)
{
  uint64_t v2 = *(void *)(a2 + 8);
  if (v2 < 0)
  {
    int v3 = *(_DWORD *)(a2 + 48);
    uint64_t v6 = *(void *)(a2 + 40);
    unint64_t v5 = *(void *)(a2 + 24) | ((unint64_t)*result << 32);
    unint64_t v4 = v2 & 0x1FFFFFFFFLL | 0x8000000000000000;
  }
  else
  {
    int v3 = 0;
    unint64_t v4 = v2 & 0x1FFFFFFFFLL;
    unint64_t v5 = *(void *)(a2 + 24) & 0x1FFFFFFFFLL;
    uint64_t v6 = *result;
  }
  *(void *)(a2 + 8) = v4;
  *(void *)(a2 + 24) = v5;
  *(void *)(a2 + 40) = v6;
  *(_DWORD *)(a2 + 48) = v3;
  return result;
}

void *sub_1D20C10F0@<X0>(void *result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *result;
  return result;
}

uint64_t sub_1D20C1100@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_DWORD *)(result + 4);
  return result;
}

uint64_t sub_1D20C1110@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(void *)(result + 8);
  return result;
}

uint64_t sub_1D20C1120@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  *a2 = *(_DWORD *)(result + 12);
  return result;
}

uint64_t sub_1D20C1130@<X0>(uint64_t a1@<X8>)
{
  uint64_t result = BNNS.SGDMomentumOptimizer.gradientBounds.getter();
  *(void *)a1 = result;
  *(unsigned char *)(a1 + 8) = v3 & 1;
  return result;
}

uint64_t keypath_get_60Tm@<X0>(uint64_t result@<X0>, BOOL *a2@<X8>)
{
  if ((*(void *)(result + 16) & 0x80000000) != 0) {
    uint64_t v2 = *(void *)(result + 16) & 1;
  }
  else {
    uint64_t v2 = *(void *)(result + 24) & 0x100000000;
  }
  *a2 = v2 != 0;
  return result;
}

unsigned __int8 *keypath_set_61Tm(unsigned __int8 *result, void *a2)
{
  uint64_t v2 = *result;
  uint64_t v4 = a2[2];
  unint64_t v3 = a2[3];
  if ((v4 & 0x80000000) != 0)
  {
    uint64_t v5 = a2[5];
    unint64_t v6 = v4 & 0xFFFFFFFF00000000 | v2 | 0x80000000;
  }
  else
  {
    uint64_t v5 = 0;
    unint64_t v6 = v4 & 0xFFFFFFFF00000001;
    BOOL v7 = v2 == 0;
    uint64_t v8 = 0x100000000;
    if (v7) {
      uint64_t v8 = 0;
    }
    unint64_t v3 = v8 & 0xFFFFFFFF00000000 | a2[3];
  }
  a2[2] = v6;
  a2[3] = v3;
  a2[5] = v5;
  return result;
}

uint64_t sub_1D20C1200@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  unint64_t v2 = *(void *)(result + 16);
  if ((v2 & 0x80000000) != 0) {
    unint64_t v3 = HIDWORD(v2);
  }
  else {
    unint64_t v3 = *(void *)(result + 32);
  }
  *a2 = v3;
  return result;
}

uint64_t sub_1D20C1224@<X0>(uint64_t result@<X0>, _DWORD *a2@<X8>)
{
  if ((*(unsigned char *)(result + 19) & 0x80) != 0) {
    uint64_t v2 = *(void *)(result + 24);
  }
  else {
    LODWORD(v2) = *(_DWORD *)(result + 36);
  }
  *a2 = v2;
  return result;
}

uint64_t dispatch thunk of BNNSOptimizer.step(parameters:gradients:accumulators:filterParameters:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return (*(uint64_t (**)(uint64_t))(a9 + 8))(a1);
}

uint64_t dispatch thunk of BNNSOptimizer.bnnsOptimizerFunction.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

uint64_t dispatch thunk of BNNSOptimizer.accumulatorCountMultiplier.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 24))();
}

uint64_t dispatch thunk of WithOptimizerAlgFields.withOptimizerAlgFields(body:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return (*(uint64_t (**)(void))(a4 + 8))();
}

__n128 __swift_memcpy54_4(uint64_t a1, uint64_t a2)
{
  __n128 result = *(__n128 *)a2;
  long long v3 = *(_OWORD *)(a2 + 16);
  long long v4 = *(_OWORD *)(a2 + 32);
  *(void *)(a1 + 46) = *(void *)(a2 + 46);
  *(_OWORD *)(a1 + 16) = v3;
  *(_OWORD *)(a1 + 32) = v4;
  *(__n128 *)a1 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.AdamOptimizer(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFF && *(unsigned char *)(a1 + 54)) {
    return (*(_DWORD *)a1 + 255);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 52);
  if (v3 <= 1) {
    int v4 = -1;
  }
  else {
    int v4 = v3 ^ 0xFF;
  }
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.AdamOptimizer(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(void *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_WORD *)(result + 52) = 0;
    *(_DWORD *)(result + 48) = 0;
    *(void *)__n128 result = a2 - 255;
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 54) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 54) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 52) = -(char)a2;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.AdamOptimizer()
{
  return &type metadata for BNNS.AdamOptimizer;
}

ValueMetadata *type metadata accessor for BNNS.RMSPropOptimizer()
{
  return &type metadata for BNNS.RMSPropOptimizer;
}

ValueMetadata *type metadata accessor for BNNS.SGDMomentumOptimizer()
{
  return &type metadata for BNNS.SGDMomentumOptimizer;
}

uint64_t getEnumTagSinglePayload for BNNS.SGDMomentumOptimizer(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0x7FFFFFFF && *(unsigned char *)(a1 + 48)) {
    return (*(_DWORD *)a1 + 0x7FFFFFFF);
  }
  unsigned int v3 = (*(_DWORD *)(a1 + 16) & 0x7FFFFFFE | (*(_DWORD *)(a1 + 16) >> 31)) ^ 0x7FFFFFFF;
  if (v3 >= 0x7FFFFFFE) {
    unsigned int v3 = -1;
  }
  return v3 + 1;
}

uint64_t storeEnumTagSinglePayload for BNNS.SGDMomentumOptimizer(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0x7FFFFFFE)
  {
    *(void *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(void *)__n128 result = a2 - 0x7FFFFFFF;
    if (a3 >= 0x7FFFFFFF) {
      *(unsigned char *)(result + 48) = 1;
    }
  }
  else
  {
    if (a3 >= 0x7FFFFFFF) {
      *(unsigned char *)(result + 48) = 0;
    }
    if (a2)
    {
      *(void *)__n128 result = 0;
      *(void *)(result + 8) = 0;
      *(void *)(result + 16) = -a2 & 0x7FFFFFFE | (a2 << 31);
      *(void *)(result + 24) = 0;
      *(void *)(result + 32) = 0;
      *(void *)(result + 40) = 0;
    }
  }
  return result;
}

uint64_t getEnumTag for BNNS.SGDMomentumOptimizer.Fields(uint64_t a1)
{
  return *(unsigned __int8 *)(a1 + 19) >> 7;
}

uint64_t destructiveProjectEnumData for BNNS.SGDMomentumOptimizer.Fields(uint64_t result)
{
  *(void *)(result + 16) &= ~0x80000000uLL;
  return result;
}

uint64_t destructiveInjectEnumTag for BNNS.SGDMomentumOptimizer.Fields(uint64_t result, int a2)
{
  *(void *)(result + 16) = *(void *)(result + 16) & 0xFFFFFFFF00000001 | (a2 << 31);
  return result;
}

ValueMetadata *type metadata accessor for BNNS.SGDMomentumOptimizer.Fields()
{
  return &type metadata for BNNS.SGDMomentumOptimizer.Fields;
}

uint64_t getEnumTagSinglePayload for BNNS.RMSPropOptimizer(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0x7FFFFFFF && *(unsigned char *)(a1 + 52)) {
    return (*(_DWORD *)a1 + 0x7FFFFFFF);
  }
  unsigned int v3 = (((*(void *)(a1 + 8) >> 33) >> 30) & 0x80000001 | (2 * ((*(void *)(a1 + 8) >> 33) & 0x3FFFFFFF))) ^ 0x7FFFFFFF;
  if (v3 >= 0x7FFFFFFE) {
    unsigned int v3 = -1;
  }
  return v3 + 1;
}

uint64_t storeEnumTagSinglePayload for BNNS.RMSPropOptimizer(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0x7FFFFFFE)
  {
    *(void *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(_DWORD *)(result + 48) = 0;
    *(void *)__n128 result = a2 - 0x7FFFFFFF;
    if (a3 >= 0x7FFFFFFF) {
      *(unsigned char *)(result + 52) = 1;
    }
  }
  else
  {
    if (a3 >= 0x7FFFFFFF) {
      *(unsigned char *)(result + 52) = 0;
    }
    if (a2)
    {
      *(void *)__n128 result = 0;
      *(void *)(result + 8) = (unint64_t)(((-a2 >> 1) & 0x3FFFFFFF) - (a2 << 30)) << 33;
      *(_OWORD *)(result + 16) = 0u;
      *(_OWORD *)(result + 32) = 0u;
      *(_DWORD *)(result + 48) = 0;
    }
  }
  return result;
}

uint64_t getEnumTag for BNNS.RMSPropOptimizer.Fields(uint64_t a1)
{
  return *(void *)(a1 + 8) >> 63;
}

uint64_t destructiveProjectEnumData for BNNS.RMSPropOptimizer.Fields(uint64_t result)
{
  *(void *)(result + 8) &= ~0x8000000000000000;
  return result;
}

uint64_t destructiveInjectEnumTag for BNNS.RMSPropOptimizer.Fields(uint64_t result, uint64_t a2)
{
  *(void *)(result + 8) = *(void *)(result + 8) & 0x1FFFFFFFFLL | (a2 << 63);
  return result;
}

ValueMetadata *type metadata accessor for BNNS.RMSPropOptimizer.Fields()
{
  return &type metadata for BNNS.RMSPropOptimizer.Fields;
}

__n128 __swift_memcpy53_4(uint64_t a1, uint64_t a2)
{
  __n128 result = *(__n128 *)a2;
  long long v3 = *(_OWORD *)(a2 + 16);
  long long v4 = *(_OWORD *)(a2 + 32);
  *(void *)(a1 + 45) = *(void *)(a2 + 45);
  *(_OWORD *)(a1 + 16) = v3;
  *(_OWORD *)(a1 + 32) = v4;
  *(__n128 *)a1 = result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.AdamOptimizer.Fields(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFF && *(unsigned char *)(a1 + 53)) {
    return (*(_DWORD *)a1 + 255);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 52);
  if (v3 <= 1) {
    int v4 = -1;
  }
  else {
    int v4 = v3 ^ 0xFF;
  }
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.AdamOptimizer.Fields(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(void *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(unsigned char *)(result + 52) = 0;
    *(_DWORD *)(result + 48) = 0;
    *(void *)__n128 result = a2 - 255;
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 53) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 53) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 52) = -(char)a2;
    }
  }
  return result;
}

uint64_t getEnumTag for BNNS.AdamOptimizer.Fields(uint64_t a1)
{
  return *(unsigned __int8 *)(a1 + 52);
}

uint64_t destructiveInjectEnumTag for BNNS.AdamOptimizer.Fields(uint64_t result, char a2)
{
  *(unsigned char *)(result + 52) = a2 & 1;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.AdamOptimizer.Fields()
{
  return &type metadata for BNNS.AdamOptimizer.Fields;
}

uint64_t sub_1D20C16C4()
{
  return MEMORY[0x1F4186498](v0, 72, 7);
}

uint64_t sub_1D20C16D4()
{
  return MEMORY[0x1F4186498](v0, 64, 7);
}

uint64_t sub_1D20C16E4()
{
  return MEMORY[0x1F4186498](v0, 68, 7);
}

uint64_t outlined init with copy of BNNSOptimizer(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(a1 + 24);
  *(void *)(a2 + 24) = v3;
  *(void *)(a2 + 32) = *(void *)(a1 + 32);
  (**(void (***)(uint64_t, uint64_t))(v3 - 8))(a2, a1);
  return a2;
}

void *__swift_project_boxed_opaque_existential_1(void *result, uint64_t a2)
{
  if ((*(_DWORD *)(*(void *)(a2 - 8) + 80) & 0x20000) != 0) {
    return (void *)(*result
  }
                    + ((*(_DWORD *)(*(void *)(a2 - 8) + 80) + 16) & ~(unint64_t)*(_DWORD *)(*(void *)(a2 - 8) + 80)));
  return result;
}

uint64_t partial apply for closure #1 in closure #4 in static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)(uint64_t a1)
{
  return closure #1 in closure #4 in static BNNS.optimizerStep(function:parameters:gradients:accumulators:filterParameters:)(a1, *(_DWORD **)(v1 + 16), *(void **)(v1 + 24), *(void *)(v1 + 32), *(char ***)(v1 + 40), *(char ***)(v1 + 48), *(char ***)(v1 + 56), *(void *)(v1 + 64));
}

double BNNS.FusedTernaryArithmeticParameters.layerParameters(inputA:inputB:inputC:output:)@<D0>(_OWORD *a1@<X0>, _OWORD *a2@<X1>, _OWORD *a3@<X2>, _OWORD *a4@<X3>, uint64_t *a5@<X8>)
{
  *(_OWORD *)&v23[116] = a2[7];
  *(_OWORD *)&v23[132] = a2[8];
  *(_OWORD *)&v23[148] = a2[9];
  *(_OWORD *)&v23[164] = a2[10];
  *(_OWORD *)&v23[52] = a2[3];
  *(_OWORD *)&v23[68] = a2[4];
  *(_OWORD *)&v23[84] = a2[5];
  *(_OWORD *)&v23[100] = a2[6];
  *(_OWORD *)&v23[4] = *a2;
  *(_OWORD *)&v23[20] = a2[1];
  *(_OWORD *)&v23[36] = a2[2];
  *(_OWORD *)&v22[116] = a3[7];
  *(_OWORD *)&v22[132] = a3[8];
  *(_OWORD *)&v22[148] = a3[9];
  *(_OWORD *)&v22[164] = a3[10];
  *(_OWORD *)&v22[52] = a3[3];
  *(_OWORD *)&v22[68] = a3[4];
  *(_OWORD *)&v22[84] = a3[5];
  *(_OWORD *)&v22[100] = a3[6];
  *(_OWORD *)&v22[4] = *a3;
  *(_OWORD *)&v22[20] = a3[1];
  *(_OWORD *)&v22[36] = a3[2];
  *(_OWORD *)&v21[100] = a4[6];
  *(_OWORD *)&v21[116] = a4[7];
  *(_OWORD *)&v21[132] = a4[8];
  *(_OWORD *)&v21[148] = a4[9];
  *(_OWORD *)&v21[164] = a4[10];
  *(_OWORD *)&v21[52] = a4[3];
  *(_OWORD *)&v21[68] = a4[4];
  *(_OWORD *)&v21[84] = a4[5];
  *(_OWORD *)&void v21[4] = *a4;
  *(_OWORD *)&v21[20] = a4[1];
  int v8 = *(unsigned __int8 *)(v5 + 8);
  int v9 = *(unsigned __int8 *)(v5 + 9);
  int v10 = *(unsigned __int8 *)(v5 + 10);
  int v11 = *(unsigned __int8 *)(v5 + 11);
  *(_OWORD *)&v21[36] = a4[2];
  uint64_t v12 = swift_slowAlloc();
  long long v13 = a1[9];
  *(_OWORD *)(v12 + 128) = a1[8];
  *(_OWORD *)(v12 + 144) = v13;
  *(_OWORD *)(v12 + 160) = a1[10];
  long long v14 = a1[5];
  *(_OWORD *)(v12 + 64) = a1[4];
  *(_OWORD *)(v12 + 80) = v14;
  long long v15 = a1[7];
  *(_OWORD *)(v12 + 96) = a1[6];
  *(_OWORD *)(v12 + 112) = v15;
  long long v16 = a1[1];
  *(_OWORD *)uint64_t v12 = *a1;
  *(_OWORD *)(v12 + 16) = v16;
  long long v17 = a1[3];
  *(_OWORD *)(v12 + 32) = a1[2];
  *(_OWORD *)(v12 + 48) = v17;
  *(_OWORD *)(v12 + 324) = *(_OWORD *)&v23[144];
  *(_OWORD *)(v12 + 340) = *(_OWORD *)&v23[160];
  *(_OWORD *)(v12 + 244) = *(_OWORD *)&v23[64];
  *(_OWORD *)(v12 + 260) = *(_OWORD *)&v23[80];
  *(_OWORD *)(v12 + 276) = *(_OWORD *)&v23[96];
  *(void *)uint64_t v5 = v12;
  *(_DWORD *)(v12 + 176) = v8;
  *(_DWORD *)(v12 + 356) = *(_DWORD *)&v23[176];
  *(_OWORD *)(v12 + 292) = *(_OWORD *)&v23[112];
  *(_OWORD *)(v12 + 308) = *(_OWORD *)&v23[128];
  *(_OWORD *)(v12 + 180) = *(_OWORD *)v23;
  *(_OWORD *)(v12 + 196) = *(_OWORD *)&v23[16];
  *(_OWORD *)(v12 + 212) = *(_OWORD *)&v23[32];
  *(_OWORD *)(v12 + 228) = *(_OWORD *)&v23[48];
  *(_DWORD *)(v12 + 360) = v9;
  *(_OWORD *)(v12 + 492) = *(_OWORD *)&v22[128];
  *(_OWORD *)(v12 + 508) = *(_OWORD *)&v22[144];
  *(_OWORD *)(v12 + 524) = *(_OWORD *)&v22[160];
  *(_DWORD *)(v12 + 540) = *(_DWORD *)&v22[176];
  *(_OWORD *)(v12 + 428) = *(_OWORD *)&v22[64];
  *(_OWORD *)(v12 + 444) = *(_OWORD *)&v22[80];
  *(_OWORD *)(v12 + 460) = *(_OWORD *)&v22[96];
  *(_OWORD *)(v12 + 476) = *(_OWORD *)&v22[112];
  *(_OWORD *)(v12 + 364) = *(_OWORD *)v22;
  *(_OWORD *)(v12 + 380) = *(_OWORD *)&v22[16];
  *(_OWORD *)(v12 + 396) = *(_OWORD *)&v22[32];
  *(_OWORD *)(v12 + 412) = *(_OWORD *)&v22[48];
  *(_DWORD *)(v12 + 544) = v10;
  *(_OWORD *)(v12 + 676) = *(_OWORD *)&v21[128];
  *(_OWORD *)(v12 + 692) = *(_OWORD *)&v21[144];
  *(_OWORD *)(v12 + 708) = *(_OWORD *)&v21[160];
  *(_DWORD *)(v12 + 724) = *(_DWORD *)&v21[176];
  *(_OWORD *)(v12 + 612) = *(_OWORD *)&v21[64];
  *(_OWORD *)(v12 + 628) = *(_OWORD *)&v21[80];
  *(_OWORD *)(v12 + 644) = *(_OWORD *)&v21[96];
  *(_OWORD *)(v12 + 660) = *(_OWORD *)&v21[112];
  *(_OWORD *)(v12 + 548) = *(_OWORD *)v21;
  *(_OWORD *)(v12 + 564) = *(_OWORD *)&v21[16];
  *(_OWORD *)(v12 + 580) = *(_OWORD *)&v21[32];
  *(_OWORD *)(v12 + 596) = *(_OWORD *)&v21[48];
  *(_DWORD *)(v12 + 728) = v11;
  type metadata accessor for BNNSLayerParametersArithmetic(0);
  a5[3] = v18;
  a5[4] = (uint64_t)&protocol witness table for BNNSLayerParametersArithmetic;
  uint64_t v19 = swift_allocObject();
  *a5 = v19;
  *(_DWORD *)(v19 + 16) = 28;
  *(void *)(v19 + 24) = v12;
  *(_DWORD *)(v19 + 32) = 0;
  *(int32x2_t *)(v19 + 36) = vdup_n_s32(0x7FC00000u);
  *(_DWORD *)(v19 + 44) = 1;
  double result = 0.0;
  *(_OWORD *)(v19 + 48) = 0u;
  *(_OWORD *)(v19 + 64) = 0u;
  return result;
}

void BNNS.FusedTernaryArithmeticParameters.inputADescriptorType.getter(unsigned char *a1@<X8>)
{
  *a1 = *(unsigned char *)(v1 + 8);
}

unsigned char *BNNS.FusedTernaryArithmeticParameters.inputADescriptorType.setter(unsigned char *result)
{
  *(unsigned char *)(v1 + 8) = *result;
  return result;
}

uint64_t (*BNNS.FusedTernaryArithmeticParameters.inputADescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedTernaryArithmeticParameters.inputBDescriptorType.getter(unsigned char *a1@<X8>)
{
  *a1 = *(unsigned char *)(v1 + 9);
}

unsigned char *BNNS.FusedTernaryArithmeticParameters.inputBDescriptorType.setter(unsigned char *result)
{
  *(unsigned char *)(v1 + 9) = *result;
  return result;
}

uint64_t (*BNNS.FusedTernaryArithmeticParameters.inputBDescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedTernaryArithmeticParameters.inputCDescriptorType.getter(unsigned char *a1@<X8>)
{
  *a1 = *(unsigned char *)(v1 + 10);
}

unsigned char *BNNS.FusedTernaryArithmeticParameters.inputCDescriptorType.setter(unsigned char *result)
{
  *(unsigned char *)(v1 + 10) = *result;
  return result;
}

uint64_t (*BNNS.FusedTernaryArithmeticParameters.inputCDescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

void BNNS.FusedTernaryArithmeticParameters.outputDescriptorType.getter(unsigned char *a1@<X8>)
{
  *a1 = *(unsigned char *)(v1 + 11);
}

unsigned char *BNNS.FusedTernaryArithmeticParameters.outputDescriptorType.setter(unsigned char *result)
{
  *(unsigned char *)(v1 + 11) = *result;
  return result;
}

uint64_t (*BNNS.FusedTernaryArithmeticParameters.outputDescriptorType.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

uint64_t (*BNNS.FusedTernaryArithmeticParameters.function.modify())()
{
  return destructiveProjectEnumData for BNNS.ActivationFunction;
}

char *BNNS.FusedTernaryArithmeticParameters.init(inputADescriptorType:inputBDescriptorType:inputCDescriptorType:outputDescriptorType:function:)@<X0>(char *result@<X0>, char *a2@<X1>, char *a3@<X2>, char *a4@<X3>, uint64_t a5@<X8>)
{
  char v5 = *result;
  char v6 = *a2;
  char v7 = *a3;
  char v8 = *a4;
  *(void *)a5 = 0;
  *(unsigned char *)(a5 + 8) = v5;
  *(unsigned char *)(a5 + 9) = v6;
  *(unsigned char *)(a5 + 10) = v7;
  *(unsigned char *)(a5 + 11) = v8;
  return result;
}

uint64_t protocol witness for FusableLayerParametersWrapperDeallocatable.deallocate() in conformance BNNS.FusedTernaryArithmeticParameters()
{
  uint64_t result = *v0;
  if (*v0) {
    JUMPOUT(0x1D26009C0);
  }
  return result;
}

uint64_t BNNS.FusedParametersLayer.__allocating_init(inputA:inputB:inputC:output:fusedLayerParameters:filterParameters:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint32_t a6, size_t a7, int (__cdecl *a8)(void **, size_t, size_t), void (__cdecl *a9)(void *))
{
  uint64_t v50 = *MEMORY[0x1E4F143B8];
  if (*(void *)(a5 + 16) != 2)
  {
    __break(1u);
    goto LABEL_20;
  }
  outlined init with copy of BNNSOptimizer(a5 + 32, (uint64_t)&v34);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParameters);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableTernaryInputLayerParametersWrapper);
  if ((swift_dynamicCast() & 1) == 0)
  {
    long long v30 = 0u;
    memset(v31, 0, sizeof(v31));
    swift_bridgeObjectRelease();
    outlined destroy of FusableTernaryInputLayerParametersWrapper?((uint64_t)&v30, &demangling cache variable for type metadata for FusableTernaryInputLayerParametersWrapper?);
    return 0;
  }
  outlined init with take of FusableLayerParametersWrapper(&v30, (uint64_t)v47);
  if (*(void *)(a5 + 16) < 2uLL) {
LABEL_20:
  }
    __break(1u);
  outlined init with copy of BNNSOptimizer(a5 + 72, (uint64_t)&v34);
  swift_bridgeObjectRelease();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParametersWrapper);
  if ((swift_dynamicCast() & 1) == 0)
  {
    long long v30 = 0u;
    memset(v31, 0, sizeof(v31));
    outlined destroy of FusableTernaryInputLayerParametersWrapper?((uint64_t)&v30, &demangling cache variable for type metadata for FusableLayerParametersWrapper?);
LABEL_16:
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v47);
    return 0;
  }
  outlined init with take of FusableLayerParametersWrapper(&v30, (uint64_t)v44);
  uint64_t v18 = v48;
  uint64_t v17 = v49;
  __swift_mutable_project_boxed_opaque_existential_1((uint64_t)v47, v48);
  (*(void (**)(void *__return_ptr, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))(v17 + 8))(v43, a1, a2, a3, a4, v18, v17);
  uint64_t v19 = v45;
  uint64_t v20 = v46;
  __swift_mutable_project_boxed_opaque_existential_1((uint64_t)v44, v45);
  (*(void (**)(void *__return_ptr, uint64_t, uint64_t, uint64_t, uint64_t))(v20 + 8))(v42, a4, a4, v19, v20);
  uint64_t v21 = v45;
  uint64_t v22 = v46;
  __swift_project_boxed_opaque_existential_1(v44, v45);
  int v23 = (*(uint64_t (**)(uint64_t, uint64_t))(v22 + 16))(v21, v22);
  if ((v23 - 2) > 3)
  {
LABEL_15:
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v42);
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v43);
    __swift_destroy_boxed_opaque_existential_1((uint64_t)v44);
    goto LABEL_16;
  }
  int v24 = v23;
  outlined init with copy of BNNSOptimizer((uint64_t)v43, (uint64_t)v41);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
  type metadata accessor for BNNSLayerParametersArithmetic(0);
  swift_dynamicCast();
  LODWORD(v30) = v34;
  *((void *)&v30 + 1) = v35;
  v31[0] = v36;
  *(_OWORD *)&v31[1] = v37;
  v31[5] = v38;
  long long v32 = v39;
  uint64_t v33 = v40;
  long long v25 = specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)((uint64_t)&v30, (uint64_t)v42, a6, a7, a8, a9, 8, v24);
  type metadata accessor for BNNS.FusedParametersLayer();
  uint64_t v26 = swift_allocObject();
  uint64_t v27 = v26;
  *(void *)(v26 + 24) = MEMORY[0x1E4FBC860];
  if (!v25)
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    goto LABEL_15;
  }
  *(void *)(v26 + 16) = v25;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<FusableLayerParametersWrapperDeallocatable?>);
  uint64_t v28 = swift_allocObject();
  *(_OWORD *)(v28 + 16) = xmmword_1D2135290;
  outlined init with copy of BNNSOptimizer((uint64_t)v47, (uint64_t)&v34);
  swift_retain();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for FusableLayerParametersWrapperDeallocatable);
  if ((swift_dynamicCast() & 1) == 0)
  {
    *(void *)(v28 + 64) = 0;
    *(_OWORD *)(v28 + 32) = 0u;
    *(_OWORD *)(v28 + 48) = 0u;
  }
  outlined init with copy of BNNSOptimizer((uint64_t)v44, (uint64_t)&v30);
  if ((swift_dynamicCast() & 1) == 0)
  {
    *(void *)(v28 + 104) = 0;
    *(_OWORD *)(v28 + 72) = 0u;
    *(_OWORD *)(v28 + 88) = 0u;
  }
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v42);
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v43);
  *(void *)(v27 + 24) = v28;
  swift_release();
  swift_bridgeObjectRelease();
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v44);
  __swift_destroy_boxed_opaque_existential_1((uint64_t)v47);
  return v27;
}

uint64_t BNNS.FusedParametersLayer.apply(batchSize:inputA:inputB:inputC:output:for:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, unint64_t a5, char a6)
{
  unint64_t v84 = a5;
  outlined init with take of UnsafeMutableRawPointer?(a2 + 136, (uint64_t)v79);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v79, (uint64_t)&v80);
  uint64_t v10 = v80;
  if (!v80) {
    goto LABEL_7;
  }
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v78);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v78, (uint64_t)&v81);
  uint64_t v11 = v81;
  if (!v81) {
    goto LABEL_7;
  }
  outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v77);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v77, (uint64_t)&v82);
  uint64_t v12 = v82;
  if (v82
    && (outlined init with take of UnsafeMutableRawPointer?(v84 + 136, (uint64_t)v76),
        outlined init with take of UnsafeMutableRawPointer?((uint64_t)v76, (uint64_t)&v83),
        v83))
  {
    char v47 = a6;
    out = v83;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
    uint64_t v13 = swift_allocObject();
    *(_OWORD *)(v13 + 16) = xmmword_1D2135DC0;
    *(void *)(v13 + 32) = v10;
    uint64_t v46 = (const void **)(v13 + 32);
    *(void *)(v13 + 40) = v11;
    *(void *)(v13 + 48) = v12;
    uint64_t v45 = *(void **)(v6 + 16);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
    uint64_t v49 = swift_allocObject();
    *(_OWORD *)(v49 + 16) = xmmword_1D2135DC0;
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v71);
    outlined init with take of BNNS.Shape((uint64_t)v71, (uint64_t)v72);
    outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)v70);
    BNNS.Shape.size.getter((uint64_t)&v62);
    unint64_t v14 = v62;
    unint64_t v15 = v63;
    unint64_t v16 = v64;
    unint64_t v17 = v65;
    unint64_t v18 = v66;
    unint64_t v41 = v68;
    unint64_t in_stride = v67;
    unint64_t v38 = v69;
    outlined init with take of BNNS.Shape((uint64_t)v72, (uint64_t)v70);
    BNNS.Shape.stride.getter((uint64_t)&v62);
    *(void *)(v49 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v14, v15, v16, v17, v18, in_stride, v41, v38, v62, v63, v64, v65, v66, v67, v68, v69);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v70);
    outlined init with take of BNNS.Shape((uint64_t)v70, (uint64_t)v73);
    outlined init with take of BNNS.Shape((uint64_t)v73, (uint64_t)&v62);
    BNNS.Shape.size.getter((uint64_t)&v57);
    long long v19 = v57;
    long long v20 = v58;
    long long v21 = v59;
    unint64_t v39 = v61;
    unint64_t v42 = v60;
    outlined init with take of BNNS.Shape((uint64_t)v73, (uint64_t)&v62);
    BNNS.Shape.stride.getter((uint64_t)&v57);
    *(void *)(v49 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v19, *((unint64_t *)&v19 + 1), v20, *((unint64_t *)&v20 + 1), v21, *((unint64_t *)&v21 + 1), v42, v39, v57, *((unint64_t *)&v57 + 1), v58, *((unint64_t *)&v58 + 1), v59, *((unint64_t *)&v59 + 1), v60, v61);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)&v62);
    outlined init with take of BNNS.Shape((uint64_t)&v62, (uint64_t)v74);
    outlined init with take of BNNS.Shape((uint64_t)v74, (uint64_t)&v57);
    BNNS.Shape.size.getter((uint64_t)&v51);
    unint64_t v22 = v52;
    long long v23 = v53;
    long long v24 = v54;
    unint64_t v25 = v55;
    unint64_t v40 = v56;
    unint64_t v43 = v51;
    outlined init with take of BNNS.Shape((uint64_t)v74, (uint64_t)&v57);
    BNNS.Shape.stride.getter((uint64_t)&v51);
    *(void *)(v49 + 48) = specialized static BNNS.calculateBatchStride(size:stride:)(v43, v22, v23, *((unint64_t *)&v23 + 1), v24, *((unint64_t *)&v24 + 1), v25, v40, v51, v52, v53, *((unint64_t *)&v53 + 1), v54, *((unint64_t *)&v54 + 1), v55, v56);
    BNNSNDArrayDescriptor.shape.getter((uint64_t)v71);
    outlined init with take of BNNS.Shape((uint64_t)v71, (uint64_t)v75);
    outlined init with take of BNNS.Shape((uint64_t)v75, (uint64_t)v70);
    BNNS.Shape.size.getter((uint64_t)&v62);
    unint64_t v26 = v62;
    unint64_t v27 = v63;
    unint64_t v28 = v64;
    unint64_t v29 = v65;
    unint64_t v30 = v66;
    unint64_t v31 = v67;
    unint64_t v32 = v68;
    unint64_t v84 = v69;
    outlined init with take of BNNS.Shape((uint64_t)v75, (uint64_t)v70);
    BNNS.Shape.stride.getter((uint64_t)&v62);
    size_t v33 = specialized static BNNS.calculateBatchStride(size:stride:)(v26, v27, v28, v29, v30, v31, v32, v84, v62, v63, v64, v65, v66, v67, v68, v69);
    int v34 = BNNSFusedFilterApplyMultiInputBatch(v45, a1, 3uLL, v46, (const size_t *)(v49 + 32), out, v33, (v47 & 1) == 0);
    swift_bridgeObjectRelease();
    uint64_t result = swift_bridgeObjectRelease();
    if (!v34) {
      return result;
    }
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *int v36 = 0;
  }
  else
  {
LABEL_7:
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v37 = 2;
  }
  return swift_willThrow();
}

uint64_t BNNS.FusedParametersLayer.applyBackward(batchSize:inputA:inputB:inputC:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingInputCGradient:generatingParameterGradients:)(size_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, _OWORD *a6, _OWORD *a7, _OWORD *a8, long long *a9, uint64_t a10)
{
  size_t v11 = a1;
  uint64_t v89 = *MEMORY[0x1E4F143B8];
  int64_t v12 = *(void *)(a10 + 16);
  uint64_t v13 = MEMORY[0x1E4FBC860];
  if (v12)
  {
    unint64_t v64 = a6;
    uint64_t v65 = a5;
    unint64_t v67 = a8;
    unint64_t v68 = a7;
    uint64_t v69 = a2;
    uint64_t v71 = a3;
    uint64_t v72 = a4;
    *(void *)&v77[0] = MEMORY[0x1E4FBC860];
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v12, 0);
    unint64_t v14 = (long long *)(a10 + 32);
    uint64_t v13 = *(void *)&v77[0];
    do
    {
      long long v15 = v14[9];
      long long v86 = v14[8];
      long long v87 = v15;
      long long v88 = v14[10];
      long long v16 = v14[5];
      long long v82 = v14[4];
      long long v83 = v16;
      long long v17 = v14[7];
      long long v84 = v14[6];
      long long v85 = v17;
      long long v18 = v14[1];
      long long v78 = *v14;
      long long v79 = v18;
      long long v19 = v14[3];
      long long v80 = v14[2];
      long long v81 = v19;
      long long v20 = (_OWORD *)swift_slowAlloc();
      long long v21 = v87;
      uint8_t v20[8] = v86;
      v20[9] = v21;
      v20[10] = v88;
      long long v22 = v83;
      uint16_t v20[4] = v82;
      void v20[5] = v22;
      long long v23 = v85;
      v20[6] = v84;
      v20[7] = v23;
      long long v24 = v79;
      *long long v20 = v78;
      v20[1] = v24;
      long long v25 = v81;
      v20[2] = v80;
      v20[3] = v25;
      if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, *(void *)(v13 + 16) + 1, 1);
        uint64_t v13 = *(void *)&v77[0];
      }
      unint64_t v27 = *(void *)(v13 + 16);
      unint64_t v26 = *(void *)(v13 + 24);
      if (v27 >= v26 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v26 > 1), v27 + 1, 1);
        uint64_t v13 = *(void *)&v77[0];
      }
      *(void *)(v13 + 16) = v27 + 1;
      *(void *)(v13 + 8 * v27 + 32) = v20;
      v14 += 11;
      --v12;
    }
    while (v12);
    a4 = v72;
    uint64_t v10 = v70;
    a3 = v71;
    a7 = v68;
    a2 = v69;
    size_t v11 = a1;
    a8 = v67;
    a6 = v64;
    a5 = v65;
  }
  long long v28 = a7[8];
  long long v29 = a7[9];
  long long v30 = a7[6];
  long long v85 = a7[7];
  long long v86 = v28;
  long long v31 = a7[10];
  long long v87 = v29;
  long long v88 = v31;
  long long v32 = a7[4];
  long long v33 = a7[5];
  long long v34 = a7[2];
  long long v81 = a7[3];
  long long v82 = v32;
  long long v83 = v33;
  long long v84 = v30;
  long long v35 = *a7;
  long long v79 = a7[1];
  long long v80 = v34;
  long long v36 = a8[9];
  unsigned char v77[8] = a8[8];
  v77[9] = v36;
  v77[10] = a8[10];
  long long v78 = v35;
  long long v37 = a8[5];
  v77[4] = a8[4];
  v77[5] = v37;
  long long v38 = a8[7];
  v77[6] = a8[6];
  v77[7] = v38;
  long long v39 = a8[1];
  v77[0] = *a8;
  v77[1] = v39;
  long long v40 = a8[3];
  v77[2] = a8[2];
  v77[3] = v40;
  long long v41 = a9[8];
  long long v42 = a9[9];
  long long v43 = a9[6];
  v76[7] = a9[7];
  unsigned char v76[8] = v41;
  long long v44 = a9[10];
  v76[9] = v42;
  v76[10] = v44;
  long long v45 = a9[4];
  long long v46 = a9[5];
  long long v47 = a9[2];
  v76[3] = a9[3];
  v76[4] = v45;
  uint64_t v74 = (char *)v13;
  int v73 = 0;
  v76[5] = v46;
  v76[6] = v43;
  long long v48 = *a9;
  v76[1] = a9[1];
  v76[2] = v47;
  long long v49 = a6[9];
  *(_OWORD *)&v75.stride[7] = a6[8];
  *(_OWORD *)&v75.data_type = v49;
  *(_OWORD *)&v75.table_data_type = a6[10];
  v76[0] = v48;
  long long v50 = a6[5];
  *(_OWORD *)&v75.size[7] = a6[4];
  *(_OWORD *)&v75.stride[1] = v50;
  long long v51 = a6[7];
  *(_OWORD *)&v75.stride[3] = a6[6];
  *(_OWORD *)&v75.stride[5] = v51;
  long long v52 = a6[1];
  *(_OWORD *)&v75.flags = *a6;
  *(_OWORD *)&v75.size[1] = v52;
  long long v53 = a6[3];
  *(_OWORD *)&v75.size[3] = a6[2];
  *(_OWORD *)&v75.size[5] = v53;
  closure #1 in closure #1 in closure #1 in closure #2 in BNNS.FusedParametersLayer.applyBackward(batchSize:inputA:inputB:inputC:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingInputCGradient:generatingParameterGradients:)(&v75, a2, a3, a4, (uint64_t)&v78, (uint64_t)v77, (uint64_t)v76, &v73, v10, v11, (uint64_t)a7, (uint64_t)a8, (uint64_t)a9, a5, (uint64_t)a6, &v74);
  if (v73)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v54 = 0;
    swift_willThrow();
    unint64_t v55 = v74;
    uint64_t v56 = *((void *)v74 + 2);
    if (v56)
    {
      swift_bridgeObjectRetain();
      for (uint64_t i = 0; i != v56; ++i)
      {
        uint64_t v58 = *(void *)&v55[8 * i + 32];
        if (v58) {
          MEMORY[0x1D26009C0](v58, -1, -1);
        }
      }
LABEL_20:
      swift_bridgeObjectRelease();
    }
  }
  else
  {
    long long v59 = v74;
    uint64_t v60 = *((void *)v74 + 2);
    if (v60)
    {
      swift_bridgeObjectRetain();
      for (uint64_t j = 0; j != v60; ++j)
      {
        uint64_t v62 = *(void *)&v59[8 * j + 32];
        if (v62) {
          MEMORY[0x1D26009C0](v62, -1, -1);
        }
      }
      goto LABEL_20;
    }
  }
  return swift_bridgeObjectRelease();
}

uint64_t closure #1 in closure #1 in closure #1 in closure #2 in BNNS.FusedParametersLayer.applyBackward(batchSize:inputA:inputB:inputC:output:outputGradient:generatingInputAGradient:generatingInputBGradient:generatingInputCGradient:generatingParameterGradients:)(BNNSNDArrayDescriptor *a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, int *a8, uint64_t a9, size_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15, char **a16)
{
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer?>);
  uint64_t v22 = swift_allocObject();
  *(_OWORD *)(v22 + 16) = xmmword_1D2135DC0;
  long long v23 = (void *)v22;
  outlined init with take of UnsafeMutableRawPointer?(a2 + 136, (uint64_t)v157);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v157, (uint64_t)&v158);
  v23[4] = v158;
  in = (void **)(v23 + 4);
  outlined init with take of UnsafeMutableRawPointer?(a3 + 136, (uint64_t)v156);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v156, (uint64_t)&v159);
  v23[5] = v159;
  outlined init with take of UnsafeMutableRawPointer?(a4 + 136, (uint64_t)v155);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v155, (uint64_t)&v160);
  v23[6] = v160;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeMutablePointer<BNNSNDArrayDescriptor>>);
  uint64_t v90 = swift_allocObject();
  *(_OWORD *)(v90 + 16) = xmmword_1D2135DC0;
  *(void *)(v90 + 32) = a5;
  *(void *)(v90 + 40) = a6;
  *(void *)(v90 + 48) = a7;
  long long v91 = *(void **)(a9 + 16);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int>);
  uint64_t v162 = swift_allocObject();
  *(_OWORD *)(v162 + 16) = xmmword_1D2135DC0;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v153);
  outlined init with take of BNNS.Shape((uint64_t)v153, (uint64_t)v130);
  outlined init with take of BNNS.Shape((uint64_t)v130, (uint64_t)&v145);
  BNNS.Shape.size.getter((uint64_t)&v140);
  long long v24 = v140;
  long long v25 = v141;
  long long v26 = v142;
  unint64_t v27 = v143;
  unint64_t v28 = v144;
  outlined init with take of BNNS.Shape((uint64_t)v130, (uint64_t)&v145);
  BNNS.Shape.stride.getter((uint64_t)&v140);
  unint64_t v29 = specialized static BNNS.calculateBatchStride(size:stride:)(v24, *((unint64_t *)&v24 + 1), v25, *((unint64_t *)&v25 + 1), v26, *((unint64_t *)&v26 + 1), v27, v28, v140, *((unint64_t *)&v140 + 1), v141, *((unint64_t *)&v141 + 1), v142, *((unint64_t *)&v142 + 1), v143, v144);
  uint64_t v30 = v162;
  uint64_t v31 = v162;
  *(void *)(v162 + 32) = v29;
  uint64_t v89 = (const size_t *)(v31 + 32);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v145);
  outlined init with take of BNNS.Shape((uint64_t)&v145, (uint64_t)v131);
  outlined init with take of BNNS.Shape((uint64_t)v131, (uint64_t)&v140);
  BNNS.Shape.size.getter((uint64_t)&v132);
  unint64_t v85 = v132;
  unint64_t v32 = v133;
  unint64_t v33 = v134;
  unint64_t v34 = v135;
  unint64_t v35 = v136;
  unint64_t v36 = v137;
  unint64_t v37 = v138;
  unint64_t v38 = v139;
  outlined init with take of BNNS.Shape((uint64_t)v131, (uint64_t)&v140);
  BNNS.Shape.stride.getter((uint64_t)&v132);
  *(void *)(v30 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v85, v32, v33, v34, v35, v36, v37, v38, v132, v133, v134, v135, v136, v137, v138, v139);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v140);
  outlined init with take of BNNS.Shape((uint64_t)&v140, (uint64_t)&v132);
  outlined init with take of BNNS.Shape((uint64_t)&v132, (uint64_t)v129);
  BNNS.Shape.size.getter((uint64_t)&v121);
  unint64_t v39 = v121;
  unint64_t v40 = v122;
  unint64_t v41 = v123;
  unint64_t v42 = v124;
  unint64_t v43 = v125;
  unint64_t v44 = v126;
  unint64_t v45 = v127;
  unint64_t v46 = v128;
  outlined init with take of BNNS.Shape((uint64_t)&v132, (uint64_t)v129);
  BNNS.Shape.stride.getter((uint64_t)&v121);
  unint64_t v47 = specialized static BNNS.calculateBatchStride(size:stride:)(v39, v40, v41, v42, v43, v44, v45, v46, v121, v122, v123, v124, v125, v126, v127, v128);
  *(void *)(v162 + 48) = v47;
  uint64_t v95 = swift_allocObject();
  *(_OWORD *)(v95 + 16) = xmmword_1D2135DC0;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v129);
  outlined init with take of BNNS.Shape((uint64_t)v129, (uint64_t)&v140);
  outlined init with take of BNNS.Shape((uint64_t)&v140, (uint64_t)v153);
  BNNS.Shape.size.getter((uint64_t)&v145);
  unint64_t v48 = v145;
  unint64_t v49 = v146;
  unint64_t v50 = v147;
  unint64_t v51 = v148;
  unint64_t v52 = v149;
  unint64_t v53 = v150;
  unint64_t v54 = v151;
  unint64_t v55 = v152;
  outlined init with take of BNNS.Shape((uint64_t)&v140, (uint64_t)v153);
  BNNS.Shape.stride.getter((uint64_t)&v145);
  *(void *)(v95 + 32) = specialized static BNNS.calculateBatchStride(size:stride:)(v48, v49, v50, v51, v52, v53, v54, v55, v145, v146, v147, v148, v149, v150, v151, v152);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v121);
  outlined init with take of BNNS.Shape((uint64_t)&v121, (uint64_t)&v145);
  outlined init with take of BNNS.Shape((uint64_t)&v145, (uint64_t)v153);
  BNNS.Shape.size.getter((uint64_t)&v116);
  long long v56 = v116;
  long long v57 = v117;
  long long v58 = v118;
  unint64_t v59 = v119;
  unint64_t v86 = v120;
  outlined init with take of BNNS.Shape((uint64_t)&v145, (uint64_t)v153);
  BNNS.Shape.stride.getter((uint64_t)&v116);
  *(void *)(v95 + 40) = specialized static BNNS.calculateBatchStride(size:stride:)(v56, *((unint64_t *)&v56 + 1), v57, *((unint64_t *)&v57 + 1), v58, *((unint64_t *)&v58 + 1), v59, v86, v116, *((unint64_t *)&v116 + 1), v117, *((unint64_t *)&v117 + 1), v118, *((unint64_t *)&v118 + 1), v119, v120);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v116);
  outlined init with take of BNNS.Shape((uint64_t)&v116, (uint64_t)v153);
  outlined init with take of BNNS.Shape((uint64_t)v153, (uint64_t)&v108);
  BNNS.Shape.size.getter((uint64_t)&v103);
  long long v60 = v103;
  long long v61 = v104;
  long long v62 = v105;
  unint64_t v63 = v106;
  unint64_t v64 = v107;
  outlined init with take of BNNS.Shape((uint64_t)v153, (uint64_t)&v108);
  BNNS.Shape.stride.getter((uint64_t)&v103);
  *(void *)(v95 + 48) = specialized static BNNS.calculateBatchStride(size:stride:)(v60, *((unint64_t *)&v60 + 1), v61, *((unint64_t *)&v61 + 1), v62, *((unint64_t *)&v62 + 1), v63, v64, v103, *((unint64_t *)&v103 + 1), v104, *((unint64_t *)&v104 + 1), v105, *((unint64_t *)&v105 + 1), v106, v107);
  outlined init with take of UnsafeMutableRawPointer?(a14 + 136, (uint64_t)v154);
  outlined init with take of UnsafeMutableRawPointer?((uint64_t)v154, (uint64_t)&v161);
  out = v161;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v116);
  outlined init with take of BNNS.Shape((uint64_t)&v116, (uint64_t)&v121);
  outlined init with take of BNNS.Shape((uint64_t)&v121, (uint64_t)v129);
  BNNS.Shape.size.getter((uint64_t)&v108);
  unint64_t v65 = v108;
  unint64_t v66 = v109;
  unint64_t v67 = v110;
  unint64_t v68 = v111;
  unint64_t v69 = v112;
  unint64_t v70 = v113;
  unint64_t v72 = v114;
  unint64_t v71 = v115;
  outlined init with take of BNNS.Shape((uint64_t)&v121, (uint64_t)v129);
  BNNS.Shape.stride.getter((uint64_t)&v108);
  size_t out_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v65, v66, v67, v68, v69, v70, v72, v71, v108, v109, v110, v111, v112, v113, v114, v115);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v108);
  outlined init with take of BNNS.Shape((uint64_t)&v108, (uint64_t)v129);
  outlined init with take of BNNS.Shape((uint64_t)v129, (uint64_t)&v103);
  BNNS.Shape.size.getter((uint64_t)&v96);
  unint64_t v73 = v96;
  unint64_t v74 = v97;
  unint64_t v75 = v98;
  unint64_t v76 = v99;
  long long v77 = v100;
  unint64_t v78 = v101;
  unint64_t v79 = v102;
  outlined init with take of BNNS.Shape((uint64_t)v129, (uint64_t)&v103);
  BNNS.Shape.stride.getter((uint64_t)&v96);
  size_t out_delta_stride = specialized static BNNS.calculateBatchStride(size:stride:)(v73, v74, v75, v76, v77, *((unint64_t *)&v77 + 1), v78, v79, v96, v97, v98, v99, v100, *((unint64_t *)&v100 + 1), v101, v102);
  long long v81 = *a16;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  char isUniquelyReferenced_nonNull_native = swift_isUniquelyReferenced_nonNull_native();
  *a16 = v81;
  if ((isUniquelyReferenced_nonNull_native & 1) == 0) {
    long long v81 = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, *((void *)v81 + 2), 0, v81);
  }
  *a16 = v81;
  swift_bridgeObjectRetain();
  int v83 = BNNSFusedFilterApplyBackwardMultiInputBatch(v91, a10, 3uLL, (const void **)in, v89, (BNNSNDArrayDescriptor **)(v90 + 32), (const size_t *)(v95 + 32), out, out_stride, a1, out_delta_stride, (BNNSNDArrayDescriptor **)v81 + 4);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease_n();
  uint64_t result = swift_bridgeObjectRelease_n();
  *a8 = v83;
  return result;
}

uint64_t outlined destroy of FusableTernaryInputLayerParametersWrapper?(uint64_t a1, uint64_t *a2)
{
  uint64_t v3 = __swift_instantiateConcreteTypeFromMangledName(a2);
  (*(void (**)(uint64_t, uint64_t))(*(void *)(v3 - 8) + 8))(a1, v3);
  return a1;
}

uint64_t outlined init with take of FusableLayerParametersWrapper(long long *a1, uint64_t a2)
{
  long long v2 = *a1;
  long long v3 = a1[1];
  *(void *)(a2 + 32) = *((void *)a1 + 4);
  *(_OWORD *)a2 = v2;
  *(_OWORD *)(a2 + 16) = v3;
  return a2;
}

uint64_t __swift_mutable_project_boxed_opaque_existential_1(uint64_t a1, uint64_t a2)
{
  if ((*(_DWORD *)(*(void *)(a2 - 8) + 80) & 0x20000) != 0)
  {
    swift_makeBoxUnique();
    return v2;
  }
  return result;
}

void *specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)(uint64_t a1, uint64_t a2, uint32_t a3, size_t a4, int (__cdecl *a5)(void **, size_t, size_t), void (__cdecl *a6)(void *), int a7, int a8)
{
  return specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)(a1, a2, a3, a4, a5, a6, a7, a8);
}

{
  uint64_t v14;
  const void **v15;
  uint64_t v16;
  const BNNSFilterType *v17;
  const void **v18;
  BNNSFilterParameters *p_filter_params;
  uint64_t v20;
  void *FusedLayer;
  BNNSFilterParameters filter_params;
  unsigned char __dst[1128];
  unsigned char __src[1128];
  unsigned char v27[40];
  uint64_t v28;

  unint64_t v28 = *MEMORY[0x1E4F143B8];
  outlined init with copy of BNNSOptimizer(a2, (uint64_t)v27);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
  type metadata accessor for BNNSLayerParametersNormalization(0);
  swift_dynamicCast();
  memcpy(__dst, __src, sizeof(__dst));
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
  unint64_t v14 = swift_allocObject();
  *(_OWORD *)(v14 + 16) = xmmword_1D2135290;
  if (!a1) {
    __break(1u);
  }
  long long v15 = (const void **)(v14 + 32);
  *(void *)(v14 + 32) = a1;
  *(void *)(v14 + 40) = __dst;
  if (a5 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<BNNSFilterType>);
    long long v16 = swift_allocObject();
    *(_DWORD *)(v16 + 32) = a7;
    long long v17 = (const BNNSFilterType *)(v16 + 32);
    *(_DWORD *)(v16 + 36) = a8;
    long long v18 = v15;
    p_BNNSFilterParameters filter_params = 0;
  }
  else
  {
    filter_params.flags = a3;
    filter_params.n_threads = a4;
    filter_params.alloc_memory = a5;
    filter_params.free_memory = a6;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<BNNSFilterType>);
    long long v20 = swift_allocObject();
    *(_DWORD *)(v20 + 32) = a7;
    long long v17 = (const BNNSFilterType *)(v20 + 32);
    *(_DWORD *)(v20 + 36) = a8;
    p_BNNSFilterParameters filter_params = &filter_params;
    long long v18 = v15;
  }
  FusedLayer = BNNSFilterCreateFusedLayer(2uLL, v17, v18, p_filter_params);
  swift_setDeallocating();
  swift_deallocClassInstance();
  swift_bridgeObjectRelease();
  return FusedLayer;
}

{
  return specialized closure #1 in static BNNS.FusedParametersLayer.makeFusedLayer<A, B>(zero:zeroType:filterTypeZero:one:oneType:filterTypeOne:filterParameters:)(a1, a2, a3, a4, a5, a6, a7, a8);
}

{
  uint64_t v14;
  const void **v15;
  uint64_t v16;
  const BNNSFilterType *v17;
  const void **v18;
  BNNSFilterParameters *p_filter_params;
  uint64_t v20;
  void *FusedLayer;
  BNNSFilterParameters filter_params;
  unsigned char __dst[720];
  unsigned char __src[720];
  unsigned char v27[40];
  uint64_t v28;

  unint64_t v28 = *MEMORY[0x1E4F143B8];
  outlined init with copy of BNNSOptimizer(a2, (uint64_t)v27);
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for BNNSFusableLayerParameters);
  type metadata accessor for BNNSLayerParametersQuantization(0);
  swift_dynamicCast();
  memcpy(__dst, __src, sizeof(__dst));
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UnsafeRawPointer>);
  unint64_t v14 = swift_allocObject();
  *(_OWORD *)(v14 + 16) = xmmword_1D2135290;
  if (!a1) {
    __break(1u);
  }
  long long v15 = (const void **)(v14 + 32);
  *(void *)(v14 + 32) = a1;
  *(void *)(v14 + 40) = __dst;
  if (a5 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<BNNSFilterType>);
    long long v16 = swift_allocObject();
    *(_DWORD *)(v16 + 32) = a7;
    long long v17 = (const BNNSFilterType *)(v16 + 32);
    *(_DWORD *)(v16 + 36) = a8;
    long long v18 = v15;
    p_BNNSFilterParameters filter_params = 0;
  }
  else
  {
    filter_params.flags = a3;
    filter_params.n_threads = a4;
    filter_params.alloc_memory = a5;
    filter_params.free_memory = a6;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<BNNSFilterType>);
    long long v20 = swift_allocObject();
    *(_DWORD *)(v20 + 32) = a7;
    long long v17 = (const BNNSFilterType *)(v20 + 32);
    *(_DWORD *)(v20 + 36) = a8;
    p_BNNSFilterParameters filter_params = &filter_params;
    long long v18 = v15;
  }
  FusedLayer = BNNSFilterCreateFusedLayer(2uLL, v17, v18, p_filter_params);
  swift_setDeallocating();
  swift_deallocClassInstance();
  swift_bridgeObjectRelease();
  return FusedLayer;
}

void *dispatch thunk of FusableTernaryInputLayerParametersWrapper.layerParameters(inputA:inputB:inputC:output:)@<X0>(uint64_t *a1@<X0>, uint64_t *a2@<X1>, uint64_t *a3@<X2>, uint64_t *a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, void *a7@<X8>)
{
  void *(*v23)(void *__return_ptr, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t);
  long long v24;
  long long v25;
  long long v26;
  long long v27;
  long long v28;
  long long v29;
  long long v30;
  long long v31;
  uint64_t v32;
  long long v33;
  long long v34;
  long long v35;
  long long v36;
  long long v37;
  long long v38;
  long long v39;
  long long v40;
  uint64_t v41;
  long long v42;
  uint64_t v44;
  long long v45;
  long long v46;
  long long v47;
  long long v48;
  long long v49;
  long long v50;
  long long v51;
  long long v52;
  uint64_t v53;
  int v54;
  uint64_t v55;
  int v56;
  uint64_t v57;
  uint64_t v58;
  long long v59;
  long long v60;
  long long v61;
  long long v62;
  long long v63;
  long long v64;
  long long v65;
  long long v66;
  uint64_t v67;
  int v68;
  uint64_t v69;
  int v70;
  uint64_t v71;
  uint64_t v72;
  long long v73;
  long long v74;
  long long v75;
  long long v76;
  long long v77;
  long long v78;
  long long v79;
  long long v80;
  uint64_t v81;
  int v82;
  uint64_t v83;
  int v84;
  uint64_t v85;
  uint64_t v86;
  long long v87;
  long long v88;
  long long v89;
  long long v90;
  long long v91;
  long long v92;
  long long v93;
  long long v94;
  uint64_t v95;
  int v96;
  uint64_t v97;
  int v98;
  uint64_t v99;

  uint64_t v7 = a1[17];
  int v8 = *((_DWORD *)a1 + 36);
  uint64_t v9 = a1[19];
  int v10 = *((_DWORD *)a1 + 40);
  uint64_t v11 = a2[17];
  int v12 = *((_DWORD *)a2 + 36);
  uint64_t v13 = a2[19];
  int v14 = *((_DWORD *)a2 + 40);
  uint64_t v15 = a3[17];
  int v16 = *((_DWORD *)a3 + 36);
  uint64_t v17 = a3[19];
  int v18 = *((_DWORD *)a3 + 40);
  uint64_t v19 = a4[17];
  int v20 = *((_DWORD *)a4 + 36);
  uint64_t v21 = a4[19];
  int v22 = *((_DWORD *)a4 + 40);
  long long v23 = *(void *(**)(void *__return_ptr, uint64_t *, uint64_t *, uint64_t *, uint64_t *, uint64_t))(a6 + 8);
  unint64_t v86 = *a1;
  long long v87 = *(_OWORD *)(a1 + 1);
  long long v88 = *(_OWORD *)(a1 + 3);
  uint64_t v89 = *(_OWORD *)(a1 + 5);
  uint64_t v90 = *(_OWORD *)(a1 + 7);
  long long v91 = *(_OWORD *)(a1 + 9);
  long long v92 = *(_OWORD *)(a1 + 11);
  long long v93 = *(_OWORD *)(a1 + 13);
  long long v94 = *(_OWORD *)(a1 + 15);
  uint64_t v95 = v7;
  unint64_t v96 = v8;
  unint64_t v97 = v9;
  unint64_t v98 = v10;
  unint64_t v99 = *(uint64_t *)((char *)a1 + 164);
  unint64_t v72 = *a2;
  unint64_t v73 = *(_OWORD *)(a2 + 1);
  unint64_t v74 = *(_OWORD *)(a2 + 3);
  unint64_t v75 = *(_OWORD *)(a2 + 5);
  unint64_t v76 = *(_OWORD *)(a2 + 7);
  long long v77 = *(_OWORD *)(a2 + 9);
  unint64_t v78 = *(_OWORD *)(a2 + 11);
  long long v24 = *(_OWORD *)(a2 + 15);
  unint64_t v79 = *(_OWORD *)(a2 + 13);
  long long v25 = *(_OWORD *)(a3 + 1);
  long long v26 = *(_OWORD *)(a3 + 3);
  unint64_t v27 = *(_OWORD *)(a3 + 5);
  unint64_t v28 = *(_OWORD *)(a3 + 7);
  unint64_t v29 = *(_OWORD *)(a3 + 9);
  uint64_t v30 = *(_OWORD *)(a3 + 11);
  uint64_t v31 = *(_OWORD *)(a3 + 13);
  unint64_t v32 = *a3;
  unint64_t v33 = *(_OWORD *)(a3 + 15);
  unint64_t v34 = *(_OWORD *)(a4 + 1);
  unint64_t v35 = *(_OWORD *)(a4 + 3);
  unint64_t v36 = *(_OWORD *)(a4 + 5);
  unint64_t v37 = *(_OWORD *)(a4 + 7);
  unint64_t v38 = *(_OWORD *)(a4 + 9);
  unint64_t v39 = *(_OWORD *)(a4 + 11);
  unint64_t v40 = *(_OWORD *)(a4 + 13);
  unint64_t v41 = *a4;
  unint64_t v42 = *(_OWORD *)(a4 + 15);
  long long v80 = v24;
  long long v81 = v11;
  long long v82 = v12;
  int v83 = v13;
  long long v84 = v14;
  unint64_t v85 = *(uint64_t *)((char *)a2 + 164);
  long long v58 = v32;
  *(void *)&long long v24 = *(uint64_t *)((char *)a4 + 164);
  unint64_t v59 = v25;
  *(void *)&long long v25 = *(uint64_t *)((char *)a3 + 164);
  long long v60 = v26;
  long long v61 = v27;
  long long v62 = v28;
  unint64_t v63 = v29;
  unint64_t v64 = v30;
  unint64_t v65 = v31;
  unint64_t v66 = v33;
  unint64_t v67 = v15;
  unint64_t v68 = v16;
  unint64_t v69 = v17;
  unint64_t v70 = v18;
  unint64_t v71 = v25;
  unint64_t v44 = v41;
  unint64_t v45 = v34;
  unint64_t v46 = v35;
  unint64_t v47 = v36;
  unint64_t v48 = v37;
  unint64_t v49 = v38;
  unint64_t v50 = v39;
  unint64_t v51 = v40;
  unint64_t v52 = v42;
  unint64_t v53 = v19;
  unint64_t v54 = v20;
  unint64_t v55 = v21;
  long long v56 = v22;
  long long v57 = v24;
  return v23(a7, &v86, &v72, &v58, &v44, a5);
}

uint64_t dispatch thunk of FusableTernaryInputLayerParametersWrapper.filterType.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

uint64_t __swift_memcpy12_8(uint64_t result, uint64_t *a2)
{
  uint64_t v2 = *a2;
  *(_DWORD *)(result + 8) = *((_DWORD *)a2 + 2);
  *(void *)uint64_t result = v2;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.FusedTernaryArithmeticParameters(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFE && *(unsigned char *)(a1 + 12)) {
    return (*(_DWORD *)a1 + 254);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 8);
  BOOL v4 = v3 >= 3;
  int v5 = v3 - 3;
  if (!v4) {
    int v5 = -1;
  }
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.FusedTernaryArithmeticParameters(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFD)
  {
    *(_DWORD *)(result + 8) = 0;
    *(void *)uint64_t result = a2 - 254;
    if (a3 >= 0xFE) {
      *(unsigned char *)(result + 12) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFE) {
      *(unsigned char *)(result + 12) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 8) = a2 + 2;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.FusedTernaryArithmeticParameters()
{
  return &type metadata for BNNS.FusedTernaryArithmeticParameters;
}

uint64_t sub_1D20C3410()
{
  return MEMORY[0x1F4186498](v0, 80, 7);
}

uint64_t BNNS.NormalizationLayer.__allocating_init(type:input:output:beta:gamma:momentum:epsilon:activation:filterParameters:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, int (__cdecl *a9)(void **, size_t, size_t), void (__cdecl *a10)(void *))
{
  uint64_t v10 = MEMORY[0x1F4188790](a1);
  uint32_t v70 = v11;
  size_t v71 = v12;
  int v14 = v13;
  float v16 = v15;
  float v18 = v17;
  v175[46] = *MEMORY[0x1E4F143B8];
  long long v20 = v19[9];
  long long v138 = v19[8];
  long long v139 = v20;
  long long v140 = v19[10];
  long long v21 = v19[5];
  long long v134 = v19[4];
  long long v135 = v21;
  long long v22 = v19[6];
  long long v137 = v19[7];
  long long v136 = v22;
  long long v23 = v19[1];
  long long v130 = *v19;
  long long v131 = v23;
  long long v24 = v19[2];
  long long v133 = v19[3];
  long long v132 = v24;
  long long v26 = v25[9];
  long long v149 = v25[8];
  long long v150 = v26;
  long long v151 = v25[10];
  long long v27 = v25[5];
  long long v145 = v25[4];
  long long v146 = v27;
  long long v28 = v25[6];
  long long v148 = v25[7];
  long long v147 = v28;
  long long v29 = v25[1];
  long long v141 = *v25;
  long long v142 = v29;
  long long v30 = v25[2];
  long long v144 = v25[3];
  long long v143 = v30;
  long long v32 = v31[9];
  long long v160 = v31[8];
  long long v161 = v32;
  long long v162 = v31[10];
  long long v33 = v31[5];
  long long v156 = v31[4];
  long long v157 = v33;
  long long v34 = v31[6];
  long long v159 = v31[7];
  long long v158 = v34;
  long long v35 = v31[1];
  long long v152 = *v31;
  long long v153 = v35;
  long long v36 = v31[2];
  long long v155 = v31[3];
  long long v154 = v36;
  long long v38 = v37[9];
  long long v171 = v37[8];
  long long v172 = v38;
  long long v173 = v37[10];
  long long v39 = v37[5];
  long long v167 = v37[4];
  long long v168 = v39;
  long long v40 = v37[6];
  long long v170 = v37[7];
  long long v169 = v40;
  long long v41 = v37[1];
  long long v163 = *v37;
  long long v164 = v41;
  long long v42 = v37[2];
  long long v166 = v37[3];
  long long v165 = v42;
  outlined init with take of BNNS.NormalizationType(v10, (uint64_t)v174);
  uint64_t v43 = *v14;
  char v44 = *((unsigned char *)v14 + 8);
  outlined init with take of BNNS.NormalizationType((uint64_t)v174, (uint64_t)v175);
  unsigned int v45 = _s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v175);
  if (v45 >= 2)
  {
    if (v45 == 2)
    {
      _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v175);
      size_t v91 = 0;
      size_t v92 = 0;
      size_t v89 = 0;
      size_t v90 = 0;
      size_t v87 = 0;
      size_t v88 = 0;
      size_t v85 = 0;
      size_t v86 = 0;
      size_t v83 = 0;
      size_t v84 = 0;
      size_t v81 = 0;
      size_t v82 = 0;
      size_t v79 = 0;
      size_t v80 = 0;
      size_t v77 = 0;
      size_t v78 = 0;
      unint64_t v74 = 0;
      unint64_t v75 = 0;
      BNNSDataType v50 = 0;
      size_t v111 = 0;
      size_t v112 = 0;
      size_t v109 = 0;
      size_t v110 = 0;
      size_t v106 = 0;
      size_t v107 = 0;
      size_t v104 = 0;
      size_t v105 = 0;
      size_t v102 = 0;
      size_t v103 = 0;
      long long v100 = 0;
      size_t v101 = 0;
      size_t v99 = 0;
      size_t v96 = 0;
      size_t v97 = 0;
      size_t v94 = 0;
      size_t v95 = 0;
      long long v93 = 0;
      BNNSDataType v98 = 0;
      size_t v51 = 0;
      uint64_t v108 = 0;
    }
    else
    {
      size_t v91 = 0;
      size_t v92 = 0;
      size_t v89 = 0;
      size_t v90 = 0;
      size_t v87 = 0;
      size_t v88 = 0;
      size_t v85 = 0;
      size_t v86 = 0;
      size_t v83 = 0;
      size_t v84 = 0;
      size_t v81 = 0;
      size_t v82 = 0;
      size_t v79 = 0;
      size_t v80 = 0;
      size_t v77 = 0;
      size_t v78 = 0;
      unint64_t v74 = 0;
      unint64_t v75 = 0;
      BNNSDataType v50 = 0;
      size_t v111 = 0;
      size_t v112 = 0;
      size_t v109 = 0;
      size_t v110 = 0;
      size_t v106 = 0;
      size_t v107 = 0;
      size_t v104 = 0;
      size_t v105 = 0;
      size_t v102 = 0;
      size_t v103 = 0;
      long long v100 = 0;
      size_t v101 = 0;
      size_t v99 = 0;
      size_t v96 = 0;
      size_t v97 = 0;
      size_t v94 = 0;
      size_t v95 = 0;
      long long v93 = 0;
      BNNSDataType v98 = 0;
      uint64_t v108 = 0;
      size_t v51 = *(void *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v175);
    }
    uint64_t v49 = 0;
    uint64_t v48 = 0;
    uint64_t v76 = 0;
    uint64_t v72 = 0;
    uint64_t v73 = 0;
  }
  else
  {
    uint64_t v46 = _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v175);
    uint64_t v47 = v46 + 184;
    outlined init with take of BNNSNDArrayDescriptor?(v46, (uint64_t)v128, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    outlined init with take of BNNSNDArrayDescriptor?(v47, (uint64_t)v129, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v128, (uint64_t)&__dst, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    uint64_t v48 = 0;
    if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)&__dst) == 1)
    {
      BNNSDataType v98 = 0;
      size_t v99 = 0;
      long long v100 = 0;
      long long v93 = 0;
      size_t v94 = 0;
      size_t v95 = 0;
      size_t v96 = 0;
      size_t v97 = 0;
      size_t v101 = 0;
      size_t v102 = 0;
      size_t v103 = 0;
      size_t v104 = 0;
      size_t v105 = 0;
      size_t v106 = 0;
      size_t v107 = 0;
      size_t v109 = 0;
      size_t v110 = 0;
      size_t v111 = 0;
      size_t v112 = 0;
      uint64_t v108 = 0;
      uint64_t v49 = 0;
      uint64_t v76 = 0;
    }
    else
    {
      outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v128, (uint64_t)__src, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      uint64_t v108 = *(void *)&__src[0];
      size_t v111 = *(void *)&__src[1];
      size_t v112 = *((void *)&__src[0] + 1);
      size_t v109 = *(void *)&__src[2];
      size_t v110 = *((void *)&__src[1] + 1);
      size_t v106 = *(void *)&__src[3];
      size_t v107 = *((void *)&__src[2] + 1);
      size_t v104 = *(void *)&__src[4];
      size_t v105 = *((void *)&__src[3] + 1);
      size_t v102 = *(void *)&__src[5];
      size_t v103 = *((void *)&__src[4] + 1);
      size_t v99 = *(void *)&__src[6];
      size_t v96 = *(void *)&__src[7];
      size_t v97 = *((void *)&__src[6] + 1);
      size_t v94 = *(void *)&__src[8];
      size_t v95 = *((void *)&__src[7] + 1);
      long long v93 = (void *)*((void *)&__src[8] + 1);
      long long v100 = (void *)*((void *)&__src[9] + 1);
      uint64_t v76 = *(void *)&__src[9];
      size_t v101 = *((void *)&__src[5] + 1);
      BNNSDataType v98 = __src[10];
      uint64_t v49 = *(void *)((char *)&__src[10] + 4);
      LODWORD(v47) = HIDWORD(__src[10]);
    }
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v129, (uint64_t)__src, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    int v69 = v47;
    if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)__src) == 1)
    {
      BNNSDataType v50 = 0;
      unint64_t v74 = 0;
      unint64_t v75 = 0;
      size_t v77 = 0;
      size_t v78 = 0;
      size_t v79 = 0;
      size_t v80 = 0;
      size_t v81 = 0;
      size_t v82 = 0;
      size_t v83 = 0;
      size_t v84 = 0;
      size_t v85 = 0;
      size_t v86 = 0;
      size_t v87 = 0;
      size_t v88 = 0;
      size_t v89 = 0;
      size_t v90 = 0;
      size_t v91 = 0;
      size_t v92 = 0;
      uint64_t v72 = 0;
      uint64_t v73 = 0;
    }
    else
    {
      outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v129, (uint64_t)v117, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
      uint64_t v73 = v117[0];
      size_t v91 = v117[2];
      size_t v92 = v117[1];
      size_t v89 = v117[4];
      size_t v90 = v117[3];
      size_t v87 = v117[6];
      size_t v88 = v117[5];
      size_t v85 = v117[8];
      size_t v86 = v117[7];
      size_t v83 = v117[10];
      size_t v84 = v117[9];
      size_t v81 = v117[12];
      size_t v82 = v117[11];
      size_t v79 = v117[14];
      size_t v80 = v117[13];
      size_t v77 = v117[16];
      size_t v78 = v117[15];
      uint64_t v72 = v117[18];
      unint64_t v74 = (void *)v117[19];
      unint64_t v75 = (void *)v117[17];
      BNNSDataType v50 = v118;
      uint64_t v48 = v119;
      int v68 = v120;
    }
    size_t v51 = 0;
  }
  *(void *)&__src[0] = v43;
  BYTE8(__src[0]) = v44;
  BNNS.ActivationFunction.bnnsActivation.getter((uint64_t)&v121);
  uint64_t v52 = v122;
  uint64_t v53 = v123;
  BNNSActivationFunction v54 = v121;
  int32_t v55 = v124;
  long long v56 = v125;
  long long v57 = v126;
  long long v58 = v127;
  outlined init with take of BNNS.NormalizationType((uint64_t)v174, (uint64_t)v117);
  if (_s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v117) == 2) {
    size_t v59 = *(void *)_s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v117);
  }
  else {
    size_t v59 = 0;
  }
  __src[8] = v171;
  __src[9] = v172;
  __src[4] = v167;
  __src[5] = v168;
  __src[6] = v169;
  __src[7] = v170;
  __src[0] = v163;
  __src[1] = v164;
  __src[2] = v165;
  __src[3] = v166;
  __src[18] = v159;
  __src[19] = v160;
  __src[20] = v161;
  __src[21] = v162;
  __src[13] = v154;
  __src[14] = v155;
  __src[15] = v156;
  __src[16] = v157;
  __src[17] = v158;
  __src[10] = v173;
  __src[11] = v152;
  __src[12] = v153;
  __src[30] = v149;
  __src[31] = v150;
  __src[26] = v145;
  __src[27] = v146;
  __src[29] = v148;
  __src[28] = v147;
  _OWORD __src[22] = v141;
  _OWORD __src[23] = v142;
  __src[25] = v144;
  __src[24] = v143;
  __src[40] = v137;
  __src[41] = v138;
  __src[42] = v139;
  __src[43] = v140;
  __src[36] = v133;
  __src[37] = v134;
  __src[38] = v135;
  __src[39] = v136;
  __src[32] = v151;
  _OWORD __src[33] = v130;
  __src[34] = v131;
  __src[35] = v132;
  memcpy(&__dst, __src, 0x2C0uLL);
  __dst.moving_mean_desc.size[0] = v112;
  __dst.moving_mean_desc.size[1] = v111;
  __dst.moving_mean_desc.size[2] = v110;
  __dst.moving_mean_desc.size[3] = v109;
  __dst.moving_mean_desc.size[4] = v107;
  __dst.moving_mean_desc.size[5] = v106;
  __dst.moving_mean_desc.size[6] = v105;
  __dst.moving_mean_desc.size[7] = v104;
  __dst.moving_mean_desc.stride[0] = v103;
  __dst.moving_mean_desc.stride[1] = v102;
  __dst.moving_mean_desc.stride[2] = v101;
  __dst.moving_mean_desc.stride[3] = v99;
  __dst.moving_mean_desc.stride[4] = v97;
  __dst.moving_mean_desc.stride[5] = v96;
  __dst.moving_mean_desc.stride[6] = v95;
  __dst.moving_mean_desc.stride[7] = v94;
  __dst.moving_mean_desc.data = v93;
  *(void *)&__dst.moving_mean_desc.flags = v108;
  *(void *)&__dst.moving_mean_desc.data_type = v76;
  __dst.moving_mean_desc.table_data = v100;
  __dst.moving_mean_desc.table_data_type = v98;
  *(void *)&__dst.moving_mean_desc.data_scale = v49;
  *((_DWORD *)&__dst.moving_mean_desc.data_bias + 1) = v69;
  *(void *)&__dst.moving_variance_desc.flags = v73;
  __dst.moving_variance_desc.size[0] = v92;
  __dst.moving_variance_desc.size[1] = v91;
  __dst.moving_variance_desc.size[2] = v90;
  __dst.moving_variance_desc.size[3] = v89;
  __dst.moving_variance_desc.size[4] = v88;
  __dst.moving_variance_desc.size[5] = v87;
  __dst.moving_variance_desc.size[6] = v86;
  __dst.moving_variance_desc.size[7] = v85;
  __dst.moving_variance_desc.stride[0] = v84;
  __dst.moving_variance_desc.stride[1] = v83;
  __dst.moving_variance_desc.stride[2] = v82;
  __dst.moving_variance_desc.stride[3] = v81;
  __dst.moving_variance_desc.stride[4] = v80;
  __dst.moving_variance_desc.stride[5] = v79;
  __dst.moving_variance_desc.stride[6] = v78;
  __dst.moving_variance_desc.stride[7] = v77;
  __dst.moving_variance_desc.data = v75;
  *(void *)&__dst.moving_variance_desc.data_type = v72;
  __dst.moving_variance_desc.table_data = v74;
  __dst.moving_variance_desc.table_data_type = v50;
  *(void *)&__dst.moving_variance_desc.data_scale = v48;
  *((_DWORD *)&__dst.moving_variance_desc.data_bias + 1) = v68;
  __dst.momentum = v18;
  __dst.epsilon = v16;
  __dst.activation.function = v54;
  *(void *)&__dst.activation.alpha = v52;
  *(void *)&__dst.activation.iscale = v53;
  __dst.activation.ishift = v55;
  __dst.activation.iscale_per_channel = v56;
  __dst.activation.ioffset_per_channel = v57;
  __dst.activation.ishift_per_channel = v58;
  __dst.num_groups = v51;
  __dst.normalization_axis = v59;
  if (a9 == (int (__cdecl *)(void **, size_t, size_t))1)
  {
    outlined init with take of BNNS.NormalizationType((uint64_t)v174, (uint64_t)v114);
    BNNSFilterType v60 = _s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v114) + 2;
    _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v114);
    BNNSFilterType v61 = v60;
    p_BNNSFilterParameters filter_params = 0;
  }
  else
  {
    filter_params.flags = v70;
    filter_params.n_threads = v71;
    filter_params.alloc_memory = a9;
    filter_params.free_memory = a10;
    outlined init with take of BNNS.NormalizationType((uint64_t)v174, (uint64_t)v114);
    BNNSFilterType v63 = _s10Accelerate4BNNSO17NormalizationTypeOWOg((uint64_t)v114) + 2;
    _s10Accelerate4BNNSO17NormalizationTypeOWOj0_((uint64_t)v114);
    p_BNNSFilterParameters filter_params = &filter_params;
    BNNSFilterType v61 = v63;
  }
  unint64_t v64 = BNNSFilterCreateLayerNormalization(v61, &__dst, p_filter_params);
  type metadata accessor for BNNS.NormalizationLayer();
  uint64_t v65 = swift_allocObject();
  uint64_t v66 = v65;
  if (v64)
  {
    *(void *)(v65 + 16) = v64;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v66;
}

uint64_t BNNS.NormalizationLayer.apply(batchSize:input:output:for:)(size_t a1, uint64_t a2, uint64_t a3, char a4)
{
  return specialized static BNNS.normalizationFilterApply(_:batchSize:input:output:for:)(v4, a1, a2, a3, a4 & 1);
}

uint64_t BNNS.NormalizationLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingBetaGradient:generatingGammaGradient:)(size_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, _OWORD *a5, uint64_t a6, uint64_t a7)
{
  return specialized static BNNS.normalizationLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingBetaGradient:generatingGammaGradient:)(v7, a1, a3, a4, a5, a6, a7);
}

uint64_t type metadata accessor for BNNS.NormalizationLayer()
{
  return self;
}

uint64_t BNNS.NormalizationLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.NormalizationLayer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.normalizationLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingBetaGradient:generatingGammaGradient:)(BNNSNDArrayDescriptor *a1, uint64_t a2, size_t a3, BNNSNDArrayDescriptor *a4, uint64_t a5, uint64_t a6, BNNSNDArrayDescriptor *a7, uint64_t a8, BNNSNDArrayDescriptor *beta_delta)
{
  long long v36 = *(void **)(a2 + 16);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v65);
  outlined init with take of BNNS.Shape((uint64_t)v65, (uint64_t)v68);
  outlined init with take of BNNS.Shape((uint64_t)v68, (uint64_t)v64);
  BNNS.Shape.size.getter((uint64_t)&v56);
  unint64_t v10 = v56;
  unint64_t v11 = v57;
  unint64_t v12 = v58;
  unint64_t v13 = v59;
  unint64_t v14 = v60;
  unint64_t v15 = v61;
  unint64_t v16 = v62;
  unint64_t v17 = v63;
  outlined init with take of BNNS.Shape((uint64_t)v68, (uint64_t)v64);
  BNNS.Shape.stride.getter((uint64_t)&v56);
  size_t v35 = specialized static BNNS.calculateBatchStride(size:stride:)(v10, v11, v12, v13, v14, v15, v16, v17, v56, v57, v58, v59, v60, v61, v62, v63);
  outlined init with take of BNNSNDArrayDescriptor?(a6 + 136, (uint64_t)v66, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v66, (uint64_t)&v67, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  long long v34 = v67;
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v55);
  outlined init with take of BNNS.Shape((uint64_t)v55, (uint64_t)&v56);
  outlined init with take of BNNS.Shape((uint64_t)&v56, (uint64_t)v64);
  BNNS.Shape.size.getter((uint64_t)&v47);
  unint64_t v18 = v47;
  unint64_t v19 = v48;
  unint64_t v20 = v49;
  unint64_t v21 = v50;
  unint64_t v22 = v51;
  unint64_t v23 = v52;
  unint64_t v25 = v53;
  unint64_t v24 = v54;
  outlined init with take of BNNS.Shape((uint64_t)&v56, (uint64_t)v64);
  BNNS.Shape.stride.getter((uint64_t)&v47);
  size_t v26 = specialized static BNNS.calculateBatchStride(size:stride:)(v18, v19, v20, v21, v22, v23, v25, v24, v47, v48, v49, v50, v51, v52, v53, v54);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v47);
  outlined init with take of BNNS.Shape((uint64_t)&v47, (uint64_t)v64);
  outlined init with take of BNNS.Shape((uint64_t)v64, (uint64_t)v46);
  BNNS.Shape.size.getter((uint64_t)&v41);
  long long v27 = v41;
  long long v28 = v42;
  long long v29 = v43;
  unint64_t v30 = v44;
  unint64_t v31 = v45;
  outlined init with take of BNNS.Shape((uint64_t)v64, (uint64_t)v46);
  BNNS.Shape.stride.getter((uint64_t)&v41);
  size_t v32 = specialized static BNNS.calculateBatchStride(size:stride:)(v27, *((unint64_t *)&v27 + 1), v28, *((unint64_t *)&v28 + 1), v29, *((unint64_t *)&v29 + 1), v30, v31, v41, *((unint64_t *)&v41 + 1), v42, *((unint64_t *)&v42 + 1), v43, *((unint64_t *)&v43 + 1), v44, v45);
  return BNNSNormalizationFilterApplyBackwardBatch(v36, a3, a4, v35, v34, v26, a7, v32, beta_delta, a1);
}

uint64_t specialized static BNNS.normalizationFilterApply(_:batchSize:input:output:for:)(uint64_t a1, size_t a2, uint64_t a3, uint64_t a4, char a5)
{
  uint64_t v48 = a4;
  unint64_t v25 = *(void **)(a1 + 16);
  outlined init with take of BNNSNDArrayDescriptor?(a3 + 136, (uint64_t)v45, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  uint64_t result = outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v45, (uint64_t)&v46, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  in = v46;
  if (!v46)
  {
    __break(1u);
    goto LABEL_7;
  }
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v42);
  outlined init with take of BNNS.Shape((uint64_t)v42, (uint64_t)v43);
  outlined init with take of BNNS.Shape((uint64_t)v43, (uint64_t)v41);
  BNNS.Shape.size.getter((uint64_t)&v33);
  unint64_t v6 = v33;
  unint64_t v7 = v34;
  unint64_t v8 = v35;
  unint64_t v9 = v36;
  unint64_t v10 = v37;
  unint64_t v11 = v38;
  unint64_t v12 = v39;
  unint64_t v13 = v40;
  outlined init with take of BNNS.Shape((uint64_t)v43, (uint64_t)v41);
  BNNS.Shape.stride.getter((uint64_t)&v33);
  size_t v22 = specialized static BNNS.calculateBatchStride(size:stride:)(v6, v7, v8, v9, v10, v11, v12, v13, v33, v34, v35, v36, v37, v38, v39, v40);
  outlined init with take of BNNSNDArrayDescriptor?(v48 + 136, (uint64_t)v44, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  uint64_t result = outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v44, (uint64_t)&v47, &demangling cache variable for type metadata for UnsafeMutableRawPointer?);
  unint64_t v21 = v47;
  if (!v47)
  {
LABEL_7:
    __break(1u);
    return result;
  }
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v33);
  outlined init with take of BNNS.Shape((uint64_t)&v33, (uint64_t)v41);
  outlined init with take of BNNS.Shape((uint64_t)v41, (uint64_t)v32);
  BNNS.Shape.size.getter((uint64_t)&v27);
  long long v14 = v27;
  long long v15 = v28;
  long long v16 = v29;
  unint64_t v17 = v30;
  unint64_t v18 = v31;
  outlined init with take of BNNS.Shape((uint64_t)v41, (uint64_t)v32);
  BNNS.Shape.stride.getter((uint64_t)&v27);
  size_t v19 = specialized static BNNS.calculateBatchStride(size:stride:)(v14, *((unint64_t *)&v14 + 1), v15, *((unint64_t *)&v15 + 1), v16, *((unint64_t *)&v16 + 1), v17, v18, v27, *((unint64_t *)&v27 + 1), v28, *((unint64_t *)&v28 + 1), v29, *((unint64_t *)&v29 + 1), v30, v31);
  uint64_t result = BNNSNormalizationFilterApplyBatch(v25, a2, in, v22, v21, v19, (a5 & 1) == 0);
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *unint64_t v20 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t specialized static BNNS.normalizationLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingBetaGradient:generatingGammaGradient:)(uint64_t a1, size_t a2, uint64_t a3, _OWORD *a4, _OWORD *a5, uint64_t a6, uint64_t a7)
{
  uint64_t v44 = *MEMORY[0x1E4F143B8];
  long long v13 = a5[9];
  *(_OWORD *)&v40.stride[7] = a5[8];
  *(_OWORD *)&v40.data_type = v13;
  *(_OWORD *)&v40.table_data_type = a5[10];
  long long v14 = a5[5];
  *(_OWORD *)&v40.size[7] = a5[4];
  *(_OWORD *)&v40.stride[1] = v14;
  long long v15 = a5[7];
  *(_OWORD *)&v40.stride[3] = a5[6];
  *(_OWORD *)&v40.stride[5] = v15;
  long long v16 = a5[1];
  *(_OWORD *)&v40.flags = *a5;
  *(_OWORD *)&v40.size[1] = v16;
  long long v17 = a5[3];
  *(_OWORD *)&v40.size[3] = a5[2];
  *(_OWORD *)&v40.size[5] = v17;
  long long v18 = a4[9];
  *(_OWORD *)&v39.stride[7] = a4[8];
  *(_OWORD *)&v39.data_type = v18;
  *(_OWORD *)&v39.table_data_type = a4[10];
  long long v19 = a4[5];
  *(_OWORD *)&v39.size[7] = a4[4];
  *(_OWORD *)&v39.stride[1] = v19;
  long long v20 = a4[7];
  *(_OWORD *)&v39.stride[3] = a4[6];
  *(_OWORD *)&v39.stride[5] = v20;
  long long v21 = a4[1];
  *(_OWORD *)&v39.flags = *a4;
  *(_OWORD *)&v39.size[1] = v21;
  long long v22 = a4[3];
  *(_OWORD *)&v39.size[3] = a4[2];
  *(_OWORD *)&v39.size[5] = v22;
  outlined init with take of BNNSNDArrayDescriptor?(a6, (uint64_t)v43, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v43) == 1)
  {
    outlined init with take of BNNSNDArrayDescriptor?(a7, (uint64_t)v42, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v42) == 1)
    {
      size_t v26 = 0;
LABEL_6:
      unint64_t v23 = 0;
      goto LABEL_9;
    }
    long long v36 = v42[8];
    long long v37 = v42[9];
    long long v38 = v42[10];
    long long v32 = v42[4];
    long long v33 = v42[5];
    long long v34 = v42[6];
    long long v35 = v42[7];
    long long v28 = v42[0];
    long long v29 = v42[1];
    long long v30 = v42[2];
    long long v31 = v42[3];
    size_t v26 = 0;
    unint64_t v23 = (BNNSNDArrayDescriptor *)&v28;
  }
  else
  {
    long long v36 = v43[8];
    long long v37 = v43[9];
    long long v38 = v43[10];
    long long v32 = v43[4];
    long long v33 = v43[5];
    long long v34 = v43[6];
    long long v35 = v43[7];
    long long v28 = v43[0];
    long long v29 = v43[1];
    long long v30 = v43[2];
    long long v31 = v43[3];
    outlined init with take of BNNSNDArrayDescriptor?(a7, (uint64_t)v41, &demangling cache variable for type metadata for BNNSNDArrayDescriptor?);
    if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v41) == 1)
    {
      size_t v26 = (BNNSNDArrayDescriptor *)&v28;
      goto LABEL_6;
    }
    v27[8] = v41[8];
    v27[9] = v41[9];
    v27[10] = v41[10];
    v27[4] = v41[4];
    v27[5] = v41[5];
    v27[6] = v41[6];
    v27[7] = v41[7];
    v27[0] = v41[0];
    v27[1] = v41[1];
    v27[2] = v41[2];
    v27[3] = v41[3];
    size_t v26 = (BNNSNDArrayDescriptor *)&v28;
    unint64_t v23 = (BNNSNDArrayDescriptor *)v27;
  }
LABEL_9:
  uint64_t result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.normalizationLayerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingBetaGradient:generatingGammaGradient:)(v23, a1, a2, &v40, (uint64_t)a5, a3, &v39, (uint64_t)a4, v26);
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *unint64_t v25 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t method lookup function for BNNS.NormalizationLayer(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.NormalizationLayer);
}

uint64_t dispatch thunk of BNNS.NormalizationLayer.apply(batchSize:input:output:for:)(uint64_t a1, uint64_t *a2, uint64_t *a3, char a4)
{
  uint64_t v5 = a2[17];
  int v6 = *((_DWORD *)a2 + 36);
  uint64_t v7 = a2[19];
  int v8 = *((_DWORD *)a2 + 40);
  uint64_t v9 = a3[17];
  int v10 = *((_DWORD *)a3 + 36);
  uint64_t v11 = a3[19];
  int v12 = *((_DWORD *)a3 + 40);
  long long v13 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, void))(*(void *)v4 + 96);
  uint64_t v29 = *a2;
  long long v30 = *(_OWORD *)(a2 + 1);
  long long v31 = *(_OWORD *)(a2 + 3);
  long long v32 = *(_OWORD *)(a2 + 5);
  long long v33 = *(_OWORD *)(a2 + 7);
  long long v34 = *(_OWORD *)(a2 + 9);
  long long v35 = *(_OWORD *)(a2 + 11);
  long long v36 = *(_OWORD *)(a2 + 13);
  long long v37 = *(_OWORD *)(a2 + 15);
  uint64_t v38 = v5;
  int v39 = v6;
  uint64_t v40 = v7;
  int v41 = v8;
  uint64_t v42 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v15 = *a3;
  long long v16 = *(_OWORD *)(a3 + 1);
  long long v17 = *(_OWORD *)(a3 + 3);
  long long v18 = *(_OWORD *)(a3 + 5);
  long long v19 = *(_OWORD *)(a3 + 7);
  long long v20 = *(_OWORD *)(a3 + 9);
  long long v21 = *(_OWORD *)(a3 + 11);
  long long v22 = *(_OWORD *)(a3 + 13);
  long long v23 = *(_OWORD *)(a3 + 15);
  uint64_t v24 = v9;
  int v25 = v10;
  uint64_t v26 = v11;
  int v27 = v12;
  uint64_t v28 = *(uint64_t *)((char *)a3 + 164);
  return v13(a1, &v29, &v15, a4 & 1);
}

uint64_t dispatch thunk of BNNS.NormalizationLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingBetaGradient:generatingGammaGradient:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v39 = *(void *)(a2 + 136);
  int v8 = *(_DWORD *)(a2 + 144);
  uint64_t v9 = *(void *)(a2 + 152);
  int v10 = *(_DWORD *)(a2 + 160);
  uint64_t v11 = *(void *)(a3 + 136);
  int v12 = *(_DWORD *)(a3 + 144);
  uint64_t v13 = *(void *)(a3 + 152);
  int v14 = *(_DWORD *)(a3 + 160);
  uint64_t v15 = *(void *)(a4 + 136);
  int v16 = *(_DWORD *)(a4 + 144);
  uint64_t v17 = *(void *)(a4 + 152);
  int v18 = *(_DWORD *)(a4 + 160);
  uint64_t v19 = *(void *)(a5 + 136);
  int v20 = *(_DWORD *)(a5 + 144);
  uint64_t v21 = *(void *)(a5 + 152);
  int v22 = *(_DWORD *)(a5 + 160);
  char v23 = *(unsigned char *)(a6 + 176);
  char v24 = *(unsigned char *)(a7 + 176);
  int v25 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, _OWORD *, _OWORD *))(*(void *)v7 + 104);
  long long v26 = *(_OWORD *)(a2 + 104);
  long long v92 = *(_OWORD *)(a2 + 88);
  long long v27 = *(_OWORD *)(a2 + 120);
  long long v93 = v26;
  long long v94 = v27;
  uint64_t v99 = *(void *)(a2 + 164);
  *(void *)&long long v27 = *(void *)a2;
  long long v87 = *(_OWORD *)(a2 + 8);
  long long v88 = *(_OWORD *)(a2 + 24);
  long long v89 = *(_OWORD *)(a2 + 40);
  long long v90 = *(_OWORD *)(a2 + 56);
  long long v91 = *(_OWORD *)(a2 + 72);
  long long v78 = *(_OWORD *)(a3 + 88);
  long long v79 = *(_OWORD *)(a3 + 104);
  long long v80 = *(_OWORD *)(a3 + 120);
  uint64_t v85 = *(void *)(a3 + 164);
  long long v73 = *(_OWORD *)(a3 + 8);
  long long v74 = *(_OWORD *)(a3 + 24);
  long long v75 = *(_OWORD *)(a3 + 40);
  long long v76 = *(_OWORD *)(a3 + 56);
  long long v77 = *(_OWORD *)(a3 + 72);
  long long v64 = *(_OWORD *)(a4 + 88);
  long long v65 = *(_OWORD *)(a4 + 104);
  long long v66 = *(_OWORD *)(a4 + 120);
  uint64_t v71 = *(void *)(a4 + 164);
  long long v59 = *(_OWORD *)(a4 + 8);
  long long v60 = *(_OWORD *)(a4 + 24);
  long long v61 = *(_OWORD *)(a4 + 40);
  long long v62 = *(_OWORD *)(a4 + 56);
  long long v63 = *(_OWORD *)(a4 + 72);
  long long v45 = *(_OWORD *)(a5 + 8);
  long long v46 = *(_OWORD *)(a5 + 24);
  long long v47 = *(_OWORD *)(a5 + 40);
  long long v48 = *(_OWORD *)(a5 + 56);
  long long v49 = *(_OWORD *)(a5 + 72);
  long long v50 = *(_OWORD *)(a5 + 88);
  long long v51 = *(_OWORD *)(a5 + 104);
  long long v52 = *(_OWORD *)(a5 + 120);
  uint64_t v57 = *(void *)(a5 + 164);
  *(void *)&long long v26 = *(void *)a3;
  uint64_t v86 = v27;
  *(void *)&long long v27 = *(void *)a4;
  uint64_t v72 = v26;
  *(void *)&long long v26 = *(void *)a5;
  uint64_t v58 = v27;
  uint64_t v44 = v26;
  long long v28 = *(_OWORD *)(a6 + 16);
  v42[0] = *(_OWORD *)a6;
  v42[1] = v28;
  long long v29 = *(_OWORD *)(a6 + 48);
  v42[2] = *(_OWORD *)(a6 + 32);
  v42[3] = v29;
  long long v30 = *(_OWORD *)(a6 + 80);
  v42[4] = *(_OWORD *)(a6 + 64);
  void v42[5] = v30;
  long long v31 = *(_OWORD *)(a6 + 112);
  v42[6] = *(_OWORD *)(a6 + 96);
  v42[7] = v31;
  long long v32 = *(_OWORD *)(a6 + 144);
  v42[8] = *(_OWORD *)(a6 + 128);
  v42[9] = v32;
  v42[10] = *(_OWORD *)(a6 + 160);
  long long v33 = *(_OWORD *)(a7 + 16);
  v40[0] = *(_OWORD *)a7;
  v40[1] = v33;
  long long v34 = *(_OWORD *)(a7 + 48);
  v40[2] = *(_OWORD *)(a7 + 32);
  v40[3] = v34;
  long long v35 = *(_OWORD *)(a7 + 80);
  v40[4] = *(_OWORD *)(a7 + 64);
  v40[5] = v35;
  long long v36 = *(_OWORD *)(a7 + 112);
  v40[6] = *(_OWORD *)(a7 + 96);
  v40[7] = v36;
  long long v37 = *(_OWORD *)(a7 + 144);
  unsigned char v40[8] = *(_OWORD *)(a7 + 128);
  v40[9] = v37;
  v40[10] = *(_OWORD *)(a7 + 160);
  uint64_t v95 = v39;
  int v96 = v8;
  uint64_t v97 = v9;
  int v98 = v10;
  uint64_t v81 = v11;
  int v82 = v12;
  uint64_t v83 = v13;
  int v84 = v14;
  uint64_t v67 = v15;
  int v68 = v16;
  uint64_t v69 = v17;
  int v70 = v18;
  uint64_t v53 = v19;
  int v54 = v20;
  uint64_t v55 = v21;
  int v56 = v22;
  char v43 = v23;
  char v41 = v24;
  return v25(a1, &v86, &v72, &v58, &v44, v42, v40);
}

void *__swift_memcpy361_8(void *a1, const void *a2)
{
  return memcpy(a1, a2, 0x169uLL);
}

uint64_t getEnumTagSinglePayload for BNNS.NormalizationType(uint64_t a1, int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0 && *(unsigned char *)(a1 + 361)) {
    return *(_DWORD *)a1 + 0x80000000;
  }
  uint64_t v2 = *(void *)(a1 + 176) >> 1;
  if (v2 > 0x80000000) {
    int v3 = ~v2;
  }
  else {
    int v3 = -1;
  }
  return (v3 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.NormalizationType(uint64_t result, int a2, int a3)
{
  if (a2 < 0)
  {
    *(_OWORD *)(result + 248) = 0u;
    *(_OWORD *)(result + 232) = 0u;
    *(_OWORD *)(result + 216) = 0u;
    *(_OWORD *)(result + 200) = 0u;
    *(_OWORD *)(result + 184) = 0u;
    *(_OWORD *)(result + 168) = 0u;
    *(_OWORD *)(result + 152) = 0u;
    *(_OWORD *)(result + 136) = 0u;
    *(_OWORD *)(result + 120) = 0u;
    *(_OWORD *)(result + 104) = 0u;
    *(_OWORD *)(result + 88) = 0u;
    *(_OWORD *)(result + 72) = 0u;
    *(_OWORD *)(result + 56) = 0u;
    *(_OWORD *)(result + 40) = 0u;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(unsigned char *)(result + 360) = 0;
    *(_OWORD *)(result + 344) = 0u;
    *(_OWORD *)(result + 328) = 0u;
    *(_OWORD *)(result + 312) = 0u;
    *(_OWORD *)(result + 296) = 0u;
    *(_OWORD *)(result + 280) = 0u;
    *(_OWORD *)(result + 264) = 0u;
    *(void *)uint64_t result = a2 ^ 0x80000000;
    if (a3 < 0) {
      *(unsigned char *)(result + 361) = 1;
    }
  }
  else
  {
    if ((a3 & 0x80000000) == 0)
    {
      if (!a2) {
        return result;
      }
LABEL_8:
      *(_OWORD *)(result + 144) = 0u;
      *(_OWORD *)(result + 160) = 0u;
      *(_OWORD *)(result + 112) = 0u;
      *(_OWORD *)(result + 128) = 0u;
      *(_OWORD *)(result + 80) = 0u;
      *(_OWORD *)(result + 96) = 0u;
      *(_OWORD *)(result + 48) = 0u;
      *(_OWORD *)(result + 64) = 0u;
      *(_OWORD *)(result + 16) = 0u;
      *(_OWORD *)(result + 32) = 0u;
      *(_OWORD *)uint64_t result = 0u;
      *(void *)(result + 176) = 2 * -a2;
      *(_OWORD *)(result + 200) = 0u;
      *(_OWORD *)(result + 216) = 0u;
      *(_OWORD *)(result + 232) = 0u;
      *(_OWORD *)(result + 248) = 0u;
      *(unsigned char *)(result + 360) = 0;
      *(_OWORD *)(result + 184) = 0u;
      result += 184;
      *(_OWORD *)(result + 80) = 0u;
      *(_OWORD *)(result + 96) = 0u;
      *(_OWORD *)(result + 112) = 0u;
      *(_OWORD *)(result + 128) = 0u;
      *(_OWORD *)(result + 144) = 0u;
      *(_OWORD *)(result + 160) = 0u;
      return result;
    }
    *(unsigned char *)(result + 361) = 0;
    if (a2) {
      goto LABEL_8;
    }
  }
  return result;
}

uint64_t destructiveInjectEnumTag for BNNS.NormalizationType(uint64_t result, char a2)
{
  char v2 = *(unsigned char *)(result + 360) & 1 | (a2 << 6);
  *(void *)(result + 176) &= 1uLL;
  *(unsigned char *)(result + 360) = v2;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.NormalizationType()
{
  return &type metadata for BNNS.NormalizationType;
}

uint64_t static vDSP.rectangularToPolar<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.rectangularToPolar<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.rectangularToPolar<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.rectangularToPolar<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.rectangularToPolar<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vDSP.rectangularToPolar<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.rectangularToPolar<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.rectangularToPolar<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:));
}

uint64_t static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:));
}

{
  return static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:));
}

uint64_t closure #1 in static vDSP.rectangularToPolar<A>(_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t *a6, unint64_t *a7, void (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(a6);
  uint64_t v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a7, a6);
  a8(a3, a1, a4, v16, a5, v17);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a6 + 8) + 16))(a4);
  if (result == v10)
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(uint64_t))(a6 + 16))(a7);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t static vDSP.polarToRectangular<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.rectangularToPolar<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.polarToRectangular<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.rectangularToPolar<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.polarToRectangular<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:));
}

{
  return static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:));
}

uint64_t static vDSP.rectangularToPolar<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(uint64_t, uint64_t, void *))
{
  uint64_t v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  _OWORD v12[2] = a2;
  v12[3] = a3;
  v12[4] = a1;
  return a5(v10, a4, v12);
}

uint64_t static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  v20[0] = a7;
  uint64_t v12 = *(void *)(a3 - 8);
  MEMORY[0x1F4188790](a1);
  int v14 = (char *)v20 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v17 = (*(uint64_t (**)(uint64_t))(*(void *)(v15 + 8) + 16))(v16);
  (*(void (**)(char *, uint64_t, uint64_t))(v12 + 16))(v14, a1, a3);
  uint64_t v18 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v12 + 8))(v14, a3);
  if (v18 == v17)
  {
    MEMORY[0x1F4188790](result);
    v20[-6] = a3;
    v20[-5] = a4;
    v20[-4] = a5;
    v20[-3] = a6;
    v20[-2] = a1;
    v20[-1] = v17;
    return (*(uint64_t (**)(void))(a6 + 16))(v20[0]);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(uint64_t result, uint64_t a2, void *a3, uint64_t a4, uint64_t (*a5)(void))
{
  if (!result)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*a3)
  {
    if (a4 >= -1) {
      return a5();
    }
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:));
}

uint64_t partial apply for closure #1 in static vDSP.polarToRectangular<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.rectangularToPolar<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:));
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.rectangularToPolar<A>(_:)(a1, a2, v2[4], v2[2], v2[3], &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:));
}

uint64_t partial apply for closure #1 in static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:));
}

{
  return partial apply for closure #1 in static vDSP.convertElements<A, B>(of:to:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(polarCoordinates:toRectangularCoordinates:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(a1, a2, *(void **)(v2 + 16), *(void *)(v2 + 24), MEMORY[0x1E4F16A20]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(a1, a2, *(void **)(v2 + 16), *(void *)(v2 + 24), MEMORY[0x1E4F16A18]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(a1, a2, *(void **)(v2 + 16), *(void *)(v2 + 24), MEMORY[0x1E4F16A10]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.convert<A, B>(rectangularCoordinates:toPolarCoordinates:)(a1, a2, *(void **)(v2 + 16), *(void *)(v2 + 24), MEMORY[0x1E4F16A08]);
}

BOOL static vDSP.DFTTransformType.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void vDSP.DFTTransformType.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int vDSP.DFTTransformType.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t vDSP.DFT.__allocating_init(previous:count:direction:transformType:ofType:)(uint64_t a1, uint64_t a2, char *a3, char *a4)
{
  return vDSP.DFT.init(previous:count:direction:transformType:ofType:)(a1, a2, a3, a4);
}

uint64_t vDSP.DFT.init(previous:count:direction:transformType:ofType:)(uint64_t a1, uint64_t a2, char *a3, char *a4)
{
  uint64_t v5 = v4;
  char v8 = *a3;
  char v9 = *a4;
  uint64_t v10 = *(void *)(*(void *)v5 + 88);
  uint64_t v11 = *(void *)(*(void *)v5 + 80);
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  char v17 = v8;
  char v16 = v9;
  uint64_t AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  uint64_t v14 = (*(uint64_t (**)(uint64_t, uint64_t, char *, char *, uint64_t, uint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness + 16))(a1, a2, &v17, &v16, v11, v10, AssociatedTypeWitness, AssociatedConformanceWitness);
  swift_release();
  if (v14)
  {
    *(unsigned char *)(v5 + 24) = v9;
    *(void *)(v5 + 16) = v14;
  }
  else
  {
    type metadata accessor for vDSP.DFT();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v5;
}

uint64_t type metadata accessor for vDSP.DFT()
{
  return __swift_instantiateGenericMetadata();
}

uint64_t vDSP.DFT.transform<A>(inputReal:inputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  uint64_t result = Array.init(unsafeUninitializedCapacity:initializingWith:)();
  __break(1u);
  return result;
}

uint64_t closure #1 in vDSP.DFT.transform<A>(inputReal:inputImaginary:)(uint64_t a1, void *a2, uint64_t *a3, uint64_t a4)
{
  *a3 = Array.init(unsafeUninitializedCapacity:initializingWith:)();
  uint64_t result = swift_bridgeObjectRelease();
  *a2 = a4;
  return result;
}

uint64_t partial apply for closure #1 in vDSP.DFT.transform<A>(inputReal:inputImaginary:)(uint64_t a1, void *a2)
{
  return closure #1 in vDSP.DFT.transform<A>(inputReal:inputImaginary:)(a1, a2, *(uint64_t **)(v2 + 32), *(void *)(v2 + 40));
}

uint64_t closure #1 in closure #1 in vDSP.DFT.transform<A>(inputReal:inputImaginary:)(uint64_t a1, void *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  uint64_t v15 = type metadata accessor for UnsafeMutableBufferPointer();
  uint64_t WitnessTable = swift_getWitnessTable();
  uint64_t result = vDSP.DFT.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a4, a5, a6, a1, a8, v15, a9, WitnessTable);
  *a2 = a7;
  return result;
}

uint64_t vDSP.DFT.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  uint64_t v15 = *(void *)(v8 + 16);
  uint64_t AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  return (*(uint64_t (**)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness + 24))(v15, a1, a2, a3, a4, a5, a6, a7, a8, AssociatedTypeWitness, AssociatedConformanceWitness);
}

uint64_t vDSP.DFT.deinit()
{
  uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
  uint64_t v2 = *(void *)(v0 + 16);
  uint64_t AssociatedConformanceWitness = swift_getAssociatedConformanceWitness();
  (*(void (**)(uint64_t, uint64_t, uint64_t))(AssociatedConformanceWitness + 32))(v2, AssociatedTypeWitness, AssociatedConformanceWitness);
  return v0;
}

uint64_t vDSP.DFT.__deallocating_deinit()
{
  vDSP.DFT.deinit();

  return swift_deallocClassInstance();
}

uint64_t static vDSP.VectorizableFloat.makeDFTSetup<A>(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, uint64_t a3, unsigned char *a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.VectorizableFloat.makeDFTSetup<A>(previous:count:direction:transformType:)(a1, a2, a3, a4, a5, a6, MEMORY[0x1E4F16810], MEMORY[0x1E4F16820]);
}

uint64_t static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t partial apply for closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t protocol witness for static vDSP_DFTFunctions.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:) in conformance vDSP.VectorizableFloat(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

uint64_t static vDSP.VectorizableDouble.makeDFTSetup<A>(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, uint64_t a3, unsigned char *a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.VectorizableFloat.makeDFTSetup<A>(previous:count:direction:transformType:)(a1, a2, a3, a4, a5, a6, MEMORY[0x1E4F16818], MEMORY[0x1E4F16828]);
}

uint64_t static vDSP.VectorizableFloat.makeDFTSetup<A>(previous:count:direction:transformType:)(uint64_t result, uint64_t a2, uint64_t a3, unsigned char *a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(uint64_t), uint64_t (*a8)(uint64_t))
{
  if (*a4)
  {
    if (result) {
      uint64_t result = *(void *)(result + 16);
    }
    if ((a2 & 0x8000000000000000) == 0) {
      return a8(result);
    }
    __break(1u);
  }
  else
  {
    if (result) {
      uint64_t result = *(void *)(result + 16);
    }
    if ((a2 & 0x8000000000000000) == 0) {
      return a7(result);
    }
  }
  __break(1u);
  return result;
}

uint64_t static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  _OWORD v11[2] = a6;
  void v11[3] = a7;
  v11[4] = a8;
  v11[5] = a9;
  v11[6] = a3;
  v11[7] = a4;
  v11[8] = a5;
  v11[9] = a1;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t, uint64_t))(a8 + 24))(a10, v11, MEMORY[0x1E4FBC848] + 8, a6, a8);
}

uint64_t partial apply for closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:));
}

void *closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(void *result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, void *a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t))
{
  if (!a3)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (!a5)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (!*a7)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (*result) {
    return (void *)a8(a2, a3, a5);
  }
LABEL_9:
  __break(1u);
  return result;
}

unint64_t lazy protocol witness table accessor for type vDSP.DFTTransformType and conformance vDSP.DFTTransformType()
{
  unint64_t result = lazy protocol witness table cache variable for type vDSP.DFTTransformType and conformance vDSP.DFTTransformType;
  if (!lazy protocol witness table cache variable for type vDSP.DFTTransformType and conformance vDSP.DFTTransformType)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.DFTTransformType and conformance vDSP.DFTTransformType);
  }
  return result;
}

_UNKNOWN **associated type witness table accessor for vDSP_FloatingPointDiscreteFourierTransformable.DFTFunctions : vDSP_DFTFunctions in Float()
{
  return &protocol witness table for vDSP.VectorizableFloat;
}

unint64_t instantiation function for generic protocol witness table for Float(uint64_t a1)
{
  unint64_t result = lazy protocol witness table accessor for type Float and conformance Float();
  *(void *)(a1 + 8) = result;
  return result;
}

_UNKNOWN **associated type witness table accessor for vDSP_FloatingPointDiscreteFourierTransformable.DFTFunctions : vDSP_DFTFunctions in Double()
{
  return &protocol witness table for vDSP.VectorizableDouble;
}

unint64_t instantiation function for generic protocol witness table for Double(uint64_t a1)
{
  unint64_t result = lazy protocol witness table accessor for type Double and conformance Double();
  *(void *)(a1 + 8) = result;
  return result;
}

unint64_t lazy protocol witness table accessor for type Double and conformance Double()
{
  unint64_t result = lazy protocol witness table cache variable for type Double and conformance Double;
  if (!lazy protocol witness table cache variable for type Double and conformance Double)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Double and conformance Double);
  }
  return result;
}

uint64_t protocol witness for static vDSP_DFTFunctions.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:) in conformance vDSP.VectorizableDouble(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

unsigned char *storeEnumTagSinglePayload for vDSP.DFTTransformType(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 1 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFF) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFE)
  {
    unsigned int v6 = ((a2 - 255) >> 8) + 1;
    *unint64_t result = a2 + 1;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D20C6284);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 1;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vDSP.DFTTransformType()
{
  return &type metadata for vDSP.DFTTransformType;
}

uint64_t type metadata completion function for vDSP.DFT()
{
  return swift_initClassMetadata2();
}

uint64_t method lookup function for vDSP.DFT(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for vDSP.DFT);
}

uint64_t dispatch thunk of vDSP.DFT.__allocating_init(previous:count:direction:transformType:ofType:)()
{
  return (*(uint64_t (**)(void))(v0 + 112))();
}

uint64_t dispatch thunk of vDSP.DFT.transform<A>(inputReal:inputImaginary:)()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 120))();
}

uint64_t dispatch thunk of vDSP.DFT.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)()
{
  return (*(uint64_t (**)(void))(*(void *)v0 + 128))();
}

uint64_t dispatch thunk of static vDSP_DFTFunctions.makeDFTSetup<A>(previous:count:direction:transformType:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  return (*(uint64_t (**)(void))(a8 + 16))();
}

uint64_t dispatch thunk of static vDSP_DFTFunctions.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11)
{
  return (*(uint64_t (**)(void))(a11 + 24))();
}

uint64_t dispatch thunk of static vDSP_DFTFunctions.destroySetup(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 32))();
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:));
}

void *partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(void *a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, MEMORY[0x1E4F167E8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in vDSP.DiscreteFourierTransform<>.transform<A, B>(inputReal:inputImaginary:outputReal:outputImaginary:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(uint64_t a1, uint64_t a2)
{
  uint64_t v10 = a1;
  long long v3 = *(_OWORD *)(v2 + 80);
  long long v8 = *(_OWORD *)(v2 + 64);
  uint64_t v4 = *(void *)(v2 + 24);
  uint64_t v5 = *(void *)(v2 + 40);
  void v7[2] = *(void *)(v2 + 56);
  long long v9 = v3;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v5 + 16))(a2, v7, MEMORY[0x1E4FBC848] + 8, v4);
}

void *partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(void *a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, MEMORY[0x1E4F167E0]);
}

void *partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableDouble.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(void *a1, uint64_t (*a2)(uint64_t, uint64_t, uint64_t))
{
  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.VectorizableFloat.transform<A, B>(dftSetup:inputReal:inputImaginary:outputReal:outputImaginary:)(a1, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), *(void *)(v2 + 48), *(void **)(v2 + 56), a2);
}

uint64_t partial apply for closure #1 in closure #1 in vDSP.DFT.transform<A>(inputReal:inputImaginary:)(uint64_t a1, void *a2)
{
  return closure #1 in closure #1 in vDSP.DFT.transform<A>(inputReal:inputImaginary:)(a1, a2, v2[4], v2[5], v2[6], v2[7], v2[8], v2[2], v2[3]);
}

BOOL static vDSP.IntegrationRule.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void vDSP.IntegrationRule.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int vDSP.IntegrationRule.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

uint64_t static vDSP.integrate<A>(_:using:stepSize:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v4, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.integrate<A>(_:using:stepSize:));
}

{
  uint64_t v4;

  uint64_t v4 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v4, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.integrate<A>(_:using:stepSize:));
}

uint64_t closure #1 in static vDSP.integrate<A>(_:using:stepSize:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v12 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v13 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.integrate<A, B>(_:using:stepSize:result:)(a3, a4, a1, a5, v12, a6, v13);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

{
  uint64_t v12;
  uint64_t v13;
  uint64_t result;

  uint64_t v12 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v13 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.integrate<A, B>(_:using:stepSize:result:)(a3, a4, a1, a5, v12, a6, v13);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.integrate<A>(_:using:stepSize:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.integrate<A>(_:using:stepSize:)(a1, a2, v2[4], v2[5], v2[2], v2[3]);
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.integrate<A>(_:using:stepSize:)(a1, a2, v2[4], v2[5], v2[2], v2[3]);
}

uint64_t static vDSP.integrate<A, B>(_:using:stepSize:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v9 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a7 + 8) + 16))(a5);
  if (result >= v9) {
    uint64_t v11 = v9;
  }
  else {
    uint64_t v11 = result;
  }
  if (v11 < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(void))(a7 + 16))(partial apply for closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:));
  }
  return result;
}

{
  uint64_t v9;
  uint64_t result;
  uint64_t v11;

  uint64_t v9 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(a7 + 8) + 16))(a5);
  if (result >= v9) {
    uint64_t v11 = v9;
  }
  else {
    uint64_t v11 = result;
  }
  if (v11 < 0)
  {
    __break(1u);
  }
  else
  {
    MEMORY[0x1F4188790](result);
    return (*(uint64_t (**)(void))(a7 + 16))(partial apply for closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:));
  }
  return result;
}

void closure #1 in closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:)(const float *a1, int a2, char a3, float **a4, vDSP_Length __N, float a6)
{
  if (!a3)
  {
    if (a1)
    {
      float v9 = a6;
      uint64_t v7 = *a4;
      if (v7)
      {
        vDSP_vrsum(a1, 1, &v9, v7, 1, __N);
        return;
      }
      goto LABEL_16;
    }
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (a3 == 1)
  {
    if (a1)
    {
      float v10 = a6;
      unsigned int v6 = *a4;
      if (v6)
      {
        vDSP_vsimps(a1, 1, &v10, v6, 1, __N);
        return;
      }
    }
    else
    {
      __break(1u);
    }
    __break(1u);
    goto LABEL_15;
  }
  if (!a1)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  float v11 = a6;
  long long v8 = *a4;
  if (!v8)
  {
LABEL_18:
    __break(1u);
    return;
  }
  vDSP_vtrapz(a1, 1, &v11, v8, 1, __N);
}

void closure #1 in closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:)(const double *a1, int a2, char a3, double **a4, vDSP_Length __N, double a6)
{
  if (!a3)
  {
    if (a1)
    {
      double v9 = a6;
      uint64_t v7 = *a4;
      if (v7)
      {
        vDSP_vrsumD(a1, 1, &v9, v7, 1, __N);
        return;
      }
      goto LABEL_16;
    }
LABEL_15:
    __break(1u);
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  if (a3 == 1)
  {
    if (a1)
    {
      double v10 = a6;
      unsigned int v6 = *a4;
      if (v6)
      {
        vDSP_vsimpsD(a1, 1, &v10, v6, 1, __N);
        return;
      }
    }
    else
    {
      __break(1u);
    }
    __break(1u);
    goto LABEL_15;
  }
  if (!a1)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  double v11 = a6;
  long long v8 = *a4;
  if (!v8)
  {
LABEL_18:
    __break(1u);
    return;
  }
  vDSP_vtrapzD(a1, 1, &v11, v8, 1, __N);
}

uint64_t partial apply for closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:)(uint64_t a1)
{
  uint64_t v2 = *(void *)(v1 + 16);
  uint64_t v3 = *(void *)(v1 + 32);
  int v4 = *(_DWORD *)(v1 + 60);
  uint64_t v5 = *(void *)(v1 + 64);
  v7[16] = *(unsigned char *)(v1 + 56);
  int v8 = v4;
  uint64_t v9 = a1;
  uint64_t v10 = v5;
  return (*(uint64_t (**)(void (*)(const float *, int), unsigned char *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:), v7, MEMORY[0x1E4FBC848] + 8, v2);
}

{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  uint64_t v4;
  unsigned char v6[24];
  uint64_t v7;
  uint64_t v8;

  uint64_t v2 = *(void *)(v1 + 16);
  uint64_t v3 = *(void *)(v1 + 32);
  int v4 = *(void *)(v1 + 64);
  v6[16] = *(unsigned char *)(v1 + 56);
  uint64_t v7 = v4;
  int v8 = a1;
  return (*(uint64_t (**)(void (*)(const double *, int), unsigned char *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:), v6, MEMORY[0x1E4FBC848] + 8, v2);
}

unint64_t lazy protocol witness table accessor for type vDSP.IntegrationRule and conformance vDSP.IntegrationRule()
{
  unint64_t result = lazy protocol witness table cache variable for type vDSP.IntegrationRule and conformance vDSP.IntegrationRule;
  if (!lazy protocol witness table cache variable for type vDSP.IntegrationRule and conformance vDSP.IntegrationRule)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vDSP.IntegrationRule and conformance vDSP.IntegrationRule);
  }
  return result;
}

uint64_t getEnumTagSinglePayload for vDSP.IntegrationRule(unsigned __int8 *a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0xFE) {
    goto LABEL_17;
  }
  if (a2 + 2 >= 0xFFFF00) {
    int v2 = 4;
  }
  else {
    int v2 = 2;
  }
  if ((a2 + 2) >> 8 < 0xFF) {
    int v3 = 1;
  }
  else {
    int v3 = v2;
  }
  if (v3 == 4)
  {
    int v4 = *(_DWORD *)(a1 + 1);
    if (v4) {
      return (*a1 | (v4 << 8)) - 2;
    }
  }
  else
  {
    if (v3 == 2)
    {
      int v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1)) {
        goto LABEL_17;
      }
      return (*a1 | (v4 << 8)) - 2;
    }
    int v4 = a1[1];
    if (a1[1]) {
      return (*a1 | (v4 << 8)) - 2;
    }
  }
LABEL_17:
  unsigned int v6 = *a1;
  BOOL v7 = v6 >= 3;
  int v8 = v6 - 3;
  if (!v7) {
    int v8 = -1;
  }
  return (v8 + 1);
}

unsigned char *storeEnumTagSinglePayload for vDSP.IntegrationRule(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 2 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 2) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFE) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFD)
  {
    unsigned int v6 = ((a2 - 254) >> 8) + 1;
    *unint64_t result = a2 + 2;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D20C6FC4);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 2;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vDSP.IntegrationRule()
{
  return &type metadata for vDSP.IntegrationRule;
}

void partial apply for closure #1 in closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:)(const double *a1, int a2)
{
  closure #1 in closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:)(a1, a2, *(unsigned char *)(v2 + 16), *(double ***)(v2 + 32), *(void *)(v2 + 40), *(double *)(v2 + 24));
}

void partial apply for closure #1 in closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:)(const float *a1, int a2)
{
  closure #1 in closure #1 in static vDSP.integrate<A, B>(_:using:stepSize:result:)(a1, a2, *(unsigned char *)(v2 + 16), *(float ***)(v2 + 24), *(void *)(v2 + 32), *(float *)(v2 + 20));
}

uint64_t BNNS.AdamWOptimizer.accumulatorCountMultiplier.getter()
{
  if (*(unsigned char *)(v0 + 52)) {
    return 3;
  }
  else {
    return 2;
  }
}

float BNNS.AdamWOptimizer.learningRate.getter()
{
  return *(float *)v0;
}

void BNNS.AdamWOptimizer.learningRate.setter(float a1)
{
  float *v1 = a1;
}

float (*BNNS.AdamWOptimizer.learningRate.modify(uint64_t a1))(uint64_t a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *v1;
  return BNNS.AdamWOptimizer.learningRate.modify;
}

float BNNS.AdamWOptimizer.learningRate.modify(uint64_t a1)
{
  float result = *(float *)(a1 + 8);
  **(float **)a1 = result;
  return result;
}

float BNNS.AdamWOptimizer.beta1.getter()
{
  return *(float *)(v0 + 4);
}

void BNNS.AdamWOptimizer.beta1.setter(float a1)
{
  *(float *)(v1 + 4) = a1;
}

float (*BNNS.AdamWOptimizer.beta1.modify(uint64_t a1))(float *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 4);
  return BNNS.AdamWOptimizer.beta1.modify;
}

float BNNS.AdamWOptimizer.beta1.modify(float *a1)
{
  float result = a1[2];
  *(float *)(*(void *)a1 + 4) = result;
  return result;
}

float BNNS.AdamWOptimizer.beta2.getter()
{
  return *(float *)(v0 + 8);
}

void BNNS.AdamWOptimizer.beta2.setter(float a1)
{
  *(float *)(v1 + 8) = a1;
}

float (*BNNS.AdamWOptimizer.beta2.modify(uint64_t a1))(float *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 8);
  return BNNS.AdamWOptimizer.beta2.modify;
}

float BNNS.AdamWOptimizer.beta2.modify(float *a1)
{
  float result = a1[2];
  *(float *)(*(void *)a1 + 8) = result;
  return result;
}

float BNNS.AdamWOptimizer.timeStep.getter()
{
  return *(float *)(v0 + 12);
}

void BNNS.AdamWOptimizer.timeStep.setter(float a1)
{
  *(float *)(v1 + 12) = a1;
}

float (*BNNS.AdamWOptimizer.timeStep.modify(uint64_t a1))(float *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 12);
  return BNNS.AdamWOptimizer.timeStep.modify;
}

float BNNS.AdamWOptimizer.timeStep.modify(float *a1)
{
  float result = a1[2];
  *(float *)(*(void *)a1 + 12) = result;
  return result;
}

float BNNS.AdamWOptimizer.epsilon.getter()
{
  return *(float *)(v0 + 16);
}

void BNNS.AdamWOptimizer.epsilon.setter(float a1)
{
  *(float *)(v1 + 16) = a1;
}

float (*BNNS.AdamWOptimizer.epsilon.modify(uint64_t a1))(float *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 16);
  return BNNS.AdamWOptimizer.epsilon.modify;
}

float BNNS.AdamWOptimizer.epsilon.modify(float *a1)
{
  float result = a1[2];
  *(float *)(*(void *)a1 + 16) = result;
  return result;
}

float BNNS.AdamWOptimizer.gradientScale.getter()
{
  return *(float *)(v0 + 20);
}

void BNNS.AdamWOptimizer.gradientScale.setter(float a1)
{
  *(float *)(v1 + 20) = a1;
}

float (*BNNS.AdamWOptimizer.gradientScale.modify(uint64_t a1))(float *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 20);
  return BNNS.AdamWOptimizer.gradientScale.modify;
}

float BNNS.AdamWOptimizer.gradientScale.modify(float *a1)
{
  float result = a1[2];
  *(float *)(*(void *)a1 + 20) = result;
  return result;
}

float BNNS.AdamWOptimizer.weightDecay.getter()
{
  return *(float *)(v0 + 24);
}

void BNNS.AdamWOptimizer.weightDecay.setter(float a1)
{
  *(float *)(v1 + 24) = a1;
}

float (*BNNS.AdamWOptimizer.weightDecay.modify(uint64_t a1))(float *a1)
{
  *(void *)a1 = v1;
  *(_DWORD *)(a1 + 8) = *(_DWORD *)(v1 + 24);
  return BNNS.AdamWOptimizer.weightDecay.modify;
}

float BNNS.AdamWOptimizer.weightDecay.modify(float *a1)
{
  float result = a1[2];
  *(float *)(*(void *)a1 + 24) = result;
  return result;
}

void BNNS.AdamWOptimizer.gradientClipping.getter(uint64_t a1@<X8>)
{
  int v2 = *(_DWORD *)(v1 + 32);
  unsigned int v3 = *(_DWORD *)(v1 + 44);
  if (v2 == 3)
  {
    unint64_t v7 = v3 | ((unint64_t)*(unsigned int *)(v1 + 48) << 32);
    char v6 = 2;
    goto LABEL_9;
  }
  if (v2 == 2)
  {
    unint64_t v7 = v3;
    char v6 = 1;
    goto LABEL_9;
  }
  if (v2 != 1)
  {
    unint64_t v7 = 0;
    char v6 = 3;
    goto LABEL_9;
  }
  float v5 = *(float *)(v1 + 36);
  float v4 = *(float *)(v1 + 40);
  if (v5 <= v4)
  {
    char v6 = 0;
    unint64_t v7 = LODWORD(v5) | ((unint64_t)LODWORD(v4) << 32);
LABEL_9:
    *(void *)a1 = v7;
    *(unsigned char *)(a1 + 8) = v6;
    return;
  }
  __break(1u);
}

void key path getter for BNNS.AdamWOptimizer.gradientClipping : BNNS.AdamWOptimizer(uint64_t a1@<X8>)
{
  BNNS.AdamWOptimizer.gradientClipping.getter((uint64_t)&v3);
  char v2 = v4;
  *(void *)a1 = v3;
  *(unsigned char *)(a1 + 8) = v2;
}

unint64_t *BNNS.AdamWOptimizer.gradientClipping.setter(unint64_t *result)
{
  unint64_t v2 = *result;
  int v3 = 0;
  LODWORD(v4) = 0;
  LODWORD(v5) = 0;
  LODWORD(v6) = 0;
  LODWORD(v7) = 0;
  switch(*((unsigned char *)result + 8))
  {
    case 1:
      LODWORD(v4) = 0;
      LODWORD(v5) = 0;
      LODWORD(v7) = 0;
      int v3 = 2;
      goto LABEL_5;
    case 2:
      LODWORD(v4) = 0;
      LODWORD(v5) = 0;
      unint64_t v7 = HIDWORD(v2);
      int v3 = 3;
LABEL_5:
      unint64_t v6 = *result;
      break;
    case 3:
      break;
    default:
      LODWORD(v6) = 0;
      LODWORD(v7) = 0;
      unint64_t v5 = HIDWORD(v2);
      int v3 = 1;
      unint64_t v4 = *result;
      break;
  }
  v1[8] = v3;
  v1[9] = v4;
  v1[10] = v5;
  v1[11] = v6;
  v1[12] = v7;
  return result;
}

unint64_t *(*BNNS.AdamWOptimizer.gradientClipping.modify(unint64_t *(*result)(unint64_t *result, char a2)))(unint64_t *result, char a2)
{
  *((void *)result + 2) = v1;
  int v2 = *(_DWORD *)(v1 + 32);
  unsigned int v3 = *(_DWORD *)(v1 + 44);
  if (v2 == 3)
  {
    unint64_t v7 = v3 | ((unint64_t)*(unsigned int *)(v1 + 48) << 32);
    char v6 = 2;
    goto LABEL_9;
  }
  if (v2 == 2)
  {
    unint64_t v7 = v3;
    char v6 = 1;
    goto LABEL_9;
  }
  if (v2 != 1)
  {
    unint64_t v7 = 0;
    char v6 = 3;
    goto LABEL_9;
  }
  float v5 = *(float *)(v1 + 36);
  float v4 = *(float *)(v1 + 40);
  if (v5 <= v4)
  {
    char v6 = 0;
    unint64_t v7 = LODWORD(v5) | ((unint64_t)LODWORD(v4) << 32);
LABEL_9:
    *(void *)float result = v7;
    *((unsigned char *)result + 8) = v6;
    return BNNS.AdamWOptimizer.gradientClipping.modify;
  }
  __break(1u);
  return result;
}

unint64_t *BNNS.AdamWOptimizer.gradientClipping.modify(unint64_t *result, char a2)
{
  unint64_t v2 = *result;
  int v3 = 0;
  LODWORD(v4) = 0;
  LODWORD(v5) = 0;
  LODWORD(v6) = 0;
  LODWORD(v7) = 0;
  if (a2)
  {
    switch(*((unsigned char *)result + 8))
    {
      case 1:
        goto LABEL_6;
      case 2:
        LODWORD(v4) = 0;
        LODWORD(v5) = 0;
        unint64_t v7 = HIDWORD(v2);
        int v3 = 3;
        unint64_t v6 = *result;
        break;
      case 3:
        break;
      default:
        LODWORD(v6) = 0;
        LODWORD(v7) = 0;
        unint64_t v5 = HIDWORD(v2);
        int v3 = 1;
        unint64_t v4 = *result;
        break;
    }
  }
  else
  {
    switch(*((unsigned char *)result + 8))
    {
      case 1:
LABEL_6:
        LODWORD(v4) = 0;
        LODWORD(v5) = 0;
        LODWORD(v7) = 0;
        int v3 = 2;
        goto LABEL_9;
      case 2:
        LODWORD(v4) = 0;
        LODWORD(v5) = 0;
        unint64_t v7 = HIDWORD(v2);
        int v3 = 3;
LABEL_9:
        unint64_t v6 = *result;
        break;
      case 3:
        break;
      default:
        LODWORD(v6) = 0;
        LODWORD(v7) = 0;
        unint64_t v5 = HIDWORD(v2);
        int v3 = 1;
        unint64_t v4 = *result;
        break;
    }
  }
  int v8 = (_DWORD *)result[2];
  v8[8] = v3;
  v8[9] = v4;
  v8[10] = v5;
  v8[11] = v6;
  v8[12] = v7;
  return result;
}

unint64_t *BNNS.AdamWOptimizer.init(learningRate:beta1:beta2:timeStep:epsilon:gradientScale:weightDecay:gradientClipping:usesAMSGrad:)@<X0>(unint64_t *result@<X0>, char a2@<W1>, uint64_t a3@<X8>, float a4@<S0>, float a5@<S1>, float a6@<S2>, float a7@<S3>, float a8@<S4>, float a9@<S5>, float a10@<S6>)
{
  unint64_t v10 = *result;
  LODWORD(v11) = 0;
  LODWORD(v12) = 0;
  LODWORD(v13) = 0;
  LODWORD(v14) = 0;
  int v15 = 0;
  switch(*((unsigned char *)result + 8))
  {
    case 1:
      LODWORD(v11) = 0;
      LODWORD(v13) = 0;
      LODWORD(v14) = 0;
      int v15 = 2;
      goto LABEL_5;
    case 2:
      LODWORD(v13) = 0;
      LODWORD(v14) = 0;
      unint64_t v11 = HIDWORD(v10);
      int v15 = 3;
LABEL_5:
      unint64_t v12 = *result;
      break;
    case 3:
      break;
    default:
      LODWORD(v11) = 0;
      LODWORD(v12) = 0;
      unint64_t v13 = HIDWORD(v10);
      int v15 = 1;
      unint64_t v14 = *result;
      break;
  }
  *(float *)a3 = a4;
  *(float *)(a3 + 4) = a5;
  *(float *)(a3 + 8) = a6;
  *(float *)(a3 + 12) = a7;
  *(float *)(a3 + 16) = a8;
  *(float *)(a3 + 20) = a9;
  *(float *)(a3 + 24) = a10;
  *(_DWORD *)(a3 + 28) = 0;
  *(_DWORD *)(a3 + 32) = v15;
  *(_DWORD *)(a3 + 36) = v14;
  *(_DWORD *)(a3 + 40) = v13;
  *(_DWORD *)(a3 + 44) = v12;
  *(_DWORD *)(a3 + 48) = v11;
  *(unsigned char *)(a3 + 52) = a2 & 1;
  return result;
}

uint64_t BNNS.AdamWOptimizer.bnnsOptimizerFunction.getter()
{
  if (*(unsigned char *)(v0 + 52)) {
    return 12;
  }
  else {
    return 10;
  }
}

uint64_t protocol witness for BNNSOptimizer.bnnsOptimizerFunction.getter in conformance BNNS.AdamWOptimizer()
{
  if (*(unsigned char *)(v0 + 52)) {
    return 12;
  }
  else {
    return 10;
  }
}

uint64_t protocol witness for BNNSOptimizer.accumulatorCountMultiplier.getter in conformance BNNS.AdamWOptimizer()
{
  if (*(unsigned char *)(v0 + 52)) {
    return 3;
  }
  else {
    return 2;
  }
}

uint64_t protocol witness for WithOptimizerAlgFields.withOptimizerAlgFields(body:) in conformance BNNS.AdamWOptimizer(uint64_t (*a1)(long long *))
{
  uint64_t v9 = *MEMORY[0x1E4F143B8];
  int v2 = *((_DWORD *)v1 + 6);
  long long v4 = *v1;
  uint64_t v5 = *((void *)v1 + 2);
  int v6 = v2;
  uint64_t v7 = *(void *)((char *)v1 + 28);
  long long v8 = *(long long *)((char *)v1 + 36);
  return a1(&v4);
}

int8x16_t BNNS.AdamOptimizer.init(learningRate:beta1:beta2:timeStep:epsilon:gradientScale:regularizationScale:gradientClipping:regularizationFunction:usesAMSGrad:)@<Q0>(unint64_t *a1@<X0>, uint64_t a2@<X1>, char a3@<W2>, int8x16_t *a4@<X8>, unsigned int a5@<S0>, int32x2_t a6@<D1>, unsigned int a7@<S2>, __int32 a8@<S3>, unsigned int a9@<S4>, unsigned int a10@<S5>, unsigned int a11@<S6>)
{
  unint64_t v11 = *a1;
  unint64_t v12 = 0;
  LODWORD(v13) = 0;
  unint64_t v14 = 0;
  uint64_t v15 = 0;
  uint64_t v16 = 0;
  switch(*((unsigned char *)a1 + 8))
  {
    case 1:
      LODWORD(v13) = 0;
      unint64_t v14 = 0;
      uint64_t v15 = 0;
      uint64_t v16 = 2;
      goto LABEL_5;
    case 2:
      unint64_t v14 = 0;
      uint64_t v15 = 0;
      unint64_t v13 = HIDWORD(v11);
      uint64_t v16 = 3;
LABEL_5:
      unint64_t v12 = *a1;
      break;
    case 3:
      break;
    default:
      LODWORD(v13) = 0;
      unint64_t v12 = 0;
      unint64_t v14 = HIDWORD(v11);
      uint64_t v15 = v11 << 32;
      uint64_t v16 = 1;
      break;
  }
  a6.i32[1] = a8;
  v17.i64[0] = a5;
  v17.i64[1] = a7;
  int8x16_t result = vorrq_s8((int8x16_t)vshll_n_s32(a6, 0x20uLL), v17);
  *a4 = result;
  a4[1].i64[0] = a9 | ((unint64_t)a10 << 32);
  a4[1].i64[1] = a11 | (unint64_t)(a2 << 32);
  a4[2].i64[0] = v16 | v15;
  a4[2].i64[1] = v14 | (v12 << 32);
  a4[3].i32[0] = v13;
  a4[3].i8[4] = 1;
  a4[3].i8[5] = a3 & 1;
  return result;
}

uint64_t BNNS.AdamOptimizer.usesAMSGrad.getter()
{
  return *(unsigned __int8 *)(v0 + 53);
}

uint64_t BNNS.AdamOptimizer.usesAMSGrad.setter(uint64_t result)
{
  *(unsigned char *)(v1 + 53) = result;
  return result;
}

unsigned char *(*BNNS.AdamOptimizer.usesAMSGrad.modify(uint64_t a1))(unsigned char *result)
{
  *(void *)a1 = v1;
  *(unsigned char *)(a1 + 8) = *(unsigned char *)(v1 + 53);
  return BNNS.AdamOptimizer.usesAMSGrad.modify;
}

unsigned char *BNNS.AdamOptimizer.usesAMSGrad.modify(unsigned char *result)
{
  *(unsigned char *)(*(void *)result + 53) = result[8];
  return result;
}

void BNNS.AdamOptimizer.gradientClipping.getter(uint64_t a1@<X8>)
{
  unint64_t v2 = *(void *)(v1 + 32);
  if (*(unsigned char *)(v1 + 52) != 1)
  {
    if (*(unsigned char *)(v1 + 28))
    {
      if (*(float *)&v2 > *((float *)&v2 + 1))
      {
        __break(1u);
        goto LABEL_20;
      }
      char v8 = 0;
    }
    else
    {
      unint64_t v2 = 0;
      char v8 = 3;
    }
LABEL_18:
    *(void *)a1 = v2;
    *(unsigned char *)(a1 + 8) = v8;
    return;
  }
  unint64_t v3 = *(void *)(v1 + 40);
  if (v2 != 1)
  {
    unint64_t v4 = HIDWORD(v3);
    unint64_t v5 = v4 | ((unint64_t)*(unsigned int *)(v1 + 48) << 32);
    if (v2 == 3)
    {
      char v6 = 2;
    }
    else
    {
      unint64_t v5 = 0;
      char v6 = 3;
    }
    BOOL v7 = v2 == 2;
    if (v2 == 2) {
      unint64_t v2 = v4;
    }
    else {
      unint64_t v2 = v5;
    }
    if (v7) {
      char v8 = 1;
    }
    else {
      char v8 = v6;
    }
    goto LABEL_18;
  }
  unint64_t v9 = HIDWORD(v2);
  if (*(float *)&v9 <= *(float *)&v3)
  {
    char v8 = 0;
    unint64_t v2 = v9 | (v3 << 32);
    goto LABEL_18;
  }
LABEL_20:
  __break(1u);
}

void key path getter for BNNS.AdamOptimizer.gradientClipping : BNNS.AdamOptimizer(uint64_t a1@<X8>)
{
  BNNS.AdamOptimizer.gradientClipping.getter((uint64_t)&v3);
  char v2 = v4;
  *(void *)a1 = v3;
  *(unsigned char *)(a1 + 8) = v2;
}

unint64_t *key path setter for BNNS.AdamOptimizer.gradientClipping : BNNS.AdamOptimizer(unint64_t *result, uint64_t a2)
{
  unint64_t v2 = *result;
  LODWORD(v3) = 0;
  uint64_t v4 = 0;
  uint64_t v5 = 0;
  unint64_t v6 = 0;
  unint64_t v7 = 0;
  switch(*((unsigned char *)result + 8))
  {
    case 1:
      uint64_t v5 = 0;
      unint64_t v6 = 0;
      LODWORD(v3) = 0;
      uint64_t v4 = 2;
      goto LABEL_5;
    case 2:
      uint64_t v5 = 0;
      unint64_t v6 = 0;
      unint64_t v3 = HIDWORD(v2);
      uint64_t v4 = 3;
LABEL_5:
      unint64_t v7 = *result;
      break;
    case 3:
      break;
    default:
      unint64_t v7 = 0;
      LODWORD(v3) = 0;
      unint64_t v6 = HIDWORD(v2);
      uint64_t v5 = *result;
      uint64_t v4 = 1;
      break;
  }
  uint64_t v8 = *(void *)(a2 + 24);
  if (*(unsigned char *)(a2 + 52) == 1)
  {
    uint64_t v9 = v4 | (v5 << 32);
    uint64_t v10 = v6 | (v7 << 32);
  }
  else
  {
    LODWORD(v3) = 0;
    uint64_t v10 = *(unsigned int *)(a2 + 40);
    v8 &= 0x1FFFFFFFFuLL;
    uint64_t v9 = v5 | (v6 << 32);
  }
  *(void *)(a2 + 24) = v8;
  *(void *)(a2 + 32) = v9;
  *(void *)(a2 + 40) = v10;
  *(_DWORD *)(a2 + 48) = v3;
  return result;
}

unint64_t *BNNS.AdamOptimizer.gradientClipping.setter(unint64_t *result)
{
  unint64_t v2 = *result;
  LODWORD(v3) = 0;
  uint64_t v4 = 0;
  uint64_t v5 = 0;
  unint64_t v6 = 0;
  unint64_t v7 = 0;
  switch(*((unsigned char *)result + 8))
  {
    case 1:
      uint64_t v5 = 0;
      unint64_t v6 = 0;
      LODWORD(v3) = 0;
      uint64_t v4 = 2;
      goto LABEL_5;
    case 2:
      uint64_t v5 = 0;
      unint64_t v6 = 0;
      unint64_t v3 = HIDWORD(v2);
      uint64_t v4 = 3;
LABEL_5:
      unint64_t v7 = *result;
      break;
    case 3:
      break;
    default:
      unint64_t v7 = 0;
      LODWORD(v3) = 0;
      unint64_t v6 = HIDWORD(v2);
      uint64_t v5 = *result;
      uint64_t v4 = 1;
      break;
  }
  uint64_t v8 = *(void *)(v1 + 24);
  if (*(unsigned char *)(v1 + 52) == 1)
  {
    uint64_t v9 = v4 | (v5 << 32);
    uint64_t v10 = v6 | (v7 << 32);
  }
  else
  {
    LODWORD(v3) = 0;
    uint64_t v10 = *(unsigned int *)(v1 + 40);
    v8 &= 0x1FFFFFFFFuLL;
    uint64_t v9 = v5 | (v6 << 32);
  }
  *(void *)(v1 + 24) = v8;
  *(void *)(v1 + 32) = v9;
  *(void *)(v1 + 40) = v10;
  *(_DWORD *)(v1 + 48) = v3;
  return result;
}

unint64_t *(*BNNS.AdamOptimizer.gradientClipping.modify(uint64_t a1))(unint64_t *result, char a2)
{
  *(void *)(a1 + 16) = v1;
  BNNS.AdamOptimizer.gradientClipping.getter(a1);
  return BNNS.AdamOptimizer.gradientClipping.modify;
}

unint64_t *BNNS.AdamOptimizer.gradientClipping.modify(unint64_t *result, char a2)
{
  unint64_t v2 = *result;
  LODWORD(v3) = 0;
  uint64_t v4 = 0;
  uint64_t v5 = 0;
  unint64_t v6 = 0;
  unint64_t v7 = 0;
  if (a2)
  {
    switch(*((unsigned char *)result + 8))
    {
      case 1:
        goto LABEL_5;
      case 2:
        goto LABEL_6;
      case 3:
        break;
      default:
        goto LABEL_4;
    }
  }
  else
  {
    switch(*((unsigned char *)result + 8))
    {
      case 1:
LABEL_5:
        uint64_t v5 = 0;
        unint64_t v6 = 0;
        LODWORD(v3) = 0;
        uint64_t v4 = 2;
        goto LABEL_7;
      case 2:
LABEL_6:
        uint64_t v5 = 0;
        unint64_t v6 = 0;
        unint64_t v3 = HIDWORD(v2);
        uint64_t v4 = 3;
LABEL_7:
        unint64_t v7 = *result;
        break;
      case 3:
        break;
      default:
LABEL_4:
        unint64_t v7 = 0;
        LODWORD(v3) = 0;
        unint64_t v6 = HIDWORD(v2);
        uint64_t v5 = *result;
        uint64_t v4 = 1;
        break;
    }
  }
  unint64_t v8 = result[2];
  uint64_t v9 = *(void *)(v8 + 24);
  if (*(unsigned char *)(v8 + 52) == 1)
  {
    uint64_t v10 = v4 | (v5 << 32);
    uint64_t v11 = v6 | (v7 << 32);
  }
  else
  {
    LODWORD(v3) = 0;
    uint64_t v11 = *(unsigned int *)(v8 + 40);
    v9 &= 0x1FFFFFFFFuLL;
    uint64_t v10 = v5 | (v6 << 32);
  }
  *(void *)(v8 + 24) = v9;
  *(void *)(v8 + 32) = v10;
  *(void *)(v8 + 40) = v11;
  *(_DWORD *)(v8 + 48) = v3;
  return result;
}

int8x16_t BNNS.SGDMomentumOptimizer.init(learningRate:momentum:gradientScale:regularizationScale:gradientClipping:usesNesterovMomentum:regularizationFunction:sgdMomentumVariant:)@<Q0>(uint64_t *a1@<X0>, char a2@<W1>, uint64_t a3@<X2>, unsigned int a4@<W3>, int8x16_t *a5@<X8>, unsigned int a6@<S0>, int32x2_t a7@<D1>, unsigned int a8@<S2>, __int32 a9@<S3>)
{
  uint64_t v9 = *a1;
  unint64_t v10 = 0;
  LODWORD(v11) = 0;
  unint64_t v12 = 0;
  uint64_t v13 = 0;
  uint64_t v14 = 0;
  switch(*((unsigned char *)a1 + 8))
  {
    case 1:
      unint64_t v10 = 0;
      unint64_t v12 = 0;
      uint64_t v13 = 0;
      uint64_t v14 = 0x200000000;
      goto LABEL_5;
    case 2:
      unint64_t v12 = 0;
      uint64_t v13 = 0;
      unint64_t v10 = v9 & 0xFFFFFFFF00000000;
      uint64_t v14 = 0x300000000;
LABEL_5:
      uint64_t v11 = *a1;
      break;
    case 3:
      break;
    default:
      unint64_t v10 = 0;
      LODWORD(v11) = 0;
      unint64_t v12 = v9 & 0xFFFFFFFF00000000;
      uint64_t v13 = *a1;
      uint64_t v14 = 0x100000000;
      break;
  }
  a7.i32[1] = a9;
  v15.i64[0] = a6;
  v15.i64[1] = a8;
  int8x16_t result = vorrq_s8((int8x16_t)vshll_n_s32(a7, 0x20uLL), v15);
  *a5 = result;
  a5[1].i64[0] = a2 & 1 | (unint64_t)(a3 << 32) | 0x80000000;
  a5[1].i64[1] = v14 | a4;
  a5[2].i64[0] = v13 | v12;
  a5[2].i64[1] = v11 | v10;
  return result;
}

void BNNS.SGDMomentumOptimizer.gradientClipping.getter(uint64_t a1@<X8>)
{
  unint64_t v3 = v1[2];
  unint64_t v2 = v1[3];
  if ((v3 & 0x80000000) != 0)
  {
    unint64_t v6 = HIDWORD(v2);
    if (v6 != 1)
    {
      uint64_t v7 = v1[5];
      if (v6 == 3)
      {
        char v8 = 2;
      }
      else
      {
        uint64_t v7 = 0;
        char v8 = 3;
      }
      BOOL v9 = v6 == 2;
      if (v6 == 2) {
        uint64_t v4 = v1[5];
      }
      else {
        uint64_t v4 = v7;
      }
      if (v9) {
        char v5 = 1;
      }
      else {
        char v5 = v8;
      }
      goto LABEL_18;
    }
    uint64_t v4 = v1[4];
    if (*(float *)&v4 <= *((float *)&v4 + 1))
    {
      char v5 = 0;
      goto LABEL_18;
    }
  }
  else
  {
    if ((v3 & 1) == 0)
    {
      uint64_t v4 = 0;
      char v5 = 3;
LABEL_18:
      *(void *)a1 = v4;
      *(unsigned char *)(a1 + 8) = v5;
      return;
    }
    unint64_t v10 = HIDWORD(v3);
    if (*((float *)&v3 + 1) <= *(float *)&v2)
    {
      char v5 = 0;
      uint64_t v4 = v10 | (v2 << 32);
      goto LABEL_18;
    }
    __break(1u);
  }
  __break(1u);
}

void key path getter for BNNS.SGDMomentumOptimizer.gradientClipping : BNNS.SGDMomentumOptimizer(uint64_t a1@<X8>)
{
  BNNS.SGDMomentumOptimizer.gradientClipping.getter((uint64_t)&v3);
  char v2 = v4;
  *(void *)a1 = v3;
  *(unsigned char *)(a1 + 8) = v2;
}

unint64_t *key path setter for BNNS.SGDMomentumOptimizer.gradientClipping : BNNS.SGDMomentumOptimizer(unint64_t *result, void *a2)
{
  unint64_t v2 = *result;
  uint64_t v3 = 0;
  uint64_t v4 = 0;
  unint64_t v5 = 0;
  LODWORD(v6) = 0;
  unint64_t v7 = 0;
  switch(*((unsigned char *)result + 8))
  {
    case 1:
      uint64_t v4 = 0;
      unint64_t v5 = 0;
      unint64_t v7 = 0;
      uint64_t v3 = 0x200000000;
      goto LABEL_7;
    case 2:
      uint64_t v4 = 0;
      unint64_t v5 = 0;
      unint64_t v7 = v2 & 0xFFFFFFFF00000000;
      uint64_t v3 = 0x300000000;
LABEL_7:
      unint64_t v6 = *result;
      uint64_t v9 = a2[2];
      uint64_t v8 = a2[3];
      if ((v9 & 0x80000000) != 0) {
        goto LABEL_8;
      }
      goto LABEL_4;
    case 3:
      goto LABEL_3;
    default:
      LODWORD(v6) = 0;
      unint64_t v7 = 0;
      unint64_t v5 = HIDWORD(v2);
      uint64_t v4 = *result;
      uint64_t v3 = 0x100000000;
LABEL_3:
      uint64_t v9 = a2[2];
      uint64_t v8 = a2[3];
      if ((v9 & 0x80000000) != 0)
      {
LABEL_8:
        uint64_t v13 = v8 | (unint64_t)v3;
        uint64_t v11 = v4 | (v5 << 32);
        unint64_t v10 = v7 | v6;
        unint64_t v12 = v9 & 0xFFFFFFFF00000001 | 0x80000000;
      }
      else
      {
LABEL_4:
        unint64_t v10 = 0;
        uint64_t v11 = a2[4];
        unint64_t v12 = v9 & 1 | (v4 << 32);
        uint64_t v13 = v8 & 0x100000000 | v5;
      }
      a2[2] = v12;
      a2[3] = v13;
      a2[4] = v11;
      a2[5] = v10;
      return result;
  }
}

unint64_t *BNNS.SGDMomentumOptimizer.gradientClipping.setter(unint64_t *result)
{
  unint64_t v2 = *result;
  uint64_t v3 = 0;
  uint64_t v4 = 0;
  unint64_t v5 = 0;
  LODWORD(v6) = 0;
  unint64_t v7 = 0;
  switch(*((unsigned char *)result + 8))
  {
    case 1:
      uint64_t v4 = 0;
      unint64_t v5 = 0;
      unint64_t v7 = 0;
      uint64_t v3 = 0x200000000;
      goto LABEL_7;
    case 2:
      uint64_t v4 = 0;
      unint64_t v5 = 0;
      unint64_t v7 = v2 & 0xFFFFFFFF00000000;
      uint64_t v3 = 0x300000000;
LABEL_7:
      unint64_t v6 = *result;
      uint64_t v9 = v1[2];
      uint64_t v8 = v1[3];
      if ((v9 & 0x80000000) != 0) {
        goto LABEL_8;
      }
      goto LABEL_4;
    case 3:
      goto LABEL_3;
    default:
      LODWORD(v6) = 0;
      unint64_t v7 = 0;
      unint64_t v5 = HIDWORD(v2);
      uint64_t v4 = *result;
      uint64_t v3 = 0x100000000;
LABEL_3:
      uint64_t v9 = v1[2];
      uint64_t v8 = v1[3];
      if ((v9 & 0x80000000) != 0)
      {
LABEL_8:
        uint64_t v13 = v8 | (unint64_t)v3;
        uint64_t v11 = v4 | (v5 << 32);
        unint64_t v10 = v7 | v6;
        unint64_t v12 = v9 & 0xFFFFFFFF00000001 | 0x80000000;
      }
      else
      {
LABEL_4:
        unint64_t v10 = 0;
        uint64_t v11 = v1[4];
        unint64_t v12 = v9 & 1 | (v4 << 32);
        uint64_t v13 = v8 & 0x100000000 | v5;
      }
      v1[2] = v12;
      v1[3] = v13;
      v1[4] = v11;
      v1[5] = v10;
      return result;
  }
}

unint64_t *(*BNNS.SGDMomentumOptimizer.gradientClipping.modify(uint64_t a1))(unint64_t *result, char a2)
{
  *(void *)(a1 + 16) = v1;
  BNNS.SGDMomentumOptimizer.gradientClipping.getter(a1);
  return BNNS.SGDMomentumOptimizer.gradientClipping.modify;
}

unint64_t *BNNS.SGDMomentumOptimizer.gradientClipping.modify(unint64_t *result, char a2)
{
  unint64_t v2 = *result;
  uint64_t v3 = 0;
  uint64_t v4 = 0;
  unint64_t v5 = 0;
  LODWORD(v6) = 0;
  unint64_t v7 = 0;
  if (a2)
  {
    switch(*((unsigned char *)result + 8))
    {
      case 1:
        goto LABEL_5;
      case 2:
        goto LABEL_6;
      case 3:
        break;
      default:
        goto LABEL_4;
    }
  }
  else
  {
    switch(*((unsigned char *)result + 8))
    {
      case 1:
LABEL_5:
        uint64_t v4 = 0;
        unint64_t v5 = 0;
        unint64_t v7 = 0;
        uint64_t v3 = 0x200000000;
        goto LABEL_7;
      case 2:
LABEL_6:
        uint64_t v4 = 0;
        unint64_t v5 = 0;
        unint64_t v7 = v2 & 0xFFFFFFFF00000000;
        uint64_t v3 = 0x300000000;
LABEL_7:
        unint64_t v6 = *result;
        break;
      case 3:
        break;
      default:
LABEL_4:
        LODWORD(v6) = 0;
        unint64_t v7 = 0;
        unint64_t v5 = HIDWORD(v2);
        uint64_t v4 = *result;
        uint64_t v3 = 0x100000000;
        break;
    }
  }
  uint64_t v8 = (void *)result[2];
  uint64_t v9 = v8[2];
  if ((v9 & 0x80000000) != 0)
  {
    uint64_t v13 = v8[3] | (unint64_t)v3;
    uint64_t v11 = v4 | (v5 << 32);
    unint64_t v10 = v7 | v6;
    unint64_t v12 = v9 & 0xFFFFFFFF00000001 | 0x80000000;
  }
  else
  {
    unint64_t v10 = 0;
    uint64_t v11 = v8[4];
    unint64_t v12 = v8[2] & 1 | (v4 << 32);
    uint64_t v13 = v8[3] & 0x100000000 | v5;
  }
  _OWORD v8[2] = v12;
  void v8[3] = v13;
  void v8[4] = v11;
  void v8[5] = v10;
  return result;
}

unint64_t BNNS.RMSPropOptimizer.init(learningRate:alpha:epsilon:centered:momentum:gradientScale:regularizationScale:gradientClipping:regularizationFunction:)@<X0>(char a1@<W0>, unint64_t *a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X8>, unsigned int a5@<S0>, unsigned int a6@<S1>, unsigned int a7@<S2>, unsigned int a8@<S3>, unsigned int a9@<S4>, unsigned int a10@<S5>)
{
  unint64_t v10 = *a2;
  unint64_t v11 = 0;
  LODWORD(v12) = 0;
  unint64_t v13 = 0;
  uint64_t v14 = 0;
  uint64_t v15 = 0;
  switch(*((unsigned char *)a2 + 8))
  {
    case 1:
      LODWORD(v12) = 0;
      unint64_t v13 = 0;
      uint64_t v14 = 0;
      uint64_t v15 = 2;
      goto LABEL_5;
    case 2:
      unint64_t v13 = 0;
      uint64_t v14 = 0;
      unint64_t v12 = HIDWORD(v10);
      uint64_t v15 = 3;
LABEL_5:
      unint64_t v11 = *a2;
      break;
    case 3:
      break;
    default:
      LODWORD(v12) = 0;
      unint64_t v11 = 0;
      unint64_t v13 = HIDWORD(v10);
      uint64_t v14 = v10 << 32;
      uint64_t v15 = 1;
      break;
  }
  uint64_t v16 = 0x100000000;
  if ((a1 & 1) == 0) {
    uint64_t v16 = 0;
  }
  unint64_t result = a10 | (unint64_t)(a3 << 32);
  *(void *)a4 = a5 | ((unint64_t)a6 << 32);
  *(void *)(a4 + 8) = v16 | a7 | 0x8000000000000000;
  *(void *)(a4 + 16) = a8 | ((unint64_t)a9 << 32);
  *(void *)(a4 + 24) = result;
  *(void *)(a4 + 32) = v15 | v14;
  *(void *)(a4 + 40) = v13 | (v11 << 32);
  *(_DWORD *)(a4 + 48) = v12;
  return result;
}

void BNNS.RMSPropOptimizer.gradientClipping.getter(uint64_t a1@<X8>)
{
  unint64_t v2 = *(void *)(v1 + 32);
  if ((*(void *)(v1 + 8) & 0x8000000000000000) != 0)
  {
    unint64_t v4 = *(void *)(v1 + 40);
    if (v2 != 1)
    {
      unint64_t v5 = HIDWORD(v4);
      unint64_t v6 = v5 | ((unint64_t)*(unsigned int *)(v1 + 48) << 32);
      if (v2 == 3)
      {
        char v7 = 2;
      }
      else
      {
        unint64_t v6 = 0;
        char v7 = 3;
      }
      BOOL v8 = v2 == 2;
      if (v2 == 2) {
        unint64_t v2 = v5;
      }
      else {
        unint64_t v2 = v6;
      }
      if (v8) {
        char v3 = 1;
      }
      else {
        char v3 = v7;
      }
      goto LABEL_18;
    }
    unint64_t v9 = HIDWORD(v2);
    if (*(float *)&v9 <= *(float *)&v4)
    {
      char v3 = 0;
      unint64_t v2 = v9 | (v4 << 32);
      goto LABEL_18;
    }
  }
  else
  {
    if ((*(unsigned char *)(v1 + 28) & 1) == 0)
    {
      unint64_t v2 = 0;
      char v3 = 3;
LABEL_18:
      *(void *)a1 = v2;
      *(unsigned char *)(a1 + 8) = v3;
      return;
    }
    if (*(float *)&v2 <= *((float *)&v2 + 1))
    {
      char v3 = 0;
      goto LABEL_18;
    }
    __break(1u);
  }
  __break(1u);
}

void key path getter for BNNS.RMSPropOptimizer.gradientClipping : BNNS.RMSPropOptimizer(uint64_t a1@<X8>)
{
  BNNS.RMSPropOptimizer.gradientClipping.getter((uint64_t)&v3);
  char v2 = v4;
  *(void *)a1 = v3;
  *(unsigned char *)(a1 + 8) = v2;
}

unint64_t *key path setter for BNNS.RMSPropOptimizer.gradientClipping : BNNS.RMSPropOptimizer(unint64_t *result, uint64_t a2)
{
  unint64_t v2 = *result;
  LODWORD(v3) = 0;
  uint64_t v4 = 0;
  uint64_t v5 = 0;
  unint64_t v6 = 0;
  unint64_t v7 = 0;
  switch(*((unsigned char *)result + 8))
  {
    case 1:
      uint64_t v5 = 0;
      unint64_t v6 = 0;
      LODWORD(v3) = 0;
      uint64_t v4 = 2;
      goto LABEL_5;
    case 2:
      uint64_t v5 = 0;
      unint64_t v6 = 0;
      unint64_t v3 = HIDWORD(v2);
      uint64_t v4 = 3;
LABEL_5:
      unint64_t v7 = *result;
      break;
    case 3:
      break;
    default:
      unint64_t v7 = 0;
      LODWORD(v3) = 0;
      unint64_t v6 = HIDWORD(v2);
      uint64_t v5 = *result;
      uint64_t v4 = 1;
      break;
  }
  uint64_t v8 = *(void *)(a2 + 8);
  uint64_t v9 = *(void *)(a2 + 24);
  if (v8 < 0)
  {
    uint64_t v12 = v4 | (v5 << 32);
    unint64_t v11 = v8 & 0x1FFFFFFFFLL | 0x8000000000000000;
    uint64_t v10 = v6 | (v7 << 32);
  }
  else
  {
    LODWORD(v3) = 0;
    uint64_t v10 = *(unsigned int *)(a2 + 40);
    unint64_t v11 = v8 & 0x1FFFFFFFFLL;
    v9 &= 0x1FFFFFFFFuLL;
    uint64_t v12 = v5 | (v6 << 32);
  }
  *(void *)(a2 + 8) = v11;
  *(void *)(a2 + 24) = v9;
  *(void *)(a2 + 32) = v12;
  *(void *)(a2 + 40) = v10;
  *(_DWORD *)(a2 + 48) = v3;
  return result;
}

unint64_t *BNNS.RMSPropOptimizer.gradientClipping.setter(unint64_t *result)
{
  unint64_t v2 = *result;
  LODWORD(v3) = 0;
  uint64_t v4 = 0;
  uint64_t v5 = 0;
  unint64_t v6 = 0;
  unint64_t v7 = 0;
  switch(*((unsigned char *)result + 8))
  {
    case 1:
      uint64_t v5 = 0;
      unint64_t v6 = 0;
      LODWORD(v3) = 0;
      uint64_t v4 = 2;
      goto LABEL_5;
    case 2:
      uint64_t v5 = 0;
      unint64_t v6 = 0;
      unint64_t v3 = HIDWORD(v2);
      uint64_t v4 = 3;
LABEL_5:
      unint64_t v7 = *result;
      break;
    case 3:
      break;
    default:
      unint64_t v7 = 0;
      LODWORD(v3) = 0;
      unint64_t v6 = HIDWORD(v2);
      uint64_t v5 = *result;
      uint64_t v4 = 1;
      break;
  }
  uint64_t v8 = *(void *)(v1 + 8);
  uint64_t v9 = *(void *)(v1 + 24);
  if (v8 < 0)
  {
    uint64_t v12 = v4 | (v5 << 32);
    unint64_t v11 = v8 & 0x1FFFFFFFFLL | 0x8000000000000000;
    uint64_t v10 = v6 | (v7 << 32);
  }
  else
  {
    LODWORD(v3) = 0;
    uint64_t v10 = *(unsigned int *)(v1 + 40);
    unint64_t v11 = v8 & 0x1FFFFFFFFLL;
    v9 &= 0x1FFFFFFFFuLL;
    uint64_t v12 = v5 | (v6 << 32);
  }
  *(void *)(v1 + 8) = v11;
  *(void *)(v1 + 24) = v9;
  *(void *)(v1 + 32) = v12;
  *(void *)(v1 + 40) = v10;
  *(_DWORD *)(v1 + 48) = v3;
  return result;
}

unint64_t *(*BNNS.RMSPropOptimizer.gradientClipping.modify(uint64_t a1))(unint64_t *result, char a2)
{
  *(void *)(a1 + 16) = v1;
  BNNS.RMSPropOptimizer.gradientClipping.getter(a1);
  return BNNS.RMSPropOptimizer.gradientClipping.modify;
}

unint64_t *BNNS.RMSPropOptimizer.gradientClipping.modify(unint64_t *result, char a2)
{
  unint64_t v2 = *result;
  LODWORD(v3) = 0;
  uint64_t v4 = 0;
  uint64_t v5 = 0;
  unint64_t v6 = 0;
  unint64_t v7 = 0;
  if (a2)
  {
    switch(*((unsigned char *)result + 8))
    {
      case 1:
        goto LABEL_5;
      case 2:
        goto LABEL_6;
      case 3:
        break;
      default:
        goto LABEL_4;
    }
  }
  else
  {
    switch(*((unsigned char *)result + 8))
    {
      case 1:
LABEL_5:
        uint64_t v5 = 0;
        unint64_t v6 = 0;
        LODWORD(v3) = 0;
        uint64_t v4 = 2;
        goto LABEL_7;
      case 2:
LABEL_6:
        uint64_t v5 = 0;
        unint64_t v6 = 0;
        unint64_t v3 = HIDWORD(v2);
        uint64_t v4 = 3;
LABEL_7:
        unint64_t v7 = *result;
        break;
      case 3:
        break;
      default:
LABEL_4:
        unint64_t v7 = 0;
        LODWORD(v3) = 0;
        unint64_t v6 = HIDWORD(v2);
        uint64_t v5 = *result;
        uint64_t v4 = 1;
        break;
    }
  }
  unint64_t v8 = result[2];
  uint64_t v9 = *(void *)(v8 + 8);
  uint64_t v10 = *(void *)(v8 + 24);
  if (v9 < 0)
  {
    uint64_t v13 = v4 | (v5 << 32);
    unint64_t v12 = v9 & 0x1FFFFFFFFLL | 0x8000000000000000;
    uint64_t v11 = v6 | (v7 << 32);
  }
  else
  {
    LODWORD(v3) = 0;
    uint64_t v11 = *(unsigned int *)(v8 + 40);
    unint64_t v12 = v9 & 0x1FFFFFFFFLL;
    v10 &= 0x1FFFFFFFFuLL;
    uint64_t v13 = v5 | (v6 << 32);
  }
  *(void *)(v8 + 8) = v12;
  *(void *)(v8 + 24) = v10;
  *(void *)(v8 + 32) = v13;
  *(void *)(v8 + 40) = v11;
  *(_DWORD *)(v8 + 48) = v3;
  return result;
}

float sub_1D20C8D38@<S0>(float *a1@<X0>, _DWORD *a2@<X8>)
{
  float result = *a1;
  *a2 = *(_DWORD *)a1;
  return result;
}

float sub_1D20C8D44(float *a1, _DWORD *a2)
{
  float result = *a1;
  *a2 = *(_DWORD *)a1;
  return result;
}

float sub_1D20C8D50@<S0>(uint64_t a1@<X0>, float *a2@<X8>)
{
  float result = *(float *)(a1 + 4);
  *a2 = result;
  return result;
}

float sub_1D20C8D5C(float *a1, uint64_t a2)
{
  float result = *a1;
  *(float *)(a2 + 4) = *a1;
  return result;
}

float sub_1D20C8D68@<S0>(uint64_t a1@<X0>, float *a2@<X8>)
{
  float result = *(float *)(a1 + 8);
  *a2 = result;
  return result;
}

float sub_1D20C8D74(float *a1, uint64_t a2)
{
  float result = *a1;
  *(float *)(a2 + 8) = *a1;
  return result;
}

float sub_1D20C8D80@<S0>(uint64_t a1@<X0>, float *a2@<X8>)
{
  float result = *(float *)(a1 + 12);
  *a2 = result;
  return result;
}

float sub_1D20C8D8C(float *a1, uint64_t a2)
{
  float result = *a1;
  *(float *)(a2 + 12) = *a1;
  return result;
}

float sub_1D20C8D98@<S0>(uint64_t a1@<X0>, float *a2@<X8>)
{
  float result = *(float *)(a1 + 16);
  *a2 = result;
  return result;
}

float sub_1D20C8DA4(float *a1, uint64_t a2)
{
  float result = *a1;
  *(float *)(a2 + 16) = *a1;
  return result;
}

float sub_1D20C8DB0@<S0>(uint64_t a1@<X0>, float *a2@<X8>)
{
  float result = *(float *)(a1 + 20);
  *a2 = result;
  return result;
}

float sub_1D20C8DBC(float *a1, uint64_t a2)
{
  float result = *a1;
  *(float *)(a2 + 20) = *a1;
  return result;
}

float sub_1D20C8DC8@<S0>(uint64_t a1@<X0>, float *a2@<X8>)
{
  float result = *(float *)(a1 + 24);
  *a2 = result;
  return result;
}

float sub_1D20C8DD4(float *a1, uint64_t a2)
{
  float result = *a1;
  *(float *)(a2 + 24) = *a1;
  return result;
}

unint64_t *sub_1D20C8DE4(unint64_t *result, _DWORD *a2)
{
  unint64_t v2 = *result;
  int v3 = 0;
  LODWORD(v4) = 0;
  LODWORD(v5) = 0;
  LODWORD(v6) = 0;
  LODWORD(v7) = 0;
  switch(*((unsigned char *)result + 8))
  {
    case 1:
      LODWORD(v4) = 0;
      LODWORD(v5) = 0;
      LODWORD(v7) = 0;
      int v3 = 2;
      goto LABEL_5;
    case 2:
      LODWORD(v4) = 0;
      LODWORD(v5) = 0;
      unint64_t v7 = HIDWORD(v2);
      int v3 = 3;
LABEL_5:
      unint64_t v6 = *result;
      break;
    case 3:
      break;
    default:
      LODWORD(v6) = 0;
      LODWORD(v7) = 0;
      unint64_t v5 = HIDWORD(v2);
      int v3 = 1;
      unint64_t v4 = *result;
      break;
  }
  a2[8] = v3;
  a2[9] = v4;
  a2[10] = v5;
  a2[11] = v6;
  a2[12] = v7;
  return result;
}

uint64_t sub_1D20C8E80@<X0>(uint64_t result@<X0>, unsigned char *a2@<X8>)
{
  *a2 = *(unsigned char *)(result + 53);
  return result;
}

unsigned char *sub_1D20C8E8C(unsigned char *result, uint64_t a2)
{
  *(unsigned char *)(a2 + 53) = *result;
  return result;
}

uint64_t getEnumTagSinglePayload for BNNS.GradientClipping(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFD && *(unsigned char *)(a1 + 9)) {
    return (*(_DWORD *)a1 + 253);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 8);
  if (v3 <= 3) {
    int v4 = -1;
  }
  else {
    int v4 = v3 ^ 0xFF;
  }
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.GradientClipping(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFC)
  {
    *(unsigned char *)(result + 8) = 0;
    *(void *)float result = a2 - 253;
    if (a3 >= 0xFD) {
      *(unsigned char *)(result + 9) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFD) {
      *(unsigned char *)(result + 9) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 8) = -(char)a2;
    }
  }
  return result;
}

uint64_t getEnumTag for BNNS.GradientClipping(uint64_t a1)
{
  if (*(unsigned __int8 *)(a1 + 8) <= 2u) {
    return *(unsigned __int8 *)(a1 + 8);
  }
  else {
    return (*(_DWORD *)a1 + 3);
  }
}

uint64_t destructiveInjectEnumTag for BNNS.GradientClipping(uint64_t result, unsigned int a2)
{
  if (a2 >= 3)
  {
    *(void *)float result = a2 - 3;
    LOBYTE(a2) = 3;
  }
  *(unsigned char *)(result + 8) = a2;
  return result;
}

ValueMetadata *type metadata accessor for BNNS.GradientClipping()
{
  return &type metadata for BNNS.GradientClipping;
}

uint64_t getEnumTagSinglePayload for BNNS.AdamWOptimizer(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFF && *(unsigned char *)(a1 + 53)) {
    return (*(_DWORD *)a1 + 255);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 52);
  BOOL v4 = v3 >= 2;
  int v5 = (v3 + 2147483646) & 0x7FFFFFFF;
  if (!v4) {
    int v5 = -1;
  }
  return (v5 + 1);
}

uint64_t storeEnumTagSinglePayload for BNNS.AdamWOptimizer(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFE)
  {
    *(void *)(result + 40) = 0;
    *(_OWORD *)(result + 24) = 0u;
    *(_OWORD *)(result + 8) = 0u;
    *(unsigned char *)(result + 52) = 0;
    *(_DWORD *)(result + 48) = 0;
    *(void *)float result = a2 - 255;
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 53) = 1;
    }
  }
  else
  {
    if (a3 >= 0xFF) {
      *(unsigned char *)(result + 53) = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 52) = a2 + 1;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.AdamWOptimizer()
{
  return &type metadata for BNNS.AdamWOptimizer;
}

uint64_t static vDSP.clip<A>(_:to:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.clip<A>(_:to:)(a1, a2, a3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.clip<A>(_:to:));
}

{
  return static vDSP.clip<A>(_:to:)(a1, a2, a3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.clip<A>(_:to:));
}

uint64_t partial apply for closure #1 in static vDSP.clip<A>(_:to:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.clip<A>(_:to:)(a1, a2, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, float))static vDSP.clip<A, B>(_:to:result:));
}

{
  return partial apply for closure #1 in static vDSP.clip<A>(_:to:)(a1, a2, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, double))static vDSP.clip<A, B>(_:to:result:));
}

uint64_t static vDSP.clip<A, B>(_:to:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7, float a8)
{
  return static vDSP.clip<A, B>(_:to:result:)(a1, a7, a8, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:));
}

uint64_t static vDSP.clip<A, B>(_:to:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7, double a8)
{
  return static vDSP.clip<A, B>(_:to:result:)(a1, a7, a8, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:));
}

uint64_t static vDSP.invertedClip<A>(_:to:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.clip<A>(_:to:)(a1, a2, a3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.invertedClip<A>(_:to:));
}

{
  return static vDSP.clip<A>(_:to:)(a1, a2, a3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.invertedClip<A>(_:to:));
}

uint64_t closure #1 in static vDSP.clip<A>(_:to:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, void (*a6)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, float), float a7, float a8)
{
  uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  a6(a3, a1, a4, v16, a5, v17, a7, a8);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.invertedClip<A, B>(_:to:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7, float a8)
{
  return static vDSP.clip<A, B>(_:to:result:)(a1, a7, a8, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.invertedClip<A, B>(_:to:result:));
}

uint64_t static vDSP.clip<A, B>(_:to:result:)(uint64_t a1, float a2, float a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  uint64_t v25 = a9;
  uint64_t v37 = *MEMORY[0x1E4F143B8];
  uint64_t v16 = *(void *)(a5 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v18 = (char *)&v24 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v21 = (*(uint64_t (**)(uint64_t))(*(void *)(v19 + 8) + 16))(v20);
  (*(void (**)(char *, uint64_t, uint64_t))(v16 + 16))(v18, a1, a5);
  uint64_t v22 = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  (*(void (**)(char *, uint64_t))(v16 + 8))(v18, a5);
  if (v22 != v21) {
    __break(1u);
  }
  float v26 = a2;
  float v27 = a3;
  uint64_t v29 = a5;
  uint64_t v30 = a6;
  uint64_t v31 = a7;
  uint64_t v32 = a8;
  uint64_t v33 = a1;
  long long v34 = &v26;
  long long v35 = &v27;
  uint64_t v36 = v21;
  return (*(uint64_t (**)(uint64_t, unsigned char *, uint64_t, uint64_t, uint64_t))(a8 + 16))(v25, v28, MEMORY[0x1E4FBC848] + 8, a6, a8);
}

uint64_t closure #1 in static vDSP.clip<A>(_:to:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, void (*a6)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, double), double a7, double a8)
{
  uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  a6(a3, a1, a4, v16, a5, v17, a7, a8);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.invertedClip<A, B>(_:to:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7, double a8)
{
  return static vDSP.clip<A, B>(_:to:result:)(a1, a7, a8, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.invertedClip<A, B>(_:to:result:));
}

uint64_t static vDSP.clip<A, B>(_:to:result:)(uint64_t a1, double a2, double a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  uint64_t v24 = a9;
  uint64_t v36 = *MEMORY[0x1E4F143B8];
  uint64_t v16 = *(void *)(a5 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v18 = (char *)&v24 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v21 = (*(uint64_t (**)(uint64_t))(*(void *)(v19 + 8) + 16))(v20);
  (*(void (**)(char *, uint64_t, uint64_t))(v16 + 16))(v18, a1, a5);
  uint64_t v22 = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  (*(void (**)(char *, uint64_t))(v16 + 8))(v18, a5);
  if (v22 != v21) {
    __break(1u);
  }
  double v25 = a2;
  double v26 = a3;
  uint64_t v28 = a5;
  uint64_t v29 = a6;
  uint64_t v30 = a7;
  uint64_t v31 = a8;
  uint64_t v32 = a1;
  uint64_t v33 = &v25;
  long long v34 = &v26;
  uint64_t v35 = v21;
  return (*(uint64_t (**)(uint64_t, unsigned char *, uint64_t, uint64_t, uint64_t))(a8 + 16))(v24, v27, MEMORY[0x1E4FBC848] + 8, a6, a8);
}

uint64_t static vDSP.threshold<A>(_:to:with:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v4 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v4, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.threshold<A>(_:to:with:));
}

{
  uint64_t v4;

  uint64_t v4 = (*(uint64_t (**)(uint64_t, uint64_t))(a4 + 16))(a3, a4);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v4, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.threshold<A>(_:to:with:));
}

uint64_t closure #1 in static vDSP.threshold<A>(_:to:with:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  uint64_t v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.threshold<A, B>(_:to:with:result:)(a3, a7, a4, a1, a5, v14, a6, v15);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t static vDSP.threshold<A, B>(_:to:with:result:)(uint64_t a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v37 = *MEMORY[0x1E4F143B8];
  uint64_t v14 = *(void *)(a5 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v16 = (char *)&v24 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  int v18 = *v17;
  HIDWORD(v24) = *((unsigned __int8 *)v17 + 4);
  int v25 = v18;
  uint64_t v21 = (*(uint64_t (**)(uint64_t))(*(void *)(v19 + 8) + 16))(v20);
  (*(void (**)(char *, uint64_t, uint64_t))(v14 + 16))(v16, a1, a5);
  uint64_t v22 = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  (*(void (**)(char *, uint64_t))(v14 + 8))(v16, a5);
  if (v22 != v21) {
    __break(1u);
  }
  float v26 = a2;
  uint64_t v28 = a5;
  uint64_t v29 = a6;
  uint64_t v30 = a7;
  uint64_t v31 = a8;
  uint64_t v32 = a1;
  int v33 = v25;
  char v34 = BYTE4(v24);
  uint64_t v35 = &v26;
  uint64_t v36 = v21;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t), unsigned char *, uint64_t, uint64_t, uint64_t))(a8 + 16))(partial apply for closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:), v27, MEMORY[0x1E4FBC848] + 8, a6, a8);
}

void closure #1 in closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:)(const float *a1, int a2, uint64_t a3, const float *__B, float **a5, vDSP_Length __N)
{
  if ((a3 & 0x100000000) == 0)
  {
    if (!a1)
    {
LABEL_19:
      __break(1u);
      goto LABEL_20;
    }
    int v7 = a3;
    unint64_t v6 = *a5;
    if (!v6)
    {
LABEL_20:
      __break(1u);
      goto LABEL_21;
    }
    if ((__N & 0x8000000000000000) == 0)
    {
      vDSP_vthrsc(a1, 1, __B, (const float *)&v7, v6, 1, __N);
      return;
    }
    __break(1u);
    goto LABEL_17;
  }
  if (a3)
  {
    if (!a1)
    {
LABEL_21:
      __break(1u);
      goto LABEL_22;
    }
    if (!*a5)
    {
LABEL_22:
      __break(1u);
      goto LABEL_23;
    }
    if ((__N & 0x8000000000000000) == 0)
    {
      vDSP_vthres(a1, 1, __B, *a5, 1, __N);
      return;
    }
LABEL_17:
    __break(1u);
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!a1)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!*a5)
  {
LABEL_24:
    __break(1u);
    return;
  }
  if ((__N & 0x8000000000000000) != 0) {
    goto LABEL_18;
  }
  vDSP_vthr(a1, 1, __B, *a5, 1, __N);
}

uint64_t closure #1 in static vDSP.threshold<A>(_:to:with:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  uint64_t v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.threshold<A, B>(_:to:with:result:)(a3, a7, a4, a1, a5, v14, a6, v15);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a5, a6);
  *a2 = result;
  return result;
}

uint64_t static vDSP.threshold<A, B>(_:to:with:result:)(uint64_t a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v36 = *MEMORY[0x1E4F143B8];
  uint64_t v14 = *(void *)(a5 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v16 = (char *)&v23 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v24 = *v17;
  HIDWORD(v23) = *((unsigned __int8 *)v17 + 8);
  uint64_t v20 = (*(uint64_t (**)(uint64_t))(*(void *)(v18 + 8) + 16))(v19);
  (*(void (**)(char *, uint64_t, uint64_t))(v14 + 16))(v16, a1, a5);
  uint64_t v21 = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  (*(void (**)(char *, uint64_t))(v14 + 8))(v16, a5);
  if (v21 != v20) {
    __break(1u);
  }
  double v25 = a2;
  uint64_t v27 = a5;
  uint64_t v28 = a6;
  uint64_t v29 = a7;
  uint64_t v30 = a8;
  uint64_t v31 = a1;
  uint64_t v32 = v24;
  char v33 = BYTE4(v23);
  char v34 = &v25;
  uint64_t v35 = v20;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t), unsigned char *, uint64_t, uint64_t, uint64_t))(a8 + 16))(partial apply for closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:), v26, MEMORY[0x1E4FBC848] + 8, a6, a8);
}

void closure #1 in closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:)(const double *a1, int a2, uint64_t a3, char a4, const double *__B, double **a6, vDSP_Length __N)
{
  if ((a4 & 1) == 0)
  {
    if (!a1)
    {
LABEL_19:
      __break(1u);
      goto LABEL_20;
    }
    uint64_t v7 = a3;
    if (!*a6)
    {
LABEL_20:
      __break(1u);
      goto LABEL_21;
    }
    if ((__N & 0x8000000000000000) == 0)
    {
      vDSP_vthrscD(a1, 1, __B, (const double *)&v7, *a6, 1, __N);
      return;
    }
    __break(1u);
    goto LABEL_17;
  }
  if (a3)
  {
    if (!a1)
    {
LABEL_21:
      __break(1u);
      goto LABEL_22;
    }
    if (!*a6)
    {
LABEL_22:
      __break(1u);
      goto LABEL_23;
    }
    if ((__N & 0x8000000000000000) == 0)
    {
      vDSP_vthresD(a1, 1, __B, *a6, 1, __N);
      return;
    }
LABEL_17:
    __break(1u);
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!a1)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!*a6)
  {
LABEL_24:
    __break(1u);
    return;
  }
  if ((__N & 0x8000000000000000) != 0) {
    goto LABEL_18;
  }
  vDSP_vthrD(a1, 1, __B, *a6, 1, __N);
}

uint64_t static vDSP.limit<A>(_:limit:withOutputConstant:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.clip<A>(_:to:)(a1, a2, a3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.limit<A>(_:limit:withOutputConstant:));
}

{
  return static vDSP.clip<A>(_:to:)(a1, a2, a3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.limit<A>(_:limit:withOutputConstant:));
}

uint64_t static vDSP.clip<A>(_:to:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(void *, uint64_t *))
{
  uint64_t v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, a4);
}

{
  uint64_t v5;

  uint64_t v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, a4);
}

uint64_t closure #1 in static vDSP.limit<A>(_:limit:withOutputConstant:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, float a6, float a7)
{
  uint64_t v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a3, a6, a7, a1, a4, v14, a5, v15);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(uint64_t a1, float a2, float a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v35 = *MEMORY[0x1E4F143B8];
  uint64_t v15 = *(void *)(a5 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v17 = (char *)&v23 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v20 = (*(uint64_t (**)(uint64_t))(*(void *)(v18 + 8) + 16))(v19);
  (*(void (**)(char *, uint64_t, uint64_t))(v15 + 16))(v17, a1, a5);
  uint64_t v21 = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  (*(void (**)(char *, uint64_t))(v15 + 8))(v17, a5);
  if (v21 != v20) {
    __break(1u);
  }
  float v24 = a2;
  float v25 = a3;
  uint64_t v27 = a5;
  uint64_t v28 = a6;
  uint64_t v29 = a7;
  uint64_t v30 = a8;
  uint64_t v31 = a1;
  uint64_t v32 = &v24;
  char v33 = &v25;
  uint64_t v34 = v20;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t), unsigned char *, uint64_t, uint64_t, uint64_t))(a8 + 16))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:), v26, MEMORY[0x1E4FBC848] + 8, a6, a8);
}

uint64_t closure #1 in static vDSP.limit<A>(_:limit:withOutputConstant:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, double a6, double a7)
{
  uint64_t v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a3, a6, a7, a1, a4, v14, a5, v15);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(uint64_t a1, double a2, double a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v35 = *MEMORY[0x1E4F143B8];
  uint64_t v15 = *(void *)(a5 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v17 = (char *)&v23 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v20 = (*(uint64_t (**)(uint64_t))(*(void *)(v18 + 8) + 16))(v19);
  (*(void (**)(char *, uint64_t, uint64_t))(v15 + 16))(v17, a1, a5);
  uint64_t v21 = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  (*(void (**)(char *, uint64_t))(v15 + 8))(v17, a5);
  if (v21 != v20) {
    __break(1u);
  }
  double v24 = a2;
  double v25 = a3;
  uint64_t v27 = a5;
  uint64_t v28 = a6;
  uint64_t v29 = a7;
  uint64_t v30 = a8;
  uint64_t v31 = a1;
  uint64_t v32 = &v24;
  char v33 = &v25;
  uint64_t v34 = v20;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t), unsigned char *, uint64_t, uint64_t, uint64_t))(a8 + 16))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:), v26, MEMORY[0x1E4FBC848] + 8, a6, a8);
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, void *a5, uint64_t a6, uint64_t (*a7)(void))
{
  if (!result)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*a5)
  {
    if ((a6 & 0x8000000000000000) == 0) {
      return a7();
    }
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t partial apply for closure #1 in static vDSP.invertedClip<A>(_:to:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.clip<A>(_:to:)(a1, a2, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, float))static vDSP.invertedClip<A, B>(_:to:result:));
}

{
  return partial apply for closure #1 in static vDSP.clip<A>(_:to:)(a1, a2, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, double))static vDSP.invertedClip<A, B>(_:to:result:));
}

uint64_t partial apply for closure #1 in static vDSP.clip<A>(_:to:)(uint64_t a1, uint64_t *a2, void (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, float))
{
  return closure #1 in static vDSP.clip<A>(_:to:)(a1, a2, *(void *)(v3 + 32), *(void *)(v3 + 16), *(void *)(v3 + 24), a3, *(float *)(v3 + 40), *(float *)(v3 + 44));
}

uint64_t partial apply for closure #1 in static vDSP.clip<A>(_:to:)(uint64_t a1, uint64_t *a2, void (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, double))
{
  return closure #1 in static vDSP.clip<A>(_:to:)(a1, a2, *(void *)(v3 + 32), *(void *)(v3 + 16), *(void *)(v3 + 24), a3, *(double *)(v3 + 40), *(double *)(v3 + 48));
}

uint64_t partial apply for closure #1 in static vDSP.threshold<A>(_:to:with:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.threshold<A>(_:to:with:)(a1, a2, *(void *)(v2 + 32), *(void *)(v2 + 48), *(void *)(v2 + 16), *(void *)(v2 + 24), *(float *)(v2 + 40));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.threshold<A>(_:to:with:)(a1, a2, *(void *)(v2 + 32), *(void *)(v2 + 48), *(void *)(v2 + 16), *(void *)(v2 + 24), *(double *)(v2 + 40));
}

uint64_t partial apply for closure #1 in static vDSP.limit<A>(_:limit:withOutputConstant:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.limit<A>(_:limit:withOutputConstant:)(a1, a2, *(void *)(v2 + 32), *(void *)(v2 + 16), *(void *)(v2 + 24), *(float *)(v2 + 40), *(float *)(v2 + 44));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.limit<A>(_:limit:withOutputConstant:)(a1, a2, *(void *)(v2 + 32), *(void *)(v2 + 16), *(void *)(v2 + 24), *(double *)(v2 + 40), *(double *)(v2 + 48));
}

uint64_t type metadata instantiation function for vDSP.ThresholdRule(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return MEMORY[0x1F41863F8](a1, a2, a3, 16);
}

uint64_t type metadata completion function for vDSP.ThresholdRule()
{
  uint64_t result = swift_checkMetadataState();
  if (v1 <= 0x3F)
  {
    swift_initEnumMetadataSinglePayload();
    return 0;
  }
  return result;
}

uint64_t *initializeBufferWithCopyOfBuffer for vDSP.ThresholdRule(uint64_t *a1, uint64_t *a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(a3 + 16);
  uint64_t v6 = *(void *)(v5 - 8);
  unsigned int v7 = *(_DWORD *)(v6 + 84);
  size_t v8 = *(void *)(v6 + 64);
  unint64_t v9 = v8;
  if (v7 <= 1)
  {
    if (v8 <= 3)
    {
      unsigned int v11 = (~(-1 << (8 * v8)) - v7 + 2) >> (8 * v8);
      if (v11 > 0xFFFE)
      {
        uint64_t v10 = 4;
      }
      else
      {
        BOOL v12 = v11 != 0;
        BOOL v13 = v11 >= 0xFF;
        uint64_t v10 = 2;
        if (!v13) {
          uint64_t v10 = v12;
        }
      }
    }
    else
    {
      uint64_t v10 = 1;
    }
    unint64_t v9 = v10 + v8;
  }
  uint64_t v14 = *(_DWORD *)(v6 + 80);
  if (v14 <= 7 && v9 <= 0x18 && (*(_DWORD *)(v6 + 80) & 0x100000) == 0)
  {
    if ((*(unsigned int (**)(uint64_t *, uint64_t, uint64_t))(v6 + 48))(a2, 2, v5))
    {
      if (v7 <= 1)
      {
        if (v8 <= 3)
        {
          unsigned int v19 = (~(-1 << (8 * v8)) - v7 + 2) >> (8 * v8);
          if (v19 > 0xFFFE)
          {
            uint64_t v18 = 4;
          }
          else
          {
            BOOL v20 = v19 != 0;
            BOOL v13 = v19 >= 0xFF;
            uint64_t v18 = 2;
            if (!v13) {
              uint64_t v18 = v20;
            }
          }
        }
        else
        {
          uint64_t v18 = 1;
        }
        v8 += v18;
      }
      memcpy(a1, a2, v8);
    }
    else
    {
      (*(void (**)(uint64_t *, uint64_t *, uint64_t))(v6 + 16))(a1, a2, v5);
      (*(void (**)(uint64_t *, void, uint64_t, uint64_t))(v6 + 56))(a1, 0, 2, v5);
    }
  }
  else
  {
    uint64_t v17 = *a2;
    *a1 = *a2;
    a1 = (uint64_t *)(v17 + ((v14 + 16) & ~v14));
    swift_retain();
  }
  return a1;
}

uint64_t destroy for vDSP.ThresholdRule(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(a2 + 16);
  uint64_t v6 = *(void *)(v3 - 8);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t, uint64_t))(v6 + 48))(a1, 2, v3);
  if (!result)
  {
    uint64_t v5 = *(uint64_t (**)(uint64_t, uint64_t))(v6 + 8);
    return v5(a1, v3);
  }
  return result;
}

void *initializeWithCopy for vDSP.ThresholdRule(void *a1, const void *a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(a3 + 16);
  uint64_t v6 = *(void *)(v5 - 8);
  if ((*(unsigned int (**)(const void *, uint64_t, uint64_t))(v6 + 48))(a2, 2, v5))
  {
    unsigned int v7 = *(_DWORD *)(v6 + 84);
    size_t v8 = *(void *)(v6 + 64);
    if (v7 <= 1)
    {
      if (v8 <= 3)
      {
        unsigned int v10 = (~(-1 << (8 * v8)) - v7 + 2) >> (8 * v8);
        if (v10 > 0xFFFE)
        {
          uint64_t v9 = 4;
        }
        else
        {
          BOOL v11 = v10 != 0;
          BOOL v12 = v10 >= 0xFF;
          uint64_t v9 = 2;
          if (!v12) {
            uint64_t v9 = v11;
          }
        }
      }
      else
      {
        uint64_t v9 = 1;
      }
      v8 += v9;
    }
    memcpy(a1, a2, v8);
  }
  else
  {
    (*(void (**)(void *, const void *, uint64_t))(v6 + 16))(a1, a2, v5);
    (*(void (**)(void *, void, uint64_t, uint64_t))(v6 + 56))(a1, 0, 2, v5);
  }
  return a1;
}

void *assignWithCopy for vDSP.ThresholdRule(void *a1, void *a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(a3 + 16);
  uint64_t v6 = *(void *)(v5 - 8);
  unsigned int v7 = *(uint64_t (**)(void *, uint64_t, uint64_t))(v6 + 48);
  int v8 = v7(a1, 2, v5);
  int v9 = v7(a2, 2, v5);
  if (v8)
  {
    if (v9)
    {
      unsigned int v10 = *(_DWORD *)(v6 + 84);
      size_t v11 = *(void *)(v6 + 64);
      if (v10 <= 1)
      {
        if (v11 > 3)
        {
LABEL_5:
          uint64_t v12 = 1;
LABEL_16:
          v11 += v12;
          goto LABEL_17;
        }
LABEL_9:
        unsigned int v15 = (~(-1 << (8 * v11)) - v10 + 2) >> (8 * v11);
        if (v15 > 0xFFFE)
        {
          uint64_t v12 = 4;
        }
        else
        {
          BOOL v16 = v15 != 0;
          BOOL v17 = v15 >= 0xFF;
          uint64_t v12 = 2;
          if (!v17) {
            uint64_t v12 = v16;
          }
        }
        goto LABEL_16;
      }
      goto LABEL_17;
    }
    (*(void (**)(void *, void *, uint64_t))(v6 + 16))(a1, a2, v5);
    (*(void (**)(void *, void, uint64_t, uint64_t))(v6 + 56))(a1, 0, 2, v5);
  }
  else
  {
    if (v9)
    {
      uint64_t v14 = *(void (**)(void *, uint64_t))(v6 + 8);
      uint64_t v13 = v6 + 8;
      v14(a1, v5);
      unsigned int v10 = *(_DWORD *)(v13 + 76);
      size_t v11 = *(void *)(v13 + 56);
      if (v10 <= 1)
      {
        if (v11 > 3) {
          goto LABEL_5;
        }
        goto LABEL_9;
      }
LABEL_17:
      memcpy(a1, a2, v11);
      return a1;
    }
    (*(void (**)(void *, void *, uint64_t))(v6 + 24))(a1, a2, v5);
  }
  return a1;
}

void *initializeWithTake for vDSP.ThresholdRule(void *a1, const void *a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(a3 + 16);
  uint64_t v6 = *(void *)(v5 - 8);
  if ((*(unsigned int (**)(const void *, uint64_t, uint64_t))(v6 + 48))(a2, 2, v5))
  {
    unsigned int v7 = *(_DWORD *)(v6 + 84);
    size_t v8 = *(void *)(v6 + 64);
    if (v7 <= 1)
    {
      if (v8 <= 3)
      {
        unsigned int v10 = (~(-1 << (8 * v8)) - v7 + 2) >> (8 * v8);
        if (v10 > 0xFFFE)
        {
          uint64_t v9 = 4;
        }
        else
        {
          BOOL v11 = v10 != 0;
          BOOL v12 = v10 >= 0xFF;
          uint64_t v9 = 2;
          if (!v12) {
            uint64_t v9 = v11;
          }
        }
      }
      else
      {
        uint64_t v9 = 1;
      }
      v8 += v9;
    }
    memcpy(a1, a2, v8);
  }
  else
  {
    (*(void (**)(void *, const void *, uint64_t))(v6 + 32))(a1, a2, v5);
    (*(void (**)(void *, void, uint64_t, uint64_t))(v6 + 56))(a1, 0, 2, v5);
  }
  return a1;
}

void *assignWithTake for vDSP.ThresholdRule(void *a1, void *a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(a3 + 16);
  uint64_t v6 = *(void *)(v5 - 8);
  unsigned int v7 = *(uint64_t (**)(void *, uint64_t, uint64_t))(v6 + 48);
  int v8 = v7(a1, 2, v5);
  int v9 = v7(a2, 2, v5);
  if (v8)
  {
    if (v9)
    {
      unsigned int v10 = *(_DWORD *)(v6 + 84);
      size_t v11 = *(void *)(v6 + 64);
      if (v10 <= 1)
      {
        if (v11 > 3)
        {
LABEL_5:
          uint64_t v12 = 1;
LABEL_16:
          v11 += v12;
          goto LABEL_17;
        }
LABEL_9:
        unsigned int v15 = (~(-1 << (8 * v11)) - v10 + 2) >> (8 * v11);
        if (v15 > 0xFFFE)
        {
          uint64_t v12 = 4;
        }
        else
        {
          BOOL v16 = v15 != 0;
          BOOL v17 = v15 >= 0xFF;
          uint64_t v12 = 2;
          if (!v17) {
            uint64_t v12 = v16;
          }
        }
        goto LABEL_16;
      }
      goto LABEL_17;
    }
    (*(void (**)(void *, void *, uint64_t))(v6 + 32))(a1, a2, v5);
    (*(void (**)(void *, void, uint64_t, uint64_t))(v6 + 56))(a1, 0, 2, v5);
  }
  else
  {
    if (v9)
    {
      uint64_t v14 = *(void (**)(void *, uint64_t))(v6 + 8);
      uint64_t v13 = v6 + 8;
      v14(a1, v5);
      unsigned int v10 = *(_DWORD *)(v13 + 76);
      size_t v11 = *(void *)(v13 + 56);
      if (v10 <= 1)
      {
        if (v11 > 3) {
          goto LABEL_5;
        }
        goto LABEL_9;
      }
LABEL_17:
      memcpy(a1, a2, v11);
      return a1;
    }
    (*(void (**)(void *, void *, uint64_t))(v6 + 40))(a1, a2, v5);
  }
  return a1;
}

uint64_t getEnumTagSinglePayload for vDSP.ThresholdRule(unsigned __int16 *a1, unsigned int a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(*(void *)(a3 + 16) - 8);
  unsigned int v5 = *(_DWORD *)(v4 + 84);
  unsigned int v6 = v5 - 2;
  uint64_t v7 = *(void *)(v4 + 64);
  if (v5 <= 1)
  {
    unsigned int v6 = 0;
    if (v7 <= 3)
    {
      unsigned int v9 = (~(-1 << (8 * v7)) - v5 + 2) >> (8 * v7);
      if (v9 > 0xFFFE)
      {
        uint64_t v8 = 4;
      }
      else
      {
        BOOL v10 = v9 != 0;
        BOOL v11 = v9 >= 0xFF;
        uint64_t v8 = 2;
        if (!v11) {
          uint64_t v8 = v10;
        }
      }
    }
    else
    {
      uint64_t v8 = 1;
    }
    v7 += v8;
  }
  if (!a2) {
    return 0;
  }
  int v12 = a2 - v6;
  if (a2 <= v6) {
    goto LABEL_30;
  }
  char v13 = 8 * v7;
  if (v7 <= 3)
  {
    unsigned int v15 = ((v12 + ~(-1 << v13)) >> v13) + 1;
    if (HIWORD(v15))
    {
      int v14 = *(_DWORD *)((char *)a1 + v7);
      if (!v14) {
        goto LABEL_30;
      }
      goto LABEL_20;
    }
    if (v15 > 0xFF)
    {
      int v14 = *(unsigned __int16 *)((char *)a1 + v7);
      if (!*(unsigned __int16 *)((char *)a1 + v7)) {
        goto LABEL_30;
      }
      goto LABEL_20;
    }
    if (v15 < 2)
    {
LABEL_30:
      if (v6)
      {
        unsigned int v19 = (*(uint64_t (**)(void))(v4 + 48))();
        if (v19 >= 3) {
          return v19 - 2;
        }
        else {
          return 0;
        }
      }
      return 0;
    }
  }
  int v14 = *((unsigned __int8 *)a1 + v7);
  if (!*((unsigned char *)a1 + v7)) {
    goto LABEL_30;
  }
LABEL_20:
  int v16 = (v14 - 1) << v13;
  if (v7 > 3) {
    int v16 = 0;
  }
  if (v7)
  {
    if (v7 <= 3) {
      int v17 = v7;
    }
    else {
      int v17 = 4;
    }
    switch(v17)
    {
      case 2:
        int v18 = *a1;
        break;
      case 3:
        int v18 = *a1 | (*((unsigned __int8 *)a1 + 2) << 16);
        break;
      case 4:
        int v18 = *(_DWORD *)a1;
        break;
      default:
        int v18 = *(unsigned __int8 *)a1;
        break;
    }
  }
  else
  {
    int v18 = 0;
  }
  return v6 + (v18 | v16) + 1;
}

void storeEnumTagSinglePayload for vDSP.ThresholdRule(char *a1, unsigned int a2, unsigned int a3, uint64_t a4)
{
  uint64_t v6 = *(void *)(*(void *)(a4 + 16) - 8);
  unsigned int v7 = *(_DWORD *)(v6 + 84);
  unsigned int v8 = v7 - 2;
  size_t v9 = *(void *)(v6 + 64);
  if (v7 <= 1)
  {
    unsigned int v8 = 0;
    if (v9 <= 3)
    {
      unsigned int v11 = (~(-1 << (8 * v9)) - v7 + 2) >> (8 * v9);
      if (v11 > 0xFFFE)
      {
        uint64_t v10 = 4;
      }
      else
      {
        BOOL v12 = v11 != 0;
        BOOL v13 = v11 >= 0xFF;
        uint64_t v10 = 2;
        if (!v13) {
          uint64_t v10 = v12;
        }
      }
    }
    else
    {
      uint64_t v10 = 1;
    }
    v9 += v10;
  }
  BOOL v13 = a3 >= v8;
  unsigned int v14 = a3 - v8;
  if (v14 != 0 && v13)
  {
    if (v9 <= 3)
    {
      unsigned int v18 = ((v14 + ~(-1 << (8 * v9))) >> (8 * v9)) + 1;
      if (HIWORD(v18))
      {
        int v15 = 4;
      }
      else if (v18 >= 0x100)
      {
        int v15 = 2;
      }
      else
      {
        int v15 = v18 > 1;
      }
    }
    else
    {
      int v15 = 1;
    }
  }
  else
  {
    int v15 = 0;
  }
  if (v8 < a2)
  {
    unsigned int v16 = ~v8 + a2;
    if (v9 < 4)
    {
      int v17 = (v16 >> (8 * v9)) + 1;
      if (v9)
      {
        int v19 = v16 & ~(-1 << (8 * v9));
        bzero(a1, v9);
        if (v9 == 3)
        {
          *(_WORD *)a1 = v19;
          a1[2] = BYTE2(v19);
        }
        else if (v9 == 2)
        {
          *(_WORD *)a1 = v19;
        }
        else
        {
          *a1 = v19;
        }
      }
    }
    else
    {
      bzero(a1, v9);
      *(_DWORD *)a1 = v16;
      int v17 = 1;
    }
    switch(v15)
    {
      case 1:
        a1[v9] = v17;
        return;
      case 2:
        *(_WORD *)&a1[v9] = v17;
        return;
      case 3:
        goto LABEL_43;
      case 4:
        *(_DWORD *)&a1[v9] = v17;
        return;
      default:
        return;
    }
  }
  switch(v15)
  {
    case 1:
      a1[v9] = 0;
      if (!a2) {
        return;
      }
      goto LABEL_30;
    case 2:
      *(_WORD *)&a1[v9] = 0;
      if (!a2) {
        return;
      }
      goto LABEL_30;
    case 3:
LABEL_43:
      __break(1u);
      JUMPOUT(0x1D20CB6FCLL);
    case 4:
      *(_DWORD *)&a1[v9] = 0;
      goto LABEL_29;
    default:
LABEL_29:
      if (a2)
      {
LABEL_30:
        BOOL v20 = *(void (**)(void))(v6 + 56);
        v20();
      }
      return;
  }
}

uint64_t getEnumTag for vDSP.ThresholdRule(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(uint64_t, uint64_t))(*(void *)(*(void *)(a2 + 16) - 8) + 48))(a1, 2);
}

uint64_t destructiveInjectEnumTag for vDSP.ThresholdRule(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(uint64_t, uint64_t, uint64_t))(*(void *)(*(void *)(a3 + 16) - 8) + 56))(a1, a2, 2);
}

uint64_t type metadata accessor for vDSP.ThresholdRule()
{
  return __swift_instantiateGenericMetadata();
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16CC0]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16CB8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:)(uint64_t a1)
{
  uint64_t v2 = *(void *)(v1 + 16);
  uint64_t v3 = *(void *)(v1 + 32);
  char v4 = *(unsigned char *)(v1 + 64);
  uint64_t v5 = *(void *)(v1 + 72);
  uint64_t v6 = *(void *)(v1 + 80);
  _OWORD v8[2] = *(void *)(v1 + 56);
  char v9 = v4;
  uint64_t v10 = v5;
  uint64_t v11 = a1;
  uint64_t v12 = v6;
  return (*(uint64_t (**)(void (*)(const double *, int), void *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:), v8, MEMORY[0x1E4FBC848] + 8, v2);
}

{
  uint64_t v1;
  uint64_t v2;
  uint64_t v3;
  char v4;
  uint64_t v5;
  _DWORD v7[5];
  char v8;
  uint64_t v9;
  uint64_t v10;

  uint64_t v2 = *(void *)(v1 + 16);
  uint64_t v3 = *(void *)(v1 + 32);
  char v4 = *(unsigned char *)(v1 + 60);
  uint64_t v5 = *(void *)(v1 + 64);
  v7[4] = *(_DWORD *)(v1 + 56);
  unsigned int v8 = v4;
  char v9 = v5;
  uint64_t v10 = a1;
  return (*(uint64_t (**)(void (*)(const float *, int), _DWORD *, uint64_t, uint64_t))(v3 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:), v7, MEMORY[0x1E4FBC848] + 8, v2);
}

void partial apply for closure #1 in closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:)(const double *a1, int a2)
{
  closure #1 in closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:)(a1, a2, *(void *)(v2 + 16), *(unsigned char *)(v2 + 24), *(const double **)(v2 + 32), *(double ***)(v2 + 40), *(void *)(v2 + 48));
}

void partial apply for closure #1 in closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:)(const float *a1, int a2)
{
  closure #1 in closure #1 in closure #1 in static vDSP.threshold<A, B>(_:to:with:result:)(a1, a2, *(unsigned int *)(v2 + 16) | ((unint64_t)*(unsigned __int8 *)(v2 + 20) << 32), *(const float **)(v2 + 24), *(float ***)(v2 + 32), *(void *)(v2 + 40));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.invertedClip<A, B>(_:to:result:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.invertedClip<A, B>(_:to:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.invertedClip<A, B>(_:to:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.invertedClip<A, B>(_:to:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16CB0]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16CA8]);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16B08]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.clip<A, B>(_:to:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16B00]);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.limit<A, B>(_:limit:withOutputConstant:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(v2 + 16);
  uint64_t v4 = *(void *)(v2 + 32);
  v6[1] = *(_OWORD *)(v2 + 56);
  uint64_t v7 = a1;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, uint64_t))(v4 + 24))(a2, v6, MEMORY[0x1E4FBC848] + 8, v3);
}

uint64_t vImage.PixelBuffer<>.permuteChannels(to:destination:)(char a1, char a2, char a3, char a4, void *a5)
{
  uint64_t v5 = (void (*)(void *, void *, uint64_t, void))MEMORY[0x1E4F17058];

  return vImage.PixelBuffer<>.permuteChannels(to:destination:)(a1, a2, a3, a4, a5, v5);
}

{
  void (*v5)(void *, void *, uint64_t, void);
  uint64_t vars8;

  uint64_t v5 = (void (*)(void *, void *, uint64_t, void))MEMORY[0x1E4F17050];

  return vImage.PixelBuffer<>.permuteChannels(to:destination:)(a1, a2, a3, a4, a5, v5);
}

{
  void (*v5)(void *, void *, uint64_t, void);
  uint64_t vars8;

  uint64_t v5 = (void (*)(void *, void *, uint64_t, void))MEMORY[0x1E4F17048];

  return vImage.PixelBuffer<>.permuteChannels(to:destination:)(a1, a2, a3, a4, a5, v5);
}

{
  void (*v5)(void *, void *, uint64_t, void);
  uint64_t vars8;

  uint64_t v5 = (void (*)(void *, void *, uint64_t, void))MEMORY[0x1E4F17060];

  return vImage.PixelBuffer<>.permuteChannels(to:destination:)(a1, a2, a3, a4, a5, v5);
}

vImage_Error specialized vImage.PixelBuffer<>._permute<A>(permuteArray:destination:permuteFunc:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v15 = *MEMORY[0x1E4F143B8];
  if ((specialized Sequence<>.max()(a1) & 0x1FC) != 0)
  {
    __break(1u);
    goto LABEL_16;
  }
  if (!*(void *)(a3 + 16))
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  vImagePixelCount v6 = *(void *)(a3 + 48);
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  vImagePixelCount v7 = *(void *)(a3 + 40);
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v6)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v7)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!*(void *)(a2 + 16))
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v8 = *(void *)(a2 + 48);
  if (v8 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v9 = *(void *)(a2 + 40);
  if (v9 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v8)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v9)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v6 != v8)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v7 != v9) {
    goto LABEL_27;
  }
  size_t v10 = *(void *)(a3 + 56);
  src.data = *(void **)(a3 + 32);
  src.vImagePixelCount height = v7;
  src.vImagePixelCount width = v6;
  src.size_t rowBytes = v10;
  size_t v11 = *(void *)(a2 + 56);
  dest.data = *(void **)(a2 + 32);
  dest.vImagePixelCount height = v7;
  dest.vImagePixelCount width = v6;
  dest.size_t rowBytes = v11;
  return vImagePermuteChannels_RGB888(&src, &dest, (const uint8_t *)(a1 + 32), 0);
}

uint64_t vImage.PixelBuffer<>._permute<A>(permuteArray:destination:permuteFunc:)(uint64_t a1, uint64_t *a2, uint64_t (*a3)(uint64_t *, uint64_t *, uint64_t, void))
{
  uint64_t v26 = *MEMORY[0x1E4F143B8];
  uint64_t v6 = *a2;
  uint64_t v7 = *v3;
  if ((specialized Sequence<>.max()(a1) & 0x1FC) != 0)
  {
    __break(1u);
    goto LABEL_6;
  }
  int v17 = a3;
  uint64_t v18 = v7;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&v22);
  uint64_t v9 = v22;
  uint64_t v8 = v23;
  type metadata accessor for vImage.PixelBuffer();
  vImage.PixelBuffer.size.getter(&v18);
  swift_bridgeObjectRelease();
  if (v9 != v18 || v8 != v19) {
LABEL_6:
  }
    __break(1u);
  uint64_t v22 = v7;
  uint64_t v18 = vImage.PixelBuffer<>.vImageBuffer.getter();
  uint64_t v19 = v10;
  uint64_t v20 = v11;
  uint64_t v21 = v12;
  uint64_t v22 = v6;
  uint64_t v22 = vImage.PixelBuffer<>.vImageBuffer.getter();
  uint64_t v23 = v13;
  uint64_t v24 = v14;
  uint64_t v25 = v15;
  return v17(&v18, &v22, a1 + 32, 0);
}

uint64_t vImage.PixelBuffer<>.permuteChannels(to:destination:)(char a1, char a2, char a3, uint64_t *a4)
{
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135DC0;
  *(unsigned char *)(inited + 32) = a1;
  *(unsigned char *)(inited + 33) = a2;
  *(unsigned char *)(inited + 34) = a3;
  specialized vImage.PixelBuffer<>._permute<A>(permuteArray:destination:permuteFunc:)(inited, *a4, *v4);
  return swift_setDeallocating();
}

uint64_t vImage.PixelBuffer<>.permuteChannels(to:destination:)(char a1, char a2, char a3, char a4, void *a5, void (*a6)(void *, void *, uint64_t, void))
{
  v25[9] = *MEMORY[0x1E4F143B8];
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
  uint64_t inited = swift_initStackObject();
  *(unsigned char *)(inited + 32) = a1;
  uint64_t v14 = inited + 32;
  *(_OWORD *)(inited + 16) = xmmword_1D2135FC0;
  *(unsigned char *)(inited + 33) = a2;
  *(unsigned char *)(inited + 34) = a3;
  *(unsigned char *)(inited + 35) = a4;
  uint64_t v15 = (void *)*a5;
  unsigned int v16 = (void *)*v6;
  if ((specialized Sequence<>.max()(inited) & 0x1FC) != 0)
  {
    __break(1u);
    goto LABEL_16;
  }
  if (!v16[2])
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v17 = v16[6];
  if (v17 < 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  uint64_t v18 = v16[5];
  if (v18 < 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (!v17)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  if (!v18)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  if (!v15[2])
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v19 = v15[6];
  if (v19 < 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v20 = v15[5];
  if (v20 < 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v19)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v20)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  if (v17 != v19)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v18 != v20) {
    goto LABEL_27;
  }
  uint64_t v21 = v16[7];
  v25[0] = v16[4];
  v25[1] = v18;
  v25[2] = v17;
  v25[3] = v21;
  uint64_t v22 = v15[7];
  v24[0] = v15[4];
  v24[1] = v18;
  v24[2] = v17;
  v24[3] = v22;
  a6(v25, v24, v14, 0);
  return swift_setDeallocating();
}

float _swift_vDSP_dotpr(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v31 = a6;
  uint64_t v33 = *MEMORY[0x1E4F143B8];
  uint64_t v10 = *(void *)(a4 - 8);
  uint64_t v11 = MEMORY[0x1F4188790](a1);
  uint64_t v13 = (char *)&v27 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v15 = *(void *)(v14 - 8);
  MEMORY[0x1F4188790](v11);
  uint64_t v17 = (char *)&v27 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = *(void (**)(char *))(v15 + 16);
  uint64_t v29 = v19;
  v18(v17);
  uint64_t v20 = *(void (**)(char *, uint64_t, uint64_t))(v10 + 16);
  uint64_t v28 = a2;
  v20(v13, a2, a4);
  uint64_t v21 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  uint64_t v30 = v21(a3, a5);
  uint64_t v22 = (*(uint64_t (**)(uint64_t))(v31 + 16))(a4);
  (*(void (**)(char *, uint64_t))(v10 + 8))(v13, a4);
  (*(void (**)(char *, uint64_t))(v15 + 8))(v17, a3);
  if (v30 != v22)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  uint64_t v23 = v21(a3, a5);
  if (v23 < 0) {
    goto LABEL_5;
  }
  float v32 = NAN;
  uint64_t v24 = MEMORY[0x1F4188790](v23);
  *(&v27 - 8) = a3;
  *(&v27 - 7) = a4;
  uint64_t v25 = v31;
  *(&v27 - 6) = a5;
  *(&v27 - 5) = v25;
  *(&v27 - 4) = v28;
  *(&v27 - 3) = (uint64_t)&v32;
  *(&v27 - 2) = v24;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t)))(a5 + 24))(partial apply for closure #1 in static vDSP.dot<A, B>(_:_:));
  return v32;
}

double _swift_vDSP_dotprD(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v31 = a6;
  v32[1] = *MEMORY[0x1E4F143B8];
  uint64_t v10 = *(void *)(a4 - 8);
  uint64_t v11 = MEMORY[0x1F4188790](a1);
  uint64_t v13 = (char *)&v27 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v15 = *(void *)(v14 - 8);
  MEMORY[0x1F4188790](v11);
  uint64_t v17 = (char *)&v27 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = *(void (**)(char *))(v15 + 16);
  uint64_t v29 = v19;
  v18(v17);
  uint64_t v20 = *(void (**)(char *, uint64_t, uint64_t))(v10 + 16);
  uint64_t v28 = a2;
  v20(v13, a2, a4);
  uint64_t v21 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  uint64_t v30 = v21(a3, a5);
  uint64_t v22 = (*(uint64_t (**)(uint64_t))(v31 + 16))(a4);
  (*(void (**)(char *, uint64_t))(v10 + 8))(v13, a4);
  (*(void (**)(char *, uint64_t))(v15 + 8))(v17, a3);
  if (v30 != v22)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  uint64_t v23 = v21(a3, a5);
  if (v23 < 0) {
    goto LABEL_5;
  }
  v32[0] = 0x7FF8000000000000;
  uint64_t v24 = MEMORY[0x1F4188790](v23);
  *(&v27 - 8) = a3;
  *(&v27 - 7) = a4;
  uint64_t v25 = v31;
  *(&v27 - 6) = a5;
  *(&v27 - 5) = v25;
  *(&v27 - 4) = v28;
  *(&v27 - 3) = (uint64_t)v32;
  *(&v27 - 2) = v24;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t)))(a5 + 24))(partial apply for closure #1 in static vDSP.dot<A, B>(_:_:));
  return *(double *)v32;
}

float static vDSP.dot<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return _swift_vDSP_dotpr(a1, a2, a3, a3, a4, a4);
}

double static vDSP.dot<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  return _swift_vDSP_dotprD(a1, a2, a3, a3, a4, a4);
}

uint64_t static vDSP.hypot<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.hypot<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.hypot<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.hypot<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.hypot<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t partial apply for closure #1 in static vDSP.hypot<A, B>(_:_:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.hypot<A, B>(_:_:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.hypot<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.hypot<A, B>(_:_:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.hypot<A, B, C>(_:_:result:));
}

uint64_t static vDSP.hypot<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.hypot<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  return static vDSP.hypot<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  char *v22;
  uint64_t v23;
  void (*v24)(char *);
  uint64_t v25;
  void (*v26)(char *, uint64_t, uint64_t);
  uint64_t (*v27)(uint64_t, uint64_t);
  uint64_t v28;
  uint64_t (*v29)(uint64_t, uint64_t);
  uint64_t v30;
  char *v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  BOOL v37;
  uint64_t v38;
  uint64_t v39;
  void (*v40)(char *, uint64_t);
  uint64_t v41;
  uint64_t result;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  void v46[2];
  uint64_t v47;
  uint64_t v48;
  char *v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;

  long long v52 = a3;
  uint64_t v53 = a6;
  uint64_t v14 = *(void *)(a5 - 8);
  long long v48 = a9;
  uint64_t v15 = MEMORY[0x1F4188790](a1);
  uint64_t v17 = (char *)v46 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = MEMORY[0x1F4188790](v15);
  uint64_t v20 = (char *)v46 - v19;
  MEMORY[0x1F4188790](v18);
  uint64_t v22 = (char *)v46 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  long long v50 = v23;
  uint64_t v24 = *(void (**)(char *))(v23 + 16);
  v46[1] = v25;
  v24(v22);
  int v54 = v14;
  uint64_t v26 = *(void (**)(char *, uint64_t, uint64_t))(v14 + 16);
  v46[0] = a2;
  v26(v20, a2, a5);
  uint64_t v27 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  long long v49 = v22;
  uint64_t v55 = a4;
  long long v47 = a7;
  uint64_t v28 = v27(a4, a7);
  uint64_t v29 = *(uint64_t (**)(uint64_t, uint64_t))(a8 + 16);
  long long v51 = a8;
  uint64_t v30 = v29(a5, a8);
  uint64_t v31 = v17;
  v26(v17, (uint64_t)v20, a5);
  if (v28 == v30)
  {
    float v32 = v51;
    uint64_t v33 = v29(a5, v51);
    uint64_t v34 = v48;
    uint64_t v36 = v52;
    uint64_t v35 = v53;
    uint64_t v37 = v33 != (*(uint64_t (**)(uint64_t))(*(void *)(v48 + 8) + 16))(v53);
    uint64_t v38 = v32;
    uint64_t v39 = v35;
  }
  else
  {
    uint64_t v34 = v48;
    uint64_t v37 = 1;
    uint64_t v38 = v51;
    uint64_t v36 = v52;
    uint64_t v39 = v53;
  }
  uint64_t v40 = *(void (**)(char *, uint64_t))(v54 + 8);
  v40(v31, a5);
  v40(v20, a5);
  char v41 = v55;
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v50 + 8))(v49, v55);
  if (v37)
  {
    __break(1u);
  }
  else
  {
    uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(v34 + 8) + 16))(v39);
    if ((result & 0x8000000000000000) == 0)
    {
      char v43 = MEMORY[0x1F4188790](result);
      v46[-10] = v41;
      v46[-9] = a5;
      uint64_t v44 = v47;
      v46[-8] = v39;
      v46[-7] = v44;
      v46[-6] = v38;
      v46[-5] = v34;
      v46[-4] = v46[0];
      v46[-3] = v36;
      v46[-2] = v43;
      return (*(uint64_t (**)(uint64_t))(v44 + 24))(v45);
    }
  }
  __break(1u);
  return result;
}

uint64_t static vDSP.hypot<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t))
{
  uint64_t v38 = a7;
  uint64_t v39 = a8;
  uint64_t v40 = a6;
  uint64_t v12 = *(void *)(a4 - 8);
  uint64_t v13 = MEMORY[0x1F4188790](a1);
  uint64_t v15 = (char *)&v34 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v17 = *(void *)(v16 - 8);
  MEMORY[0x1F4188790](v13);
  uint64_t v19 = (char *)&v34 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v20 = *(void (**)(char *))(v17 + 16);
  uint64_t v35 = v21;
  v20(v19);
  uint64_t v22 = *(void (**)(char *, uint64_t, uint64_t))(v12 + 16);
  uint64_t v37 = a2;
  v22(v15, a2, a4);
  uint64_t v23 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  uint64_t v36 = a5;
  uint64_t v24 = v23(a3, a5);
  uint64_t v25 = (*(uint64_t (**)(uint64_t))(v40 + 16))(a4);
  (*(void (**)(char *, uint64_t))(v12 + 8))(v15, a4);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v17 + 8))(v19, a3);
  if (v24 == v25)
  {
    uint64_t v27 = v35;
    uint64_t v28 = v36;
    uint64_t v29 = v23(a3, v36);
    uint64_t v30 = MEMORY[0x1F4188790](v29);
    *(&v34 - 6) = a3;
    *(&v34 - 5) = a4;
    uint64_t v31 = v40;
    *(&v34 - 4) = v28;
    *(&v34 - 3) = v31;
    uint64_t v33 = v37;
    uint64_t v32 = v38;
    *(&v34 - 2) = v27;
    *(&v34 - 1) = v33;
    return v39(v30, v32);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in static vDSP.hypot<A, B>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, unint64_t *a10, void (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(a9);
  uint64_t v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a10, a9);
  a11(a3, a4, a1, a5, a6, v16, a7, a8, v17);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  *a2 = result;
  return result;
}

void *closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:)(void *result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, uint64_t, uint64_t, void, uint64_t, uint64_t))
{
  if (!a2)
  {
    __break(1u);
    goto LABEL_6;
  }
  if (!a4)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  if (*result) {
    return (void *)a7(a2, 1, a4, 1, *result, 1, a6);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  return static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:), (uint64_t (*)(uint64_t, uint64_t))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, char *a14, uint64_t a15)
{
  return static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15);
}

{
  return static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15);
}

{
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  char *v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  char *v35;
  void (*v36)(char *);
  uint64_t v37;
  void (*v38)(char *, uint64_t, uint64_t);
  uint64_t (*v39)(uint64_t);
  uint64_t v40;
  uint64_t v41;
  uint64_t (*v42)(void);
  uint64_t v43;
  char *v44;
  uint64_t v45;
  BOOL v46;
  void (*v47)(char *, uint64_t);
  char *v48;
  uint64_t v49;
  uint64_t (*v50)(uint64_t, uint64_t);
  uint64_t v51;
  uint64_t v52;
  uint64_t v53;
  uint64_t v54;
  uint64_t v55;
  uint64_t v56;
  BOOL v57;
  uint64_t v58;
  void (*v59)(uint64_t (*)(uint64_t), uint64_t);
  uint64_t result;
  uint64_t v61;
  uint64_t v62;
  uint64_t v63;
  uint64_t v64;
  uint64_t v65;
  uint64_t v66;
  uint64_t v67;
  uint64_t v68;
  uint64_t v69;
  uint64_t v70;
  uint64_t (*v71)(uint64_t);
  uint64_t v72;
  char *v73;
  uint64_t v74;
  void (*v75)(char *, uint64_t, uint64_t);
  uint64_t v76;
  uint64_t v77;
  uint64_t v78;
  uint64_t v79;
  uint64_t v80;
  uint64_t v81;
  char *v82;
  char *v83;
  uint64_t v84;
  uint64_t v85;
  uint64_t v86;
  uint64_t v87;
  uint64_t v88;
  uint64_t v89;

  uint64_t v81 = a5;
  long long v77 = a3;
  long long v78 = a4;
  uint64_t v86 = a15;
  long long v87 = a10;
  long long v89 = a9;
  uint64_t v85 = *(void *)(a9 - 8);
  long long v79 = a12;
  uint64_t v18 = MEMORY[0x1F4188790](a1);
  uint64_t v83 = (char *)&v68 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v72 = v20;
  int v84 = *(void *)(v20 - 8);
  uint64_t v21 = MEMORY[0x1F4188790](v18);
  uint64_t v71 = (uint64_t (*)(uint64_t))((char *)&v68 - ((v22 + 15) & 0xFFFFFFFFFFFFFFF0));
  uint64_t v23 = MEMORY[0x1F4188790](v21);
  int v82 = (char *)&v68 - v24;
  uint64_t v26 = *(void *)(v25 - 8);
  uint64_t v27 = MEMORY[0x1F4188790](v23);
  uint64_t v29 = (char *)&v68 - ((v28 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v31 = *(void *)(v30 - 8);
  uint64_t v32 = MEMORY[0x1F4188790](v27);
  long long v73 = (char *)&v68 - ((v33 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1F4188790](v32);
  uint64_t v35 = (char *)&v68 - v34;
  long long v74 = v31;
  uint64_t v36 = *(void (**)(char *))(v31 + 16);
  int v70 = v37;
  v36((char *)&v68 - v34);
  long long v76 = v26;
  uint64_t v38 = *(void (**)(char *, uint64_t, uint64_t))(v26 + 16);
  uint64_t v69 = a2;
  v38(v29, a2, a7);
  uint64_t v39 = *(uint64_t (**)(uint64_t))(a11 + 16);
  uint64_t v40 = a6;
  long long v88 = a11;
  char v41 = v39(a6);
  uint64_t v42 = *(uint64_t (**)(void))(v79 + 16);
  long long v75 = (void (*)(char *, uint64_t, uint64_t))v29;
  char v43 = a7;
  uint64_t v44 = v73;
  long long v80 = v43;
  long long v45 = v42();
  ((void (*)(char *, char *, uint64_t))v36)(v44, v35, v40);
  if (v41 == v45)
  {
    char v41 = ((uint64_t (*)(uint64_t, uint64_t))v39)(v40, v88);
    uint64_t v39 = (uint64_t (*)(uint64_t))v86;
    long long v46 = v41 != (*(uint64_t (**)(uint64_t))(*(void *)(v86 + 8) + 16))(v87);
  }
  else
  {
    long long v46 = 1;
  }
  long long v47 = *(void (**)(char *, uint64_t))(v74 + 8);
  v47(v44, v40);
  (*(void (**)(void (*)(char *, uint64_t, uint64_t), uint64_t))(v76 + 8))(v75, v80);
  v47(v35, v40);
  if (v46)
  {
    __break(1u);
  }
  else
  {
    long long v75 = *(void (**)(char *, uint64_t, uint64_t))(v84 + 16);
    long long v76 = v40;
    long long v48 = v82;
    char v41 = v72;
    v75(v82, v77, v72);
    long long v49 = v89;
    (*(void (**)(char *, uint64_t, uint64_t))(v85 + 16))(v83, v78, v89);
    long long v50 = *(uint64_t (**)(uint64_t, uint64_t))(a13 + 16);
    long long v51 = v50(v41, a13);
    long long v73 = a14;
    long long v74 = v51;
    long long v52 = (*((uint64_t (**)(uint64_t, char *))v73 + 2))(v49, v73);
    uint64_t v39 = v71;
    v75((char *)v71, (uint64_t)v48, v41);
    int v68 = a13;
    if (v74 == v52)
    {
      uint64_t v53 = v50(v41, a13);
      int v54 = v86;
      uint64_t v55 = v87;
      int v56 = v81;
      uint64_t v57 = v53 != (*(uint64_t (**)(uint64_t))(*(void *)(v86 + 8) + 16))(v87);
      uint64_t v58 = v88;
      goto LABEL_6;
    }
  }
  uint64_t v57 = 1;
  uint64_t v55 = v87;
  uint64_t v58 = v88;
  int v54 = v86;
  int v56 = v81;
LABEL_6:
  long long v59 = *(void (**)(uint64_t (*)(uint64_t), uint64_t))(v84 + 8);
  v59(v39, v41);
  (*(void (**)(char *, uint64_t))(v85 + 8))(v83, v89);
  uint64_t result = ((uint64_t (*)(char *, uint64_t))v59)(v82, v41);
  long long v61 = v76;
  if (v57)
  {
    __break(1u);
  }
  else
  {
    uint64_t result = (*(uint64_t (**)(uint64_t))(*(void *)(v54 + 8) + 16))(v55);
    if ((result & 0x8000000000000000) == 0)
    {
      long long v62 = MEMORY[0x1F4188790](result);
      long long v63 = v80;
      *(&v68 - 16) = v61;
      *(&v68 - 15) = v63;
      long long v64 = v89;
      *(&v68 - 14) = v41;
      *(&v68 - 13) = v64;
      *(&v68 - 12) = v55;
      *(&v68 - 11) = v58;
      long long v65 = v68;
      *(&v68 - 10) = v79;
      *(&v68 - 9) = v65;
      *(&v68 - 8) = (uint64_t)v73;
      *(&v68 - 7) = v54;
      long long v66 = v77;
      *(&v68 - 6) = v69;
      *(&v68 - 5) = v66;
      *(&v68 - 4) = v78;
      *(&v68 - 3) = v56;
      *(&v68 - 2) = v62;
      return (*(uint64_t (**)(uint64_t))(v58 + 24))(v67);
    }
  }
  __break(1u);
  return result;
}

uint64_t static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t (*a14)(uint64_t, uint64_t))
{
  uint64_t v97 = a8;
  uint64_t v98 = a4;
  uint64_t v100 = a3;
  long long v94 = *(uint64_t (**)(char *, uint64_t))(a8 - 8);
  uint64_t v17 = MEMORY[0x1F4188790](a1);
  long long v93 = (char *)&v74 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v99 = v19;
  uint64_t v101 = *(void *)(v19 - 8);
  uint64_t v20 = MEMORY[0x1F4188790](v17);
  long long v90 = (char *)&v74 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v22 = MEMORY[0x1F4188790](v20);
  long long v92 = (char *)&v74 - v23;
  uint64_t v25 = *(void *)(v24 - 8);
  uint64_t v26 = MEMORY[0x1F4188790](v22);
  uint64_t v28 = (char *)&v74 - ((v27 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v30 = *(void *)(v29 - 8);
  uint64_t v31 = MEMORY[0x1F4188790](v26);
  long long v88 = (char *)&v74 - ((v32 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1F4188790](v31);
  uint64_t v34 = (char *)&v74 - v33;
  uint64_t v35 = *(void (**)(char *, uint64_t, uint64_t))(v30 + 16);
  uint64_t v96 = v36;
  uint64_t v83 = v35;
  uint64_t v84 = v30 + 16;
  ((void (*)(char *))v35)((char *)&v74 - v33);
  uint64_t v37 = a2;
  (*(void (**)(char *, uint64_t, uint64_t))(v25 + 16))(v28, a2, a6);
  uint64_t v38 = *(uint64_t (**)(uint64_t, uint64_t))(a9 + 16);
  uint64_t v91 = a9;
  uint64_t v85 = v38;
  uint64_t v86 = a9 + 16;
  uint64_t v39 = v38(a5, a9);
  uint64_t v40 = *(uint64_t (**)(uint64_t, uint64_t))(a10 + 16);
  uint64_t v81 = a10;
  uint64_t v41 = v40(a6, a10);
  uint64_t v42 = *(void (**)(char *, uint64_t))(v25 + 8);
  uint64_t v82 = a6;
  v42(v28, a6);
  char v43 = *(uint64_t (**)(char *, uint64_t))(v30 + 8);
  uint64_t v87 = a5;
  uint64_t v89 = v30 + 8;
  uint64_t result = v43(v34, a5);
  if (v39 != v41)
  {
    __break(1u);
    goto LABEL_6;
  }
  long long v80 = v43;
  uint64_t v76 = v37;
  long long v45 = v92;
  uint64_t v46 = v101;
  long long v47 = *(void (**)(char *, uint64_t, uint64_t))(v101 + 16);
  uint64_t v48 = v99;
  uint64_t v79 = v101 + 16;
  long long v78 = v47;
  v47(v92, v100, v99);
  long long v50 = v93;
  long long v49 = v94;
  uint64_t v51 = v97;
  (*((void (**)(char *, uint64_t, uint64_t))v94 + 2))(v93, v98, v97);
  long long v52 = *(uint64_t (**)(uint64_t, uint64_t))(a11 + 16);
  uint64_t v95 = a11;
  long long v77 = v52;
  uint64_t v53 = v52(v48, a11);
  int v54 = *(uint64_t (**)(uint64_t, uint64_t))(a12 + 16);
  uint64_t v75 = a12;
  uint64_t v55 = v54(v51, a12);
  (*((void (**)(char *, uint64_t))v49 + 1))(v50, v51);
  int v56 = *(uint64_t (**)(char *, uint64_t))(v46 + 8);
  uint64_t v101 = v46 + 8;
  uint64_t result = v56(v45, v48);
  if (v53 != v55)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v57 = v87;
  uint64_t v58 = v88;
  v83(v88, v96, v87);
  long long v94 = v56;
  long long v59 = v90;
  uint64_t v60 = v99;
  v78(v90, v100, v99);
  uint64_t v61 = v91;
  long long v62 = v85;
  uint64_t v63 = v85(v57, v91);
  uint64_t v64 = v77(v60, v95);
  v94(v59, v60);
  uint64_t result = v80(v58, v57);
  if (v63 == v64)
  {
    uint64_t v65 = v96;
    uint64_t v66 = v62(v57, v61);
    uint64_t v67 = MEMORY[0x1F4188790](v66);
    uint64_t v68 = v82;
    *(&v74 - 12) = v57;
    *(&v74 - 11) = v68;
    uint64_t v69 = v97;
    *(&v74 - 10) = v99;
    *(&v74 - 9) = v69;
    uint64_t v70 = v81;
    *(&v74 - 8) = v61;
    *(&v74 - 7) = v70;
    uint64_t v71 = v75;
    *(&v74 - 6) = v95;
    *(&v74 - 5) = v71;
    uint64_t v72 = v76;
    *(&v74 - 4) = v65;
    *(&v74 - 3) = v72;
    uint64_t v73 = v98;
    *(&v74 - 2) = v100;
    *(&v74 - 1) = v73;
    return a14(v67, a13);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t closure #1 in static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t *a15, unint64_t *a16, void (*a17)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v19 = __swift_instantiateConcreteTypeFromMangledName(a15);
  uint64_t v20 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a16, a15);
  a17(a3, a4, a5, a6, a1, a7, a8, a9, a10, v19, a11, a12, a13, a14, v20);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a11 + 16))(a7, a11);
  *a2 = result;
  return result;
}

void *closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(void *result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, void, uint64_t, uint64_t))
{
  if (!a2)
  {
    __break(1u);
    goto LABEL_8;
  }
  if (!a4)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (!a6)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (!a8)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (*result) {
    return (void *)a11(a2, 1, a4, 1, a6, 1, a8, 1, *result, 1, a10);
  }
LABEL_11:
  __break(1u);
  return result;
}

float static vDSP.distanceSquared<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v31 = a6;
  uint64_t v33 = *MEMORY[0x1E4F143B8];
  uint64_t v10 = *(void *)(a4 - 8);
  uint64_t v11 = MEMORY[0x1F4188790](a1);
  uint64_t v13 = (char *)&v27 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v15 = *(void *)(v14 - 8);
  MEMORY[0x1F4188790](v11);
  uint64_t v17 = (char *)&v27 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = *(void (**)(char *))(v15 + 16);
  uint64_t v29 = v19;
  v18(v17);
  uint64_t v20 = *(void (**)(char *, uint64_t, uint64_t))(v10 + 16);
  uint64_t v28 = a2;
  v20(v13, a2, a4);
  uint64_t v21 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  uint64_t v30 = v21(a3, a5);
  uint64_t v22 = (*(uint64_t (**)(uint64_t))(*(void *)(v31 + 8) + 16))(a4);
  (*(void (**)(char *, uint64_t))(v10 + 8))(v13, a4);
  (*(void (**)(char *, uint64_t))(v15 + 8))(v17, a3);
  if (v30 != v22)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  uint64_t v23 = v21(a3, a5);
  if (v23 < 0) {
    goto LABEL_5;
  }
  float v32 = NAN;
  uint64_t v24 = MEMORY[0x1F4188790](v23);
  *(&v27 - 8) = a3;
  *(&v27 - 7) = a4;
  uint64_t v25 = v31;
  *(&v27 - 6) = a5;
  *(&v27 - 5) = v25;
  *(&v27 - 4) = v28;
  *(&v27 - 3) = (uint64_t)&v32;
  *(&v27 - 2) = v24;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t)))(a5 + 24))(partial apply for closure #1 in static vDSP.distanceSquared<A, B>(_:_:));
  return v32;
}

double static vDSP.distanceSquared<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v31 = a6;
  v32[1] = *MEMORY[0x1E4F143B8];
  uint64_t v10 = *(void *)(a4 - 8);
  uint64_t v11 = MEMORY[0x1F4188790](a1);
  uint64_t v13 = (char *)&v27 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v15 = *(void *)(v14 - 8);
  MEMORY[0x1F4188790](v11);
  uint64_t v17 = (char *)&v27 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = *(void (**)(char *))(v15 + 16);
  uint64_t v29 = v19;
  v18(v17);
  uint64_t v20 = *(void (**)(char *, uint64_t, uint64_t))(v10 + 16);
  uint64_t v28 = a2;
  v20(v13, a2, a4);
  uint64_t v21 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  uint64_t v30 = v21(a3, a5);
  uint64_t v22 = (*(uint64_t (**)(uint64_t))(*(void *)(v31 + 8) + 16))(a4);
  (*(void (**)(char *, uint64_t))(v10 + 8))(v13, a4);
  (*(void (**)(char *, uint64_t))(v15 + 8))(v17, a3);
  if (v30 != v22)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  uint64_t v23 = v21(a3, a5);
  if (v23 < 0) {
    goto LABEL_5;
  }
  v32[0] = 0x7FF8000000000000;
  uint64_t v24 = MEMORY[0x1F4188790](v23);
  *(&v27 - 8) = a3;
  *(&v27 - 7) = a4;
  uint64_t v25 = v31;
  *(&v27 - 6) = a5;
  *(&v27 - 5) = v25;
  *(&v27 - 4) = v28;
  *(&v27 - 3) = (uint64_t)v32;
  *(&v27 - 2) = v24;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t)))(a5 + 24))(partial apply for closure #1 in static vDSP.distanceSquared<A, B>(_:_:));
  return *(double *)v32;
}

uint64_t partial apply for closure #1 in static vDSP.hypot<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.hypot<A, B>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  return closure #1 in static vDSP.hypot<A, B>(_:_:)(a1, a2, v5[6], v5[7], v5[2], v5[3], v5[4], v5[5], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

{
  return partial apply for closure #1 in static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(a1, a2, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, &lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

uint64_t partial apply for closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

{
  return partial apply for closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

uint64_t partial apply for closure #1 in static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(uint64_t a1, uint64_t *a2, uint64_t *a3, unint64_t *a4, void (*a5)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  return closure #1 in static vDSP.hypot<A, B, C, D>(x0:x1:y0:y1:)(a1, a2, v5[10], v5[11], v5[12], v5[13], v5[2], v5[3], v5[4], v5[5], v5[6], v5[7], v5[8], v5[9], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 56);
  uint64_t v5 = *(void *)(v3 + 120);
  uint64_t v6 = *(void *)(v3 + 128);
  void v9[2] = *(void *)(v3 + 16);
  long long v10 = *(_OWORD *)(v3 + 24);
  long long v11 = *(_OWORD *)(v3 + 40);
  uint64_t v12 = v4;
  long long v7 = *(_OWORD *)(v3 + 80);
  long long v13 = *(_OWORD *)(v3 + 64);
  long long v14 = v7;
  long long v15 = *(_OWORD *)(v3 + 104);
  uint64_t v16 = v5;
  uint64_t v17 = a1;
  uint64_t v18 = a2;
  uint64_t v19 = v6;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, void))(v13 + 24))(a3, v9, MEMORY[0x1E4FBC848] + 8, v10);
}

uint64_t partial apply for closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.distanceSquared<A, B>(_:_:));
}

{
  return partial apply for closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.distanceSquared<A, B>(_:_:));
}

uint64_t partial apply for closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = v3[3];
  uint64_t v5 = v3[5];
  uint64_t v6 = v3[7];
  _OWORD v8[2] = a1;
  void v8[3] = a2;
  void v8[4] = v6;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(*(void *)(v5 + 8) + 24))(a3, v8, MEMORY[0x1E4FBC848] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(a1, a2, MEMORY[0x1E4F168C0]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(a1, a2, MEMORY[0x1E4F168B8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:));
}

void *partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(void *a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, MEMORY[0x1E4F16D90]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, MEMORY[0x1E4F16D88]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 64);
  uint64_t v5 = *(void *)(v3 + 88);
  uint64_t v6 = *(void *)(v3 + 136);
  long long v7 = *(_OWORD *)(v3 + 32);
  v10[1] = *(_OWORD *)(v3 + 16);
  _OWORD v10[2] = v7;
  v10[3] = *(_OWORD *)(v3 + 48);
  uint64_t v11 = v4;
  long long v12 = *(_OWORD *)(v3 + 72);
  uint64_t v13 = v5;
  long long v8 = *(_OWORD *)(v3 + 120);
  long long v14 = *(_OWORD *)(v3 + 104);
  long long v15 = v8;
  uint64_t v16 = a1;
  uint64_t v17 = a2;
  uint64_t v18 = v6;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, void))(v12 + 24))(a3, v10, MEMORY[0x1E4FBC848] + 8, v7);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 32);
  uint64_t v5 = *(void *)(v3 + 72);
  uint64_t v6 = *(void *)(v3 + 104);
  uint64_t v7 = *(void *)(v3 + 144);
  v10[1] = *(_OWORD *)(v3 + 16);
  uint64_t v11 = v4;
  long long v12 = *(_OWORD *)(v3 + 40);
  long long v13 = *(_OWORD *)(v3 + 56);
  uint64_t v14 = v5;
  long long v8 = *(_OWORD *)(v3 + 128);
  long long v17 = *(_OWORD *)(v3 + 112);
  long long v15 = *(_OWORD *)(v3 + 80);
  uint64_t v16 = v6;
  uint64_t v18 = a1;
  uint64_t v19 = a2;
  long long v20 = v8;
  uint64_t v21 = v7;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, void))(v15 + 24))(a3, v10, MEMORY[0x1E4FBC848] + 8, v12);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 48);
  uint64_t v5 = *(void *)(v3 + 88);
  uint64_t v6 = *(void *)(v3 + 152);
  long long v7 = *(_OWORD *)(v3 + 120);
  v9[1] = *(_OWORD *)(v3 + 104);
  void v9[2] = v7;
  void v9[3] = *(_OWORD *)(v3 + 136);
  uint64_t v10 = a1;
  uint64_t v11 = a2;
  uint64_t v12 = v6;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, uint64_t))(v5 + 16))(a3, v9, MEMORY[0x1E4FBC848] + 8, v4);
}

void *partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(void *a1, uint64_t (*a2)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, void, uint64_t, uint64_t))
{
  return closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C, D, E>(x0:x1:y0:y1:result:)(a1, v2[2], v2[3], v2[4], v2[5], v2[6], v2[7], v2[8], v2[9], v2[10], a2);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.taperedMerge<A, B, C>(_:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:));
}

void *partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:)(void *a1)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:)(a1, MEMORY[0x1E4F16B38]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:)(a1, MEMORY[0x1E4F16B30]);
}

void *partial apply for closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:)(void *a1, uint64_t (*a2)(uint64_t, uint64_t, uint64_t, uint64_t, void, uint64_t, uint64_t))
{
  return closure #1 in closure #1 in closure #1 in static vDSP.hypot<A, B, C>(_:_:result:)(a1, v2[2], v2[3], v2[4], v2[5], v2[6], a2);
}

uint64_t partial apply for closure #1 in static vDSP.dot<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.dot<A, B>(_:_:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.dot<A, B>(_:_:));
}

{
  return partial apply for closure #1 in static vDSP.dot<A, B>(_:_:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.dot<A, B>(_:_:));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.dot<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(a1, a2, MEMORY[0x1E4F168D0]);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(a1, a2, MEMORY[0x1E4F168C8]);
}

uint64_t partial apply for closure #1 in static vDSP.dot<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = v3[3];
  uint64_t v5 = v3[5];
  uint64_t v6 = v3[7];
  _OWORD v8[2] = a1;
  void v8[3] = a2;
  void v8[4] = v6;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E4FBC848] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.distanceSquared<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(void))
{
  uint64_t result = *(void *)(v3 + 16);
  if (result)
  {
    if (a1) {
      return a3();
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

void one-time initialization function for gaussian1Dx3()
{
  static vImage.ConvolutionKernel.gaussian1Dx3 = (uint64_t)&outlined read-only object #0 of one-time initialization function for gaussian1Dx3;
}

uint64_t static vImage.ConvolutionKernel.gaussian1Dx3.getter()
{
  return static vImage.ConvolutionKernel.gaussian1Dx3.getter(&one-time initialization token for gaussian1Dx3);
}

void one-time initialization function for gaussian1Dx5()
{
  static vImage.ConvolutionKernel.gaussian1Dx5 = (uint64_t)&outlined read-only object #0 of one-time initialization function for gaussian1Dx5;
}

uint64_t static vImage.ConvolutionKernel.gaussian1Dx5.getter()
{
  return static vImage.ConvolutionKernel.gaussian1Dx3.getter(&one-time initialization token for gaussian1Dx5);
}

void one-time initialization function for gaussian1Dx7()
{
  static vImage.ConvolutionKernel.gaussian1Dx7 = (uint64_t)&outlined read-only object #0 of one-time initialization function for gaussian1Dx7;
}

uint64_t static vImage.ConvolutionKernel.gaussian1Dx7.getter()
{
  return static vImage.ConvolutionKernel.gaussian1Dx3.getter(&one-time initialization token for gaussian1Dx7);
}

uint64_t static vImage.ConvolutionKernel.gaussian1Dx3.getter(void *a1)
{
  if (*a1 != -1) {
    swift_once();
  }

  return swift_bridgeObjectRetain();
}

const vImage_Buffer *vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(uint64_t a1, uint64_t a2, Pixel_16U *a3, uint64_t *a4, float a5, uint64_t a6, uint64_t a7)
{
  uint64_t v31 = *MEMORY[0x1E4F143B8];
  uint64_t v10 = *v7;
  uint64_t v28 = *a4;
  uint64_t v29 = v10;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&v30);
  vImage.PixelBuffer.size.getter(&v27);
  uint64_t v26 = v10;
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  if (*(_OWORD *)&v30.data != v27)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if ((*(unsigned char *)(a1 + 16) & 1) == 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if ((*(unsigned char *)(a2 + 16) & 1) == 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  uint64_t result = (const vImage_Buffer *)(*(uint64_t (**)(void))(a7 + 32))();
  if (((unint64_t)result & 0x8000000000000000) != 0)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  uint64_t v12 = result;
  if (result)
  {
    unint64_t v13 = 0;
    uint64_t v14 = 32;
    while (v12 != (const vImage_Buffer *)v13)
    {
      *(void *)&long long v27 = v26;
      swift_bridgeObjectRetain();
      swift_bridgeObjectRetain();
      uint64_t v15 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v13 >= *(void *)(v15 + 16)) {
        goto LABEL_19;
      }
      uint64_t v16 = *(void *)(v15 + v14);
      swift_bridgeObjectRelease();
      uint64_t v17 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v13 >= *(void *)(v17 + 16)) {
        goto LABEL_20;
      }
      uint64_t v18 = *(void *)(v17 + v14);
      swift_bridgeObjectRelease();
      swift_bridgeObjectRelease();
      swift_bridgeObjectRelease();
      if (v16)
      {
        if (v18 && v16 == v18) {
          goto LABEL_22;
        }
      }
      else if (!v18)
      {
        goto LABEL_27;
      }
      uint64_t v19 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v13 >= *(void *)(v19 + 16)) {
        goto LABEL_21;
      }
      long long v24 = *(_OWORD *)(v19 + v14 + 16);
      long long v25 = *(_OWORD *)(v19 + v14);
      swift_bridgeObjectRelease();
      *(_OWORD *)&v30.data = v25;
      *(_OWORD *)&v30.vImagePixelCount width = v24;
      uint64_t result = closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(&v30, a5, (uint64_t)a4, v13, a1, a2, a3);
      v14 += 32;
      if (v12 == (const vImage_Buffer *)++v13) {
        return result;
      }
    }
    __break(1u);
LABEL_19:
    __break(1u);
LABEL_20:
    __break(1u);
LABEL_21:
    __break(1u);
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  return result;
}

const vImage_Buffer *closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(vImage_Buffer *a1, float a2, uint64_t a3, unint64_t a4, uint64_t a5, uint64_t a6, Pixel_16U *a7)
{
  uint64_t v19 = *MEMORY[0x1E4F143B8];
  type metadata accessor for vImage.PixelBuffer();
  uint64_t v13 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a4 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  if (*(void *)(v13 + 16) <= a4) {
    goto LABEL_5;
  }
  uint64_t v14 = v13 + 32 * a4;
  long long v16 = *(_OWORD *)(v14 + 48);
  long long v17 = *(_OWORD *)(v14 + 32);
  swift_bridgeObjectRelease();
  *(_OWORD *)&dest.data = v17;
  *(_OWORD *)&dest.vImagePixelCount width = v16;
  return closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(&dest, a1, a5, a6, a7, a2);
}

const vImage_Buffer *closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(const vImage_Buffer *dest, vImage_Buffer *src, uint64_t a3, uint64_t a4, Pixel_16U *a5, float a6)
{
  unint64_t v6 = *(void *)(a3 + 16);
  if (HIDWORD(v6))
  {
    __break(1u);
    goto LABEL_8;
  }
  kernelY_vImagePixelCount width = *(void *)(a4 + 16);
  if (HIDWORD(kernelY_width))
  {
LABEL_8:
    __break(1u);
    return dest;
  }
  if (*((unsigned char *)a5 + 2) == 1)
  {
    Pixel_16U backgroundColor = 0;
    vImage_Flags flags = dword_1D2137AC4[(__int16)*a5];
  }
  else
  {
    Pixel_16U backgroundColor = *a5;
    vImage_Flags flags = 4;
  }
  return (const vImage_Buffer *)vImageSepConvolve_Planar8(src, dest, 0, 0, 0, (const float *)(a3 + 32), v6, (const float *)(a4 + 32), kernelY_width, a6, backgroundColor, flags);
}

uint64_t vImage.EdgeMode.backgroundColor.getter@<X0>(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  uint64_t v4 = *(void *)(a1 - 8);
  MEMORY[0x1F4188790](a1);
  unint64_t v6 = (char *)&v10 - ((v5 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(char *))(v4 + 16))(v6);
  uint64_t v7 = *(void *)(a1 + 16);
  uint64_t v8 = *(void *)(v7 - 8);
  if ((*(unsigned int (**)(char *, uint64_t, uint64_t))(v8 + 48))(v6, 3, v7))
  {
    (*(void (**)(uint64_t, uint64_t, uint64_t, uint64_t))(v8 + 56))(a2, 1, 1, v7);
    return (*(uint64_t (**)(char *, uint64_t))(v4 + 8))(v6, a1);
  }
  else
  {
    (*(void (**)(uint64_t, char *, uint64_t))(v8 + 32))(a2, v6, v7);
    return (*(uint64_t (**)(uint64_t, void, uint64_t, uint64_t))(v8 + 56))(a2, 0, 1, v7);
  }
}

uint64_t vImage.EdgeMode.vImageFlags.getter(uint64_t a1)
{
  MEMORY[0x1F4188790](a1);
  uint64_t v3 = (char *)&v9 - ((v2 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(char *))(v4 + 16))(v3);
  uint64_t v5 = *(void *)(a1 + 16);
  uint64_t v6 = *(void *)(v5 - 8);
  int v7 = (*(uint64_t (**)(char *, uint64_t, uint64_t))(v6 + 48))(v3, 3, v5);
  uint64_t result = 2;
  switch(v7)
  {
    case 1:
      return result;
    case 2:
      uint64_t result = 64;
      break;
    case 3:
      uint64_t result = 8;
      break;
    default:
      (*(void (**)(char *, uint64_t))(v6 + 8))(v3, v5);
      uint64_t result = 4;
      break;
  }
  return result;
}

const vImage_Buffer *vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(const vImage_Buffer *result, uint64_t a2, Pixel_16U *a3, uint64_t a4, float a5)
{
  uint64_t v18 = *MEMORY[0x1E4F143B8];
  uint64_t v6 = *(void **)v5;
  if (!*(void *)(*(void *)v5 + 16))
  {
    __break(1u);
    goto LABEL_22;
  }
  vImagePixelCount v7 = v6[6];
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  vImagePixelCount v8 = v6[5];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v8)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  uint64_t v9 = *(void **)a4;
  if (!*(void *)(*(void *)a4 + 16))
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  uint64_t v10 = v9[6];
  if (v10 < 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  uint64_t v11 = v9[5];
  if (v11 < 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (!v10)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v11)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (v7 != v10)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (v8 != v11)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if ((result->width & 1) == 0)
  {
LABEL_33:
    __break(1u);
LABEL_34:
    __break(1u);
  }
  if ((*(unsigned char *)(a2 + 16) & 1) == 0) {
    goto LABEL_34;
  }
  uint64_t v12 = (void *)v6[4];
  uint64_t v13 = (void *)v9[4];
  if (v12)
  {
    if (!v13 || v12 != v13) {
      goto LABEL_20;
    }
    __break(1u);
  }
  if (v13)
  {
LABEL_20:
    size_t v14 = v6[7];
    src.data = v12;
    src.vImagePixelCount height = v8;
    src.vImagePixelCount width = v7;
    src.size_t rowBytes = v14;
    size_t v15 = v9[7];
    dest.data = v13;
    dest.vImagePixelCount height = v8;
    dest.vImagePixelCount width = v7;
    dest.size_t rowBytes = v15;
    return closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(&dest, &src, (uint64_t)result, a2, a3, a5);
  }
  __break(1u);
  return result;
}

const vImage_Buffer *vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:useFloat16Accumulator:destination:)(const vImage_Buffer *result, uint64_t a2, Pixel_16F *a3, char a4, uint64_t a5, float a6)
{
  uint64_t v20 = *MEMORY[0x1E4F143B8];
  vImagePixelCount v7 = *(void **)v6;
  if (!*(void *)(*(void *)v6 + 16))
  {
    __break(1u);
    goto LABEL_25;
  }
  vImagePixelCount v8 = v7[6];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  vImagePixelCount v9 = v7[5];
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v8)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (!v9)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  uint64_t v10 = *(void **)a5;
  if (!*(void *)(*(void *)a5 + 16))
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  uint64_t v11 = v10[6];
  if (v11 < 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  uint64_t v12 = v10[5];
  if (v12 < 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v11)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v12)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (v8 != v11)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (v9 != v12)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if ((result->width & 1) == 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if ((*(unsigned char *)(a2 + 16) & 1) == 0)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  uint64_t v13 = (void *)v7[4];
  size_t v14 = (void *)v10[4];
  if (v13)
  {
    if (v14)
    {
      if (v13 != v14)
      {
        if ((a4 & 1) == 0)
        {
LABEL_19:
          unsigned int v15 = 0;
LABEL_23:
          size_t v16 = v7[7];
          src.data = v13;
          src.vImagePixelCount height = v9;
          src.vImagePixelCount width = v8;
          src.size_t rowBytes = v16;
          size_t v17 = v10[7];
          dest.data = v14;
          dest.vImagePixelCount height = v9;
          dest.vImagePixelCount width = v8;
          dest.size_t rowBytes = v17;
          return closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:useFloat16Accumulator:destination:)(&dest, &src, (uint64_t)result, a2, a3, v15, a6);
        }
LABEL_22:
        unsigned int v15 = 4096;
        goto LABEL_23;
      }
LABEL_38:
      __break(1u);
    }
LABEL_21:
    if ((a4 & 1) == 0) {
      goto LABEL_19;
    }
    goto LABEL_22;
  }
  if (v14) {
    goto LABEL_21;
  }
  __break(1u);
  return result;
}

const vImage_Buffer *closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:useFloat16Accumulator:destination:)(const vImage_Buffer *dest, vImage_Buffer *src, uint64_t a3, uint64_t a4, Pixel_16F *a5, unsigned int a6, float a7)
{
  unint64_t v7 = *(void *)(a3 + 16);
  if (HIDWORD(v7))
  {
    __break(1u);
    goto LABEL_8;
  }
  kernelY_vImagePixelCount width = *(void *)(a4 + 16);
  if (HIDWORD(kernelY_width))
  {
LABEL_8:
    __break(1u);
    return dest;
  }
  if (*((unsigned char *)a5 + 2) == 1)
  {
    Pixel_16F backgroundColor = 0;
    int v10 = dword_1D2137AC4[(__int16)*a5];
  }
  else
  {
    Pixel_16F backgroundColor = *a5;
    int v10 = 4;
  }
  return (const vImage_Buffer *)vImageSepConvolve_Planar16F(src, dest, 0, 0, 0, (const float *)(a3 + 32), v7, (const float *)(a4 + 32), kernelY_width, a7, backgroundColor, v10 | a6);
}

const vImage_Buffer *vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(uint64_t a1, uint64_t a2, int *a3, uint64_t *a4, float a5, uint64_t a6, uint64_t a7)
{
  uint64_t v31 = *MEMORY[0x1E4F143B8];
  uint64_t v10 = *v7;
  uint64_t v28 = *a4;
  uint64_t v29 = v10;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&v30);
  vImage.PixelBuffer.size.getter(&v27);
  uint64_t v26 = v10;
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  if (*(_OWORD *)&v30.data != v27)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if ((*(unsigned char *)(a1 + 16) & 1) == 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if ((*(unsigned char *)(a2 + 16) & 1) == 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  uint64_t result = (const vImage_Buffer *)(*(uint64_t (**)(void))(a7 + 32))();
  if (((unint64_t)result & 0x8000000000000000) != 0)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  uint64_t v12 = result;
  if (result)
  {
    unint64_t v13 = 0;
    uint64_t v14 = 32;
    while (v12 != (const vImage_Buffer *)v13)
    {
      *(void *)&long long v27 = v26;
      swift_bridgeObjectRetain();
      swift_bridgeObjectRetain();
      uint64_t v15 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v13 >= *(void *)(v15 + 16)) {
        goto LABEL_19;
      }
      uint64_t v16 = *(void *)(v15 + v14);
      swift_bridgeObjectRelease();
      uint64_t v17 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v13 >= *(void *)(v17 + 16)) {
        goto LABEL_20;
      }
      uint64_t v18 = *(void *)(v17 + v14);
      swift_bridgeObjectRelease();
      swift_bridgeObjectRelease();
      swift_bridgeObjectRelease();
      if (v16)
      {
        if (v18 && v16 == v18) {
          goto LABEL_22;
        }
      }
      else if (!v18)
      {
        goto LABEL_27;
      }
      uint64_t v19 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v13 >= *(void *)(v19 + 16)) {
        goto LABEL_21;
      }
      long long v24 = *(_OWORD *)(v19 + v14 + 16);
      long long v25 = *(_OWORD *)(v19 + v14);
      swift_bridgeObjectRelease();
      *(_OWORD *)&v30.data = v25;
      *(_OWORD *)&v30.vImagePixelCount width = v24;
      uint64_t result = closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(&v30, a5, (uint64_t)a4, v13, a1, a2, a3);
      v14 += 32;
      if (v12 == (const vImage_Buffer *)++v13) {
        return result;
      }
    }
    __break(1u);
LABEL_19:
    __break(1u);
LABEL_20:
    __break(1u);
LABEL_21:
    __break(1u);
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  return result;
}

const vImage_Buffer *closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(vImage_Buffer *a1, float a2, uint64_t a3, unint64_t a4, uint64_t a5, uint64_t a6, int *a7)
{
  uint64_t v19 = *MEMORY[0x1E4F143B8];
  type metadata accessor for vImage.PixelBuffer();
  uint64_t v13 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a4 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_5:
    __break(1u);
  }
  if (*(void *)(v13 + 16) <= a4) {
    goto LABEL_5;
  }
  uint64_t v14 = v13 + 32 * a4;
  long long v16 = *(_OWORD *)(v14 + 48);
  long long v17 = *(_OWORD *)(v14 + 32);
  swift_bridgeObjectRelease();
  *(_OWORD *)&dest.data = v17;
  *(_OWORD *)&dest.vImagePixelCount width = v16;
  return closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(&dest, a1, a5, a6, a7, a2);
}

const vImage_Buffer *closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(const vImage_Buffer *dest, vImage_Buffer *src, uint64_t a3, uint64_t a4, int *a5, float a6)
{
  unint64_t v6 = *(void *)(a3 + 16);
  if (HIDWORD(v6))
  {
    __break(1u);
    goto LABEL_8;
  }
  kernelY_vImagePixelCount width = *(void *)(a4 + 16);
  if (HIDWORD(kernelY_width))
  {
LABEL_8:
    __break(1u);
    return dest;
  }
  if (*((unsigned char *)a5 + 4) == 1)
  {
    vImage_Flags v8 = dword_1D2137AC4[*a5];
    Pixel_F v9 = 0.0;
  }
  else
  {
    Pixel_F v9 = *(float *)a5;
    vImage_Flags v8 = 4;
  }
  return (const vImage_Buffer *)vImageSepConvolve_PlanarF(src, dest, 0, 0, 0, (const float *)(a3 + 32), v6, (const float *)(a4 + 32), kernelY_width, a6, v9, v8);
}

const vImage_Buffer *vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(const vImage_Buffer *result, uint64_t a2, int *a3, uint64_t a4, float a5)
{
  uint64_t v18 = *MEMORY[0x1E4F143B8];
  unint64_t v6 = *(void **)v5;
  if (!*(void *)(*(void *)v5 + 16))
  {
    __break(1u);
    goto LABEL_22;
  }
  vImagePixelCount v7 = v6[6];
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  vImagePixelCount v8 = v6[5];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  if (!v7)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (!v8)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  Pixel_F v9 = *(void **)a4;
  if (!*(void *)(*(void *)a4 + 16))
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  uint64_t v10 = v9[6];
  if (v10 < 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  uint64_t v11 = v9[5];
  if (v11 < 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (!v10)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v11)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (v7 != v10)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (v8 != v11)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if ((result->width & 1) == 0)
  {
LABEL_33:
    __break(1u);
LABEL_34:
    __break(1u);
  }
  if ((*(unsigned char *)(a2 + 16) & 1) == 0) {
    goto LABEL_34;
  }
  uint64_t v12 = (void *)v6[4];
  uint64_t v13 = (void *)v9[4];
  if (v12)
  {
    if (!v13 || v12 != v13) {
      goto LABEL_20;
    }
    __break(1u);
  }
  if (v13)
  {
LABEL_20:
    size_t v14 = v6[7];
    src.data = v12;
    src.vImagePixelCount height = v8;
    src.vImagePixelCount width = v7;
    src.size_t rowBytes = v14;
    size_t v15 = v9[7];
    dest.data = v13;
    dest.vImagePixelCount height = v8;
    dest.vImagePixelCount width = v7;
    dest.size_t rowBytes = v15;
    return closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(&dest, &src, (uint64_t)result, a2, a3, a5);
  }
  __break(1u);
  return result;
}

unint64_t *vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:)(unint64_t *result, char *a2, uint64_t a3)
{
  uint64_t v22 = *MEMORY[0x1E4F143B8];
  uint64_t v4 = *(void **)v3;
  if (!*(void *)(*(void *)v3 + 16))
  {
    __break(1u);
    goto LABEL_27;
  }
  vImagePixelCount v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  vImagePixelCount v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (!v5)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v6)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  vImagePixelCount v7 = *(void **)a3;
  if (!*(void *)(*(void *)a3 + 16))
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  uint64_t v8 = v7[6];
  if (v8 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  uint64_t v9 = v7[5];
  if (v9 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v8)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (!v9)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (v5 != v8)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (v6 != v9)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  uint64_t v10 = (void *)v4[4];
  uint64_t v11 = (void *)v7[4];
  if (v10)
  {
    if (!v11 || v10 != v11) {
      goto LABEL_18;
    }
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_18:
  unint64_t v12 = *result;
  if ((*result & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (HIDWORD(v12))
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  unint64_t v13 = result[1];
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_40:
    __break(1u);
LABEL_41:
    __break(1u);
  }
  if (HIDWORD(v13)) {
    goto LABEL_41;
  }
  uint64_t v14 = *a2;
  char v15 = a2[1];
  size_t v16 = v4[7];
  src.data = v10;
  src.vImagePixelCount height = v6;
  src.vImagePixelCount width = v5;
  src.size_t rowBytes = v16;
  size_t v17 = v7[7];
  dest.data = v11;
  dest.vImagePixelCount height = v6;
  dest.vImagePixelCount width = v5;
  dest.size_t rowBytes = v17;
  if (v15)
  {
    Pixel_8 v18 = 0;
    vImage_Flags flags = dword_1D2137AC4[v14];
  }
  else
  {
    Pixel_8 v18 = v14;
    vImage_Flags flags = 4;
  }
  return (unint64_t *)vImageBoxConvolve_Planar8(&src, &dest, 0, 0, 0, v13 | 1, v12 | 1, v18, flags);
}

unint64_t *vImage.PixelBuffer<>.tentConvolve(kernelSize:edgeMode:destination:)(unint64_t *result, char *a2, uint64_t a3)
{
  uint64_t v22 = *MEMORY[0x1E4F143B8];
  uint64_t v4 = *(void **)v3;
  if (!*(void *)(*(void *)v3 + 16))
  {
    __break(1u);
    goto LABEL_27;
  }
  vImagePixelCount v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  vImagePixelCount v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (!v5)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v6)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  vImagePixelCount v7 = *(void **)a3;
  if (!*(void *)(*(void *)a3 + 16))
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  uint64_t v8 = v7[6];
  if (v8 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  uint64_t v9 = v7[5];
  if (v9 < 0)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v8)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (!v9)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (v5 != v8)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (v6 != v9)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  uint64_t v10 = (void *)v4[4];
  uint64_t v11 = (void *)v7[4];
  if (v10)
  {
    if (!v11 || v10 != v11) {
      goto LABEL_18;
    }
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_18:
  unint64_t v12 = *result;
  if ((*result & 0x8000000000000000) != 0)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (HIDWORD(v12))
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  unint64_t v13 = result[1];
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_40:
    __break(1u);
LABEL_41:
    __break(1u);
  }
  if (HIDWORD(v13)) {
    goto LABEL_41;
  }
  uint64_t v14 = *a2;
  char v15 = a2[1];
  size_t v16 = v4[7];
  src.data = v10;
  src.vImagePixelCount height = v6;
  src.vImagePixelCount width = v5;
  src.size_t rowBytes = v16;
  size_t v17 = v7[7];
  dest.data = v11;
  dest.vImagePixelCount height = v6;
  dest.vImagePixelCount width = v5;
  dest.size_t rowBytes = v17;
  if (v15)
  {
    Pixel_8 v18 = 0;
    vImage_Flags flags = dword_1D2137AC4[v14];
  }
  else
  {
    Pixel_8 v18 = v14;
    vImage_Flags flags = 4;
  }
  return (unint64_t *)vImageTentConvolve_Planar8(&src, &dest, 0, 0, 0, v13 | 1, v12 | 1, v18, flags);
}

uint64_t vImage.PixelBuffer<>.boxConvolved(kernelSize:edgeMode:)@<X0>(uint64_t *a1@<X0>, int *a2@<X1>, uint64_t *a3@<X8>)
{
  return vImage.PixelBuffer<>.boxConvolved(kernelSize:edgeMode:)(a1, a2, (uint64_t (*)(void *, int *, uint64_t *))vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:), a3);
}

unint64_t *vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:)(unint64_t *result, int *a2, uint64_t a3)
{
  uint64_t v24 = *MEMORY[0x1E4F143B8];
  uint64_t v4 = *(void **)v3;
  if (!*(void *)(*(void *)v3 + 16))
  {
    __break(1u);
    goto LABEL_30;
  }
  vImagePixelCount v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  vImagePixelCount v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v5)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v6)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  vImagePixelCount v7 = *(void **)a3;
  if (!*(void *)(*(void *)a3 + 16))
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  uint64_t v8 = v7[6];
  if (v8 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  uint64_t v9 = v7[5];
  if (v9 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v8)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v9)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (v5 != v8)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v6 != v9)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  uint64_t v10 = (void *)v4[4];
  uint64_t v11 = (void *)v7[4];
  if (v10)
  {
    if (!v11 || v10 != v11) {
      goto LABEL_18;
    }
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_18:
  unint64_t v12 = *result;
  if ((*result & 0x8000000000000000) != 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (HIDWORD(v12))
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  unint64_t v13 = result[1];
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_43:
    __break(1u);
LABEL_44:
    __break(1u);
  }
  if (HIDWORD(v13)) {
    goto LABEL_44;
  }
  uint64_t v14 = *a2;
  int v15 = *((unsigned __int8 *)a2 + 4);
  if (v15)
  {
    uint64_t v16 = MEMORY[0x1E4FBC860];
  }
  else
  {
    size_t v17 = v11;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    uint64_t v18 = swift_allocObject();
    uint64_t v11 = v17;
    uint64_t v16 = v18;
    *(_OWORD *)(v18 + 16) = xmmword_1D2135FC0;
    *(_WORD *)(v18 + 32) = v14;
    *(unsigned char *)(v18 + 34) = BYTE2(v14);
    *(unsigned char *)(v18 + 35) = BYTE3(v14);
  }
  size_t v19 = v4[7];
  src.data = v10;
  src.vImagePixelCount height = v6;
  src.vImagePixelCount width = v5;
  src.size_t rowBytes = v19;
  size_t v20 = v7[7];
  dest.data = v11;
  dest.vImagePixelCount height = v6;
  dest.vImagePixelCount width = v5;
  dest.size_t rowBytes = v20;
  if (v15) {
    vImage_Flags flags = dword_1D2137AC4[v14];
  }
  else {
    vImage_Flags flags = 4;
  }
  vImageBoxConvolve_ARGB8888(&src, &dest, 0, 0, 0, v13 | 1, v12 | 1, (const uint8_t *)(v16 + 32), flags);
  return (unint64_t *)swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.tentConvolved(kernelSize:edgeMode:)@<X0>(uint64_t *a1@<X0>, int *a2@<X1>, uint64_t *a3@<X8>)
{
  return vImage.PixelBuffer<>.boxConvolved(kernelSize:edgeMode:)(a1, a2, (uint64_t (*)(void *, int *, uint64_t *))vImage.PixelBuffer<>.tentConvolve(kernelSize:edgeMode:destination:), a3);
}

uint64_t vImage.PixelBuffer<>.boxConvolved(kernelSize:edgeMode:)@<X0>(uint64_t *a1@<X0>, int *a2@<X1>, uint64_t (*a3)(void *, int *, uint64_t *)@<X2>, uint64_t *a4@<X8>)
{
  uint64_t v6 = *v4;
  if (!*(void *)(*v4 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  uint64_t v7 = *(void *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v5 = *(void *)(v6 + 40);
  if (v5 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (!v7)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (v5)
  {
    uint64_t v9 = *a1;
    uint64_t v8 = a1[1];
    int v10 = *a2;
    char v21 = *((unsigned char *)a2 + 4);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    uint64_t v11 = swift_allocObject();
    *(_OWORD *)(v11 + 16) = xmmword_1D2135280;
    uint64_t v12 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v7, v5);
    uint64_t v14 = v13;
    uint64_t v16 = v15;
    uint64_t v18 = v17;
    type metadata accessor for vImage.BufferReference();
    size_t v19 = (void *)swift_allocObject();
    v19[2] = v12;
    v19[3] = v14;
    v19[4] = v16;
    void v19[5] = v18;
    *(void *)(v11 + 32) = v12;
    *(void *)(v11 + 40) = v14;
    *(void *)(v11 + 48) = v16;
    *(void *)(v11 + 56) = v18;
    *(void *)(v11 + 64) = v19;
    v27[1] = v8;
    v27[2] = v6;
    v27[0] = v9;
    int v25 = v10;
    char v26 = v21;
    uint64_t v24 = v11;
    uint64_t result = a3(v27, &v25, &v24);
    *a4 = v11;
    return result;
  }
LABEL_11:
  __break(1u);

  uint64_t result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

unint64_t *vImage.PixelBuffer<>.tentConvolve(kernelSize:edgeMode:destination:)(unint64_t *result, int *a2, uint64_t a3)
{
  uint64_t v24 = *MEMORY[0x1E4F143B8];
  uint64_t v4 = *(void **)v3;
  if (!*(void *)(*(void *)v3 + 16))
  {
    __break(1u);
    goto LABEL_30;
  }
  vImagePixelCount v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  vImagePixelCount v6 = v4[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v5)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v6)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  uint64_t v7 = *(void **)a3;
  if (!*(void *)(*(void *)a3 + 16))
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  uint64_t v8 = v7[6];
  if (v8 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  uint64_t v9 = v7[5];
  if (v9 < 0)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v8)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (!v9)
  {
LABEL_38:
    __break(1u);
    goto LABEL_39;
  }
  if (v5 != v8)
  {
LABEL_39:
    __break(1u);
    goto LABEL_40;
  }
  if (v6 != v9)
  {
LABEL_40:
    __break(1u);
    goto LABEL_41;
  }
  int v10 = (void *)v4[4];
  uint64_t v11 = (void *)v7[4];
  if (v10)
  {
    if (!v11 || v10 != v11) {
      goto LABEL_18;
    }
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_18:
  unint64_t v12 = *result;
  if ((*result & 0x8000000000000000) != 0)
  {
LABEL_41:
    __break(1u);
    goto LABEL_42;
  }
  if (HIDWORD(v12))
  {
LABEL_42:
    __break(1u);
    goto LABEL_43;
  }
  unint64_t v13 = result[1];
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_43:
    __break(1u);
LABEL_44:
    __break(1u);
  }
  if (HIDWORD(v13)) {
    goto LABEL_44;
  }
  uint64_t v14 = *a2;
  int v15 = *((unsigned __int8 *)a2 + 4);
  if (v15)
  {
    uint64_t v16 = MEMORY[0x1E4FBC860];
  }
  else
  {
    uint64_t v17 = v11;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    uint64_t v18 = swift_allocObject();
    uint64_t v11 = v17;
    uint64_t v16 = v18;
    *(_OWORD *)(v18 + 16) = xmmword_1D2135FC0;
    *(_WORD *)(v18 + 32) = v14;
    *(unsigned char *)(v18 + 34) = BYTE2(v14);
    *(unsigned char *)(v18 + 35) = BYTE3(v14);
  }
  size_t v19 = v4[7];
  src.data = v10;
  src.vImagePixelCount height = v6;
  src.vImagePixelCount width = v5;
  src.size_t rowBytes = v19;
  size_t v20 = v7[7];
  dest.data = v11;
  dest.vImagePixelCount height = v6;
  dest.vImagePixelCount width = v5;
  dest.size_t rowBytes = v20;
  if (v15) {
    vImage_Flags flags = dword_1D2137AC4[v14];
  }
  else {
    vImage_Flags flags = 4;
  }
  vImageTentConvolve_ARGB8888(&src, &dest, 0, 0, 0, v13 | 1, v12 | 1, (const uint8_t *)(v16 + 32), flags);
  return (unint64_t *)swift_bridgeObjectRelease();
}

uint64_t vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:)(unint64_t *a1, unsigned __int8 *a2, uint64_t *a3, uint64_t a4, uint64_t a5)
{
  return vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:)(a1, a2, a3, a4, a5, (uint64_t (*)(_OWORD *, uint64_t *, unint64_t, void, void, void, unsigned __int8 *, uint64_t, uint64_t))closure #1 in vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:));
}

vImage_Error closure #1 in vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:)(const vImage_Buffer *a1, uint64_t a2, unint64_t a3, uint32_t a4, uint32_t a5, Pixel_8 a6, char *a7)
{
  uint64_t v20 = *MEMORY[0x1E4F143B8];
  type metadata accessor for vImage.PixelBuffer();
  uint64_t v13 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_8:
    __break(1u);
  }
  if (*(void *)(v13 + 16) <= a3) {
    goto LABEL_8;
  }
  uint64_t v14 = v13 + 32 * a3;
  long long v17 = *(_OWORD *)(v14 + 48);
  long long v18 = *(_OWORD *)(v14 + 32);
  swift_bridgeObjectRelease();
  *(_OWORD *)&dest.data = v18;
  *(_OWORD *)&dest.vImagePixelCount width = v17;
  if (a7[1] == 1) {
    vImage_Flags flags = dword_1D2137AC4[*a7];
  }
  else {
    vImage_Flags flags = 4;
  }
  return vImageBoxConvolve_Planar8(a1, &dest, 0, 0, 0, a4, a5, a6, flags);
}

uint64_t vImage.PixelBuffer<>.tentConvolve(kernelSize:edgeMode:destination:)(unint64_t *a1, unsigned __int8 *a2, uint64_t *a3, uint64_t a4, uint64_t a5)
{
  return vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:)(a1, a2, a3, a4, a5, (uint64_t (*)(_OWORD *, uint64_t *, unint64_t, void, void, void, unsigned __int8 *, uint64_t, uint64_t))closure #1 in vImage.PixelBuffer<>.tentConvolve(kernelSize:edgeMode:destination:));
}

uint64_t vImage.PixelBuffer<>.boxConvolve(kernelSize:edgeMode:destination:)(unint64_t *a1, unsigned __int8 *a2, uint64_t *a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(_OWORD *, uint64_t *, unint64_t, void, void, void, unsigned __int8 *, uint64_t, uint64_t))
{
  uint64_t v36 = *MEMORY[0x1E4F143B8];
  uint64_t v10 = *v6;
  uint64_t v33 = *a3;
  uint64_t v34 = v10;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(v35);
  vImage.PixelBuffer.size.getter(&v32);
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  if (v35[0] != v32)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  unint64_t v11 = *a1;
  if ((*a1 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (HIDWORD(v11))
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  unint64_t v12 = a1[1];
  if ((v12 & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  if (HIDWORD(v12))
  {
LABEL_31:
    __break(1u);
LABEL_32:
    __break(1u);
LABEL_33:
    __break(1u);
  }
  unsigned int v13 = *a2;
  if (a2[1]) {
    unsigned int v13 = 0;
  }
  unsigned int v26 = v13;
  uint64_t v25 = *(void *)(a4 + 16);
  uint64_t result = (*(uint64_t (**)(void))(a5 + 32))();
  if (result < 0) {
    goto LABEL_32;
  }
  uint64_t v15 = result;
  if (result)
  {
    unint64_t v16 = 0;
    unsigned int v23 = v12 | 1;
    unsigned int v24 = v11 | 1;
    uint64_t v17 = 32;
    while (v15 != v16)
    {
      *(void *)&long long v32 = v10;
      swift_bridgeObjectRetain();
      swift_bridgeObjectRetain();
      uint64_t v18 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v16 >= *(void *)(v18 + 16)) {
        goto LABEL_23;
      }
      uint64_t v19 = *(void *)(v18 + v17);
      swift_bridgeObjectRelease();
      uint64_t v20 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v16 >= *(void *)(v20 + 16)) {
        goto LABEL_24;
      }
      uint64_t v21 = *(void *)(v20 + v17);
      swift_bridgeObjectRelease();
      swift_bridgeObjectRelease();
      swift_bridgeObjectRelease();
      if (v19)
      {
        if (v21 && v19 == v21) {
          goto LABEL_26;
        }
      }
      else if (!v21)
      {
        goto LABEL_33;
      }
      uint64_t v22 = vImage.PixelBuffer<>.vImageBuffers.getter();
      if (v16 >= *(void *)(v22 + 16)) {
        goto LABEL_25;
      }
      long long v30 = *(_OWORD *)(v22 + v17 + 16);
      long long v31 = *(_OWORD *)(v22 + v17);
      swift_bridgeObjectRelease();
      v35[0] = v31;
      v35[1] = v30;
      uint64_t result = a6(v35, a3, v16, v23, v24, v26, a2, v25, a5);
      v17 += 32;
      if (v15 == ++v16) {
        return result;
      }
    }
    __break(1u);
LABEL_23:
    __break(1u);
LABEL_24:
    __break(1u);
LABEL_25:
    __break(1u);
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  return result;
}

vImage_Error closure #1 in vImage.PixelBuffer<>.tentConvolve(kernelSize:edgeMode:destination:)(const vImage_Buffer *a1, uint64_t a2, unint64_t a3, uint32_t a4, uint32_t a5, Pixel_8 a6, char *a7)
{
  uint64_t v20 = *MEMORY[0x1E4F143B8];
  type metadata accessor for vImage.PixelBuffer();
  uint64_t v13 = vImage.PixelBuffer<>.vImageBuffers.getter();
  if ((a3 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_8:
    __break(1u);
  }
  if (*(void *)(v13 + 16) <= a3) {
    goto LABEL_8;
  }
  uint64_t v14 = v13 + 32 * a3;
  long long v17 = *(_OWORD *)(v14 + 48);
  long long v18 = *(_OWORD *)(v14 + 32);
  swift_bridgeObjectRelease();
  *(_OWORD *)&dest.data = v18;
  *(_OWORD *)&dest.vImagePixelCount width = v17;
  if (a7[1] == 1) {
    vImage_Flags flags = dword_1D2137AC4[*a7];
  }
  else {
    vImage_Flags flags = 4;
  }
  return vImageTentConvolve_Planar8(a1, &dest, 0, 0, 0, a4, a5, a6, flags);
}

uint64_t vImage.ConvolutionKernel2D.width.getter()
{
  return *(void *)v0;
}

uint64_t vImage.ConvolutionKernel2D.height.getter()
{
  return *(void *)(v0 + 8);
}

uint64_t vImage.ConvolutionKernel2D.values.getter()
{
  return swift_bridgeObjectRetain();
}

uint64_t vImage.ConvolutionKernel2D.init(values:width:height:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, unint64_t *a5@<X8>)
{
  unint64_t v5 = a3 & ~(a3 >> 63);
  if ((v5 & 1) == 0 || a3 < 1 || a2 < 1 || (unint64_t v6 = a2 & ~(a2 >> 63), (v6 & 1) == 0))
  {
    __break(1u);
    goto LABEL_10;
  }
  uint64_t v7 = result;
  uint64_t result = MEMORY[0x1D25FF9B0](result, a4);
  if (!is_mul_ok(v6, v5))
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if ((result & 0x8000000000000000) == 0 && result == v6 * v5)
  {
    *a5 = v6;
    a5[1] = v5;
    a5[2] = v7;
    return result;
  }
LABEL_11:
  __break(1u);
  return result;
}

double vImage.ConvolutionKernel2D.init(values:size:)@<D0>(uint64_t a1@<X0>, uint64_t *a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X8>)
{
  vImage.ConvolutionKernel2D.init(values:width:height:)(a1, *a2, a2[1], a3, (unint64_t *)&v7);
  uint64_t v5 = v8;
  double result = *(double *)&v7;
  *(_OWORD *)a4 = v7;
  *(void *)(a4 + 16) = v5;
  return result;
}

uint64_t vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(uint64_t result, unint64_t a2, uint64_t a3, uint64_t a4)
{
  uint64_t v28 = *MEMORY[0x1E4F143B8];
  uint64_t v5 = (void *)*v4;
  if (!v5[2])
  {
    __break(1u);
    goto LABEL_26;
  }
  unint64_t v6 = *(void **)a4;
  if (!*(void *)(*(void *)a4 + 16))
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  uint64_t v9 = (void *)result;
  uint64_t v10 = (void *)v5[4];
  unint64_t v11 = (void *)v6[4];
  if (v10)
  {
    if (v11) {
      BOOL v12 = v10 == v11;
    }
    else {
      BOOL v12 = 0;
    }
    if (!v12) {
      goto LABEL_11;
    }
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_11:
  vImagePixelCount v13 = v5[6];
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  vImagePixelCount v14 = v5[5];
  if ((v14 & 0x8000000000000000) != 0)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  if (!v13)
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  if (!v14)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  uint64_t v15 = v6[6];
  if (v15 < 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  uint64_t v16 = v6[5];
  if (v16 < 0)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v15)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (!v16)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (v13 != v15)
  {
LABEL_35:
    __break(1u);
LABEL_36:
    __break(1u);
  }
  if (v14 != v16) {
    goto LABEL_36;
  }
  if (*(unsigned char *)(a3 + 4))
  {
    uint64_t v17 = MEMORY[0x1E4FBC860];
  }
  else
  {
    int v18 = *(_DWORD *)a3;
    int v24 = HIWORD(*(_DWORD *)a3);
    int v25 = *(_DWORD *)a3 >> 8;
    int v23 = HIBYTE(*(_DWORD *)a3);
    uint64_t v19 = (void *)result;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    uint64_t v20 = swift_allocObject();
    uint64_t v9 = v19;
    uint64_t v17 = v20;
    *(_OWORD *)(v20 + 16) = xmmword_1D2135FC0;
    *(unsigned char *)(v20 + 32) = v18;
    *(unsigned char *)(v20 + 33) = v25;
    *(unsigned char *)(v20 + 34) = v24;
    *(unsigned char *)(v20 + 35) = v23;
  }
  size_t v21 = v5[7];
  src.data = v10;
  src.vImagePixelCount height = v14;
  src.vImagePixelCount width = v13;
  src.size_t rowBytes = v21;
  size_t v22 = v6[7];
  dest.data = v11;
  dest.vImagePixelCount height = v14;
  dest.vImagePixelCount width = v13;
  dest.size_t rowBytes = v22;
  closure #1 in closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(&dest, &src, v9, a2 | ((HIDWORD(a2) & 1) << 32), v17, (int *)a3);
  return swift_bridgeObjectRelease();
}

const vImage_Buffer *closure #1 in closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(const vImage_Buffer *dest, vImage_Buffer *src, void *a3, uint64_t a4, uint64_t a5, int *a6)
{
  unint64_t v6 = a3[1];
  if (HIDWORD(v6))
  {
    __break(1u);
    goto LABEL_10;
  }
  if (HIDWORD(*a3))
  {
LABEL_10:
    __break(1u);
    return dest;
  }
  if (*((unsigned char *)a6 + 4) == 1) {
    vImage_Flags flags = dword_1D2137AC4[*a6];
  }
  else {
    vImage_Flags flags = 4;
  }
  float v8 = *(float *)&a4;
  if ((a4 & 0x100000000) != 0) {
    float v8 = 0.0;
  }
  return (const vImage_Buffer *)vImageConvolveFloatKernel_ARGB8888(src, dest, 0, 0, 0, (const float *)(a3[2] + 32), v6, *a3, v8, (const uint8_t *)(a5 + 32), flags);
}

uint64_t vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, float a5)
{
  uint64_t v30 = *MEMORY[0x1E4F143B8];
  unint64_t v6 = *(void **)v5;
  if (!*(void *)(*(void *)v5 + 16))
  {
    __break(1u);
    goto LABEL_25;
  }
  vImagePixelCount v7 = v6[6];
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  vImagePixelCount v8 = v6[5];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_26:
    __break(1u);
    goto LABEL_27;
  }
  if (!v7)
  {
LABEL_27:
    __break(1u);
    goto LABEL_28;
  }
  if (!v8)
  {
LABEL_28:
    __break(1u);
    goto LABEL_29;
  }
  uint64_t v9 = *(void **)a4;
  if (!*(void *)(*(void *)a4 + 16))
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  uint64_t v10 = v9[6];
  if (v10 < 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  uint64_t v11 = v9[5];
  if (v11 < 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v10)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v11)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  if (v7 != v10)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  if (v8 != v11)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  uint64_t v13 = result;
  if ((*(unsigned char *)(result + 16) & 1) == 0)
  {
LABEL_36:
    __break(1u);
LABEL_37:
    __break(1u);
  }
  uint64_t v14 = a2;
  if ((*(unsigned char *)(a2 + 16) & 1) == 0) {
    goto LABEL_37;
  }
  uint64_t v16 = (void *)v6[4];
  uint64_t v17 = (void *)v9[4];
  if (v16)
  {
    if (!v17 || v16 != v17) {
      goto LABEL_20;
    }
    __break(1u);
  }
  if (!v17)
  {
    __break(1u);
    return result;
  }
LABEL_20:
  if (*(unsigned char *)(a3 + 4))
  {
    uint64_t v18 = MEMORY[0x1E4FBC860];
  }
  else
  {
    int v19 = *(_DWORD *)a3;
    int v26 = HIWORD(*(_DWORD *)a3);
    int v27 = *(_DWORD *)a3 >> 8;
    int v25 = HIBYTE(*(_DWORD *)a3);
    uint64_t v21 = result;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    uint64_t v22 = swift_allocObject();
    uint64_t v13 = v21;
    uint64_t v14 = a2;
    uint64_t v18 = v22;
    *(_OWORD *)(v22 + 16) = xmmword_1D2135FC0;
    *(unsigned char *)(v22 + 32) = v19;
    *(unsigned char *)(v22 + 33) = v27;
    *(unsigned char *)(v22 + 34) = v26;
    *(unsigned char *)(v22 + 35) = v25;
  }
  size_t v23 = v6[7];
  src.data = v16;
  src.vImagePixelCount height = v8;
  src.vImagePixelCount width = v7;
  src.size_t rowBytes = v23;
  size_t v24 = v9[7];
  dest.data = v17;
  dest.vImagePixelCount height = v8;
  dest.vImagePixelCount width = v7;
  dest.size_t rowBytes = v24;
  closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(&dest, &src, v13, v14, v18, (int *)a3, a5);
  return swift_bridgeObjectRelease();
}

const vImage_Buffer *closure #1 in closure #1 in vImage.PixelBuffer<>.separableConvolve(horizontalKernel:verticalKernel:bias:edgeMode:destination:)(const vImage_Buffer *dest, vImage_Buffer *src, uint64_t a3, uint64_t a4, uint64_t a5, int *a6, float a7)
{
  unint64_t v7 = *(void *)(a3 + 16);
  if (HIDWORD(v7))
  {
    __break(1u);
    goto LABEL_8;
  }
  kernelY_vImagePixelCount width = *(void *)(a4 + 16);
  if (HIDWORD(kernelY_width))
  {
LABEL_8:
    __break(1u);
    return dest;
  }
  if (*((unsigned char *)a6 + 4) == 1) {
    vImage_Flags flags = dword_1D2137AC4[*a6];
  }
  else {
    vImage_Flags flags = 4;
  }
  return (const vImage_Buffer *)vImageSepConvolve_ARGB8888(src, dest, 0, 0, 0, (const float *)(a3 + 32), v7, (const float *)(a4 + 32), kernelY_width, a7, (const uint8_t *)(a5 + 32), flags);
}

uint64_t vImage.PixelBuffer<>.convolve(with:divisor:bias:edgeMode:destination:)(uint64_t *a1, uint64_t a2, unint64_t a3, int *a4, void **a5)
{
  unint64_t v6 = a5;
  unint64_t v8 = a3;
  uint64_t v9 = *a1;
  uint64_t v10 = a1[1];
  uint64_t v11 = a1[2];
  if ((a2 & 0x100000000) == 0)
  {
    int v12 = a2;
    uint64_t v13 = *a1;
LABEL_15:
    uint64_t v26 = *a4;
    if (a4[1])
    {
      int v27 = dword_1D2137AC4[v26];
      uint64_t v28 = MEMORY[0x1E4FBC860];
    }
    else
    {
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
      uint64_t v28 = swift_allocObject();
      *(_OWORD *)(v28 + 16) = xmmword_1D2135FC0;
      *(_WORD *)(v28 + 32) = v26;
      *(unsigned char *)(v28 + 34) = BYTE2(v26);
      *(unsigned char *)(v28 + 35) = BYTE3(v26);
      int v27 = 4;
    }
    specialized vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(v13, v10, v11, v12, v8 | ((HIDWORD(v8) & 1) << 32), v28 + 32, v27, *v6, (void (*)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, int *, uint64_t *, int))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int), 0, (void (*)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, int *, int *, uint64_t *, int))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned Int32, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int), 0, *v5);
    return swift_bridgeObjectRelease();
  }
  uint64_t v32 = a1[1];
  uint64_t v14 = *(void *)(v11 + 16);
  if (v14)
  {
    uint64_t v31 = *a1;
    uint64_t v33 = MEMORY[0x1E4FBC860];
    swift_bridgeObjectRetain();
    uint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v14, 0);
    uint64_t v16 = v33;
    uint64_t v17 = (__int16 *)(v11 + 32);
    unint64_t v18 = *(void *)(v33 + 16);
    do
    {
      int v20 = *v17++;
      int v19 = v20;
      unint64_t v21 = *(void *)(v33 + 24);
      unint64_t v22 = v18 + 1;
      if (v18 >= v21 >> 1) {
        uint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v21 > 1, v18 + 1, 1);
      }
      *(void *)(v33 + 16) = v22;
      *(_DWORD *)(v33 + 4 * v18++ + 32) = v19;
      --v14;
    }
    while (v14);
    unint64_t v8 = a3;
    uint64_t v9 = v31;
    unint64_t v6 = a5;
  }
  else
  {
    uint64_t v16 = MEMORY[0x1E4FBC860];
    unint64_t v22 = *(void *)(MEMORY[0x1E4FBC860] + 16);
    uint64_t result = swift_bridgeObjectRetain();
    if (!v22)
    {
      swift_bridgeObjectRelease();
      int v12 = 0;
LABEL_14:
      uint64_t v13 = v9;
      swift_bridgeObjectRelease();
      uint64_t v10 = v32;
      goto LABEL_15;
    }
  }
  uint64_t v23 = 0;
  int v12 = 0;
  while (1)
  {
    int v24 = *(_DWORD *)(v16 + 4 * v23 + 32);
    BOOL v25 = __OFADD__(v12, v24);
    v12 += v24;
    if (v25) {
      break;
    }
    if (v22 == ++v23)
    {
      swift_bridgeObjectRelease();
      goto LABEL_14;
    }
  }
  __break(1u);
  return result;
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const int16_t *a6, uint32_t a7, uint32_t a8, int32_t *divisor, uint8_t *backgroundColor, vImage_Flags flags)
{
  return vImageConvolve_ARGB8888(a1, a2, a3, a4, a5, a6, a7, a8, *divisor, *(const uint8_t **)backgroundColor, flags);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned Int32, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const int16_t *a6, uint32_t a7, uint32_t a8, int32_t *divisor, uint8_t *backgroundColor, const uint8_t *__attribute__((__org_typedef(Pixel_8888))) *flags, vImage_Flags a12)
{
  return vImageConvolveWithBias_ARGB8888(a1, a2, a3, a4, a5, a6, a7, a8, *divisor, *(_DWORD *)backgroundColor, *flags, a12);
}

uint64_t specialized vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t result, uint64_t a2, uint64_t a3, int a4, unint64_t a5, uint64_t a6, int a7, void *a8, void (*a9)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, int *, uint64_t *, int), uint64_t a10, void (*a11)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, int *, int *, uint64_t *, int), uint64_t a12, void *a13)
{
  uint16_t v20[4] = *MEMORY[0x1E4F143B8];
  if (!a13[2])
  {
    __break(1u);
    goto LABEL_20;
  }
  if (!a8[2])
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v13 = a13[4];
  uint64_t v14 = a8[4];
  if (v13)
  {
    if (!v14 || v13 != v14)
    {
LABEL_8:
      uint64_t v15 = a13[6];
      if ((v15 & 0x8000000000000000) == 0)
      {
        uint64_t v16 = a13[5];
        if ((v16 & 0x8000000000000000) == 0)
        {
          if (v15)
          {
            if (v16)
            {
              uint64_t v17 = a8[6];
              if ((v17 & 0x8000000000000000) == 0)
              {
                uint64_t v18 = a8[5];
                if ((v18 & 0x8000000000000000) == 0)
                {
                  if (v17)
                  {
                    if (v18)
                    {
                      if (v15 == v17)
                      {
                        if (v16 == v18)
                        {
                          uint64_t v19 = a13[7];
                          v20[0] = v13;
                          v20[1] = v16;
                          void v20[2] = v15;
                          v20[3] = v19;
                          return specialized closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)((uint64_t)v20, (uint64_t)a8, a5 | ((HIDWORD(a5) & 1) << 32), a11, a12, result, a2, a3, a2, result, a4, a6, a7, a9);
                        }
LABEL_30:
                        __break(1u);
                      }
LABEL_29:
                      __break(1u);
                      goto LABEL_30;
                    }
LABEL_28:
                    __break(1u);
                    goto LABEL_29;
                  }
LABEL_27:
                  __break(1u);
                  goto LABEL_28;
                }
LABEL_26:
                __break(1u);
                goto LABEL_27;
              }
LABEL_25:
              __break(1u);
              goto LABEL_26;
            }
LABEL_24:
            __break(1u);
            goto LABEL_25;
          }
LABEL_23:
          __break(1u);
          goto LABEL_24;
        }
LABEL_22:
        __break(1u);
        goto LABEL_23;
      }
LABEL_21:
      __break(1u);
      goto LABEL_22;
    }
    __break(1u);
  }
  if (v14) {
    goto LABEL_8;
  }
  __break(1u);
  return result;
}

uint64_t specialized vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t result, uint64_t a2, uint64_t a3, int a4, unint64_t a5, char a6, int a7, void *a8, void (*a9)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, int *, char *, int), uint64_t a10, void (*a11)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, int *, int *, char *, int), uint64_t a12, void *a13)
{
  uint16_t v20[4] = *MEMORY[0x1E4F143B8];
  if (!a13[2])
  {
    __break(1u);
    goto LABEL_20;
  }
  if (!a8[2])
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v13 = a13[4];
  uint64_t v14 = a8[4];
  if (v13)
  {
    if (!v14 || v13 != v14)
    {
LABEL_8:
      uint64_t v15 = a13[6];
      if ((v15 & 0x8000000000000000) == 0)
      {
        uint64_t v16 = a13[5];
        if ((v16 & 0x8000000000000000) == 0)
        {
          if (v15)
          {
            if (v16)
            {
              uint64_t v17 = a8[6];
              if ((v17 & 0x8000000000000000) == 0)
              {
                uint64_t v18 = a8[5];
                if ((v18 & 0x8000000000000000) == 0)
                {
                  if (v17)
                  {
                    if (v18)
                    {
                      if (v15 == v17)
                      {
                        if (v16 == v18)
                        {
                          uint64_t v19 = a13[7];
                          v20[0] = v13;
                          v20[1] = v16;
                          void v20[2] = v15;
                          v20[3] = v19;
                          return specialized closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)((uint64_t)v20, (uint64_t)a8, a5 | ((HIDWORD(a5) & 1) << 32), a11, a12, result, a2, a3, a2, result, a4, a6, a7, a9);
                        }
LABEL_30:
                        __break(1u);
                      }
LABEL_29:
                      __break(1u);
                      goto LABEL_30;
                    }
LABEL_28:
                    __break(1u);
                    goto LABEL_29;
                  }
LABEL_27:
                  __break(1u);
                  goto LABEL_28;
                }
LABEL_26:
                __break(1u);
                goto LABEL_27;
              }
LABEL_25:
              __break(1u);
              goto LABEL_26;
            }
LABEL_24:
            __break(1u);
            goto LABEL_25;
          }
LABEL_23:
          __break(1u);
          goto LABEL_24;
        }
LABEL_22:
        __break(1u);
        goto LABEL_23;
      }
LABEL_21:
      __break(1u);
      goto LABEL_22;
    }
    __break(1u);
  }
  if (v14) {
    goto LABEL_8;
  }
  __break(1u);
  return result;
}

uint64_t specialized vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t result, uint64_t a2, uint64_t a3, unint64_t a4, uint64_t a5, int a6, void *a7, void (*a8)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, float *, uint64_t *, int), float a9, uint64_t a10, void (*a11)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, float *, int *, uint64_t *, int), uint64_t a12, void *a13)
{
  uint16_t v20[4] = *MEMORY[0x1E4F143B8];
  if (!a13[2])
  {
    __break(1u);
    goto LABEL_20;
  }
  if (!a7[2])
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v13 = a13[4];
  uint64_t v14 = a7[4];
  if (v13)
  {
    if (!v14 || v13 != v14)
    {
LABEL_8:
      uint64_t v15 = a13[6];
      if ((v15 & 0x8000000000000000) == 0)
      {
        uint64_t v16 = a13[5];
        if ((v16 & 0x8000000000000000) == 0)
        {
          if (v15)
          {
            if (v16)
            {
              uint64_t v17 = a7[6];
              if ((v17 & 0x8000000000000000) == 0)
              {
                uint64_t v18 = a7[5];
                if ((v18 & 0x8000000000000000) == 0)
                {
                  if (v17)
                  {
                    if (v18)
                    {
                      if (v15 == v17)
                      {
                        if (v16 == v18)
                        {
                          uint64_t v19 = a13[7];
                          v20[0] = v13;
                          v20[1] = v16;
                          void v20[2] = v15;
                          v20[3] = v19;
                          return specialized closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)((uint64_t)v20, (uint64_t)a7, a4 | ((HIDWORD(a4) & 1) << 32), a11, a9, a12, result, a2, a3, a2, result, a5, a6, a8);
                        }
LABEL_30:
                        __break(1u);
                      }
LABEL_29:
                      __break(1u);
                      goto LABEL_30;
                    }
LABEL_28:
                    __break(1u);
                    goto LABEL_29;
                  }
LABEL_27:
                  __break(1u);
                  goto LABEL_28;
                }
LABEL_26:
                __break(1u);
                goto LABEL_27;
              }
LABEL_25:
              __break(1u);
              goto LABEL_26;
            }
LABEL_24:
            __break(1u);
            goto LABEL_25;
          }
LABEL_23:
          __break(1u);
          goto LABEL_24;
        }
LABEL_22:
        __break(1u);
        goto LABEL_23;
      }
LABEL_21:
      __break(1u);
      goto LABEL_22;
    }
    __break(1u);
  }
  if (v14) {
    goto LABEL_8;
  }
  __break(1u);
  return result;
}

uint64_t specialized vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t result, uint64_t a2, uint64_t a3, unint64_t a4, unint64_t a5, int a6, void *a7, void (*a8)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, float *, int *, int), float a9, uint64_t a10, void (*a11)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, float *, int *, int *, int), uint64_t a12, void *a13)
{
  void v21[4] = *MEMORY[0x1E4F143B8];
  if (!a13[2])
  {
    __break(1u);
    goto LABEL_20;
  }
  if (!a7[2])
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  uint64_t v13 = a13[4];
  uint64_t v14 = a7[4];
  if (v13)
  {
    if (!v14 || v13 != v14)
    {
LABEL_8:
      uint64_t v15 = a13[6];
      if ((v15 & 0x8000000000000000) == 0)
      {
        uint64_t v16 = a13[5];
        if ((v16 & 0x8000000000000000) == 0)
        {
          if (v15)
          {
            if (v16)
            {
              uint64_t v17 = a7[6];
              if ((v17 & 0x8000000000000000) == 0)
              {
                uint64_t v18 = a7[5];
                if ((v18 & 0x8000000000000000) == 0)
                {
                  if (v17)
                  {
                    if (v18)
                    {
                      if (v15 == v17)
                      {
                        if (v16 == v18)
                        {
                          uint64_t v19 = a13[7];
                          v21[0] = v13;
                          v21[1] = v16;
                          v21[2] = v15;
                          v21[3] = v19;
                          uint64_t v20 = a5 | ((HIDWORD(a5) & 1) << 32);
                          return specialized closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)((uint64_t)v21, (uint64_t)a7, a4 | ((HIDWORD(a4) & 1) << 32), a11, a9, a12, result, a2, a3, a2, result, v20, SBYTE4(v20), a6, a8);
                        }
LABEL_30:
                        __break(1u);
                      }
LABEL_29:
                      __break(1u);
                      goto LABEL_30;
                    }
LABEL_28:
                    __break(1u);
                    goto LABEL_29;
                  }
LABEL_27:
                  __break(1u);
                  goto LABEL_28;
                }
LABEL_26:
                __break(1u);
                goto LABEL_27;
              }
LABEL_25:
              __break(1u);
              goto LABEL_26;
            }
LABEL_24:
            __break(1u);
            goto LABEL_25;
          }
LABEL_23:
          __break(1u);
          goto LABEL_24;
        }
LABEL_22:
        __break(1u);
        goto LABEL_23;
      }
LABEL_21:
      __break(1u);
      goto LABEL_22;
    }
    __break(1u);
  }
  if (v14) {
    goto LABEL_8;
  }
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t *a1, uint64_t a2, uint64_t a3, uint64_t a4, int a5, uint64_t *a6, char *a7, uint64_t a8, void (*a9)(uint64_t, uint64_t, void, void, void, unint64_t, uint64_t, uint64_t, uint64_t, char *, uint64_t, int, uint64_t), uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14)
{
  uint64_t v39 = *MEMORY[0x1E4F143B8];
  uint64_t v28 = a1[1];
  uint64_t v29 = *a1;
  uint64_t v30 = a1[2];
  uint64_t v18 = *a6;
  uint64_t v19 = *v14;
  uint64_t v20 = vImage.PixelBuffer<>.vImageBuffer.getter();
  uint64_t v35 = v18;
  type metadata accessor for vImage.PixelBuffer();
  uint64_t result = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (v20)
  {
    if (result) {
      BOOL v22 = v20 == result;
    }
    else {
      BOOL v22 = 0;
    }
    if (!v22) {
      goto LABEL_9;
    }
    __break(1u);
  }
  if (!result)
  {
    __break(1u);
    return result;
  }
LABEL_9:
  v34[0] = v19;
  swift_bridgeObjectRetain();
  vImage.PixelBuffer.size.getter(&v35);
  uint64_t v24 = v35;
  uint64_t v23 = v36;
  vImage.PixelBuffer.size.getter(v34);
  swift_bridgeObjectRelease();
  if (v24 != v34[0] || v23 != v34[1]) {
    __break(1u);
  }
  uint64_t v35 = v19;
  uint64_t v35 = vImage.PixelBuffer<>.vImageBuffer.getter();
  uint64_t v36 = v25;
  uint64_t v37 = v26;
  uint64_t v38 = v27;
  return closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)((uint64_t)&v35, v18, a3, a9, a10, v29, v28, v30, v28, v29, a2, a4, a5, a7, a8, *(void *)(a11 + 16), a12, a13, a14);
}

uint64_t vImage.PixelBuffer<>.convolve(with:divisor:bias:edgeMode:destination:)(uint64_t *a1, uint64_t a2, unint64_t a3, char *a4, void **a5)
{
  unint64_t v6 = a5;
  uint64_t v9 = *a1;
  uint64_t v10 = a1[1];
  uint64_t v11 = a1[2];
  if ((a2 & 0x100000000) == 0)
  {
    int v12 = a2;
LABEL_15:
    if (a4[1] == 1)
    {
      char v25 = 0;
      int v26 = dword_1D2137AC4[*a4];
    }
    else
    {
      char v25 = *a4;
      int v26 = 4;
    }
    return specialized vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(v9, v10, v11, v12, a3 | ((HIDWORD(a3) & 1) << 32), v25, v26, *v6, (void (*)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, int *, char *, int))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned UInt8, @unowned UInt32) -> (@unowned Int), 0, (void (*)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, int *, int *, char *, int))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned Int32, @unowned UInt8, @unowned UInt32) -> (@unowned Int), 0, *v5);
  }
  uint64_t v13 = *(void *)(v11 + 16);
  if (v13)
  {
    uint64_t v28 = a1[1];
    uint64_t v29 = *a1;
    uint64_t v30 = MEMORY[0x1E4FBC860];
    swift_bridgeObjectRetain();
    uint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v13, 0);
    uint64_t v15 = v30;
    uint64_t v16 = (__int16 *)(v11 + 32);
    unint64_t v17 = *(void *)(v30 + 16);
    do
    {
      int v19 = *v16++;
      int v18 = v19;
      unint64_t v20 = *(void *)(v30 + 24);
      unint64_t v21 = v17 + 1;
      if (v17 >= v20 >> 1) {
        uint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v20 > 1, v17 + 1, 1);
      }
      *(void *)(v30 + 16) = v21;
      *(_DWORD *)(v30 + 4 * v17++ + 32) = v18;
      --v13;
    }
    while (v13);
    uint64_t v10 = v28;
    uint64_t v9 = v29;
    unint64_t v6 = a5;
  }
  else
  {
    uint64_t v15 = MEMORY[0x1E4FBC860];
    unint64_t v21 = *(void *)(MEMORY[0x1E4FBC860] + 16);
    uint64_t result = swift_bridgeObjectRetain();
    if (!v21)
    {
      swift_bridgeObjectRelease();
      int v12 = 0;
LABEL_14:
      swift_bridgeObjectRelease();
      goto LABEL_15;
    }
  }
  uint64_t v22 = 0;
  int v12 = 0;
  while (1)
  {
    int v23 = *(_DWORD *)(v15 + 4 * v22 + 32);
    BOOL v24 = __OFADD__(v12, v23);
    v12 += v23;
    if (v24) {
      break;
    }
    if (v21 == ++v22)
    {
      swift_bridgeObjectRelease();
      goto LABEL_14;
    }
  }
  __break(1u);
  return result;
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned UInt8, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const int16_t *a6, uint32_t a7, uint32_t a8, int32_t *divisor, Pixel_8 *flags, vImage_Flags a11)
{
  return vImageConvolve_Planar8(a1, a2, a3, a4, a5, a6, a7, a8, *divisor, *flags, a11);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned Int32, @unowned UInt8, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const int16_t *a6, uint32_t a7, uint32_t a8, int32_t *divisor, int32_t *flags, Pixel_8 *a11, vImage_Flags a12)
{
  return vImageConvolveWithBias_Planar8(a1, a2, a3, a4, a5, a6, a7, a8, *divisor, *flags, *a11, a12);
}

uint64_t vImage.PixelBuffer<>.convolve(with:divisor:bias:edgeMode:destination:)(uint64_t *a1, uint64_t a2, uint64_t a3, char *a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v8 = a6;
  int v10 = a2;
  if ((a2 & 0x100000000) != 0)
  {
    uint64_t v11 = a1[2];
    uint64_t v12 = *(void *)(v11 + 16);
    if (v12)
    {
      uint64_t v43 = MEMORY[0x1E4FBC860];
      swift_bridgeObjectRetain();
      uint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v12, 0);
      uint64_t v14 = v43;
      uint64_t v15 = (__int16 *)(v11 + 32);
      unint64_t v16 = *(void *)(v43 + 16);
      do
      {
        int v18 = *v15++;
        int v17 = v18;
        uint64_t v43 = v14;
        unint64_t v19 = *(void *)(v14 + 24);
        unint64_t v20 = v16 + 1;
        if (v16 >= v19 >> 1)
        {
          uint64_t result = specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(v19 > 1, v16 + 1, 1);
          uint64_t v14 = v43;
        }
        *(void *)(v14 + 16) = v20;
        *(_DWORD *)(v14 + 4 * v16++ + 32) = v17;
        --v12;
      }
      while (v12);
LABEL_9:
      uint64_t v21 = 0;
      int v10 = 0;
      while (1)
      {
        int v22 = *(_DWORD *)(v14 + 4 * v21 + 32);
        BOOL v23 = __OFADD__(v10, v22);
        v10 += v22;
        if (v23) {
          break;
        }
        if (v20 == ++v21)
        {
          swift_bridgeObjectRelease();
          goto LABEL_13;
        }
      }
      __break(1u);
      goto LABEL_27;
    }
    uint64_t v14 = MEMORY[0x1E4FBC860];
    unint64_t v20 = *(void *)(MEMORY[0x1E4FBC860] + 16);
    uint64_t result = swift_bridgeObjectRetain();
    if (v20) {
      goto LABEL_9;
    }
    swift_bridgeObjectRelease();
    int v10 = 0;
LABEL_13:
    swift_bridgeObjectRelease();
    uint64_t v8 = a6;
  }
  uint64_t v24 = *a4;
  int v25 = a4[1];
  if (a4[1]) {
    char v26 = 0;
  }
  else {
    char v26 = *a4;
  }
  uint64_t result = (*(uint64_t (**)(void, uint64_t))(a7 + 32))(*(void *)(v8 + 16), a7);
  if (result < 0)
  {
LABEL_27:
    __break(1u);
    return result;
  }
  if (result)
  {
    char v27 = v26;
    if (v25) {
      int v28 = dword_1D2137AC4[v24];
    }
    else {
      int v28 = 4;
    }
    unint64_t v29 = 0;
    char v30 = v27;
    int v31 = v28;
    do
    {
      uint64_t v32 = result;
      vImage.PixelBuffer<>.subscript.getter(v29, &v43);
      vImage.PixelBuffer<>.subscript.getter(v29, &v42);
      int v41 = v10;
      int v39 = a3;
      char v40 = BYTE4(a3) & 1;
      char v38 = v30;
      uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
      swift_getAssociatedConformanceWitness();
      uint64_t v34 = type metadata accessor for vImage.PixelBuffer();
      vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(a1, (uint64_t)&v41, (uint64_t)&v39, (uint64_t)&v38, v31, &v42, (char *)specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned UInt8, @unowned UInt32) -> (@unowned Int), 0, (void (*)(uint64_t, uint64_t, void, void, void, unint64_t, uint64_t, uint64_t, uint64_t, char *, uint64_t, int, uint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Int16>?, @unowned UInt32, @unowned UInt32, @unowned Int32, @unowned Int32, @unowned UInt8, @unowned UInt32) -> (@unowned Int), 0, v34, AssociatedTypeWitness, MEMORY[0x1E4FBC0F8], MEMORY[0x1E4FBC150]);
      swift_bridgeObjectRelease();
      swift_bridgeObjectRelease();
      uint64_t result = v32;
      ++v29;
    }
    while (v32 != v29);
  }
  return result;
}

uint64_t vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(uint64_t *a1, unint64_t a2, long long *a3, void **a4)
{
  uint64_t v21 = a4;
  unint64_t v22 = a2;
  if (a3[1])
  {
    int v20 = dword_1D2137AC4[*(void *)a3];
    uint64_t v7 = MEMORY[0x1E4FBC860];
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
    uint64_t v7 = swift_allocObject();
    long long v8 = *a3;
    *(_OWORD *)(v7 + 16) = xmmword_1D2135FC0;
    *(_OWORD *)(v7 + 32) = v8;
    int v20 = 4;
  }
  void v19[5] = v7;
  uint64_t v9 = *v4;
  uint64_t v10 = swift_allocObject();
  *(void *)(v10 + 16) = v9;
  v19[2] = partial apply for implicit closure #2 in implicit closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:);
  v19[3] = MEMORY[0x1F4188790](v10);
  uint64_t v11 = swift_allocObject();
  *(void *)(v11 + 16) = v9;
  v18[2] = partial apply for implicit closure #4 in implicit closure #3 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:);
  v18[3] = MEMORY[0x1F4188790](v11);
  uint64_t v12 = v7 + 32;
  uint64_t v14 = *a1;
  uint64_t v13 = a1[1];
  uint64_t v15 = a1[2];
  unint64_t v16 = *v21;
  swift_bridgeObjectRetain_n();
  specialized vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(v14, v13, v15, v22 | ((HIDWORD(v22) & 1) << 32), v12, v20, v16, (void (*)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, float *, uint64_t *, int))partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned UnsafePointer<Float>?, @unowned UInt32) -> (@unowned Int), 0.0, (uint64_t)v19, (void (*)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, float *, int *, uint64_t *, int))partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float, @unowned UnsafePointer<Float>?, @unowned UInt32) -> (@unowned Int), (uint64_t)v18, v9);
  swift_release();
  swift_release();
  return swift_bridgeObjectRelease();
}

vImage_Error vImage.PixelBuffer<>.vImageConvolve_ARGBFFFF_wrapper(_:_:_:_:_:_:_:_:unusedDivisor:_:_:)(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, const Pixel_FFFF backgroundColor, vImage_Flags flags)
{
  return vImageConvolve_ARGBFFFF(src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, backgroundColor, flags);
}

vImage_Error vImage.PixelBuffer<>.vImageConvolveWithBias_ARGBFFFF_wrapper(_:_:_:_:_:_:_:_:unusedDivisor:_:_:_:)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const float *a6, uint32_t a7, uint32_t a8, double a9, float a10, float *backgroundColor, vImage_Flags flags)
{
  return vImageConvolveWithBias_ARGBFFFF(a1, a2, a3, a4, a5, a6, a7, a8, a10, backgroundColor, flags);
}

uint64_t vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(uint64_t *a1, unint64_t a2, unsigned int *a3, void **a4)
{
  unint64_t v18 = a2;
  unsigned int v7 = *a3;
  if (*((unsigned char *)a3 + 4) == 1)
  {
    int v19 = dword_1D2137AC4[v7];
    unsigned int v7 = 0;
  }
  else
  {
    int v19 = 4;
  }
  long long v8 = *v4;
  uint64_t v9 = swift_allocObject();
  *(void *)(v9 + 16) = v8;
  void v17[2] = partial apply for implicit closure #2 in implicit closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:);
  v17[3] = MEMORY[0x1F4188790](v9);
  uint64_t v10 = swift_allocObject();
  *(void *)(v10 + 16) = v8;
  v16[2] = partial apply for implicit closure #4 in implicit closure #3 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:);
  void v16[3] = MEMORY[0x1F4188790](v10);
  uint64_t v11 = *a1;
  uint64_t v12 = a1[1];
  uint64_t v13 = a1[2];
  uint64_t v14 = *a4;
  swift_bridgeObjectRetain_n();
  char v20 = 0;
  specialized vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(v11, v12, v13, v18 | ((HIDWORD(v18) & 1) << 32), v7, v19, v14, (void (*)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, float *, int *, int))partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float?, @unowned UInt32) -> (@unowned Int), 0.0, (uint64_t)v17, (void (*)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, float *, int *, int *, int))partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float, @unowned Float?, @unowned UInt32) -> (@unowned Int), (uint64_t)v16, v8);
  swift_release();
  return swift_release();
}

const vImage_Buffer *vImage.PixelBuffer<>.vImageConvolve_PlanarF_wrapper(_:_:_:_:_:_:_:_:unusedDivisor:_:_:)(const vImage_Buffer *result, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const float *a6, uint32_t a7, uint32_t a8, uint64_t flags, vImage_Flags a10)
{
  if ((flags & 0x100000000) == 0) {
    return (const vImage_Buffer *)vImageConvolve_PlanarF(result, a2, a3, a4, a5, a6, a7, a8, *(Pixel_F *)&flags, a10);
  }
  __break(1u);
  return result;
}

const vImage_Buffer *vImage.PixelBuffer<>.vImageConvolveWithBias_PlanarF_wrapper(_:_:_:_:_:_:_:_:unusedDivisor:_:_:_:)(const vImage_Buffer *result, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const float *a6, uint32_t a7, uint32_t a8, double a9, float a10, uint64_t flags, vImage_Flags a12)
{
  if ((flags & 0x100000000) == 0) {
    return (const vImage_Buffer *)vImageConvolveWithBias_PlanarF(result, a2, a3, a4, a5, a6, a7, a8, a10, *(Pixel_F *)&flags, a12);
  }
  __break(1u);
  return result;
}

uint64_t *vImage.PixelBuffer<>.convolve(with:bias:edgeMode:useFloat16Accumulator:destination:)(uint64_t *result, unint64_t a2, uint64_t a3, char a4, uint64_t a5)
{
  uint64_t v34 = *MEMORY[0x1E4F143B8];
  unint64_t v6 = (void *)*v5;
  if (!v6[2])
  {
    __break(1u);
    goto LABEL_29;
  }
  unsigned int v7 = *(void **)a5;
  if (!*(void *)(*(void *)a5 + 16))
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  uint64_t v8 = a3;
  uint64_t v9 = (uint64_t)result;
  uint64_t v10 = (void *)v6[4];
  uint64_t v11 = (void *)v7[4];
  if (v10)
  {
    if (v11) {
      BOOL v12 = v10 == v11;
    }
    else {
      BOOL v12 = 0;
    }
    if (!v12) {
      goto LABEL_11;
    }
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_11:
  vImagePixelCount v13 = v6[6];
  if ((v13 & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  vImagePixelCount v14 = v6[5];
  if ((v14 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v13)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v14)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  uint64_t v15 = v7[6];
  if (v15 < 0)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  uint64_t v16 = v7[5];
  if (v16 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (!v15)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v16)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (v13 != v15)
  {
LABEL_38:
    __break(1u);
LABEL_39:
    __break(1u);
  }
  if (v14 != v16) {
    goto LABEL_39;
  }
  uint64_t v17 = *result;
  unint64_t v18 = result[1];
  if (*(unsigned char *)(a3 + 8))
  {
    uint64_t v19 = MEMORY[0x1E4FBC860];
  }
  else
  {
    uint64_t v20 = *(void *)a3;
    uint64_t v26 = *(void *)a3 >> 16;
    uint64_t v27 = HIDWORD(*(void *)a3);
    uint64_t v28 = HIWORD(*(void *)a3);
    unint64_t v30 = a2;
    char v29 = a4;
    uint64_t v21 = v10;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    uint64_t v22 = swift_allocObject();
    uint64_t v10 = v21;
    a4 = v29;
    a2 = v30;
    uint64_t v8 = a3;
    uint64_t v19 = v22;
    *(_OWORD *)(v22 + 16) = xmmword_1D2135FC0;
    *(_WORD *)(v22 + 32) = v20;
    *(_WORD *)(v22 + 34) = v26;
    *(_WORD *)(v22 + 36) = v27;
    *(_WORD *)(v22 + 38) = v28;
  }
  if (a4) {
    unsigned int v23 = 4096;
  }
  else {
    unsigned int v23 = 0;
  }
  size_t v24 = v6[7];
  src.data = v10;
  src.vImagePixelCount height = v14;
  src.vImagePixelCount width = v13;
  src.size_t rowBytes = v24;
  size_t v25 = v7[7];
  dest.data = v11;
  dest.vImagePixelCount height = v14;
  dest.vImagePixelCount width = v13;
  dest.size_t rowBytes = v25;
  closure #1 in closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:useFloat16Accumulator:destination:)(&dest, a2 | ((HIDWORD(a2) & 1) << 32), &src, v9, v18, v17, v19, v8, v23);
  return (uint64_t *)swift_bridgeObjectRelease();
}

const vImage_Buffer *closure #1 in closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:useFloat16Accumulator:destination:)(const vImage_Buffer *dest, uint64_t a2, vImage_Buffer *src, uint64_t a4, unint64_t kernel_height, uint64_t kernel_width, uint64_t a7, uint64_t a8, unsigned int a9)
{
  unint64_t v9 = (kernel_width | kernel_height) >> 32;
  if ((a2 & 0x100000000) == 0)
  {
    if (!v9)
    {
      if (*(unsigned char *)(a8 + 8) == 1) {
        int v10 = dword_1D2137AC4[*(void *)a8];
      }
      else {
        int v10 = 4;
      }
      return (const vImage_Buffer *)vImageConvolveWithBias_ARGB16F(src, dest, 0, 0, 0, (const float *)(*(void *)(a4 + 16) + 32), kernel_height, kernel_width, *(float *)&a2, (const uint16_t *)(a7 + 32), v10 | a9);
    }
    __break(1u);
LABEL_13:
    __break(1u);
    return dest;
  }
  if (v9) {
    goto LABEL_13;
  }
  if (*(unsigned char *)(a8 + 8) == 1) {
    int v11 = dword_1D2137AC4[*(void *)a8];
  }
  else {
    int v11 = 4;
  }
  return (const vImage_Buffer *)vImageConvolve_ARGB16F(src, dest, 0, 0, 0, (const float *)(*(void *)(a4 + 16) + 32), kernel_height, kernel_width, (const uint16_t *)(a7 + 32), v11 | a9);
}

const vImage_Buffer *vImage.PixelBuffer<>.convolve(with:bias:edgeMode:useFloat16Accumulator:destination:)(const vImage_Buffer *result, unint64_t a2, uint64_t a3, char a4, uint64_t a5)
{
  uint64_t v23 = *MEMORY[0x1E4F143B8];
  unint64_t v6 = *(void **)v5;
  if (!*(void *)(*(void *)v5 + 16))
  {
    __break(1u);
    goto LABEL_29;
  }
  unsigned int v7 = *(void **)a5;
  if (!*(void *)(*(void *)a5 + 16))
  {
LABEL_29:
    __break(1u);
    goto LABEL_30;
  }
  uint64_t v8 = (void *)v6[4];
  unint64_t v9 = (void *)v7[4];
  if (v8)
  {
    if (v9) {
      BOOL v10 = v8 == v9;
    }
    else {
      BOOL v10 = 0;
    }
    if (!v10) {
      goto LABEL_11;
    }
    __break(1u);
  }
  if (!v9)
  {
    __break(1u);
    return result;
  }
LABEL_11:
  vImagePixelCount v11 = v6[6];
  if ((v11 & 0x8000000000000000) != 0)
  {
LABEL_30:
    __break(1u);
    goto LABEL_31;
  }
  vImagePixelCount v12 = v6[5];
  if ((v12 & 0x8000000000000000) != 0)
  {
LABEL_31:
    __break(1u);
    goto LABEL_32;
  }
  if (!v11)
  {
LABEL_32:
    __break(1u);
    goto LABEL_33;
  }
  if (!v12)
  {
LABEL_33:
    __break(1u);
    goto LABEL_34;
  }
  uint64_t v13 = v7[6];
  if (v13 < 0)
  {
LABEL_34:
    __break(1u);
    goto LABEL_35;
  }
  uint64_t v14 = v7[5];
  if (v14 < 0)
  {
LABEL_35:
    __break(1u);
    goto LABEL_36;
  }
  if (!v13)
  {
LABEL_36:
    __break(1u);
    goto LABEL_37;
  }
  if (!v14)
  {
LABEL_37:
    __break(1u);
    goto LABEL_38;
  }
  if (v11 != v13)
  {
LABEL_38:
    __break(1u);
LABEL_39:
    __break(1u);
  }
  if (v12 != v14) {
    goto LABEL_39;
  }
  if (a4) {
    unsigned int v15 = 4096;
  }
  else {
    unsigned int v15 = 0;
  }
  uint64_t data = (uint64_t)result->data;
  unint64_t height = result->height;
  if (*(unsigned char *)(a3 + 2)) {
    Pixel_16F v18 = 0;
  }
  else {
    Pixel_16F v18 = *(_WORD *)a3;
  }
  size_t v19 = v6[7];
  src.uint64_t data = v8;
  src.unint64_t height = v12;
  src.vImagePixelCount width = v11;
  src.size_t rowBytes = v19;
  size_t v20 = v7[7];
  dest.uint64_t data = v9;
  dest.unint64_t height = v12;
  dest.vImagePixelCount width = v11;
  dest.size_t rowBytes = v20;
  return closure #1 in closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:useFloat16Accumulator:destination:)(&dest, a2 | ((HIDWORD(a2) & 1) << 32), &src, (uint64_t)result, height, data, v18, (__int16 *)a3, v15);
}

const vImage_Buffer *closure #1 in closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:useFloat16Accumulator:destination:)(const vImage_Buffer *dest, uint64_t a2, vImage_Buffer *src, uint64_t a4, unint64_t kernel_height, uint64_t kernel_width, Pixel_16F backgroundColor, __int16 *a8, unsigned int a9)
{
  unint64_t v9 = (kernel_width | kernel_height) >> 32;
  if ((a2 & 0x100000000) == 0)
  {
    if (!v9)
    {
      if (*((unsigned char *)a8 + 2) == 1) {
        int v10 = dword_1D2137AC4[*a8];
      }
      else {
        int v10 = 4;
      }
      return (const vImage_Buffer *)vImageConvolveWithBias_Planar16F(src, dest, 0, 0, 0, (const float *)(*(void *)(a4 + 16) + 32), kernel_height, kernel_width, *(float *)&a2, backgroundColor, v10 | a9);
    }
    __break(1u);
LABEL_13:
    __break(1u);
    return dest;
  }
  if (v9) {
    goto LABEL_13;
  }
  if (*((unsigned char *)a8 + 2) == 1) {
    int v11 = dword_1D2137AC4[*a8];
  }
  else {
    int v11 = 4;
  }
  return (const vImage_Buffer *)vImageConvolve_Planar16F(src, dest, 0, 0, 0, (const float *)(*(void *)(a4 + 16) + 32), kernel_height, kernel_width, backgroundColor, v11 | a9);
}

uint64_t vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(uint64_t *a1, unint64_t a2, int *a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  unsigned int v7 = v6;
  uint64_t v33 = a1;
  uint64_t v34 = a4;
  unint64_t v9 = a3;
  uint64_t v35 = a5;
  unint64_t v36 = a2;
  int v10 = *a3;
  int v11 = *((unsigned __int8 *)a3 + 4);
  vImagePixelCount v12 = *(uint64_t (**)(void))(a6 + 32);
  uint64_t v32 = *(void *)(a5 + 16);
  uint64_t result = v12();
  if (result < 0)
  {
    __break(1u);
  }
  else if (result)
  {
    unint64_t v14 = 0;
    if (v11) {
      int v15 = 0;
    }
    else {
      int v15 = v10;
    }
    int v28 = v15;
    uint64_t v27 = HIDWORD(v36) & 1;
    unint64_t v30 = v9;
    int v31 = v6;
    uint64_t v29 = result;
    do
    {
      vImage.PixelBuffer<>.subscript.getter(v14, &v45);
      vImage.PixelBuffer<>.subscript.getter(v14, &v44);
      int v43 = 0;
      int v41 = v36;
      char v42 = v27;
      int v39 = v28;
      char v40 = 0;
      if (*((unsigned char *)v9 + 4) == 1) {
        int v16 = dword_1D2137AC4[*v9];
      }
      else {
        int v16 = 4;
      }
      int v38 = v16;
      ++v14;
      uint64_t v17 = *v7;
      Pixel_16F v18 = (void *)swift_allocObject();
      uint64_t v19 = v32;
      v18[2] = v32;
      v18[3] = a6;
      void v18[4] = v17;
      uint64_t v20 = swift_bridgeObjectRetain();
      uint64_t v37 = &v27;
      MEMORY[0x1F4188790](v20);
      v26[2] = partial apply for implicit closure #2 in implicit closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:);
      v26[3] = v18;
      uint64_t v21 = (void *)swift_allocObject();
      v21[2] = v19;
      v21[3] = a6;
      void v21[4] = v17;
      uint64_t v22 = MEMORY[0x1F4188790](v21);
      v25[2] = partial apply for implicit closure #4 in implicit closure #3 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:);
      v25[3] = v22;
      uint64_t AssociatedTypeWitness = swift_getAssociatedTypeWitness();
      swift_getAssociatedConformanceWitness();
      uint64_t v24 = type metadata accessor for vImage.PixelBuffer();
      swift_bridgeObjectRetain();
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for Float?);
      vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(v33, (uint64_t)&v43, (uint64_t)&v41, (uint64_t)&v39, v38, &v44, (char *)partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float?, @unowned UInt32) -> (@unowned Int), (uint64_t)v26, (void (*)(uint64_t, uint64_t, void, void, void, unint64_t, uint64_t, uint64_t, uint64_t, char *, uint64_t, int, uint64_t))partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float, @unowned Float?, @unowned UInt32) -> (@unowned Int), (uint64_t)v25, v24, AssociatedTypeWitness, MEMORY[0x1E4FBB470], MEMORY[0x1E4FBB470]);
      swift_bridgeObjectRelease();
      swift_release();
      unsigned int v7 = v31;
      swift_release();
      swift_bridgeObjectRelease();
      uint64_t result = v29;
      unint64_t v9 = v30;
    }
    while (v29 != v14);
  }
  return result;
}

uint64_t specialized closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, void (*a4)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, int *, int *, uint64_t *, int), uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, int a11, uint64_t a12, int a13, void (*a14)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, int *, uint64_t *, int))
{
  uint64_t v24 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a2 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  long long v16 = *(_OWORD *)(a2 + 48);
  v23[0] = *(_OWORD *)(a2 + 32);
  v23[1] = v16;
  int v21 = a11;
  unint64_t v17 = a10 | a9;
  uint64_t v22 = a12;
  if ((a3 & 0x100000000) != 0)
  {
    if (!HIDWORD(v17))
    {
      swift_bridgeObjectRetain();
      a14(a1, v23, 0, 0, 0, a8 + 32, a9, a10, &v21, &v22, a13);
      return swift_bridgeObjectRelease();
    }
LABEL_10:
    __break(1u);
  }
  int v20 = a3;
  if (HIDWORD(v17))
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  swift_bridgeObjectRetain();
  a4(a1, v23, 0, 0, 0, a8 + 32, a9, a10, &v21, &v20, &v22, a13);
  return swift_bridgeObjectRelease();
}

uint64_t specialized closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, void (*a4)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, int *, int *, char *, int), uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, int a11, char a12, int a13, void (*a14)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, int *, char *, int))
{
  uint64_t v24 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a2 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  long long v16 = *(_OWORD *)(a2 + 48);
  v23[0] = *(_OWORD *)(a2 + 32);
  v23[1] = v16;
  int v22 = a11;
  unint64_t v17 = a10 | a9;
  char v21 = a12;
  if ((a3 & 0x100000000) != 0)
  {
    if (!HIDWORD(v17))
    {
      swift_bridgeObjectRetain();
      a14(a1, v23, 0, 0, 0, a8 + 32, a9, a10, &v22, &v21, a13);
      return swift_bridgeObjectRelease();
    }
LABEL_10:
    __break(1u);
  }
  int v20 = a3;
  if (HIDWORD(v17))
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  swift_bridgeObjectRetain();
  a4(a1, v23, 0, 0, 0, a8 + 32, a9, a10, &v22, &v20, &v21, a13);
  return swift_bridgeObjectRelease();
}

uint64_t specialized closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, void (*a4)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, float *, int *, uint64_t *, int), float a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, int a13, void (*a14)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, float *, uint64_t *, int))
{
  uint64_t v24 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a2 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  long long v16 = *(_OWORD *)(a2 + 48);
  v23[0] = *(_OWORD *)(a2 + 32);
  v23[1] = v16;
  float v21 = a5;
  uint64_t v22 = a12;
  unint64_t v17 = a11 | a10;
  if ((a3 & 0x100000000) != 0)
  {
    if (!HIDWORD(v17))
    {
      swift_bridgeObjectRetain();
      a14(a1, v23, 0, 0, 0, a9 + 32, a10, a11, &v21, &v22, a13);
      return swift_bridgeObjectRelease();
    }
LABEL_10:
    __break(1u);
  }
  int v20 = a3;
  if (HIDWORD(v17))
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  swift_bridgeObjectRetain();
  a4(a1, v23, 0, 0, 0, a9 + 32, a10, a11, &v21, &v20, &v22, a13);
  return swift_bridgeObjectRelease();
}

uint64_t specialized closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, void (*a4)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, float *, int *, int *, int), float a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, int a12, char a13, int a14, void (*a15)(uint64_t, _OWORD *, void, void, void, uint64_t, uint64_t, uint64_t, float *, int *, int))
{
  uint64_t v26 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a2 + 16))
  {
    __break(1u);
    goto LABEL_9;
  }
  long long v17 = *(_OWORD *)(a2 + 48);
  v25[0] = *(_OWORD *)(a2 + 32);
  v25[1] = v17;
  float v22 = a5;
  int v23 = a12;
  char v24 = a13 & 1;
  unint64_t v18 = a11 | a10;
  if ((a3 & 0x100000000) != 0)
  {
    if (!HIDWORD(v18))
    {
      swift_bridgeObjectRetain();
      a15(a1, v25, 0, 0, 0, a9 + 32, a10, a11, &v22, &v23, a14);
      return swift_bridgeObjectRelease();
    }
LABEL_10:
    __break(1u);
  }
  int v21 = a3;
  if (HIDWORD(v18))
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  swift_bridgeObjectRetain();
  a4(a1, v25, 0, 0, 0, a9 + 32, a10, a11, &v22, &v21, &v23, a14);
  return swift_bridgeObjectRelease();
}

uint64_t closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t a1, uint64_t a2, uint64_t a3, void (*a4)(uint64_t, uint64_t, void, void, void, unint64_t, uint64_t, uint64_t, uint64_t, char *, uint64_t, int, uint64_t), uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, int a13, char *a14, uint64_t a15, uint64_t a16, uint64_t a17, uint64_t a18, uint64_t a19)
{
  v30[4] = *MEMORY[0x1E4F143B8];
  type metadata accessor for vImage.PixelBuffer();
  v30[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v30[1] = v19;
  v30[2] = v20;
  v30[3] = v21;
  return closure #1 in closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)((uint64_t)v30, a3, a4, a5, a1, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, a16, a17, a18, a19);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>._convolve<A, B, C, D>(kernel:divisor:bias:backgroundColor:flags:destination:convolveFunc:convolveWithBiasFunc:)(uint64_t a1, uint64_t a2, void (*a3)(uint64_t, uint64_t, void, void, void, unint64_t, uint64_t, uint64_t, uint64_t, char *, uint64_t, int, uint64_t), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, int a13, char *a14, uint64_t a15, uint64_t a16, uint64_t a17, uint64_t a18, uint64_t a19)
{
  uint64_t v45 = a5;
  uint64_t v37 = a4;
  int v38 = a3;
  uint64_t v46 = a2;
  uint64_t v39 = a18;
  int v40 = a13;
  uint64_t v41 = a12;
  uint64_t v42 = a11;
  uint64_t v43 = a1;
  uint64_t v20 = type metadata accessor for Optional();
  uint64_t v21 = *(void *)(v20 - 8);
  uint64_t v22 = MEMORY[0x1F4188790](v20);
  char v24 = (char *)&v37 - v23;
  uint64_t v25 = *(void *)(a19 - 8);
  MEMORY[0x1F4188790](v22);
  uint64_t v27 = (char *)&v37 - ((v26 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(char *, uint64_t, uint64_t))(v21 + 16))(v24, v46, v20);
  int v28 = (*(uint64_t (**)(char *, uint64_t, uint64_t))(v25 + 48))(v24, 1, a19);
  uint64_t v46 = a10;
  uint64_t v44 = a9;
  unint64_t v29 = a10 | a9;
  if (v28 != 1)
  {
    (*(void (**)(char *, char *, uint64_t))(v25 + 32))(v27, v24, a19);
    if (HIDWORD(v29))
    {
LABEL_30:
      __break(1u);
      goto LABEL_31;
    }
    swift_bridgeObjectRetain();
    uint64_t v32 = v39;
    if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && (a8 < 0 || (a8 & 0x4000000000000000) != 0))
    {
      if (MEMORY[0x1D25FF9C0](a8, v32))
      {
        uint64_t v47 = a8;
        type metadata accessor for _ArrayBuffer();
        swift_getWitnessTable();
        uint64_t v36 = Array.init<A>(_:)();
        destructiveProjectEnumData for BNNS.ActivationFunction(v36);
        swift_unknownObjectRetain();
        unint64_t v33 = _ContiguousArrayBuffer.firstElementAddress.getter();
        swift_release();
        goto LABEL_27;
      }
      swift_bridgeObjectRelease();
      unint64_t v33 = 0;
    }
    else
    {
      swift_bridgeObjectRelease();
      if (_swift_isClassOrObjCExistentialType()) {
        unint64_t v33 = (a8 & 0xFFFFFFFFFFFFFF8)
      }
            + ((*(unsigned __int8 *)(*(void *)(v32 - 8) + 80) + 32) & ~(unint64_t)*(unsigned __int8 *)(*(void *)(v32 - 8) + 80));
      else {
        unint64_t v33 = a8
      }
            + ((*(unsigned __int8 *)(*(void *)(v32 - 8) + 80) + 32) & ~(unint64_t)*(unsigned __int8 *)(*(void *)(v32 - 8) + 80));
    }
    if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && (a8 < 0 || (a8 & 0x4000000000000000) != 0))
    {
      specialized _ArrayBuffer._nonNative.getter(a8);
      swift_unknownObjectRetain();
      if (v33) {
        goto LABEL_27;
      }
    }
    else
    {
      _swift_isClassOrObjCExistentialType();
      swift_bridgeObjectRetain();
      if (v33)
      {
LABEL_27:
        v38(v45, v43, 0, 0, 0, v33, v44, v46, v42, v27, v41, v40, v37);
        (*(void (**)(char *, uint64_t))(v25 + 8))(v27, a19);
        return swift_unknownObjectRelease();
      }
    }
    unint64_t v33 = ~*(_DWORD *)(*(void *)(v32 - 8) + 80) | 0xFFFFFFFFFFFFFF00;
    goto LABEL_27;
  }
  unint64_t v30 = *(void (**)(char *, uint64_t))(v21 + 8);
  v21 += 8;
  v30(v24, v20);
  if (HIDWORD(v29))
  {
    __break(1u);
    goto LABEL_30;
  }
  uint64_t v27 = a14;
  swift_bridgeObjectRetain();
  uint64_t v21 = v39;
  if ((_swift_isClassOrObjCExistentialType() & 1) == 0
    || (a8 & 0x8000000000000000) == 0 && (a8 & 0x4000000000000000) == 0)
  {
    swift_bridgeObjectRelease();
    if (_swift_isClassOrObjCExistentialType()) {
      unint64_t v31 = (a8 & 0xFFFFFFFFFFFFFF8)
    }
          + ((*(unsigned __int8 *)(*(void *)(v21 - 8) + 80) + 32) & ~(unint64_t)*(unsigned __int8 *)(*(void *)(v21 - 8) + 80));
    else {
      unint64_t v31 = a8
    }
          + ((*(unsigned __int8 *)(*(void *)(v21 - 8) + 80) + 32) & ~(unint64_t)*(unsigned __int8 *)(*(void *)(v21 - 8) + 80));
    goto LABEL_15;
  }
LABEL_31:
  if (MEMORY[0x1D25FF9C0](a8, v21))
  {
    uint64_t v47 = a8;
    type metadata accessor for _ArrayBuffer();
    swift_getWitnessTable();
    uint64_t v35 = Array.init<A>(_:)();
    destructiveProjectEnumData for BNNS.ActivationFunction(v35);
    swift_unknownObjectRetain();
    unint64_t v31 = _ContiguousArrayBuffer.firstElementAddress.getter();
    swift_release();
    goto LABEL_20;
  }
  swift_bridgeObjectRelease();
  unint64_t v31 = 0;
LABEL_15:
  if ((_swift_isClassOrObjCExistentialType() & 1) != 0 && (a8 < 0 || (a8 & 0x4000000000000000) != 0))
  {
    specialized _ArrayBuffer._nonNative.getter(a8);
    swift_unknownObjectRetain();
    if (v31) {
      goto LABEL_20;
    }
    goto LABEL_19;
  }
  _swift_isClassOrObjCExistentialType();
  swift_bridgeObjectRetain();
  if (!v31) {
LABEL_19:
  }
    unint64_t v31 = ~*(_DWORD *)(*(void *)(v21 - 8) + 80) | 0xFFFFFFFFFFFFFF00;
LABEL_20:
  ((void (*)(uint64_t, uint64_t, void, void, void, unint64_t, uint64_t, uint64_t, uint64_t, uint64_t, int))v27)(v45, v43, 0, 0, 0, v31, v44, v46, v42, v41, v40);
  return swift_unknownObjectRelease();
}

vImage_Buffer **vImage.PixelBuffer<>.convolve(with:divisors:biases:edgeMode:destination:)(vImage_Buffer **result, unint64_t *a2, unint64_t *a3, unint64_t *a4, void *a5, int a6, char a7, unsigned int a8, unsigned int a9, unsigned int a10, unsigned int a11, unsigned int *a12, uint64_t a13)
{
  uint64_t v125 = *MEMORY[0x1E4F143B8];
  uint64_t v21 = *(void **)v15;
  if (!*(void *)(*(void *)v15 + 16)) {
    goto LABEL_68;
  }
  uint64_t v22 = *(void **)a13;
  if (!*(void *)(*(void *)a13 + 16)) {
    goto LABEL_69;
  }
  LOBYTE(v20) = a7;
  unint64_t v18 = *result;
  uint64_t v19 = result[1];
  unint64_t v24 = (unint64_t)result[2];
  unint64_t v104 = a2[1];
  unint64_t v105 = *a2;
  unint64_t v16 = *a3;
  unint64_t v17 = a3[1];
  unint64_t v108 = a3[2];
  unint64_t v14 = a4[1];
  unint64_t v106 = a2[2];
  unint64_t v107 = *a4;
  unint64_t v25 = a4[2];
  uint64_t v102 = *a12;
  unsigned __int8 v26 = *((unsigned char *)a12 + 4);
  unint64_t v13 = v21[4];
  uint64_t v27 = v22[4];
  if (!v13)
  {
LABEL_7:
    if (!v27)
    {
      __break(1u);
      return result;
    }
    goto LABEL_8;
  }
  if (v27 && v13 == v27)
  {
    __break(1u);
    goto LABEL_7;
  }
LABEL_8:
  uint64_t v28 = v21[6];
  if (v28 < 0) {
    goto LABEL_70;
  }
  unint64_t v13 = v21[5];
  if ((v13 & 0x8000000000000000) != 0) {
    goto LABEL_71;
  }
  if (!v28) {
    goto LABEL_72;
  }
  if (!v13) {
    goto LABEL_73;
  }
  uint64_t v29 = v22[6];
  if (v29 < 0) {
    goto LABEL_74;
  }
  uint64_t v30 = v22[5];
  if (v30 < 0) {
    goto LABEL_75;
  }
  if (!v29) {
    goto LABEL_76;
  }
  if (!v30) {
    goto LABEL_77;
  }
  if (v28 != v29) {
    goto LABEL_78;
  }
  if (v13 != v30) {
    goto LABEL_79;
  }
  uint64_t v91 = a5;
  long long v92 = v22;
  LODWORD(v109) = a6;
  long long v93 = v21;
  unsigned __int8 v103 = v26;
  swift_bridgeObjectRetain();
  unint64_t v94 = (unint64_t)v19;
  unint64_t v95 = (unint64_t)v18;
  unint64_t v113 = v18;
  unint64_t v114 = v19;
  unint64_t v115 = v24;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRelease();
  unint64_t v116 = v105;
  unint64_t v117 = v104;
  unint64_t v118 = v106;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  unint64_t v97 = v17;
  unint64_t v98 = v16;
  unint64_t v119 = v16;
  unint64_t v120 = v17;
  unint64_t v121 = v108;
  unint64_t v18 = &v112;
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  swift_bridgeObjectRetain();
  unint64_t v96 = v24;
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  unint64_t v122 = v107;
  unint64_t v123 = v14;
  unint64_t v99 = v25;
  unint64_t v100 = v14;
  unint64_t v124 = v25;
  uint64_t v19 = (vImage_Buffer *)MEMORY[0x1E4FBC860];
  v112.uint64_t data = (void *)MEMORY[0x1E4FBC860];
  specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 4, 0);
  uint64_t data = v112.data;
  unint64_t v17 = (unint64_t)v113;
  unint64_t v16 = *((void *)v112.data + 2);
  unint64_t v13 = *((void *)v112.data + 3);
  int64_t v32 = v13 >> 1;
  unint64_t v14 = v16 + 1;
  unsigned int v101 = a8;
  if (v13 >> 1 <= v16) {
    goto LABEL_80;
  }
  while (1)
  {
    data[2] = v14;
    data[v16 + 4] = v17;
    unint64_t v33 = v18[3].data;
    int64_t v34 = v16 + 2;
    if (v32 < v34)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v13 > 1), v34, 1);
      uint64_t data = v18->data;
    }
    data[2] = v34;
    data[v14 + 4] = v33;
    size_t rowBytes = v18[3].rowBytes;
    unint64_t v37 = data[2];
    unint64_t v36 = data[3];
    unint64_t v38 = v37 + 1;
    if (v37 >= v36 >> 1) {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v36 > 1), v37 + 1, 1);
    }
    uint64_t v39 = (uint64_t)v18->data;
    *(void *)(v39 + 16) = v38;
    *(void *)(v39 + 8 * v37 + 32) = rowBytes;
    vImagePixelCount width = v18[4].width;
    unint64_t v41 = *(void *)(v39 + 24);
    int64_t v42 = v37 + 2;
    if (v42 > (uint64_t)(v41 >> 1))
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v41 > 1), v42, 1);
      uint64_t v39 = (uint64_t)v18->data;
    }
    *(void *)(v39 + 16) = v42;
    *(void *)(v39 + 8 * v38 + 32) = width;
    int64_t v43 = specialized Set.init<A>(_:)(v39);
    swift_release();
    uint64_t v44 = *(void *)(v43 + 16);
    swift_bridgeObjectRelease();
    if (v44 == 1)
    {
      v18->uint64_t data = v19;
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 4, 0);
      uint64_t v46 = v18->data;
      unint64_t v17 = v18[2].width;
      int64_t v42 = *((void *)v18->data + 2);
      unint64_t v45 = *((void *)v18->data + 3);
      int64_t v47 = v45 >> 1;
      int64_t v43 = v42 + 1;
      if (v45 >> 1 > v42) {
        goto LABEL_27;
      }
    }
    else
    {
      __break(1u);
    }
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v45 > 1), v43, 1);
    uint64_t v46 = v18->data;
    unint64_t v45 = *((void *)v18->data + 3);
    int64_t v47 = v45 >> 1;
LABEL_27:
    v46[2] = v43;
    v46[v42 + 4] = v17;
    vImagePixelCount height = v18[3].height;
    int64_t v49 = v42 + 2;
    if (v47 < v49)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v45 > 1), v49, 1);
      uint64_t v46 = v18->data;
    }
    v46[2] = v49;
    v46[v43 + 4] = height;
    long long v50 = v18[4].data;
    unint64_t v52 = v46[2];
    unint64_t v51 = v46[3];
    unint64_t v53 = v52 + 1;
    if (v52 >= v51 >> 1) {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v51 > 1), v52 + 1, 1);
    }
    uint64_t v54 = (uint64_t)v18->data;
    *(void *)(v54 + 16) = v53;
    *(void *)(v54 + 8 * v52 + 32) = v50;
    size_t v55 = v18[4].rowBytes;
    unint64_t v56 = *(void *)(v54 + 24);
    int64_t v57 = v52 + 2;
    if (v57 > (uint64_t)(v56 >> 1))
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v56 > 1), v57, 1);
      uint64_t v54 = (uint64_t)v18->data;
    }
    *(void *)(v54 + 16) = v57;
    *(void *)(v54 + 8 * v53 + 32) = v55;
    unint64_t v58 = specialized Set.init<A>(_:)(v54);
    swift_release();
    uint64_t v59 = *(void *)(v58 + 16);
    swift_bridgeObjectRelease();
    if (v59 != 1)
    {
      __break(1u);
LABEL_84:
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v17 > 1), v58, 1);
      uint64_t v60 = v18->data;
      goto LABEL_37;
    }
    if ((v20 & 1) == 0)
    {
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for vImage.ConvolutionKernel2D<Int16>);
      swift_arrayDestroy();
      __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Int32>);
      uint64_t v20 = swift_allocObject();
      *(_OWORD *)(v20 + 16) = xmmword_1D2135FC0;
      *(void *)(v20 + 32) = v91;
      *(_DWORD *)(v20 + 40) = v109;
      *(_DWORD *)(v20 + 44) = v109;
      goto LABEL_63;
    }
    v18->uint64_t data = v19;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, 4, 0);
    uint64_t v60 = v18->data;
    int64_t v57 = v18[2].rowBytes;
    size_t v55 = *((void *)v18->data + 2);
    unint64_t v17 = *((void *)v18->data + 3);
    unint64_t v58 = v55 + 1;
    swift_bridgeObjectRetain();
    if (v55 >= v17 >> 1) {
      goto LABEL_84;
    }
LABEL_37:
    v60[2] = v58;
    v60[v55 + 4] = v57;
    vImagePixelCount v61 = v18[3].width;
    unint64_t v62 = v60[3];
    swift_bridgeObjectRetain();
    if (v58 >= v62 >> 1)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v62 > 1), v55 + 2, 1);
      uint64_t v60 = v18->data;
    }
    v60[2] = v55 + 2;
    v60[v58 + 4] = v61;
    vImagePixelCount v63 = v18[4].height;
    unint64_t v65 = v60[2];
    unint64_t v64 = v60[3];
    unint64_t v14 = v65 + 1;
    swift_bridgeObjectRetain();
    if (v65 >= v64 >> 1) {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v64 > 1), v65 + 1, 1);
    }
    uint64_t v66 = v18->data;
    v66[2] = v14;
    v66[v65 + 4] = v63;
    uint64_t v67 = v18[5].data;
    unint64_t v68 = v66[3];
    swift_bridgeObjectRetain();
    uint64_t v19 = (vImage_Buffer *)MEMORY[0x1E4FBC860];
    if (v14 >= v68 >> 1)
    {
      specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((void *)(v68 > 1), v65 + 2, 1);
      uint64_t v66 = v18->data;
    }
    v66[2] = v65 + 2;
    size_t v109 = (char *)(v66 + 4);
    v66[v14 + 4] = v67;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for vImage.ConvolutionKernel2D<Int16>);
    swift_arrayDestroy();
    int64_t v69 = v66[2];
    if (!v69) {
      break;
    }
    uint64_t v91 = v66;
    v18->uint64_t data = v19;
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v69, 0);
    unint64_t v17 = 0;
    uint64_t v20 = (uint64_t)v18->data;
    while (1)
    {
      uint64_t v70 = *(void *)&v109[8 * v17];
      int64_t v71 = *(void *)(v70 + 16);
      if (v71) {
        break;
      }
      unint64_t v77 = v19->width;
      swift_bridgeObjectRetain();
      if (v77)
      {
        p_uint64_t data = &v19->data;
        goto LABEL_53;
      }
      swift_bridgeObjectRelease();
      LODWORD(v16) = 0;
LABEL_57:
      swift_bridgeObjectRelease();
      v18->uint64_t data = (void *)v20;
      unint64_t v81 = *(void *)(v20 + 16);
      unint64_t v80 = *(void *)(v20 + 24);
      unint64_t v14 = v81 + 1;
      if (v81 >= v80 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v80 > 1), v81 + 1, 1);
        uint64_t v20 = (uint64_t)v18->data;
      }
      ++v17;
      *(void *)(v20 + 16) = v14;
      *(_DWORD *)(v20 + 4 * v81 + 32) = v16;
      if (v17 == v69)
      {
        swift_release();
        goto LABEL_63;
      }
    }
    v111.uint64_t data = v19;
    swift_bridgeObjectRetain();
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)(0, v71, 0);
    p_uint64_t data = &v19->data;
    uint64_t v73 = (__int16 *)(v70 + 32);
    unint64_t v14 = p_data[2];
    do
    {
      int v75 = *v73++;
      int v74 = v75;
      v111.uint64_t data = p_data;
      unint64_t v76 = p_data[3];
      unint64_t v77 = v14 + 1;
      if (v14 >= v76 >> 1)
      {
        specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v76 > 1), v14 + 1, 1);
        p_uint64_t data = v111.data;
      }
      p_data[2] = v77;
      *((_DWORD *)p_data + v14++ + 8) = v74;
      --v71;
    }
    while (v71);
    unint64_t v18 = &v112;
    uint64_t v19 = (vImage_Buffer *)MEMORY[0x1E4FBC860];
LABEL_53:
    uint64_t v78 = 0;
    LODWORD(v16) = 0;
    while (1)
    {
      unint64_t v13 = *((unsigned int *)p_data + v78 + 8);
      BOOL v79 = __OFADD__(v16, v13);
      unint64_t v16 = (v16 + v13);
      if (v79) {
        break;
      }
      if (v77 == ++v78)
      {
        swift_bridgeObjectRelease();
        goto LABEL_57;
      }
    }
    __break(1u);
LABEL_68:
    __break(1u);
LABEL_69:
    __break(1u);
LABEL_70:
    __break(1u);
LABEL_71:
    __break(1u);
LABEL_72:
    __break(1u);
LABEL_73:
    __break(1u);
LABEL_74:
    __break(1u);
LABEL_75:
    __break(1u);
LABEL_76:
    __break(1u);
LABEL_77:
    __break(1u);
LABEL_78:
    __break(1u);
LABEL_79:
    __break(1u);
LABEL_80:
    specialized ContiguousArray._createNewBuffer(bufferIsUnique:minimumCapacity:growForAppend:)((char *)(v13 > 1), v14, 1);
    uint64_t data = v18->data;
    unint64_t v13 = *((void *)v18->data + 3);
    int64_t v32 = v13 >> 1;
  }
  swift_release();
  uint64_t v20 = MEMORY[0x1E4FBC860];
LABEL_63:
  unsigned __int8 v82 = v103;
  if (v103)
  {
    uint64_t v83 = MEMORY[0x1E4FBC860];
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    uint64_t v84 = swift_allocObject();
    unsigned __int8 v82 = v103;
    uint64_t v83 = v84;
    *(_OWORD *)(v84 + 16) = xmmword_1D2135FC0;
    *(_WORD *)(v84 + 32) = v102;
    *(unsigned char *)(v84 + 34) = BYTE2(v102);
    unint64_t v18 = &v112;
    *(unsigned char *)(v84 + 35) = BYTE3(v102);
  }
  long long v85 = *((_OWORD *)v93 + 3);
  *(_OWORD *)&v18->uint64_t data = *((_OWORD *)v93 + 2);
  *(_OWORD *)&v18->vImagePixelCount width = v85;
  long long v86 = *((_OWORD *)v92 + 3);
  *(_OWORD *)&v111.uint64_t data = *((_OWORD *)v92 + 2);
  *(_OWORD *)&v111.vImagePixelCount width = v86;
  uint64_t v87 = *(void *)(v96 + 16);
  uint64_t v88 = *(void *)(v106 + 16);
  uint64_t v89 = *(void *)(v108 + 16);
  uint64_t v90 = *(void *)(v99 + 16);
  v110[0] = v95;
  v110[1] = v94;
  v110[2] = v96;
  v110[3] = v105;
  v110[4] = v104;
  v110[5] = v106;
  v110[6] = v98;
  v110[7] = v97;
  v110[8] = v108;
  v110[9] = v107;
  v110[10] = v100;
  v110[11] = v99;
  closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in closure #5 in vImage.PixelBuffer<>.convolve(with:divisors:biases:edgeMode:destination:)((const int16_t *)(v99 + 32), v90, (const int16_t *)(v96 + 32), v87, (const int16_t *)(v106 + 32), v88, (const int16_t *)(v108 + 32), v89, &v112, &v111, v110, v20, v101 | ((unint64_t)a9 << 32), a10 | ((unint64_t)a11 << 32), v83, v102 | ((unint64_t)v82 << 32));
  swift_bridgeObjectRelease();
  return (vImage_Buffer **)swift_bridgeObjectRelease();
}

const int16_t *closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in closure #5 in vImage.PixelBuffer<>.convolve(with:divisors:biases:edgeMode:destination:)(const int16_t *result, uint64_t a2, const int16_t *a3, uint64_t a4, const int16_t *a5, uint64_t a6, const int16_t *a7, uint64_t a8, const vImage_Buffer *a9, const vImage_Buffer *a10, unint64_t *a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16)
{
  unint64_t v18 = *a11;
  unint64_t v17 = a11[1];
  v21[0] = a3;
  v21[1] = a5;
  v21[2] = a7;
  v21[3] = result;
  if ((v17 | v18) >> 32)
  {
    __break(1u);
  }
  else
  {
    biases[0] = a13;
    biases[1] = a14;
    if ((a16 & 0x100000000) != 0) {
      vImage_Flags flags = dword_1D2137AC4[(int)a16];
    }
    else {
      vImage_Flags flags = 4;
    }
    return (const int16_t *)vImageConvolveMultiKernel_ARGB8888(a9, a10, 0, 0, 0, v21, v17, v18, (const int32_t *)(a12 + 32), (const int32_t *)biases, (const uint8_t *)(a15 + 32), flags);
  }
  return result;
}

uint64_t sub_1D20D4A74()
{
  swift_bridgeObjectRelease();

  return MEMORY[0x1F4186498](v0, 24, 7);
}

vImage_Error partial apply for implicit closure #2 in implicit closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, uint32_t kernel_height, uint32_t kernel_width, const Pixel_FFFF backgroundColor, vImage_Flags flags)
{
  return vImageConvolve_ARGBFFFF(src, dest, tempBuffer, srcOffsetToROI_X, srcOffsetToROI_Y, kernel, kernel_height, kernel_width, backgroundColor, flags);
}

uint64_t partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned UnsafePointer<Float>?, @unowned UInt32) -> (@unowned Int)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float *a9)
{
  return (*(uint64_t (**)(float))(v9 + 16))(*a9);
}

vImage_Error partial apply for implicit closure #4 in implicit closure #3 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const float *a6, uint32_t a7, uint32_t a8, double a9, float a10, float *backgroundColor, vImage_Flags flags)
{
  return vImageConvolveWithBias_ARGBFFFF(a1, a2, a3, a4, a5, a6, a7, a8, a10, backgroundColor, flags);
}

uint64_t partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float, @unowned UnsafePointer<Float>?, @unowned UInt32) -> (@unowned Int)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float *a9, float *a10)
{
  return (*(uint64_t (**)(float, float))(v10 + 16))(*a9, *a10);
}

const vImage_Buffer *partial apply for implicit closure #2 in implicit closure #1 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(const vImage_Buffer *result, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const float *a6, uint32_t a7, uint32_t a8, double a9, double a10, double a11, double a12, double a13, double a14, double a15, double a16, Pixel_F a17, char a18, vImage_Flags a19)
{
  if ((a18 & 1) == 0) {
    return (const vImage_Buffer *)vImageConvolve_PlanarF(result, a2, a3, a4, a5, a6, a7, a8, a17, a19);
  }
  __break(1u);
  return result;
}

uint64_t partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float?, @unowned UInt32) -> (@unowned Int)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float *a9)
{
  return partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float?, @unowned UInt32) -> (@unowned Int)(a1, a2, a3, a4, a5, a6, a7, a8, a9);
}

{
  uint64_t v9;

  return (*(uint64_t (**)(float))(v9 + 16))(*a9);
}

const vImage_Buffer *partial apply for implicit closure #4 in implicit closure #3 in vImage.PixelBuffer<>.convolve(with:bias:edgeMode:destination:)(const vImage_Buffer *result, const vImage_Buffer *a2, void *a3, vImagePixelCount a4, vImagePixelCount a5, const float *a6, uint32_t a7, uint32_t a8, double a9, float a10, double a11, double a12, double a13, double a14, double a15, double a16, Pixel_F a17, char a18, vImage_Flags a19)
{
  if ((a18 & 1) == 0) {
    return (const vImage_Buffer *)vImageConvolveWithBias_PlanarF(result, a2, a3, a4, a5, a6, a7, a8, a10, a17, a19);
  }
  __break(1u);
  return result;
}

uint64_t partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float, @unowned Float?, @unowned UInt32) -> (@unowned Int)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float *a9, float *a10)
{
  return partial apply for thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned UInt, @unowned UInt, @unowned UnsafePointer<Float>?, @unowned UInt32, @unowned UInt32, @unowned Float, @unowned Float, @unowned Float?, @unowned UInt32) -> (@unowned Int)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10);
}

{
  uint64_t v10;

  return (*(uint64_t (**)(float, float))(v10 + 16))(*a9, *a10);
}

ValueMetadata *type metadata accessor for vImage.ConvolutionKernel()
{
  return &type metadata for vImage.ConvolutionKernel;
}

uint64_t type metadata instantiation function for vImage.ConvolutionKernel2D(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return MEMORY[0x1F41863F8](a1, a2, a3, 24);
}

uint64_t initializeBufferWithCopyOfBuffer for vImage.ConvolutionKernel2D(uint64_t a1, uint64_t a2)
{
  *(_OWORD *)a1 = *(_OWORD *)a2;
  *(void *)(a1 + 16) = *(void *)(a2 + 16);
  swift_bridgeObjectRetain();
  return a1;
}

uint64_t destroy for vImage.ConvolutionKernel2D()
{
  return swift_bridgeObjectRelease();
}

void *assignWithCopy for vImage.ConvolutionKernel2D(void *a1, void *a2)
{
  *a1 = *a2;
  a1[1] = a2[1];
  a1[2] = a2[2];
  swift_bridgeObjectRetain();
  swift_bridgeObjectRelease();
  return a1;
}

__n128 __swift_memcpy24_8(__n128 *a1, __n128 *a2)
{
  __n128 result = *a2;
  a1[1].n128_u64[0] = a2[1].n128_u64[0];
  *a1 = result;
  return result;
}

uint64_t assignWithTake for vImage.ConvolutionKernel2D(uint64_t a1, uint64_t a2)
{
  *(_OWORD *)a1 = *(_OWORD *)a2;
  *(void *)(a1 + 16) = *(void *)(a2 + 16);
  swift_bridgeObjectRelease();
  return a1;
}

uint64_t getEnumTagSinglePayload for vImage.ConvolutionKernel2D(uint64_t a1, int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0 && *(unsigned char *)(a1 + 24)) {
    return *(_DWORD *)a1 + 0x80000000;
  }
  unint64_t v2 = *(void *)(a1 + 16);
  if (v2 >= 0xFFFFFFFF) {
    LODWORD(v2) = -1;
  }
  return (v2 + 1);
}

uint64_t storeEnumTagSinglePayload for vImage.ConvolutionKernel2D(uint64_t result, int a2, int a3)
{
  if (a2 < 0)
  {
    *(void *)(result + 8) = 0;
    *(void *)(result + 16) = 0;
    *(void *)__n128 result = a2 ^ 0x80000000;
    if (a3 < 0) {
      *(unsigned char *)(result + 24) = 1;
    }
  }
  else
  {
    if ((a3 & 0x80000000) == 0)
    {
      if (!a2) {
        return result;
      }
LABEL_8:
      *(void *)(result + 16) = (a2 - 1);
      return result;
    }
    *(unsigned char *)(result + 24) = 0;
    if (a2) {
      goto LABEL_8;
    }
  }
  return result;
}

uint64_t type metadata accessor for vImage.ConvolutionKernel2D()
{
  return __swift_instantiateGenericMetadata();
}

uint64_t type metadata instantiation function for vImage.EdgeMode(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return MEMORY[0x1F41863F8](a1, a2, a3, 8);
}

uint64_t type metadata completion function for vImage.EdgeMode()
{
  uint64_t result = swift_checkMetadataState();
  if (v1 <= 0x3F)
  {
    swift_initEnumMetadataSinglePayload();
    return 0;
  }
  return result;
}

void *initializeBufferWithCopyOfBuffer for vImage.EdgeMode(void *a1, unsigned __int16 *a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(a3 + 16);
  uint64_t v6 = *(void *)(v5 - 8);
  uint64_t v7 = *(unsigned int *)(v6 + 84);
  size_t v8 = *(void *)(v6 + 64);
  unint64_t v9 = v8;
  if (v7 <= 2)
  {
    if (v8 <= 3)
    {
      unsigned int v11 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v11 > 0xFFFE)
      {
        uint64_t v10 = 4;
      }
      else
      {
        BOOL v12 = v11 != 0;
        BOOL v13 = v11 >= 0xFF;
        uint64_t v10 = 2;
        if (!v13) {
          uint64_t v10 = v12;
        }
      }
    }
    else
    {
      uint64_t v10 = 1;
    }
    unint64_t v9 = v10 + v8;
  }
  uint64_t v14 = *(_DWORD *)(v6 + 80);
  if (v14 > 7 || v9 > 0x18 || (*(_DWORD *)(v6 + 80) & 0x100000) != 0)
  {
    uint64_t v17 = *(void *)a2;
    *a1 = *(void *)a2;
    a1 = (void *)(v17 + ((v14 + 16) & ~v14));
    swift_retain();
    return a1;
  }
  if (v7 <= 2)
  {
    if (v8 <= 3)
    {
      unsigned int v19 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v19 > 0xFFFE)
      {
        int v18 = *(_DWORD *)((char *)a2 + v8);
        if (!v18) {
          goto LABEL_37;
        }
        goto LABEL_27;
      }
      if (v19 > 0xFE)
      {
        int v18 = *(unsigned __int16 *)((char *)a2 + v8);
        if (!*(unsigned __int16 *)((char *)a2 + v8)) {
          goto LABEL_37;
        }
        goto LABEL_27;
      }
      if (!v19)
      {
LABEL_37:
        if (!v7) {
          goto LABEL_53;
        }
        goto LABEL_38;
      }
    }
    int v18 = *((unsigned __int8 *)a2 + v8);
    if (!*((unsigned char *)a2 + v8)) {
      goto LABEL_37;
    }
LABEL_27:
    int v20 = (v18 - 1) << (8 * v8);
    if (v8 > 3) {
      int v20 = 0;
    }
    if (v8)
    {
      if (v8 <= 3) {
        int v21 = v8;
      }
      else {
        int v21 = 4;
      }
      switch(v21)
      {
        case 2:
          int v22 = *a2;
          break;
        case 3:
          int v22 = *a2 | (*((unsigned __int8 *)a2 + 2) << 16);
          break;
        case 4:
          int v22 = *(_DWORD *)a2;
          break;
        default:
          int v22 = *(unsigned __int8 *)a2;
          break;
      }
    }
    else
    {
      int v22 = 0;
    }
    if (v7 + (v22 | v20) != -1) {
      goto LABEL_39;
    }
    goto LABEL_53;
  }
LABEL_38:
  if ((*(unsigned int (**)(unsigned __int16 *, uint64_t, uint64_t))(v6 + 48))(a2, v7, v5))
  {
LABEL_39:
    if (v7 <= 2)
    {
      if (v8 <= 3)
      {
        unsigned int v24 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
        if (v24 > 0xFFFE)
        {
          uint64_t v23 = 4;
        }
        else
        {
          BOOL v25 = v24 != 0;
          BOOL v13 = v24 >= 0xFF;
          uint64_t v23 = 2;
          if (!v13) {
            uint64_t v23 = v25;
          }
        }
      }
      else
      {
        uint64_t v23 = 1;
      }
      v8 += v23;
    }
    memcpy(a1, a2, v8);
    return a1;
  }
LABEL_53:
  (*(void (**)(void *, unsigned __int16 *, uint64_t))(v6 + 16))(a1, a2, v5);
  if (v7 <= 2)
  {
    if (v8 <= 3)
    {
      unsigned int v26 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v26 > 0xFFFE)
      {
        *(_DWORD *)((char *)a1 + v8) = 0;
        return a1;
      }
      if (v26 > 0xFE)
      {
        *(_WORD *)((char *)a1 + v8) = 0;
        return a1;
      }
      if (!v26) {
        return a1;
      }
    }
    *((unsigned char *)a1 + v8) = 0;
  }
  return a1;
}

uint64_t destroy for vImage.EdgeMode(unsigned __int16 *a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(a2 + 16);
  uint64_t v4 = *(void *)(v3 - 8);
  unsigned int v5 = *(_DWORD *)(v4 + 84);
  uint64_t v6 = *(void *)(v4 + 64);
  if (v5 > 2)
  {
LABEL_19:
    uint64_t v14 = *(void *)(v3 - 8);
    uint64_t result = (*(uint64_t (**)(unsigned __int16 *))(v4 + 48))(a1);
    uint64_t v4 = v14;
    if (result) {
      return result;
    }
    goto LABEL_25;
  }
  char v7 = 8 * v6;
  if (v6 <= 3)
  {
    unsigned int v9 = (~(-1 << v7) - v5 + 3) >> v7;
    if (v9 > 0xFFFE)
    {
      int v8 = *(_DWORD *)((char *)a1 + v6);
      if (!v8) {
        goto LABEL_18;
      }
      goto LABEL_10;
    }
    if (v9 > 0xFE)
    {
      int v8 = *(unsigned __int16 *)((char *)a1 + v6);
      if (!*(unsigned __int16 *)((char *)a1 + v6)) {
        goto LABEL_18;
      }
      goto LABEL_10;
    }
    if (!v9)
    {
LABEL_18:
      if (v5) {
        goto LABEL_19;
      }
LABEL_25:
      BOOL v13 = *(uint64_t (**)(unsigned __int16 *, uint64_t))(v4 + 8);
      return v13(a1, v3);
    }
  }
  int v8 = *((unsigned __int8 *)a1 + v6);
  if (!*((unsigned char *)a1 + v6)) {
    goto LABEL_18;
  }
LABEL_10:
  int v10 = (v8 - 1) << v7;
  if (v6 > 3) {
    int v10 = 0;
  }
  if (v6)
  {
    if (v6 > 3) {
      LODWORD(v6) = 4;
    }
    switch((int)v6)
    {
      case 2:
        LODWORD(v6) = *a1;
        break;
      case 3:
        LODWORD(v6) = *a1 | (*((unsigned __int8 *)a1 + 2) << 16);
        break;
      case 4:
        LODWORD(v6) = *(_DWORD *)a1;
        break;
      default:
        LODWORD(v6) = *(unsigned __int8 *)a1;
        break;
    }
  }
  int v12 = v5 + (v6 | v10);
  uint64_t result = (v12 + 1);
  if (v12 == -1) {
    goto LABEL_25;
  }
  return result;
}

char *initializeWithCopy for vImage.EdgeMode(char *a1, unsigned __int16 *a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(a3 + 16);
  uint64_t v6 = *(void *)(v5 - 8);
  uint64_t v7 = *(unsigned int *)(v6 + 84);
  size_t v8 = *(void *)(v6 + 64);
  if (v7 <= 2)
  {
    if (v8 <= 3)
    {
      unsigned int v10 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v10 > 0xFFFE)
      {
        int v9 = *(_DWORD *)((char *)a2 + v8);
        if (!v9) {
          goto LABEL_20;
        }
        goto LABEL_10;
      }
      if (v10 > 0xFE)
      {
        int v9 = *(unsigned __int16 *)((char *)a2 + v8);
        if (!*(unsigned __int16 *)((char *)a2 + v8)) {
          goto LABEL_20;
        }
        goto LABEL_10;
      }
      if (!v10)
      {
LABEL_20:
        if (!v7) {
          goto LABEL_36;
        }
        goto LABEL_21;
      }
    }
    int v9 = *((unsigned __int8 *)a2 + v8);
    if (!*((unsigned char *)a2 + v8)) {
      goto LABEL_20;
    }
LABEL_10:
    int v11 = (v9 - 1) << (8 * v8);
    if (v8 > 3) {
      int v11 = 0;
    }
    if (v8)
    {
      if (v8 <= 3) {
        int v12 = *(void *)(v6 + 64);
      }
      else {
        int v12 = 4;
      }
      switch(v12)
      {
        case 2:
          int v13 = *a2;
          break;
        case 3:
          int v13 = *a2 | (*((unsigned __int8 *)a2 + 2) << 16);
          break;
        case 4:
          int v13 = *(_DWORD *)a2;
          break;
        default:
          int v13 = *(unsigned __int8 *)a2;
          break;
      }
    }
    else
    {
      int v13 = 0;
    }
    if (v7 + (v13 | v11) != -1) {
      goto LABEL_22;
    }
    goto LABEL_36;
  }
LABEL_21:
  if ((*(unsigned int (**)(unsigned __int16 *, uint64_t, uint64_t))(v6 + 48))(a2, v7, v5))
  {
LABEL_22:
    if (v7 <= 2)
    {
      if (v8 <= 3)
      {
        unsigned int v15 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
        if (v15 > 0xFFFE)
        {
          uint64_t v14 = 4;
        }
        else
        {
          BOOL v16 = v15 != 0;
          BOOL v17 = v15 >= 0xFF;
          uint64_t v14 = 2;
          if (!v17) {
            uint64_t v14 = v16;
          }
        }
      }
      else
      {
        uint64_t v14 = 1;
      }
      v8 += v14;
    }
    memcpy(a1, a2, v8);
    return a1;
  }
LABEL_36:
  (*(void (**)(char *, unsigned __int16 *, uint64_t))(v6 + 16))(a1, a2, v5);
  if (v7 <= 2)
  {
    if (v8 <= 3)
    {
      unsigned int v18 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v18 > 0xFFFE)
      {
        *(_DWORD *)&a1[v8] = 0;
        return a1;
      }
      if (v18 > 0xFE)
      {
        *(_WORD *)&a1[v8] = 0;
        return a1;
      }
      if (!v18) {
        return a1;
      }
    }
    a1[v8] = 0;
  }
  return a1;
}

unsigned __int16 *assignWithCopy for vImage.EdgeMode(unsigned __int16 *a1, unsigned __int8 *a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(a3 + 16);
  uint64_t v6 = *(void *)(v5 - 8);
  uint64_t v7 = *(unsigned int *)(v6 + 84);
  size_t v8 = *(void *)(v6 + 64);
  if (v7 > 2) {
    goto LABEL_21;
  }
  if (v8 > 3) {
    goto LABEL_3;
  }
  unsigned int v10 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
  if (v10 > 0xFFFE)
  {
    int v9 = *(_DWORD *)((char *)a1 + v8);
    if (!v9) {
      goto LABEL_20;
    }
    goto LABEL_10;
  }
  if (v10 > 0xFE)
  {
    int v9 = *(unsigned __int16 *)((char *)a1 + v8);
    if (!*(unsigned __int16 *)((char *)a1 + v8)) {
      goto LABEL_20;
    }
    goto LABEL_10;
  }
  if (v10)
  {
LABEL_3:
    int v9 = *((unsigned __int8 *)a1 + v8);
    if (!*((unsigned char *)a1 + v8)) {
      goto LABEL_20;
    }
LABEL_10:
    int v11 = (v9 - 1) << (8 * v8);
    if (v8 > 3) {
      int v11 = 0;
    }
    if (v8)
    {
      if (v8 <= 3) {
        int v12 = *(void *)(v6 + 64);
      }
      else {
        int v12 = 4;
      }
      switch(v12)
      {
        case 2:
          int v13 = *a1;
          break;
        case 3:
          int v13 = *a1 | (*((unsigned __int8 *)a1 + 2) << 16);
          break;
        case 4:
          int v13 = *(_DWORD *)a1;
          break;
        default:
          int v13 = *(unsigned __int8 *)a1;
          break;
      }
    }
    else
    {
      int v13 = 0;
    }
    if ((v13 | v11) + v7 != -1) {
      goto LABEL_30;
    }
    goto LABEL_33;
  }
LABEL_20:
  if (!v7) {
    goto LABEL_33;
  }
LABEL_21:
  uint64_t v14 = *(unsigned int (**)(unsigned __int8 *, uint64_t, uint64_t))(v6 + 48);
  if (!v14((unsigned __int8 *)a1, v7, v5))
  {
    if (v7 > 2)
    {
LABEL_74:
      if (v14(a2, v7, v5)) {
        goto LABEL_75;
      }
      goto LABEL_100;
    }
LABEL_33:
    if (v8 > 3) {
      goto LABEL_34;
    }
    unsigned int v18 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
    if (v18 > 0xFFFE)
    {
      int v16 = *(_DWORD *)&a2[v8];
      if (v16) {
        goto LABEL_55;
      }
    }
    else
    {
      if (v18 <= 0xFE)
      {
        if (!v18) {
          goto LABEL_72;
        }
LABEL_34:
        int v16 = a2[v8];
        if (!a2[v8]) {
          goto LABEL_72;
        }
LABEL_55:
        int v22 = (v16 - 1) << (8 * v8);
        if (v8 > 3) {
          int v22 = 0;
        }
        if (v8)
        {
          if (v8 <= 3) {
            int v23 = v8;
          }
          else {
            int v23 = 4;
          }
          switch(v23)
          {
            case 2:
              int v24 = *(unsigned __int16 *)a2;
              break;
            case 3:
              int v24 = *(unsigned __int16 *)a2 | (a2[2] << 16);
              break;
            case 4:
              int v24 = *(_DWORD *)a2;
              break;
            default:
              int v24 = *a2;
              break;
          }
        }
        else
        {
          int v24 = 0;
        }
        if (v7 + (v24 | v22) != -1)
        {
LABEL_75:
          (*(void (**)(unsigned __int16 *, uint64_t))(v6 + 8))(a1, v5);
          if (v7 <= 2)
          {
            if (v8 > 3) {
              goto LABEL_70;
            }
            goto LABEL_77;
          }
LABEL_83:
          memcpy(a1, a2, v8);
          return a1;
        }
LABEL_100:
        (*(void (**)(unsigned __int16 *, unsigned __int8 *, uint64_t))(v6 + 24))(a1, a2, v5);
        return a1;
      }
      int v16 = *(unsigned __int16 *)&a2[v8];
      if (*(_WORD *)&a2[v8]) {
        goto LABEL_55;
      }
    }
LABEL_72:
    if (!v7) {
      goto LABEL_100;
    }
    uint64_t v14 = *(unsigned int (**)(unsigned __int8 *, uint64_t, uint64_t))(v6 + 48);
    goto LABEL_74;
  }
  if (v7 > 2)
  {
LABEL_67:
    if (v14(a2, v7, v5)) {
      goto LABEL_68;
    }
    goto LABEL_88;
  }
LABEL_30:
  if (v8 > 3) {
    goto LABEL_31;
  }
  unsigned int v17 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
  if (v17 > 0xFFFE)
  {
    int v15 = *(_DWORD *)&a2[v8];
    if (v15) {
      goto LABEL_45;
    }
    goto LABEL_65;
  }
  if (v17 > 0xFE)
  {
    int v15 = *(unsigned __int16 *)&a2[v8];
    if (*(_WORD *)&a2[v8]) {
      goto LABEL_45;
    }
LABEL_65:
    if (!v7) {
      goto LABEL_88;
    }
    uint64_t v14 = *(unsigned int (**)(unsigned __int8 *, uint64_t, uint64_t))(v6 + 48);
    goto LABEL_67;
  }
  if (!v17) {
    goto LABEL_65;
  }
LABEL_31:
  int v15 = a2[v8];
  if (!a2[v8]) {
    goto LABEL_65;
  }
LABEL_45:
  int v19 = (v15 - 1) << (8 * v8);
  if (v8 > 3) {
    int v19 = 0;
  }
  if (v8)
  {
    if (v8 <= 3) {
      int v20 = v8;
    }
    else {
      int v20 = 4;
    }
    switch(v20)
    {
      case 2:
        int v21 = *(unsigned __int16 *)a2;
        break;
      case 3:
        int v21 = *(unsigned __int16 *)a2 | (a2[2] << 16);
        break;
      case 4:
        int v21 = *(_DWORD *)a2;
        break;
      default:
        int v21 = *a2;
        break;
    }
  }
  else
  {
    int v21 = 0;
  }
  if (v7 + (v21 | v19) != -1)
  {
LABEL_68:
    if (v7 <= 2)
    {
      if (v8 > 3)
      {
LABEL_70:
        uint64_t v25 = 1;
LABEL_82:
        v8 += v25;
        goto LABEL_83;
      }
LABEL_77:
      unsigned int v26 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v26 > 0xFFFE)
      {
        uint64_t v25 = 4;
      }
      else
      {
        BOOL v27 = v26 != 0;
        BOOL v28 = v26 >= 0xFF;
        uint64_t v25 = 2;
        if (!v28) {
          uint64_t v25 = v27;
        }
      }
      goto LABEL_82;
    }
    goto LABEL_83;
  }
LABEL_88:
  (*(void (**)(unsigned __int16 *, unsigned __int8 *, uint64_t))(v6 + 16))(a1, a2, v5);
  if (v7 <= 2)
  {
    if (v8 <= 3)
    {
      unsigned int v29 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v29 > 0xFFFE)
      {
        *(_DWORD *)((char *)a1 + v8) = 0;
        return a1;
      }
      if (v29 > 0xFE)
      {
        *(unsigned __int16 *)((char *)a1 + v8) = 0;
        return a1;
      }
      if (!v29) {
        return a1;
      }
    }
    *((unsigned char *)a1 + v8) = 0;
  }
  return a1;
}

char *initializeWithTake for vImage.EdgeMode(char *a1, unsigned __int16 *a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(a3 + 16);
  uint64_t v6 = *(void *)(v5 - 8);
  uint64_t v7 = *(unsigned int *)(v6 + 84);
  size_t v8 = *(void *)(v6 + 64);
  if (v7 <= 2)
  {
    if (v8 <= 3)
    {
      unsigned int v10 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v10 > 0xFFFE)
      {
        int v9 = *(_DWORD *)((char *)a2 + v8);
        if (!v9) {
          goto LABEL_20;
        }
        goto LABEL_10;
      }
      if (v10 > 0xFE)
      {
        int v9 = *(unsigned __int16 *)((char *)a2 + v8);
        if (!*(unsigned __int16 *)((char *)a2 + v8)) {
          goto LABEL_20;
        }
        goto LABEL_10;
      }
      if (!v10)
      {
LABEL_20:
        if (!v7) {
          goto LABEL_36;
        }
        goto LABEL_21;
      }
    }
    int v9 = *((unsigned __int8 *)a2 + v8);
    if (!*((unsigned char *)a2 + v8)) {
      goto LABEL_20;
    }
LABEL_10:
    int v11 = (v9 - 1) << (8 * v8);
    if (v8 > 3) {
      int v11 = 0;
    }
    if (v8)
    {
      if (v8 <= 3) {
        int v12 = *(void *)(v6 + 64);
      }
      else {
        int v12 = 4;
      }
      switch(v12)
      {
        case 2:
          int v13 = *a2;
          break;
        case 3:
          int v13 = *a2 | (*((unsigned __int8 *)a2 + 2) << 16);
          break;
        case 4:
          int v13 = *(_DWORD *)a2;
          break;
        default:
          int v13 = *(unsigned __int8 *)a2;
          break;
      }
    }
    else
    {
      int v13 = 0;
    }
    if (v7 + (v13 | v11) != -1) {
      goto LABEL_22;
    }
    goto LABEL_36;
  }
LABEL_21:
  if ((*(unsigned int (**)(unsigned __int16 *, uint64_t, uint64_t))(v6 + 48))(a2, v7, v5))
  {
LABEL_22:
    if (v7 <= 2)
    {
      if (v8 <= 3)
      {
        unsigned int v15 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
        if (v15 > 0xFFFE)
        {
          uint64_t v14 = 4;
        }
        else
        {
          BOOL v16 = v15 != 0;
          BOOL v17 = v15 >= 0xFF;
          uint64_t v14 = 2;
          if (!v17) {
            uint64_t v14 = v16;
          }
        }
      }
      else
      {
        uint64_t v14 = 1;
      }
      v8 += v14;
    }
    memcpy(a1, a2, v8);
    return a1;
  }
LABEL_36:
  (*(void (**)(char *, unsigned __int16 *, uint64_t))(v6 + 32))(a1, a2, v5);
  if (v7 <= 2)
  {
    if (v8 <= 3)
    {
      unsigned int v18 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v18 > 0xFFFE)
      {
        *(_DWORD *)&a1[v8] = 0;
        return a1;
      }
      if (v18 > 0xFE)
      {
        *(_WORD *)&a1[v8] = 0;
        return a1;
      }
      if (!v18) {
        return a1;
      }
    }
    a1[v8] = 0;
  }
  return a1;
}

unsigned __int16 *assignWithTake for vImage.EdgeMode(unsigned __int16 *a1, unsigned __int8 *a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(a3 + 16);
  uint64_t v6 = *(void *)(v5 - 8);
  uint64_t v7 = *(unsigned int *)(v6 + 84);
  size_t v8 = *(void *)(v6 + 64);
  if (v7 > 2) {
    goto LABEL_21;
  }
  if (v8 > 3) {
    goto LABEL_3;
  }
  unsigned int v10 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
  if (v10 > 0xFFFE)
  {
    int v9 = *(_DWORD *)((char *)a1 + v8);
    if (!v9) {
      goto LABEL_20;
    }
    goto LABEL_10;
  }
  if (v10 > 0xFE)
  {
    int v9 = *(unsigned __int16 *)((char *)a1 + v8);
    if (!*(unsigned __int16 *)((char *)a1 + v8)) {
      goto LABEL_20;
    }
    goto LABEL_10;
  }
  if (v10)
  {
LABEL_3:
    int v9 = *((unsigned __int8 *)a1 + v8);
    if (!*((unsigned char *)a1 + v8)) {
      goto LABEL_20;
    }
LABEL_10:
    int v11 = (v9 - 1) << (8 * v8);
    if (v8 > 3) {
      int v11 = 0;
    }
    if (v8)
    {
      if (v8 <= 3) {
        int v12 = *(void *)(v6 + 64);
      }
      else {
        int v12 = 4;
      }
      switch(v12)
      {
        case 2:
          int v13 = *a1;
          break;
        case 3:
          int v13 = *a1 | (*((unsigned __int8 *)a1 + 2) << 16);
          break;
        case 4:
          int v13 = *(_DWORD *)a1;
          break;
        default:
          int v13 = *(unsigned __int8 *)a1;
          break;
      }
    }
    else
    {
      int v13 = 0;
    }
    if ((v13 | v11) + v7 != -1) {
      goto LABEL_30;
    }
    goto LABEL_33;
  }
LABEL_20:
  if (!v7) {
    goto LABEL_33;
  }
LABEL_21:
  uint64_t v14 = *(unsigned int (**)(unsigned __int8 *, uint64_t, uint64_t))(v6 + 48);
  if (!v14((unsigned __int8 *)a1, v7, v5))
  {
    if (v7 > 2)
    {
LABEL_74:
      if (v14(a2, v7, v5)) {
        goto LABEL_75;
      }
      goto LABEL_100;
    }
LABEL_33:
    if (v8 > 3) {
      goto LABEL_34;
    }
    unsigned int v18 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
    if (v18 > 0xFFFE)
    {
      int v16 = *(_DWORD *)&a2[v8];
      if (v16) {
        goto LABEL_55;
      }
    }
    else
    {
      if (v18 <= 0xFE)
      {
        if (!v18) {
          goto LABEL_72;
        }
LABEL_34:
        int v16 = a2[v8];
        if (!a2[v8]) {
          goto LABEL_72;
        }
LABEL_55:
        int v22 = (v16 - 1) << (8 * v8);
        if (v8 > 3) {
          int v22 = 0;
        }
        if (v8)
        {
          if (v8 <= 3) {
            int v23 = v8;
          }
          else {
            int v23 = 4;
          }
          switch(v23)
          {
            case 2:
              int v24 = *(unsigned __int16 *)a2;
              break;
            case 3:
              int v24 = *(unsigned __int16 *)a2 | (a2[2] << 16);
              break;
            case 4:
              int v24 = *(_DWORD *)a2;
              break;
            default:
              int v24 = *a2;
              break;
          }
        }
        else
        {
          int v24 = 0;
        }
        if (v7 + (v24 | v22) != -1)
        {
LABEL_75:
          (*(void (**)(unsigned __int16 *, uint64_t))(v6 + 8))(a1, v5);
          if (v7 <= 2)
          {
            if (v8 > 3) {
              goto LABEL_70;
            }
            goto LABEL_77;
          }
LABEL_83:
          memcpy(a1, a2, v8);
          return a1;
        }
LABEL_100:
        (*(void (**)(unsigned __int16 *, unsigned __int8 *, uint64_t))(v6 + 40))(a1, a2, v5);
        return a1;
      }
      int v16 = *(unsigned __int16 *)&a2[v8];
      if (*(_WORD *)&a2[v8]) {
        goto LABEL_55;
      }
    }
LABEL_72:
    if (!v7) {
      goto LABEL_100;
    }
    uint64_t v14 = *(unsigned int (**)(unsigned __int8 *, uint64_t, uint64_t))(v6 + 48);
    goto LABEL_74;
  }
  if (v7 > 2)
  {
LABEL_67:
    if (v14(a2, v7, v5)) {
      goto LABEL_68;
    }
    goto LABEL_88;
  }
LABEL_30:
  if (v8 > 3) {
    goto LABEL_31;
  }
  unsigned int v17 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
  if (v17 > 0xFFFE)
  {
    int v15 = *(_DWORD *)&a2[v8];
    if (v15) {
      goto LABEL_45;
    }
    goto LABEL_65;
  }
  if (v17 > 0xFE)
  {
    int v15 = *(unsigned __int16 *)&a2[v8];
    if (*(_WORD *)&a2[v8]) {
      goto LABEL_45;
    }
LABEL_65:
    if (!v7) {
      goto LABEL_88;
    }
    uint64_t v14 = *(unsigned int (**)(unsigned __int8 *, uint64_t, uint64_t))(v6 + 48);
    goto LABEL_67;
  }
  if (!v17) {
    goto LABEL_65;
  }
LABEL_31:
  int v15 = a2[v8];
  if (!a2[v8]) {
    goto LABEL_65;
  }
LABEL_45:
  int v19 = (v15 - 1) << (8 * v8);
  if (v8 > 3) {
    int v19 = 0;
  }
  if (v8)
  {
    if (v8 <= 3) {
      int v20 = v8;
    }
    else {
      int v20 = 4;
    }
    switch(v20)
    {
      case 2:
        int v21 = *(unsigned __int16 *)a2;
        break;
      case 3:
        int v21 = *(unsigned __int16 *)a2 | (a2[2] << 16);
        break;
      case 4:
        int v21 = *(_DWORD *)a2;
        break;
      default:
        int v21 = *a2;
        break;
    }
  }
  else
  {
    int v21 = 0;
  }
  if (v7 + (v21 | v19) != -1)
  {
LABEL_68:
    if (v7 <= 2)
    {
      if (v8 > 3)
      {
LABEL_70:
        uint64_t v25 = 1;
LABEL_82:
        v8 += v25;
        goto LABEL_83;
      }
LABEL_77:
      unsigned int v26 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v26 > 0xFFFE)
      {
        uint64_t v25 = 4;
      }
      else
      {
        BOOL v27 = v26 != 0;
        BOOL v28 = v26 >= 0xFF;
        uint64_t v25 = 2;
        if (!v28) {
          uint64_t v25 = v27;
        }
      }
      goto LABEL_82;
    }
    goto LABEL_83;
  }
LABEL_88:
  (*(void (**)(unsigned __int16 *, unsigned __int8 *, uint64_t))(v6 + 32))(a1, a2, v5);
  if (v7 <= 2)
  {
    if (v8 <= 3)
    {
      unsigned int v29 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v29 > 0xFFFE)
      {
        *(_DWORD *)((char *)a1 + v8) = 0;
        return a1;
      }
      if (v29 > 0xFE)
      {
        *(unsigned __int16 *)((char *)a1 + v8) = 0;
        return a1;
      }
      if (!v29) {
        return a1;
      }
    }
    *((unsigned char *)a1 + v8) = 0;
  }
  return a1;
}

uint64_t getEnumTagSinglePayload for vImage.EdgeMode(unsigned __int16 *a1, unsigned int a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(*(void *)(a3 + 16) - 8);
  unsigned int v5 = *(_DWORD *)(v4 + 84);
  unsigned int v6 = v5 - 3;
  uint64_t v7 = *(void *)(v4 + 64);
  if (v5 <= 2)
  {
    unsigned int v6 = 0;
    if (v7 <= 3)
    {
      unsigned int v9 = (~(-1 << (8 * v7)) - v5 + 3) >> (8 * v7);
      if (v9 > 0xFFFE)
      {
        uint64_t v8 = 4;
      }
      else
      {
        BOOL v10 = v9 != 0;
        BOOL v11 = v9 >= 0xFF;
        uint64_t v8 = 2;
        if (!v11) {
          uint64_t v8 = v10;
        }
      }
    }
    else
    {
      uint64_t v8 = 1;
    }
    v7 += v8;
  }
  if (!a2) {
    return 0;
  }
  int v12 = a2 - v6;
  if (a2 <= v6) {
    goto LABEL_30;
  }
  char v13 = 8 * v7;
  if (v7 > 3) {
    goto LABEL_13;
  }
  unsigned int v15 = ((v12 + ~(-1 << v13)) >> v13) + 1;
  if (HIWORD(v15))
  {
    int v14 = *(_DWORD *)((char *)a1 + v7);
    if (v14) {
      goto LABEL_20;
    }
  }
  else
  {
    if (v15 <= 0xFF)
    {
      if (v15 < 2) {
        goto LABEL_30;
      }
LABEL_13:
      int v14 = *((unsigned __int8 *)a1 + v7);
      if (!*((unsigned char *)a1 + v7)) {
        goto LABEL_30;
      }
LABEL_20:
      int v16 = (v14 - 1) << v13;
      if (v7 > 3) {
        int v16 = 0;
      }
      if (v7)
      {
        if (v7 <= 3) {
          int v17 = v7;
        }
        else {
          int v17 = 4;
        }
        switch(v17)
        {
          case 2:
            int v18 = *a1;
            break;
          case 3:
            int v18 = *a1 | (*((unsigned __int8 *)a1 + 2) << 16);
            break;
          case 4:
            int v18 = *(_DWORD *)a1;
            break;
          default:
            int v18 = *(unsigned __int8 *)a1;
            break;
        }
      }
      else
      {
        int v18 = 0;
      }
      return v6 + (v18 | v16) + 1;
    }
    int v14 = *(unsigned __int16 *)((char *)a1 + v7);
    if (*(unsigned __int16 *)((char *)a1 + v7)) {
      goto LABEL_20;
    }
  }
LABEL_30:
  if (!v6) {
    return 0;
  }
  if (!v5) {
    return 0;
  }
  unsigned int v19 = (*(uint64_t (**)(void))(v4 + 48))();
  BOOL v11 = v19 >= 3;
  uint64_t result = v19 - 3;
  if (result == 0 || !v11) {
    return 0;
  }
  return result;
}

void storeEnumTagSinglePayload for vImage.EdgeMode(char *a1, unsigned int a2, unsigned int a3, uint64_t a4)
{
  uint64_t v6 = *(void *)(*(void *)(a4 + 16) - 8);
  unsigned int v7 = *(_DWORD *)(v6 + 84);
  size_t v8 = *(void *)(v6 + 64);
  size_t v9 = v8;
  if (v7 <= 2) {
    unsigned int v10 = 0;
  }
  else {
    unsigned int v10 = v7 - 3;
  }
  if (v7 <= 2)
  {
    if (v8 <= 3)
    {
      unsigned int v12 = (~(-1 << (8 * v8)) - v7 + 3) >> (8 * v8);
      if (v12 > 0xFFFE)
      {
        uint64_t v11 = 4;
      }
      else
      {
        BOOL v13 = v12 != 0;
        BOOL v14 = v12 >= 0xFF;
        uint64_t v11 = 2;
        if (!v14) {
          uint64_t v11 = v13;
        }
      }
    }
    else
    {
      uint64_t v11 = 1;
    }
    size_t v9 = v11 + v8;
  }
  BOOL v14 = a3 >= v10;
  unsigned int v15 = a3 - v10;
  if (v15 != 0 && v14)
  {
    if (v9 <= 3)
    {
      unsigned int v19 = ((v15 + ~(-1 << (8 * v9))) >> (8 * v9)) + 1;
      if (HIWORD(v19))
      {
        int v16 = 4;
      }
      else if (v19 >= 0x100)
      {
        int v16 = 2;
      }
      else
      {
        int v16 = v19 > 1;
      }
    }
    else
    {
      int v16 = 1;
    }
  }
  else
  {
    int v16 = 0;
  }
  if (v10 < a2)
  {
    unsigned int v17 = ~v10 + a2;
    if (v9 < 4)
    {
      int v18 = (v17 >> (8 * v9)) + 1;
      if (v9)
      {
        int v20 = v17 & ~(-1 << (8 * v9));
        bzero(a1, v9);
        if (v9 == 3)
        {
          *(_WORD *)a1 = v20;
          a1[2] = BYTE2(v20);
        }
        else if (v9 == 2)
        {
          *(_WORD *)a1 = v20;
        }
        else
        {
          *a1 = v20;
        }
      }
    }
    else
    {
      bzero(a1, v9);
      *(_DWORD *)a1 = v17;
      int v18 = 1;
    }
    switch(v16)
    {
      case 1:
        a1[v9] = v18;
        return;
      case 2:
        *(_WORD *)&a1[v9] = v18;
        return;
      case 3:
        goto LABEL_60;
      case 4:
        *(_DWORD *)&a1[v9] = v18;
        return;
      default:
        return;
    }
  }
  switch(v16)
  {
    case 1:
      a1[v9] = 0;
      if (!a2) {
        return;
      }
      goto LABEL_33;
    case 2:
      *(_WORD *)&a1[v9] = 0;
      if (!a2) {
        return;
      }
      goto LABEL_33;
    case 3:
LABEL_60:
      __break(1u);
      JUMPOUT(0x1D20D6830);
    case 4:
      *(_DWORD *)&a1[v9] = 0;
      goto LABEL_32;
    default:
LABEL_32:
      if (a2)
      {
LABEL_33:
        if (a2 + 3 <= v7)
        {
          if (a2 != -3)
          {
            int v24 = *(void (**)(void))(v6 + 56);
            v24();
          }
        }
        else
        {
          if (v8 <= 3) {
            int v21 = ~(-1 << (8 * v8));
          }
          else {
            int v21 = -1;
          }
          if (v8)
          {
            int v22 = v21 & (a2 - v7 + 2);
            if (v8 <= 3) {
              int v23 = v8;
            }
            else {
              int v23 = 4;
            }
            bzero(a1, v8);
            switch(v23)
            {
              case 2:
                *(_WORD *)a1 = v22;
                break;
              case 3:
                *(_WORD *)a1 = v22;
                a1[2] = BYTE2(v22);
                break;
              case 4:
                *(_DWORD *)a1 = v22;
                break;
              default:
                *a1 = v22;
                break;
            }
          }
        }
      }
      return;
  }
}

uint64_t getEnumTag for vImage.EdgeMode(unsigned __int16 *a1, uint64_t a2)
{
  uint64_t v2 = *(void *)(*(void *)(a2 + 16) - 8);
  unsigned int v3 = *(_DWORD *)(v2 + 84);
  uint64_t v4 = *(void *)(v2 + 64);
  if (v3 > 2) {
    return (*(uint64_t (**)(void))(v2 + 48))();
  }
  char v5 = 8 * v4;
  if (v4 > 3) {
    goto LABEL_3;
  }
  unsigned int v7 = (~(-1 << v5) - v3 + 3) >> v5;
  if (v7 > 0xFFFE)
  {
    int v6 = *(_DWORD *)((char *)a1 + v4);
    if (v6) {
      goto LABEL_10;
    }
  }
  else
  {
    if (v7 <= 0xFE)
    {
      if (!v7) {
        goto LABEL_18;
      }
LABEL_3:
      int v6 = *((unsigned __int8 *)a1 + v4);
      if (!*((unsigned char *)a1 + v4)) {
        goto LABEL_18;
      }
LABEL_10:
      int v8 = (v6 - 1) << v5;
      if (v4 > 3) {
        int v8 = 0;
      }
      if (v4)
      {
        if (v4 > 3) {
          LODWORD(v4) = 4;
        }
        switch((int)v4)
        {
          case 2:
            LODWORD(v4) = *a1;
            break;
          case 3:
            LODWORD(v4) = *a1 | (*((unsigned __int8 *)a1 + 2) << 16);
            break;
          case 4:
            LODWORD(v4) = *(_DWORD *)a1;
            break;
          default:
            LODWORD(v4) = *(unsigned __int8 *)a1;
            break;
        }
      }
      return v3 + (v4 | v8) + 1;
    }
    int v6 = *(unsigned __int16 *)((char *)a1 + v4);
    if (*(unsigned __int16 *)((char *)a1 + v4)) {
      goto LABEL_10;
    }
  }
LABEL_18:
  if (v3) {
    return (*(uint64_t (**)(void))(v2 + 48))();
  }
  return 0;
}

void destructiveInjectEnumTag for vImage.EdgeMode(char *a1, unsigned int a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(*(void *)(a3 + 16) - 8);
  unsigned int v5 = *(_DWORD *)(v4 + 84);
  size_t v6 = *(void *)(v4 + 64);
  if (v5 > 2)
  {
    int v7 = 0;
  }
  else if (v6 <= 3)
  {
    unsigned int v10 = (~(-1 << (8 * v6)) - v5 + 3) >> (8 * v6);
    if (v10 > 0xFFFE)
    {
      int v7 = 4;
    }
    else if (v10 >= 0xFF)
    {
      int v7 = 2;
    }
    else
    {
      int v7 = v10 != 0;
    }
  }
  else
  {
    int v7 = 1;
  }
  if (v5 < a2)
  {
    unsigned int v8 = ~v5 + a2;
    if (v6 < 4)
    {
      int v9 = (v8 >> (8 * v6)) + 1;
      if (v6)
      {
        int v11 = v8 & ~(-1 << (8 * v6));
        bzero(a1, v6);
        if (v6 == 3)
        {
          *(_WORD *)a1 = v11;
          a1[2] = BYTE2(v11);
        }
        else if (v6 == 2)
        {
          *(_WORD *)a1 = v11;
        }
        else
        {
          *a1 = v11;
        }
      }
    }
    else
    {
      bzero(a1, v6);
      *(_DWORD *)a1 = v8;
      int v9 = 1;
    }
    switch(v7)
    {
      case 1:
        a1[v6] = v9;
        return;
      case 2:
        *(_WORD *)&a1[v6] = v9;
        return;
      case 3:
        goto LABEL_34;
      case 4:
        *(_DWORD *)&a1[v6] = v9;
        return;
      default:
        return;
    }
  }
  switch(v7)
  {
    case 1:
      a1[v6] = 0;
      if (!a2) {
        return;
      }
      goto LABEL_21;
    case 2:
      *(_WORD *)&a1[v6] = 0;
      if (!a2) {
        return;
      }
      goto LABEL_21;
    case 3:
LABEL_34:
      __break(1u);
      JUMPOUT(0x1D20D6BD4);
    case 4:
      *(_DWORD *)&a1[v6] = 0;
      goto LABEL_20;
    default:
LABEL_20:
      if (a2)
      {
LABEL_21:
        unsigned int v12 = *(void (**)(void))(v4 + 56);
        v12();
      }
      return;
  }
}

uint64_t type metadata accessor for vImage.EdgeMode()
{
  return __swift_instantiateGenericMetadata();
}

uint64_t sub_1D20D6C14()
{
  swift_bridgeObjectRelease();

  return MEMORY[0x1F4186498](v0, 40, 7);
}

uint64_t static vDSP.add<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.add<A>(_:_:)(a1, a2, a3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.add<A>(_:_:));
}

{
  return static vDSP.add<A>(_:_:)(a1, a2, a3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.add<A>(_:_:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A>(_:_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.add<A>(_:_:)(a1, a2, *(void *)(v2 + 40), *(void *)(v2 + 16), *(void *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float))static vDSP.add<A, B>(_:_:result:), *(float *)(v2 + 32));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.add<A>(_:_:)(a1, a2, *(void *)(v2 + 40), *(void *)(v2 + 16), *(void *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double))static vDSP.add<A, B>(_:_:result:), *(double *)(v2 + 32));
}

uint64_t static vDSP.add<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  return static vDSP.add<A, B>(_:_:result:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B>(_:_:result:));
}

uint64_t static vDSP.add<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  return static vDSP.add<A, B>(_:_:result:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B>(_:_:result:));
}

uint64_t static vDSP.add<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.add<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.add<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.add<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.add<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C>(_:_:result:));
}

{
  return static vDSP.add<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C>(_:_:result:));
}

uint64_t closure #1 in static vDSP.add<A, B>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, unint64_t *a10, void (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v16 = __swift_instantiateConcreteTypeFromMangledName(a9);
  uint64_t v17 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a10, a9);
  a11(a3, a4, a1, a5, a6, v16, a7, a8, v17);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  *a2 = result;
  return result;
}

uint64_t static vDSP.subtract<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.subtract<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.subtract<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.subtract<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.subtract<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B, C>(_:_:result:));
}

{
  return static vDSP.subtract<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B, C>(_:_:result:));
}

uint64_t static vDSP.subtract<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, void *))
{
  uint64_t v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  v18[2] = a3;
  v18[3] = a4;
  void v18[4] = a5;
  v18[5] = a6;
  v18[6] = a1;
  v18[7] = a2;
  return a8(v16, a7, v18);
}

uint64_t closure #1 in static vDSP.subtract<A, B>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t *a9, unint64_t *a10, void (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v17 = __swift_instantiateConcreteTypeFromMangledName(a9);
  uint64_t v18 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(a10, a9);
  a11(a3, a4, a1, a5, a6, v17, a7, a8, v18);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
  *a2 = result;
  return result;
}

uint64_t static vDSP.subtract<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v49 = a2;
  uint64_t v50 = a8;
  uint64_t v47 = a7;
  uint64_t v13 = *(void *)(a4 - 8);
  uint64_t v14 = MEMORY[0x1F4188790](a1);
  uint64_t v46 = (char *)&v42 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v16 = MEMORY[0x1F4188790](v14);
  uint64_t v18 = (char *)&v42 - v17;
  uint64_t v20 = *(void *)(v19 - 8);
  MEMORY[0x1F4188790](v16);
  int v22 = (char *)&v42 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v43 = v24;
  uint64_t v44 = v23;
  uint64_t v25 = *(uint64_t (**)(uint64_t))(*(void *)(v24 + 8) + 16);
  uint64_t v45 = v26;
  uint64_t v27 = v25(v26);
  BOOL v28 = *(void (**)(char *, uint64_t, uint64_t))(v20 + 16);
  uint64_t v42 = a1;
  v28(v22, a1, a5);
  unsigned int v29 = *(void (**)(char *, uint64_t, uint64_t))(v13 + 16);
  v29(v18, v49, a4);
  uint64_t v30 = *(uint64_t (**)(uint64_t))(v50 + 16);
  uint64_t v48 = a5;
  uint64_t v31 = a5;
  int64_t v32 = v46;
  uint64_t v33 = v30(v31);
  v29(v32, (uint64_t)v18, a4);
  if (v33 == v27)
  {
    uint64_t v34 = v47;
    BOOL v35 = (*(uint64_t (**)(uint64_t, uint64_t))(v47 + 16))(a4, v47) != v27;
  }
  else
  {
    BOOL v35 = 1;
    uint64_t v34 = v47;
  }
  unint64_t v36 = *(void (**)(char *, uint64_t))(v13 + 8);
  v36(v32, a4);
  v36(v18, a4);
  uint64_t v37 = v48;
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v20 + 8))(v22, v48);
  if (v35)
  {
    __break(1u);
  }
  else
  {
    uint64_t v39 = MEMORY[0x1F4188790](a10);
    *(&v42 - 10) = a4;
    *(&v42 - 9) = v37;
    *(&v42 - 8) = v45;
    *(&v42 - 7) = v34;
    uint64_t v40 = v43;
    *(&v42 - 6) = v50;
    *(&v42 - 5) = v40;
    uint64_t v41 = v42;
    *(&v42 - 4) = v49;
    *(&v42 - 3) = v41;
    *(&v42 - 2) = v27;
    return (*(uint64_t (**)(uint64_t))(v40 + 16))(v39);
  }
  return result;
}

uint64_t static vDSP.multiply<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.add<A>(_:_:)(a1, a2, a3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.multiply<A>(_:_:));
}

{
  return static vDSP.add<A>(_:_:)(a1, a2, a3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.multiply<A>(_:_:));
}

uint64_t closure #1 in static vDSP.add<A>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, void (*a6)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float), float a7)
{
  uint64_t v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  a6(a3, a1, a4, v14, a5, v15, a7);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.multiply<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  return static vDSP.add<A, B>(_:_:result:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B>(_:_:result:));
}

uint64_t closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, void *a3, uint64_t a4, uint64_t (*a5)(void))
{
  if (!a1) {
    goto LABEL_6;
  }
  if (!*a3) {
    goto LABEL_7;
  }
  if (a4 < 0)
  {
    __break(1u);
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  return a5();
}

uint64_t closure #1 in static vDSP.add<A>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, void (*a6)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double), double a7)
{
  uint64_t v14 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v15 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  a6(a3, a1, a4, v14, a5, v15, a7);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.multiply<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  return static vDSP.add<A, B>(_:_:result:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B>(_:_:result:));
}

uint64_t static vDSP.multiply<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.add<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.add<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.multiply<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.add<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(_:_:result:));
}

{
  return static vDSP.add<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(_:_:result:));
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, void *a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, uint64_t, uint64_t))
{
  if (!a3)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (!result)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (*a5)
  {
    if ((a6 & 0x8000000000000000) == 0) {
      return a7(a3, 1, result, 1);
    }
    __break(1u);
    goto LABEL_7;
  }
LABEL_9:
  __break(1u);
  return result;
}

uint64_t static vDSP.divide<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v3 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.divide<A>(_:_:));
}

{
  uint64_t v3;

  uint64_t v3 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.divide<A>(_:_:));
}

{
  return static vDSP.add<A>(_:_:)(a1, a2, a3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.divide<A>(_:_:));
}

{
  return static vDSP.add<A>(_:_:)(a1, a2, a3, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.divide<A>(_:_:));
}

uint64_t closure #1 in static vDSP.divide<A>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, float a6)
{
  uint64_t v12 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v13 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.divide<A, B>(_:_:result:)(a3, a6, a1, a4, v12, a5, v13);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.divide<A, B>(_:_:result:)(uint64_t a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v13 = *(void *)(a4 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v15 = &v21[-((v14 + 15) & 0xFFFFFFFFFFFFFFF0)];
  uint64_t v18 = (*(uint64_t (**)(uint64_t))(*(void *)(v16 + 8) + 16))(v17);
  (*(void (**)(unsigned char *, uint64_t, uint64_t))(v13 + 16))(v15, a1, a4);
  uint64_t v19 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  uint64_t result = (*(uint64_t (**)(unsigned char *, uint64_t))(v13 + 8))(v15, a4);
  if (v19 == v18)
  {
    MEMORY[0x1F4188790](result);
    *(void *)&v21[-64] = a4;
    *(void *)&v21[-56] = a5;
    *(void *)&v21[-48] = a6;
    *(void *)&v21[-40] = a7;
    *(void *)&v21[-32] = a1;
    *(float *)&v21[-24] = a2;
    *(void *)&v21[-16] = v18;
    return (*(uint64_t (**)(void))(a7 + 16))(partial apply for closure #1 in static vDSP.divide<A, B>(_:_:result:));
  }
  else
  {
    __break(1u);
  }
  return result;
}

void closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(const float *a1, int a2, float **a3, vDSP_Length __N, float a5)
{
  if (!a1)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  float v5 = a5;
  if (*a3)
  {
    if ((__N & 0x8000000000000000) == 0)
    {
      vDSP_vsdiv(a1, 1, &v5, *a3, 1, __N);
      return;
    }
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
}

uint64_t closure #1 in static vDSP.divide<A>(_:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, double a6)
{
  uint64_t v12 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v13 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.divide<A, B>(_:_:result:)(a3, a6, a1, a4, v12, a5, v13);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.divide<A, B>(_:_:result:)(uint64_t a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  uint64_t v13 = *(void *)(a4 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v15 = &v21[-((v14 + 15) & 0xFFFFFFFFFFFFFFF0)];
  uint64_t v18 = (*(uint64_t (**)(uint64_t))(*(void *)(v16 + 8) + 16))(v17);
  (*(void (**)(unsigned char *, uint64_t, uint64_t))(v13 + 16))(v15, a1, a4);
  uint64_t v19 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  uint64_t result = (*(uint64_t (**)(unsigned char *, uint64_t))(v13 + 8))(v15, a4);
  if (v19 == v18)
  {
    MEMORY[0x1F4188790](result);
    *(void *)&v21[-64] = a4;
    *(void *)&v21[-56] = a5;
    *(void *)&v21[-48] = a6;
    *(void *)&v21[-40] = a7;
    *(void *)&v21[-32] = a1;
    *(double *)&v21[-24] = a2;
    *(void *)&v21[-16] = v18;
    return (*(uint64_t (**)(void))(a7 + 16))(partial apply for closure #1 in static vDSP.divide<A, B>(_:_:result:));
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, void *a3, uint64_t a4, uint64_t (*a5)(uint64_t, uint64_t))
{
  if (!a1) {
    goto LABEL_6;
  }
  if (!*a3) {
    goto LABEL_7;
  }
  if (a4 < 0)
  {
    __break(1u);
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  return a5(a1, 1);
}

uint64_t static vDSP.add<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(void *, uint64_t *))
{
  uint64_t v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, a4);
}

{
  uint64_t v5;

  uint64_t v5 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  return specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v5, a4);
}

uint64_t static vDSP.divide<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  return static vDSP.add<A, B>(_:_:result:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.divide<A, B>(_:_:result:));
}

uint64_t static vDSP.add<A, B>(_:_:result:)(uint64_t a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  v22[0] = a8;
  uint64_t v14 = *(void *)(a4 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v16 = (char *)v22 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v19 = (*(uint64_t (**)(uint64_t))(*(void *)(v17 + 8) + 16))(v18);
  (*(void (**)(char *, uint64_t, uint64_t))(v14 + 16))(v16, a1, a4);
  uint64_t v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v14 + 8))(v16, a4);
  if (v20 == v19)
  {
    MEMORY[0x1F4188790](result);
    v22[-8] = a4;
    v22[-7] = a5;
    v22[-6] = a6;
    v22[-5] = a7;
    v22[-4] = a1;
    *(float *)&v22[-3] = a2;
    v22[-2] = v19;
    return (*(uint64_t (**)(void))(a7 + 16))(v22[0]);
  }
  else
  {
    __break(1u);
  }
  return result;
}

void closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(const float *__B, int a2, float **a3, vDSP_Length __N, float a5)
{
  uint64_t v6 = *MEMORY[0x1E4F143B8];
  float __A = a5;
  if (!__B) {
    goto LABEL_6;
  }
  if (!*a3) {
    goto LABEL_7;
  }
  if ((__N & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  vDSP_svdiv(&__A, __B, 1, *a3, 1, __N);
}

uint64_t static vDSP.divide<A, B>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  return static vDSP.add<A, B>(_:_:result:)(a1, a7, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.divide<A, B>(_:_:result:));
}

uint64_t static vDSP.add<A, B>(_:_:result:)(uint64_t a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  v22[0] = a8;
  uint64_t v14 = *(void *)(a4 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v16 = (char *)v22 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v19 = (*(uint64_t (**)(uint64_t))(*(void *)(v17 + 8) + 16))(v18);
  (*(void (**)(char *, uint64_t, uint64_t))(v14 + 16))(v16, a1, a4);
  uint64_t v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v14 + 8))(v16, a4);
  if (v20 == v19)
  {
    MEMORY[0x1F4188790](result);
    v22[-8] = a4;
    v22[-7] = a5;
    v22[-6] = a6;
    v22[-5] = a7;
    v22[-4] = a1;
    *(double *)&v22[-3] = a2;
    v22[-2] = v19;
    return (*(uint64_t (**)(void))(a7 + 16))(v22[0]);
  }
  else
  {
    __break(1u);
  }
  return result;
}

void closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(const double *__B, int a2, double **a3, vDSP_Length __N, double a5)
{
  v5[1] = *(double *)MEMORY[0x1E4F143B8];
  v5[0] = a5;
  if (!__B) {
    goto LABEL_6;
  }
  if (!*a3) {
    goto LABEL_7;
  }
  if ((__N & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  vDSP_svdivD(v5, __B, 1, *a3, 1, __N);
}

uint64_t static vDSP.divide<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  return static vDSP.add<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.divide<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.add<A, B>(_:_:)(a1, a2, a3, a4, a5, a6, (uint64_t)partial apply for closure #1 in static vDSP.divide<A, B>(_:_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.divide<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.add<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.divide<A, B, C>(_:_:result:));
}

{
  return static vDSP.add<A, B, C>(_:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.divide<A, B, C>(_:_:result:));
}

uint64_t static vDSP.add<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, void *))
{
  uint64_t v16 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  v18[2] = a3;
  v18[3] = a4;
  void v18[4] = a5;
  v18[5] = a6;
  v18[6] = a1;
  v18[7] = a2;
  return a8(v16, a7, v18);
}

uint64_t static vDSP.add<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  uint64_t v49 = a8;
  uint64_t v51 = a2;
  uint64_t v52 = a7;
  uint64_t v13 = *(void *)(a5 - 8);
  uint64_t v14 = MEMORY[0x1F4188790](a1);
  uint64_t v48 = (char *)&v44 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v16 = MEMORY[0x1F4188790](v14);
  uint64_t v18 = (char *)&v44 - v17;
  uint64_t v20 = *(void *)(v19 - 8);
  MEMORY[0x1F4188790](v16);
  int v22 = (char *)&v44 - ((v21 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v45 = v24;
  uint64_t v46 = v23;
  uint64_t v25 = *(uint64_t (**)(uint64_t))(*(void *)(v24 + 8) + 16);
  uint64_t v47 = v26;
  uint64_t v27 = v25(v26);
  BOOL v28 = *(void (**)(char *, uint64_t, uint64_t))(v20 + 16);
  uint64_t v44 = a1;
  v28(v22, a1, a4);
  unsigned int v29 = *(void (**)(char *, uint64_t, uint64_t))(v13 + 16);
  v29(v18, v51, a5);
  uint64_t v30 = *(uint64_t (**)(uint64_t))(v52 + 16);
  uint64_t v50 = a4;
  uint64_t v31 = a4;
  int64_t v32 = v48;
  uint64_t v33 = v30(v31);
  v29(v32, (uint64_t)v18, a5);
  if (v33 == v27)
  {
    uint64_t v34 = v49;
    BOOL v35 = (*(uint64_t (**)(uint64_t, uint64_t))(v49 + 16))(a5, v49) != v27;
  }
  else
  {
    BOOL v35 = 1;
    uint64_t v34 = v49;
  }
  unint64_t v36 = *(void (**)(char *, uint64_t))(v13 + 8);
  v36(v32, a5);
  v36(v18, a5);
  uint64_t v37 = v50;
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v20 + 8))(v22, v50);
  if (v35)
  {
    __break(1u);
  }
  else
  {
    uint64_t v39 = MEMORY[0x1F4188790](a10);
    *(&v44 - 10) = v37;
    *(&v44 - 9) = a5;
    uint64_t v40 = v52;
    *(&v44 - 8) = v47;
    *(&v44 - 7) = v40;
    uint64_t v42 = v44;
    uint64_t v41 = v45;
    *(&v44 - 6) = v34;
    *(&v44 - 5) = v41;
    uint64_t v43 = v51;
    *(&v44 - 4) = v42;
    *(&v44 - 3) = v43;
    *(&v44 - 2) = v27;
    return (*(uint64_t (**)(uint64_t))(v41 + 16))(v39);
  }
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, void *a5, uint64_t a6, uint64_t (*a7)(void))
{
  if (!result)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (!a3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (*a5)
  {
    if ((a6 & 0x8000000000000000) == 0) {
      return a7();
    }
    __break(1u);
    goto LABEL_7;
  }
LABEL_9:
  __break(1u);
  return result;
}

uint64_t static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  return static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:));
}

{
  return static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:));
}

uint64_t static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13)
{
  uint64_t v48 = a8;
  uint64_t v49 = a4;
  uint64_t v55 = a1;
  uint64_t v56 = a2;
  uint64_t v15 = *(void *)(a6 - 8);
  uint64_t v16 = MEMORY[0x1F4188790](a1);
  uint64_t v18 = (char *)&v44 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v19 = MEMORY[0x1F4188790](v16);
  uint64_t v21 = (char *)&v44 - v20;
  uint64_t v23 = *(void *)(v22 - 8);
  MEMORY[0x1F4188790](v19);
  uint64_t v25 = (char *)&v44 - ((v24 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v45 = v27;
  uint64_t v46 = v26;
  BOOL v28 = *(uint64_t (**)(uint64_t))(*(void *)(v27 + 8) + 16);
  uint64_t v47 = v29;
  uint64_t v30 = v28(v29);
  uint64_t v53 = v23;
  (*(void (**)(char *, uint64_t, uint64_t))(v23 + 16))(v25, v55, a5);
  uint64_t v50 = v15;
  uint64_t v31 = *(void (**)(char *, uint64_t, uint64_t))(v15 + 16);
  v31(v21, v56, a6);
  int64_t v32 = *(uint64_t (**)(uint64_t, uint64_t))(a9 + 16);
  uint64_t v51 = v25;
  uint64_t v54 = a5;
  uint64_t v44 = a9;
  uint64_t v33 = v32(a5, a9);
  uint64_t v52 = v21;
  v31(v18, (uint64_t)v21, a6);
  if (v33 != v30)
  {
    BOOL v35 = *(void (**)(char *, uint64_t))(v50 + 8);
    v35(v18, a6);
    goto LABEL_6;
  }
  uint64_t v34 = (*(uint64_t (**)(uint64_t, uint64_t))(a10 + 16))(a6, a10);
  BOOL v35 = *(void (**)(char *, uint64_t))(v50 + 8);
  v35(v18, a6);
  if (v34 != v30)
  {
LABEL_6:
    v35(v52, a6);
    uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v53 + 8))(v51, v54);
    goto LABEL_7;
  }
  uint64_t v37 = v48;
  uint64_t v36 = v49;
  uint64_t v38 = (*(uint64_t (**)(uint64_t))(*(void *)(a12 + 8) + 16))(v48);
  v35(v52, a6);
  uint64_t v39 = v54;
  uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v53 + 8))(v51, v54);
  if (v38 == v30)
  {
    uint64_t v41 = MEMORY[0x1F4188790](a13);
    *(&v44 - 12) = v39;
    *(&v44 - 11) = a6;
    *(&v44 - 10) = v47;
    *(&v44 - 9) = v37;
    uint64_t v42 = v45;
    *(&v44 - 8) = v44;
    *(&v44 - 7) = a10;
    *(&v44 - 6) = v42;
    *(&v44 - 5) = a12;
    uint64_t v43 = v55;
    *(&v44 - 4) = v36;
    *(&v44 - 3) = v43;
    *(&v44 - 2) = v56;
    *(&v44 - 1) = v30;
    return (*(uint64_t (**)(uint64_t))(v42 + 16))(v41);
  }
LABEL_7:
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, void *a5, void *a6, uint64_t a7, uint64_t (*a8)(uint64_t))
{
  if (!result)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (!a3)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (!*a5)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (*a6)
  {
    if ((a7 & 0x8000000000000000) == 0) {
      return a8(result);
    }
    __break(1u);
    goto LABEL_8;
  }
LABEL_11:
  __break(1u);
  return result;
}

uint64_t static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  return static vDSP.multiply<A, B>(addition:_:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:), a7);
}

uint64_t static vDSP.multiply<A, B, C>(addition:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, uint64_t a10)
{
  return static vDSP.multiply<A, B, C>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:));
}

uint64_t static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  return static vDSP.multiply<A, B>(addition:_:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:), a7);
}

uint64_t static vDSP.multiply<A, B, C>(addition:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, uint64_t a10)
{
  return static vDSP.multiply<A, B, C>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:));
}

uint64_t static vDSP.multiply<A, B, C>(addition:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:), (uint64_t (*)(uint64_t, uint64_t, unsigned char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:), (uint64_t (*)(uint64_t, uint64_t, unsigned char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.multiply<A, B, C, D>(addition:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  return static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (char *)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

{
  return static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (char *)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

uint64_t closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t *a11, unint64_t *a12, void (*a13)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v33 = a4;
  uint64_t v34 = a8;
  uint64_t v36 = a7;
  uint64_t v37 = a2;
  BOOL v35 = a13;
  uint64_t v31 = a9;
  uint64_t v32 = a1;
  uint64_t v30 = a12;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v17 = MEMORY[0x1F4188790](TupleTypeMetadata2 - 8);
  uint64_t v19 = (char *)&v28 - v18;
  uint64_t v20 = *(int *)(v17 + 56);
  uint64_t v21 = &v19[v20];
  uint64_t v22 = a3 + v20;
  uint64_t v23 = *(void *)(a5 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v23 + 16))(v19, a3, a5);
  uint64_t v24 = *(void *)(a6 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v24 + 16))(v21, v22, a6);
  uint64_t v29 = __swift_instantiateConcreteTypeFromMangledName(a11);
  uint64_t v25 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(v30, a11);
  uint64_t v26 = v36;
  v35(v19, v21, v33, v32, a5, a6, v36, v29, v34, v31, a10, v25);
  (*(void (**)(char *, uint64_t))(v24 + 8))(v21, a6);
  (*(void (**)(char *, uint64_t))(v23 + 8))(v19, a5);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a10 + 16))(v26, a10);
  *uint64_t v37 = result;
  return result;
}

uint64_t static vDSP.multiply<A, B>(subtraction:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  return static vDSP.multiply<A, B>(addition:_:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.multiply<A, B>(subtraction:_:), a7);
}

uint64_t closure #1 in static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, void (*a8)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, uint64_t), float a9)
{
  uint64_t v28 = a7;
  uint64_t v29 = a8;
  uint64_t v30 = a2;
  uint64_t v26 = a1;
  uint64_t v27 = a6;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v14 = MEMORY[0x1F4188790](TupleTypeMetadata2 - 8);
  uint64_t v16 = (char *)&v26 - v15;
  uint64_t v17 = *(int *)(v14 + 56);
  uint64_t v18 = &v16[v17];
  uint64_t v19 = a3 + v17;
  uint64_t v20 = *(void *)(a4 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v20 + 16))(v16, a3, a4);
  uint64_t v21 = *(void *)(a5 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v21 + 16))(v18, v19, a5);
  uint64_t v22 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v23 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v24 = v27;
  v29(v16, v18, v26, a4, a5, v22, v27, v28, a9, v23);
  (*(void (**)(char *, uint64_t))(v21 + 8))(v18, a5);
  (*(void (**)(char *, uint64_t))(v20 + 8))(v16, a4);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(v24 + 16))(a4, v24);
  *uint64_t v30 = result;
  return result;
}

uint64_t static vDSP.multiply<A, B, C>(subtraction:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, uint64_t a10)
{
  return static vDSP.multiply<A, B, C>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:));
}

uint64_t static vDSP.multiply<A, B>(subtraction:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  return static vDSP.multiply<A, B>(addition:_:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.multiply<A, B>(subtraction:_:), a7);
}

uint64_t closure #1 in static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, void (*a8)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, uint64_t), double a9)
{
  uint64_t v28 = a7;
  uint64_t v29 = a8;
  uint64_t v30 = a2;
  uint64_t v26 = a1;
  uint64_t v27 = a6;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v14 = MEMORY[0x1F4188790](TupleTypeMetadata2 - 8);
  uint64_t v16 = (char *)&v26 - v15;
  uint64_t v17 = *(int *)(v14 + 56);
  uint64_t v18 = &v16[v17];
  uint64_t v19 = a3 + v17;
  uint64_t v20 = *(void *)(a4 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v20 + 16))(v16, a3, a4);
  uint64_t v21 = *(void *)(a5 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v21 + 16))(v18, v19, a5);
  uint64_t v22 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v23 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v24 = v27;
  v29(v16, v18, v26, a4, a5, v22, v27, v28, a9, v23);
  (*(void (**)(char *, uint64_t))(v21 + 8))(v18, a5);
  (*(void (**)(char *, uint64_t))(v20 + 8))(v16, a4);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(v24 + 16))(a4, v24);
  *uint64_t v30 = result;
  return result;
}

uint64_t static vDSP.multiply<A, B, C>(subtraction:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, uint64_t a10)
{
  return static vDSP.multiply<A, B, C>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:));
}

uint64_t static vDSP.multiply<A, B, C>(subtraction:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:), (uint64_t (*)(uint64_t, uint64_t, unsigned char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:), (uint64_t (*)(uint64_t, uint64_t, unsigned char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.multiply<A, B, C, D>(subtraction:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  return static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (char *)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:));
}

{
  return static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (char *)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:));
}

uint64_t static vDSP.add<A, B>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  return static vDSP.multiply<A, B>(addition:_:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:), a7);
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  char *v18;
  void (*v19)(char *, uint64_t, uint64_t);
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  void (*v23)(char *, uint64_t);
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  char *v33;
  uint64_t v34;

  uint64_t v28 = a5;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v27 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v14 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v16 = (char *)&v25 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1F4188790](v14);
  uint64_t v18 = (char *)&v25 - v17;
  uint64_t v19 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a3 - 8) + 16);
  v19((char *)&v25 - v17, a1, a3);
  *(float *)&v18[*(int *)(TupleTypeMetadata2 + 48)] = a7;
  uint64_t v20 = a6;
  uint64_t v26 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  uint64_t v21 = *(int *)(TupleTypeMetadata2 + 48);
  v19(v16, (uint64_t)v18, a3);
  *(float *)&v16[v21] = a7;
  uint64_t v29 = a3;
  uint64_t v30 = a4;
  uint64_t v31 = v28;
  uint64_t v32 = v20;
  uint64_t v33 = v16;
  uint64_t v34 = a2;
  uint64_t v22 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v26, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:));
  uint64_t v23 = *(void (**)(char *, uint64_t))(v27 + 8);
  v23(v18, TupleTypeMetadata2);
  v23(v16, TupleTypeMetadata2);
  return v22;
}

uint64_t static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(void *, uint64_t *), float a8)
{
  uint64_t v29 = a6;
  uint64_t v30 = a7;
  uint64_t v31 = a5;
  uint64_t v27 = a2;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v28 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v13 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v15 = (char *)&v26 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v16 = MEMORY[0x1F4188790](v13);
  uint64_t v18 = (char *)&v26 - v17;
  uint64_t v26 = (char *)&v26 + *(int *)(v16 + 48) - v17;
  uint64_t v19 = v26;
  uint64_t v20 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a3 - 8) + 16);
  v20((char *)&v26 - v17, a1, a3);
  uint64_t v21 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a4 - 8) + 16);
  v21(v19, v27, a4);
  uint64_t v27 = (*(uint64_t (**)(uint64_t))(v31 + 16))(a3);
  uint64_t v22 = &v15[*(int *)(TupleTypeMetadata2 + 48)];
  v20(v15, (uint64_t)v18, a3);
  v21(v22, (uint64_t)v26, a4);
  uint64_t v32 = a3;
  uint64_t v33 = a4;
  uint64_t v34 = v31;
  uint64_t v35 = v29;
  uint64_t v36 = v15;
  float v37 = a8;
  uint64_t v23 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v27, v30);
  uint64_t v24 = *(void (**)(char *, uint64_t))(v28 + 8);
  v24(v18, TupleTypeMetadata2);
  v24(v15, TupleTypeMetadata2);
  return v23;
}

uint64_t static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, uint64_t a10)
{
  return static vDSP.multiply<A, B, C>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  char *v22;
  void (*v23)(char *, uint64_t, uint64_t);
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t (*v27)(char *, uint64_t);
  uint64_t result;
  uint64_t v29;
  char *v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  void v36[2];
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;

  uint64_t v38 = a2;
  uint64_t v39 = a8;
  uint64_t v42 = a7;
  uint64_t v43 = a6;
  float v37 = a5;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v15 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v16 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  v36[0] = (char *)v36 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = MEMORY[0x1F4188790](v16);
  uint64_t v20 = (char *)v36 - v19;
  MEMORY[0x1F4188790](v18);
  uint64_t v22 = (char *)v36 - v21;
  uint64_t v23 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a4 - 8) + 16);
  v23((char *)v36 - v21, a1, a4);
  *(float *)&v22[*(int *)(TupleTypeMetadata2 + 48)] = a9;
  uint64_t v40 = a10;
  uint64_t v41 = a3;
  uint64_t v24 = (*(uint64_t (**)(uint64_t))(*(void *)(a10 + 8) + 16))(v43);
  uint64_t v25 = *(int *)(TupleTypeMetadata2 + 48);
  v23(v20, (uint64_t)v22, a4);
  *(float *)&v20[v25] = a9;
  uint64_t v26 = (*(uint64_t (**)(uint64_t))(v42 + 16))(a4);
  uint64_t v27 = *(uint64_t (**)(char *, uint64_t))(v15 + 8);
  v36[1] = v15 + 8;
  uint64_t result = v27(v20, TupleTypeMetadata2);
  if (v26 == v24)
  {
    uint64_t v29 = *(int *)(TupleTypeMetadata2 + 48);
    uint64_t v30 = (char *)v36[0];
    uint64_t v31 = ((uint64_t (*)(void, char *, uint64_t))v23)(v36[0], v22, a4);
    *(float *)&v30[v29] = a9;
    MEMORY[0x1F4188790](v31);
    uint64_t v32 = v37;
    v36[-10] = a4;
    v36[-9] = v32;
    uint64_t v33 = v42;
    v36[-8] = v43;
    v36[-7] = v33;
    uint64_t v34 = v40;
    v36[-6] = v39;
    v36[-5] = v34;
    uint64_t v35 = v38;
    v36[-4] = v30;
    v36[-3] = v35;
    v36[-2] = v24;
    (*(void (**)(uint64_t (*)(uint64_t)))(v34 + 16))(partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
    v27(v22, TupleTypeMetadata2);
    return v27(v30, TupleTypeMetadata2);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t static vDSP.multiply<A, B, C>(addition:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, uint64_t a10, uint64_t a11)
{
  uint64_t v69 = a8;
  uint64_t v70 = a2;
  uint64_t v71 = a7;
  uint64_t v72 = a3;
  uint64_t v73 = a6;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v68 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v15 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  unint64_t v62 = (char *)v56 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v17 = MEMORY[0x1F4188790](v15);
  int v74 = (char *)v56 - v18;
  uint64_t v19 = MEMORY[0x1F4188790](v17);
  uint64_t v21 = (char *)v56 - v20;
  uint64_t v22 = MEMORY[0x1F4188790](v19);
  uint64_t v24 = (char *)v56 - v23;
  uint64_t v25 = (char *)v56 + *(int *)(v22 + 48) - v23;
  uint64_t v26 = *(void *)(a4 - 8);
  uint64_t v27 = *(void (**)(char *, uint64_t, uint64_t))(v26 + 16);
  uint64_t v28 = v26 + 16;
  v27((char *)v56 - v23, a1, a4);
  uint64_t v29 = a5;
  uint64_t v64 = a5;
  uint64_t v30 = *(void *)(a5 - 8);
  uint64_t v31 = *(char **)(v30 + 16);
  uint64_t v32 = v30 + 16;
  ((void (*)(char *, uint64_t, uint64_t))v31)(v25, v70, v29);
  uint64_t v63 = a10;
  uint64_t v70 = (*(uint64_t (**)(uint64_t))(*(void *)(a10 + 8) + 16))(v73);
  uint64_t v33 = TupleTypeMetadata2;
  unint64_t v65 = &v21[*(int *)(TupleTypeMetadata2 + 48)];
  uint64_t v34 = v65;
  uint64_t v59 = v24;
  v27(v21, (uint64_t)v24, a4);
  uint64_t v35 = v34;
  vImagePixelCount v61 = v25;
  uint64_t v36 = v25;
  uint64_t v37 = v33;
  uint64_t v38 = v64;
  ((void (*)(char *, char *, uint64_t))v31)(v35, v36, v64);
  uint64_t v39 = (*(uint64_t (**)(uint64_t))(v71 + 16))(a4);
  uint64_t v40 = &v74[*(int *)(v33 + 48)];
  uint64_t v67 = v21;
  uint64_t v57 = a4;
  uint64_t v58 = v28;
  uint64_t v60 = (void (*)(char *, char *, uint64_t))v27;
  ((void (*)(void))v27)();
  uint64_t v41 = v70;
  uint64_t v42 = v65;
  unint64_t v65 = v31;
  v56[1] = v32;
  ((void (*)(char *, char *, uint64_t))v31)(v40, v42, v38);
  BOOL v43 = v39 != v41 || (*(uint64_t (**)(uint64_t))(v69 + 16))(v38) != v41;
  uint64_t v44 = v38;
  uint64_t v45 = *(void (**)(char *, uint64_t))(v68 + 8);
  v45(v74, v37);
  uint64_t result = ((uint64_t (*)(char *, uint64_t))v45)(v67, v37);
  if (v43)
  {
    __break(1u);
  }
  else
  {
    uint64_t v47 = *(int *)(v37 + 48);
    uint64_t v48 = v37;
    uint64_t v49 = v62;
    uint64_t v50 = &v62[v47];
    uint64_t v51 = v59;
    uint64_t v52 = v57;
    v60(v62, v59, v57);
    uint64_t v53 = ((uint64_t (*)(char *, char *, uint64_t))v65)(v50, v61, v44);
    MEMORY[0x1F4188790](v53);
    v56[-10] = v52;
    v56[-9] = v44;
    uint64_t v54 = v71;
    v56[-8] = v73;
    v56[-7] = v54;
    uint64_t v55 = v63;
    v56[-6] = v69;
    v56[-5] = v55;
    v56[-4] = v49;
    *(float *)&v56[-3] = a9;
    v56[-2] = v70;
    (*(void (**)(uint64_t))(v55 + 16))(a11);
    v45(v51, v48);
    return ((uint64_t (*)(char *, uint64_t))v45)(v49, v48);
  }
  return result;
}

uint64_t closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, float a10@<S0>, uint64_t a11, uint64_t a12)
{
  uint64_t v32 = a9;
  uint64_t v29 = a8;
  uint64_t v30 = a3;
  uint64_t v27 = a6;
  uint64_t v28 = a1;
  uint64_t v31 = a12;
  uint64_t v26 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v18 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v19 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v21 = (char *)&v26 - v20;
  uint64_t v22 = *(int *)(v19 + 48);
  uint64_t v23 = &v21[v22];
  uint64_t v24 = a2 + v22;
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a4 - 8) + 16))(v21, a2, a4);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a5 - 8) + 16))(v23, v24, a5);
  uint64_t v34 = a4;
  uint64_t v35 = a5;
  uint64_t v36 = v27;
  uint64_t v37 = a7;
  uint64_t v38 = v29;
  uint64_t v39 = v26;
  uint64_t v40 = v21;
  float v41 = a10;
  uint64_t v42 = v28;
  uint64_t v43 = v30;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(a7 + 24))(v31, v33, MEMORY[0x1E4FBC848] + 8, a4, a7);
  return (*(uint64_t (**)(char *, uint64_t))(v18 + 8))(v21, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(uint64_t a1, float a2, uint64_t a3, uint64_t a4, uint64_t a5, void *a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, float *))
{
  uint64_t v10 = *MEMORY[0x1E4F143B8];
  float v9 = a2;
  if (!a4) {
    goto LABEL_7;
  }
  if (!a1)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (!*a6) {
    goto LABEL_9;
  }
  if (a7 < 0)
  {
    __break(1u);
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  return a8(a4, 1, a1, 1, &v9);
}

uint64_t static vDSP.add<A, B>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  return static vDSP.multiply<A, B>(addition:_:)(a1, a2, a3, a4, a5, a6, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:), a7);
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v14;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  char *v18;
  void (*v19)(char *, uint64_t, uint64_t);
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  void (*v23)(char *, uint64_t);
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  char *v33;
  uint64_t v34;

  uint64_t v28 = a5;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v27 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v14 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v16 = (char *)&v25 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1F4188790](v14);
  uint64_t v18 = (char *)&v25 - v17;
  uint64_t v19 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a3 - 8) + 16);
  v19((char *)&v25 - v17, a1, a3);
  *(double *)&v18[*(int *)(TupleTypeMetadata2 + 48)] = a7;
  uint64_t v20 = a6;
  uint64_t v26 = (*(uint64_t (**)(uint64_t, uint64_t))(a6 + 16))(a4, a6);
  uint64_t v21 = *(int *)(TupleTypeMetadata2 + 48);
  v19(v16, (uint64_t)v18, a3);
  *(double *)&v16[v21] = a7;
  uint64_t v29 = a3;
  uint64_t v30 = a4;
  uint64_t v31 = v28;
  uint64_t v32 = v20;
  uint64_t v33 = v16;
  uint64_t v34 = a2;
  uint64_t v22 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v26, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:));
  uint64_t v23 = *(void (**)(char *, uint64_t))(v27 + 8);
  v23(v18, TupleTypeMetadata2);
  v23(v16, TupleTypeMetadata2);
  return v22;
}

uint64_t static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t (*a7)(void *, uint64_t *), double a8)
{
  uint64_t v29 = a6;
  uint64_t v30 = a7;
  uint64_t v31 = a5;
  uint64_t v27 = a2;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v28 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v13 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v15 = (char *)&v26 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v16 = MEMORY[0x1F4188790](v13);
  uint64_t v18 = (char *)&v26 - v17;
  uint64_t v26 = (char *)&v26 + *(int *)(v16 + 48) - v17;
  uint64_t v19 = v26;
  uint64_t v20 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a3 - 8) + 16);
  v20((char *)&v26 - v17, a1, a3);
  uint64_t v21 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a4 - 8) + 16);
  v21(v19, v27, a4);
  uint64_t v27 = (*(uint64_t (**)(uint64_t))(v31 + 16))(a3);
  uint64_t v22 = &v15[*(int *)(TupleTypeMetadata2 + 48)];
  v20(v15, (uint64_t)v18, a3);
  v21(v22, (uint64_t)v26, a4);
  uint64_t v32 = a3;
  uint64_t v33 = a4;
  uint64_t v34 = v31;
  uint64_t v35 = v29;
  uint64_t v36 = v15;
  double v37 = a8;
  uint64_t v23 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v27, v30);
  uint64_t v24 = *(void (**)(char *, uint64_t))(v28 + 8);
  v24(v18, TupleTypeMetadata2);
  v24(v15, TupleTypeMetadata2);
  return v23;
}

uint64_t static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, uint64_t a10)
{
  return static vDSP.multiply<A, B, C>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  char *v22;
  void (*v23)(char *, uint64_t, uint64_t);
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t (*v27)(char *, uint64_t);
  uint64_t result;
  uint64_t v29;
  char *v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  void v36[2];
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;

  uint64_t v38 = a2;
  uint64_t v39 = a8;
  uint64_t v42 = a7;
  uint64_t v43 = a6;
  double v37 = a5;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v15 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v16 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  v36[0] = (char *)v36 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = MEMORY[0x1F4188790](v16);
  uint64_t v20 = (char *)v36 - v19;
  MEMORY[0x1F4188790](v18);
  uint64_t v22 = (char *)v36 - v21;
  uint64_t v23 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a4 - 8) + 16);
  v23((char *)v36 - v21, a1, a4);
  *(double *)&v22[*(int *)(TupleTypeMetadata2 + 48)] = a9;
  uint64_t v40 = a10;
  float v41 = a3;
  uint64_t v24 = (*(uint64_t (**)(uint64_t))(*(void *)(a10 + 8) + 16))(v43);
  uint64_t v25 = *(int *)(TupleTypeMetadata2 + 48);
  v23(v20, (uint64_t)v22, a4);
  *(double *)&v20[v25] = a9;
  uint64_t v26 = (*(uint64_t (**)(uint64_t))(v42 + 16))(a4);
  uint64_t v27 = *(uint64_t (**)(char *, uint64_t))(v15 + 8);
  v36[1] = v15 + 8;
  uint64_t result = v27(v20, TupleTypeMetadata2);
  if (v26 == v24)
  {
    uint64_t v29 = *(int *)(TupleTypeMetadata2 + 48);
    uint64_t v30 = (char *)v36[0];
    uint64_t v31 = ((uint64_t (*)(void, char *, uint64_t))v23)(v36[0], v22, a4);
    *(double *)&v30[v29] = a9;
    MEMORY[0x1F4188790](v31);
    uint64_t v32 = v37;
    v36[-10] = a4;
    v36[-9] = v32;
    uint64_t v33 = v42;
    v36[-8] = v43;
    v36[-7] = v33;
    uint64_t v34 = v40;
    v36[-6] = v39;
    v36[-5] = v34;
    uint64_t v35 = v38;
    v36[-4] = v30;
    v36[-3] = v35;
    v36[-2] = v24;
    (*(void (**)(uint64_t (*)(uint64_t)))(v34 + 16))(partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
    v27(v22, TupleTypeMetadata2);
    return v27(v30, TupleTypeMetadata2);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t static vDSP.multiply<A, B, C>(addition:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, uint64_t a10, uint64_t a11)
{
  uint64_t v69 = a8;
  uint64_t v70 = a2;
  uint64_t v71 = a7;
  uint64_t v72 = a3;
  uint64_t v73 = a6;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v68 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v15 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  unint64_t v62 = (char *)v56 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v17 = MEMORY[0x1F4188790](v15);
  int v74 = (char *)v56 - v18;
  uint64_t v19 = MEMORY[0x1F4188790](v17);
  uint64_t v21 = (char *)v56 - v20;
  uint64_t v22 = MEMORY[0x1F4188790](v19);
  uint64_t v24 = (char *)v56 - v23;
  uint64_t v25 = (char *)v56 + *(int *)(v22 + 48) - v23;
  uint64_t v26 = *(void *)(a4 - 8);
  uint64_t v27 = *(void (**)(char *, uint64_t, uint64_t))(v26 + 16);
  uint64_t v28 = v26 + 16;
  v27((char *)v56 - v23, a1, a4);
  uint64_t v29 = a5;
  uint64_t v64 = a5;
  uint64_t v30 = *(void *)(a5 - 8);
  uint64_t v31 = *(char **)(v30 + 16);
  uint64_t v32 = v30 + 16;
  ((void (*)(char *, uint64_t, uint64_t))v31)(v25, v70, v29);
  uint64_t v63 = a10;
  uint64_t v70 = (*(uint64_t (**)(uint64_t))(*(void *)(a10 + 8) + 16))(v73);
  uint64_t v33 = TupleTypeMetadata2;
  unint64_t v65 = &v21[*(int *)(TupleTypeMetadata2 + 48)];
  uint64_t v34 = v65;
  uint64_t v59 = v24;
  v27(v21, (uint64_t)v24, a4);
  uint64_t v35 = v34;
  vImagePixelCount v61 = v25;
  uint64_t v36 = v25;
  uint64_t v37 = v33;
  uint64_t v38 = v64;
  ((void (*)(char *, char *, uint64_t))v31)(v35, v36, v64);
  uint64_t v39 = (*(uint64_t (**)(uint64_t))(v71 + 16))(a4);
  uint64_t v40 = &v74[*(int *)(v33 + 48)];
  uint64_t v67 = v21;
  uint64_t v57 = a4;
  uint64_t v58 = v28;
  uint64_t v60 = (void (*)(char *, char *, uint64_t))v27;
  ((void (*)(void))v27)();
  uint64_t v41 = v70;
  uint64_t v42 = v65;
  unint64_t v65 = v31;
  v56[1] = v32;
  ((void (*)(char *, char *, uint64_t))v31)(v40, v42, v38);
  BOOL v43 = v39 != v41 || (*(uint64_t (**)(uint64_t))(v69 + 16))(v38) != v41;
  uint64_t v44 = v38;
  uint64_t v45 = *(void (**)(char *, uint64_t))(v68 + 8);
  v45(v74, v37);
  uint64_t result = ((uint64_t (*)(char *, uint64_t))v45)(v67, v37);
  if (v43)
  {
    __break(1u);
  }
  else
  {
    uint64_t v47 = *(int *)(v37 + 48);
    uint64_t v48 = v37;
    uint64_t v49 = v62;
    uint64_t v50 = &v62[v47];
    uint64_t v51 = v59;
    uint64_t v52 = v57;
    v60(v62, v59, v57);
    uint64_t v53 = ((uint64_t (*)(char *, char *, uint64_t))v65)(v50, v61, v44);
    MEMORY[0x1F4188790](v53);
    v56[-10] = v52;
    v56[-9] = v44;
    uint64_t v54 = v71;
    v56[-8] = v73;
    v56[-7] = v54;
    uint64_t v55 = v63;
    v56[-6] = v69;
    v56[-5] = v55;
    v56[-4] = v49;
    *(double *)&v56[-3] = a9;
    v56[-2] = v70;
    (*(void (**)(uint64_t))(v55 + 16))(a11);
    v45(v51, v48);
    return ((uint64_t (*)(char *, uint64_t))v45)(v49, v48);
  }
  return result;
}

uint64_t closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, double a10@<D0>, uint64_t a11, uint64_t a12)
{
  uint64_t v32 = a9;
  uint64_t v29 = a8;
  uint64_t v30 = a3;
  uint64_t v27 = a6;
  uint64_t v28 = a1;
  uint64_t v31 = a12;
  uint64_t v26 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v18 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v19 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v21 = (char *)&v26 - v20;
  uint64_t v22 = *(int *)(v19 + 48);
  uint64_t v23 = &v21[v22];
  uint64_t v24 = a2 + v22;
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a4 - 8) + 16))(v21, a2, a4);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a5 - 8) + 16))(v23, v24, a5);
  uint64_t v34 = a4;
  uint64_t v35 = a5;
  uint64_t v36 = v27;
  uint64_t v37 = a7;
  uint64_t v38 = v29;
  uint64_t v39 = v26;
  uint64_t v40 = v21;
  double v41 = a10;
  uint64_t v42 = v28;
  uint64_t v43 = v30;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(a7 + 24))(v31, v33, MEMORY[0x1E4FBC848] + 8, a4, a7);
  return (*(uint64_t (**)(char *, uint64_t))(v18 + 8))(v21, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(uint64_t a1, double a2, uint64_t a3, uint64_t a4, uint64_t a5, void *a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, void *))
{
  v9[1] = *MEMORY[0x1E4F143B8];
  *(double *)float v9 = a2;
  if (!a4) {
    goto LABEL_7;
  }
  if (!a1)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (!*a6) {
    goto LABEL_9;
  }
  if (a7 < 0)
  {
    __break(1u);
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  return a8(a4, 1, a1, 1, v9);
}

uint64_t closure #1 in static vDSP.add<A, B>(multiplication:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v26 = a2;
  uint64_t v25 = a7;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v15 = MEMORY[0x1F4188790](TupleTypeMetadata2 - 8);
  uint64_t v17 = (char *)&v24 - v16;
  uint64_t v18 = *(int *)(v15 + 56);
  uint64_t v19 = *(void *)(a5 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v19 + 16))((char *)&v24 - v16, a3, a5);
  float v20 = *(float *)(a3 + v18);
  *(float *)&v17[v18] = v20;
  uint64_t v21 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v22 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.add<A, B, C>(multiplication:_:result:)((uint64_t)v17, a4, a1, a5, a6, v21, v25, a8, v20, v22);
  (*(void (**)(char *, uint64_t))(v19 + 8))(v17, a5);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
  *uint64_t v26 = result;
  return result;
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  double v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t result;
  uint64_t v24;
  uint64_t v25;
  uint64_t *v26;

  uint64_t v26 = a2;
  uint64_t v25 = a7;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v15 = MEMORY[0x1F4188790](TupleTypeMetadata2 - 8);
  uint64_t v17 = (char *)&v24 - v16;
  uint64_t v18 = *(int *)(v15 + 56);
  uint64_t v19 = *(void *)(a5 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v19 + 16))((char *)&v24 - v16, a3, a5);
  float v20 = *(double *)(a3 + v18);
  *(double *)&v17[v18] = v20;
  uint64_t v21 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v22 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.add<A, B, C>(multiplication:_:result:)((uint64_t)v17, a4, a1, a5, a6, v21, v25, a8, v20, v22);
  (*(void (**)(char *, uint64_t))(v19 + 8))(v17, a5);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(a6, a8);
  *uint64_t v26 = result;
  return result;
}

uint64_t closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11)
{
  uint64_t v21 = a6;
  uint64_t v22 = a7;
  uint64_t v24 = a1;
  uint64_t v25 = a4;
  uint64_t v23 = a3;
  uint64_t v26 = a9;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v15 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v16 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v18 = (char *)&v21 - v17;
  uint64_t v19 = *(int *)(v16 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a5 - 8) + 16))((char *)&v21 - v17, a2, a5);
  *(_DWORD *)&v18[v19] = *(_DWORD *)(a2 + v19);
  uint64_t v28 = a5;
  uint64_t v29 = v21;
  uint64_t v30 = v22;
  uint64_t v31 = a8;
  uint64_t v32 = a10;
  uint64_t v33 = a11;
  uint64_t v34 = v23;
  uint64_t v35 = v18;
  uint64_t v36 = v24;
  uint64_t v37 = v25;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t, uint64_t))(a8 + 24))(partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:), v27, MEMORY[0x1E4FBC848] + 8, a5, a8);
  return (*(uint64_t (**)(char *, uint64_t))(v15 + 8))(v18, TupleTypeMetadata2);
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  char v27[16];
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  char *v35;
  uint64_t v36;
  uint64_t v37;

  uint64_t v21 = a6;
  uint64_t v22 = a7;
  uint64_t v24 = a1;
  uint64_t v25 = a4;
  uint64_t v23 = a3;
  uint64_t v26 = a9;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v15 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v16 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v18 = (char *)&v21 - v17;
  uint64_t v19 = *(int *)(v16 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a5 - 8) + 16))((char *)&v21 - v17, a2, a5);
  *(void *)&v18[v19] = *(void *)(a2 + v19);
  uint64_t v28 = a5;
  uint64_t v29 = v21;
  uint64_t v30 = v22;
  uint64_t v31 = a8;
  uint64_t v32 = a10;
  uint64_t v33 = a11;
  uint64_t v34 = v23;
  uint64_t v35 = v18;
  uint64_t v36 = v24;
  uint64_t v37 = v25;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t, uint64_t))(a8 + 24))(partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:), v27, MEMORY[0x1E4FBC848] + 8, a5, a8);
  return (*(uint64_t (**)(char *, uint64_t))(v15 + 8))(v18, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13)
{
  uint64_t v27 = a5;
  uint64_t v28 = a6;
  uint64_t v29 = a9;
  uint64_t v30 = a3;
  uint64_t v25 = a1;
  uint64_t v26 = a2;
  uint64_t v23 = a11;
  uint64_t v24 = a13;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v17 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v18 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  float v20 = (char *)&v23 - v19;
  uint64_t v21 = *(int *)(v18 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a7 - 8) + 16))((char *)&v23 - v19, a4, a7);
  *(_DWORD *)&v20[v21] = *(_DWORD *)(a4 + v21);
  uint64_t v32 = a7;
  uint64_t v33 = a8;
  uint64_t v34 = a10;
  uint64_t v35 = v23;
  uint64_t v36 = a12;
  uint64_t v37 = v24;
  uint64_t v38 = v20;
  uint64_t v39 = v25;
  uint64_t v40 = v26;
  uint64_t v41 = v27;
  uint64_t v42 = v28;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t, uint64_t))(v36 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:), v31, MEMORY[0x1E4FBC848] + 8, a8, v36);
  return (*(uint64_t (**)(char *, uint64_t))(v17 + 8))(v20, TupleTypeMetadata2);
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  char *v20;
  uint64_t v21;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  char v31[16];
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  char *v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;

  uint64_t v27 = a5;
  uint64_t v28 = a6;
  uint64_t v29 = a9;
  uint64_t v30 = a3;
  uint64_t v25 = a1;
  uint64_t v26 = a2;
  uint64_t v23 = a11;
  uint64_t v24 = a13;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v17 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v18 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  float v20 = (char *)&v23 - v19;
  uint64_t v21 = *(int *)(v18 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a7 - 8) + 16))((char *)&v23 - v19, a4, a7);
  *(void *)&v20[v21] = *(void *)(a4 + v21);
  uint64_t v32 = a7;
  uint64_t v33 = a8;
  uint64_t v34 = a10;
  uint64_t v35 = v23;
  uint64_t v36 = a12;
  uint64_t v37 = v24;
  uint64_t v38 = v20;
  uint64_t v39 = v25;
  uint64_t v40 = v26;
  uint64_t v41 = v27;
  uint64_t v42 = v28;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t, uint64_t))(v36 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:), v31, MEMORY[0x1E4FBC848] + 8, a8, v36);
  return (*(uint64_t (**)(char *, uint64_t))(v17 + 8))(v20, TupleTypeMetadata2);
}

void closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(const float *a1, uint64_t a2, uint64_t a3, const float *a4, uint64_t a5, float **a6, vDSP_Length a7)
{
  uint64_t v12 = *MEMORY[0x1E4F143B8];
  float __B = *(float *)(a3 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  if (!a4) {
    goto LABEL_7;
  }
  if (!a1)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (!*a6) {
    goto LABEL_9;
  }
  if ((a7 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  vDSP_vsma(a4, 1, &__B, a1, 1, *a6, 1, a7);
}

void closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(const double *a1, uint64_t a2, uint64_t a3, const double *a4, uint64_t a5, double **a6, vDSP_Length a7)
{
  v11[1] = *(double *)MEMORY[0x1E4F143B8];
  v11[0] = *(double *)(a3 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  if (!a4) {
    goto LABEL_7;
  }
  if (!a1)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (!*a6) {
    goto LABEL_9;
  }
  if ((a7 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  vDSP_vsmaD(a4, 1, v11, a1, 1, *a6, 1, a7);
}

uint64_t static vDSP.add<A, B, C>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:), (uint64_t (*)(uint64_t, uint64_t, unsigned char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:), (uint64_t (*)(uint64_t, uint64_t, unsigned char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.add<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  return static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (char *)partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:));
}

{
  return static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (char *)partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:));
}

uint64_t static vDSP.multiply<A, B, C>(addition:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t (*a11)(uint64_t, uint64_t, unsigned char *))
{
  uint64_t v33 = a6;
  uint64_t v34 = a2;
  uint64_t v35 = a3;
  uint64_t v39 = a8;
  uint64_t v40 = a11;
  uint64_t v37 = a7;
  uint64_t v38 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v36 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v14 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v16 = (char *)&v30 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v17 = MEMORY[0x1F4188790](v14);
  uint64_t v19 = (char *)&v30 - v18;
  uint64_t v31 = (char *)&v30 + *(int *)(v17 + 48) - v18;
  float v20 = v31;
  uint64_t v32 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a4 - 8) + 16);
  v32((char *)&v30 - v18, a1, a4);
  uint64_t v21 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a5 - 8) + 16);
  v21(v20, v34, a5);
  uint64_t v30 = a9;
  uint64_t v22 = v35;
  uint64_t v23 = v33;
  uint64_t v34 = (*(uint64_t (**)(uint64_t, uint64_t))(a9 + 16))(v33, a9);
  uint64_t v24 = &v16[*(int *)(TupleTypeMetadata2 + 48)];
  v32(v16, (uint64_t)v19, a4);
  v21(v24, (uint64_t)v31, a5);
  uint64_t v43 = a4;
  uint64_t v44 = a5;
  uint64_t v45 = v23;
  uint64_t v46 = v37;
  uint64_t v47 = v39;
  uint64_t v48 = v30;
  uint64_t v49 = v16;
  uint64_t v50 = v22;
  uint64_t v25 = v40(v34, v38, v42);
  uint64_t v26 = *(void (**)(char *, uint64_t))(v36 + 8);
  uint64_t v27 = v19;
  uint64_t v28 = TupleTypeMetadata2;
  v26(v27, TupleTypeMetadata2);
  v26(v16, v28);
  return v25;
}

uint64_t static vDSP.multiply<A, B, C, D>(addition:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, char *a13)
{
  uint64_t v72 = a4;
  uint64_t v73 = a8;
  uint64_t v62 = a7;
  uint64_t v63 = a3;
  uint64_t v69 = a10;
  uint64_t v70 = a2;
  uint64_t v71 = a9;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v68 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v17 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  vImagePixelCount v61 = (char *)v56 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v19 = MEMORY[0x1F4188790](v17);
  int v74 = (char *)v56 - v20;
  uint64_t v21 = MEMORY[0x1F4188790](v19);
  uint64_t v23 = (char *)v56 - v22;
  uint64_t v24 = MEMORY[0x1F4188790](v21);
  uint64_t v26 = (char *)v56 - v25;
  uint64_t v27 = (char *)v56 + *(int *)(v24 + 48) - v25;
  uint64_t v28 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a5 - 8) + 16);
  v28((char *)v56 - v25, a1, a5);
  uint64_t v29 = a6;
  uint64_t v65 = a6;
  uint64_t v30 = *(void *)(a6 - 8);
  uint64_t v31 = *(char **)(v30 + 16);
  uint64_t v32 = v30 + 16;
  ((void (*)(char *, uint64_t, uint64_t))v31)(v27, v70, v29);
  uint64_t v64 = a12;
  uint64_t v70 = (*(uint64_t (**)(uint64_t))(*(void *)(a12 + 8) + 16))(v73);
  uint64_t v66 = &v23[*(int *)(TupleTypeMetadata2 + 48)];
  uint64_t v33 = v66;
  uint64_t v58 = v26;
  uint64_t v34 = v26;
  uint64_t v35 = TupleTypeMetadata2;
  uint64_t v36 = v65;
  v28(v23, (uint64_t)v34, a5);
  uint64_t v60 = v27;
  ((void (*)(char *, char *, uint64_t))v31)(v33, v27, v36);
  uint64_t v37 = (*(uint64_t (**)(uint64_t))(v71 + 16))(a5);
  uint64_t v38 = &v74[*(int *)(v35 + 48)];
  uint64_t v67 = v23;
  uint64_t v57 = a5;
  uint64_t v59 = (void (*)(char *, char *, uint64_t))v28;
  ((void (*)(void))v28)();
  uint64_t v39 = v66;
  uint64_t v40 = v70;
  uint64_t v66 = v31;
  v56[1] = v32;
  ((void (*)(char *, char *, uint64_t))v31)(v38, v39, v36);
  BOOL v41 = v37 != v40 || (*(uint64_t (**)(uint64_t))(v69 + 16))(v36) != v40;
  uint64_t v42 = *(void (**)(char *, uint64_t))(v68 + 8);
  v42(v74, v35);
  uint64_t result = ((uint64_t (*)(char *, uint64_t))v42)(v67, v35);
  if (v41)
  {
    __break(1u);
  }
  else
  {
    int v74 = a13;
    uint64_t v44 = *(int *)(v35 + 48);
    uint64_t v45 = v35;
    uint64_t v46 = v61;
    uint64_t v47 = &v61[v44];
    uint64_t v49 = v57;
    uint64_t v48 = v58;
    v59(v61, v58, v57);
    uint64_t v50 = ((uint64_t (*)(char *, char *, uint64_t))v66)(v47, v60, v36);
    MEMORY[0x1F4188790](v50);
    v56[-12] = v49;
    v56[-11] = v36;
    uint64_t v52 = v73;
    uint64_t v51 = v74;
    v56[-10] = v62;
    v56[-9] = v52;
    uint64_t v53 = v69;
    v56[-8] = v71;
    v56[-7] = v53;
    uint64_t v55 = v63;
    uint64_t v54 = v64;
    v56[-6] = a11;
    v56[-5] = v54;
    v56[-4] = v46;
    v56[-3] = v55;
    v56[-2] = v70;
    (*(void (**)(char *))(v54 + 16))(v51);
    v42(v48, v45);
    return ((uint64_t (*)(char *, uint64_t))v42)(v46, v45);
  }
  return result;
}

uint64_t closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, long long a11, uint64_t a12, uint64_t a13)
{
  uint64_t v29 = a8;
  uint64_t v27 = a7;
  uint64_t v30 = a3;
  uint64_t v31 = a4;
  uint64_t v28 = a1;
  uint64_t v33 = a9;
  uint64_t v32 = a13;
  uint64_t v26 = a12;
  long long v25 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v17 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v18 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v20 = (char *)&v25 - v19;
  uint64_t v21 = *(int *)(v18 + 48);
  uint64_t v22 = &v20[v21];
  uint64_t v23 = a2 + v21;
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a5 - 8) + 16))(v20, a2, a5);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a6 - 8) + 16))(v22, v23, a6);
  uint64_t v35 = a5;
  uint64_t v36 = a6;
  uint64_t v37 = v27;
  uint64_t v38 = v29;
  uint64_t v39 = a10;
  long long v40 = v25;
  uint64_t v41 = v26;
  uint64_t v42 = v20;
  uint64_t v43 = v30;
  uint64_t v44 = v28;
  uint64_t v45 = v31;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(v39 + 24))(v32, v34, MEMORY[0x1E4FBC848] + 8, a5, v39);
  return (*(uint64_t (**)(char *, uint64_t))(v17 + 8))(v20, TupleTypeMetadata2);
}

uint64_t static vDSP.subtract<A, B, C>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  return static vDSP.subtract<A, B, C>(multiplication:_:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:), (uint64_t (*)(uint64_t, uint64_t, unsigned char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.subtract<A, B, C>(multiplication:_:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:), (uint64_t (*)(uint64_t, uint64_t, unsigned char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  return static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (char *)partial apply for closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:));
}

{
  return static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (char *)partial apply for closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:));
}

uint64_t static vDSP.subtract<A, B, C>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t (*a11)(uint64_t, uint64_t, unsigned char *))
{
  uint64_t v35 = a4;
  uint64_t v36 = a2;
  uint64_t v37 = a3;
  uint64_t v41 = a8;
  uint64_t v42 = a11;
  uint64_t v39 = a9;
  uint64_t v40 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v38 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v15 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v17 = (char *)&v32 - ((v16 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = MEMORY[0x1F4188790](v15);
  uint64_t v20 = (char *)&v32 - v19;
  uint64_t v33 = (char *)&v32 + *(int *)(v18 + 48) - v19;
  uint64_t v21 = v33;
  uint64_t v34 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a5 - 8) + 16);
  v34((char *)&v32 - v19, a1, a5);
  uint64_t v22 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a6 - 8) + 16);
  v22(v21, v36, a6);
  uint64_t v23 = a7;
  uint64_t v32 = a7;
  uint64_t v24 = *(uint64_t (**)(uint64_t, uint64_t))(a7 + 16);
  uint64_t v25 = v37;
  uint64_t v26 = v35;
  uint64_t v36 = v24(v35, v23);
  uint64_t v27 = &v17[*(int *)(TupleTypeMetadata2 + 48)];
  v34(v17, (uint64_t)v20, a5);
  v22(v27, (uint64_t)v33, a6);
  uint64_t v45 = v26;
  uint64_t v46 = a5;
  uint64_t v47 = a6;
  uint64_t v48 = v32;
  uint64_t v49 = v41;
  uint64_t v50 = v39;
  uint64_t v51 = v17;
  uint64_t v52 = v25;
  uint64_t v28 = v42(v36, v40, v44);
  uint64_t v29 = *(void (**)(char *, uint64_t))(v38 + 8);
  uint64_t v30 = TupleTypeMetadata2;
  v29(v20, TupleTypeMetadata2);
  v29(v17, v30);
  return v28;
}

uint64_t closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t *a11, unint64_t *a12, void (*a13)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v37 = a5;
  uint64_t v38 = a2;
  uint64_t v34 = a1;
  uint64_t v35 = a4;
  uint64_t v36 = a13;
  uint64_t v32 = a10;
  uint64_t v33 = a9;
  uint64_t v31 = a12;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v18 = MEMORY[0x1F4188790](TupleTypeMetadata2 - 8);
  uint64_t v20 = (char *)&v29 - v19;
  uint64_t v21 = *(int *)(v18 + 56);
  uint64_t v22 = &v20[v21];
  uint64_t v23 = a3 + v21;
  uint64_t v24 = *(void *)(a6 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v24 + 16))(v20, a3, a6);
  uint64_t v25 = *(void *)(a7 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v25 + 16))(v22, v23, a7);
  uint64_t v30 = __swift_instantiateConcreteTypeFromMangledName(a11);
  uint64_t v26 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(v31, a11);
  uint64_t v27 = v37;
  v36(v20, v22, v35, v34, v37, a6, a7, v30, a8, v33, v32, v26);
  (*(void (**)(char *, uint64_t))(v25 + 8))(v22, a7);
  (*(void (**)(char *, uint64_t))(v24 + 8))(v20, a6);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a8 + 16))(v27, a8);
  *uint64_t v38 = result;
  return result;
}

uint64_t static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, char *a13)
{
  uint64_t v72 = a4;
  uint64_t v73 = a8;
  uint64_t v62 = a5;
  uint64_t v63 = a3;
  uint64_t v69 = a11;
  uint64_t v70 = a2;
  uint64_t v71 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v68 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v17 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  vImagePixelCount v61 = (char *)v56 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v19 = MEMORY[0x1F4188790](v17);
  int v74 = (char *)v56 - v20;
  uint64_t v21 = MEMORY[0x1F4188790](v19);
  uint64_t v23 = (char *)v56 - v22;
  uint64_t v24 = MEMORY[0x1F4188790](v21);
  uint64_t v26 = (char *)v56 - v25;
  uint64_t v27 = (char *)v56 + *(int *)(v24 + 48) - v25;
  uint64_t v28 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a6 - 8) + 16);
  v28((char *)v56 - v25, a1, a6);
  uint64_t v29 = a7;
  uint64_t v65 = a7;
  uint64_t v30 = *(void *)(a7 - 8);
  uint64_t v31 = *(char **)(v30 + 16);
  uint64_t v32 = v30 + 16;
  ((void (*)(char *, uint64_t, uint64_t))v31)(v27, v70, v29);
  uint64_t v64 = a12;
  uint64_t v70 = (*(uint64_t (**)(uint64_t))(*(void *)(a12 + 8) + 16))(v73);
  uint64_t v66 = &v23[*(int *)(TupleTypeMetadata2 + 48)];
  uint64_t v33 = v66;
  uint64_t v58 = v26;
  uint64_t v34 = v26;
  uint64_t v35 = TupleTypeMetadata2;
  uint64_t v36 = v65;
  v28(v23, (uint64_t)v34, a6);
  uint64_t v60 = v27;
  ((void (*)(char *, char *, uint64_t))v31)(v33, v27, v36);
  uint64_t v37 = (*(uint64_t (**)(uint64_t))(v71 + 16))(a6);
  uint64_t v38 = &v74[*(int *)(v35 + 48)];
  uint64_t v67 = v23;
  uint64_t v57 = a6;
  uint64_t v59 = (void (*)(char *, char *, uint64_t))v28;
  ((void (*)(void))v28)();
  uint64_t v39 = v66;
  uint64_t v40 = v70;
  uint64_t v66 = v31;
  v56[1] = v32;
  ((void (*)(char *, char *, uint64_t))v31)(v38, v39, v36);
  BOOL v41 = v37 != v40 || (*(uint64_t (**)(uint64_t))(v69 + 16))(v36) != v40;
  uint64_t v42 = *(void (**)(char *, uint64_t))(v68 + 8);
  v42(v74, v35);
  uint64_t result = ((uint64_t (*)(char *, uint64_t))v42)(v67, v35);
  if (v41)
  {
    __break(1u);
  }
  else
  {
    int v74 = a13;
    uint64_t v44 = *(int *)(v35 + 48);
    uint64_t v45 = v35;
    uint64_t v46 = v61;
    uint64_t v47 = &v61[v44];
    uint64_t v49 = v57;
    uint64_t v48 = v58;
    v59(v61, v58, v57);
    uint64_t v50 = ((uint64_t (*)(char *, char *, uint64_t))v66)(v47, v60, v36);
    MEMORY[0x1F4188790](v50);
    v56[-12] = v62;
    v56[-11] = v49;
    uint64_t v52 = v73;
    uint64_t v51 = v74;
    v56[-10] = v36;
    v56[-9] = v52;
    uint64_t v53 = v71;
    v56[-8] = a9;
    v56[-7] = v53;
    uint64_t v54 = v64;
    v56[-6] = v69;
    v56[-5] = v54;
    uint64_t v55 = v63;
    v56[-4] = v46;
    v56[-3] = v55;
    v56[-2] = v70;
    (*(void (**)(char *))(v54 + 16))(v51);
    v42(v48, v45);
    return ((uint64_t (*)(char *, uint64_t))v42)(v46, v45);
  }
  return result;
}

uint64_t closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14)
{
  uint64_t v30 = a8;
  uint64_t v29 = a5;
  uint64_t v32 = a3;
  uint64_t v33 = a4;
  uint64_t v31 = a1;
  uint64_t v35 = a9;
  uint64_t v34 = a14;
  uint64_t v28 = a13;
  uint64_t v27 = a12;
  uint64_t v26 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v18 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v19 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v21 = (char *)&v26 - v20;
  uint64_t v22 = *(int *)(v19 + 48);
  uint64_t v23 = &v21[v22];
  uint64_t v24 = a2 + v22;
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a6 - 8) + 16))(v21, a2, a6);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a7 - 8) + 16))(v23, v24, a7);
  uint64_t v37 = v29;
  uint64_t v38 = a6;
  uint64_t v39 = a7;
  uint64_t v40 = v30;
  uint64_t v41 = v26;
  uint64_t v42 = a11;
  uint64_t v43 = v27;
  uint64_t v44 = v28;
  uint64_t v45 = v21;
  uint64_t v46 = v32;
  uint64_t v47 = v31;
  uint64_t v48 = v33;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(v42 + 24))(v34, v36, MEMORY[0x1E4FBC848] + 8, a6, v42);
  return (*(uint64_t (**)(char *, uint64_t))(v18 + 8))(v21, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, void *a7, uint64_t a8, uint64_t (*a9)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  if (!a3)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (!a5)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (!result)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (*a7)
  {
    if ((a8 & 0x8000000000000000) == 0) {
      return a9(a3, 1, a5, 1, result, 1);
    }
    __break(1u);
    goto LABEL_8;
  }
LABEL_11:
  __break(1u);
  return result;
}

uint64_t static vDSP.add<A, B>(multiplication:multiplication:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7, float a8)
{
  uint64_t v44 = a6;
  uint64_t v45 = a5;
  uint64_t v38 = a2;
  uint64_t v40 = a1;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v43 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v13 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v41 = (char *)v37 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1F4188790](v13);
  uint64_t v16 = (char *)v37 - v15;
  uint64_t v17 = swift_getTupleTypeMetadata2();
  uint64_t v42 = *(void *)(v17 - 8);
  uint64_t v18 = MEMORY[0x1F4188790](v17);
  uint64_t v20 = (char *)v37 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v21 = MEMORY[0x1F4188790](v18);
  uint64_t v23 = (char *)v37 - v22;
  uint64_t v24 = *(int *)(v21 + 48);
  uint64_t v39 = v21;
  uint64_t v25 = *(void *)(a3 - 8);
  v37[0] = *(void *)(v25 + 16);
  v37[1] = v25 + 16;
  ((void (*)(char *, uint64_t, uint64_t))v37[0])((char *)v37 - v22, v40, a3);
  *(float *)&v23[v24] = a7;
  uint64_t v26 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a4 - 8) + 16);
  v26(v16, v38, a4);
  *(float *)&v16[*(int *)(TupleTypeMetadata2 + 48)] = a8;
  uint64_t v40 = (*(uint64_t (**)(uint64_t))(v45 + 16))(a3);
  uint64_t v27 = *(int *)(v17 + 48);
  ((void (*)(char *, char *, uint64_t))v37[0])(v20, v23, a3);
  *(float *)&v20[v27] = a7;
  uint64_t v28 = TupleTypeMetadata2;
  uint64_t v29 = *(int *)(TupleTypeMetadata2 + 48);
  uint64_t v30 = v41;
  v26(v41, (uint64_t)v16, a4);
  *(float *)&v30[v29] = a8;
  uint64_t v46 = a3;
  uint64_t v47 = a4;
  uint64_t v48 = v45;
  uint64_t v49 = v44;
  uint64_t v50 = v20;
  uint64_t v51 = v30;
  uint64_t v31 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v40, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.add<A, B>(multiplication:multiplication:));
  uint64_t v32 = *(void (**)(char *, uint64_t))(v43 + 8);
  v32(v16, v28);
  uint64_t v33 = *(void (**)(char *, uint64_t))(v42 + 8);
  uint64_t v34 = v23;
  uint64_t v35 = v39;
  v33(v34, v39);
  v33(v20, v35);
  v32(v30, v28);
  return v31;
}

uint64_t closure #1 in static vDSP.add<A, B>(multiplication:multiplication:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v33 = a7;
  uint64_t v34 = a8;
  uint64_t v35 = a2;
  uint64_t v32 = a1;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v13 = TupleTypeMetadata2 - 8;
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v15 = (char *)v31 - v14;
  uint64_t v16 = swift_getTupleTypeMetadata2();
  uint64_t v17 = MEMORY[0x1F4188790](v16 - 8);
  uint64_t v19 = (char *)v31 - v18;
  uint64_t v20 = *(int *)(v17 + 56);
  uint64_t v21 = *(void *)(a5 - 8);
  uint64_t v22 = *(void (**)(char *, uint64_t, uint64_t))(v21 + 16);
  v31[1] = a3;
  v22((char *)v31 - v18, a3, a5);
  float v23 = *(float *)(a3 + v20);
  *(float *)&v19[v20] = v23;
  uint64_t v24 = *(int *)(v13 + 56);
  uint64_t v25 = *(void *)(a6 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v25 + 16))(v15, a4, a6);
  float v26 = *(float *)(a4 + v24);
  *(float *)&v15[v24] = v26;
  uint64_t v27 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v28 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v29 = v33;
  static vDSP.add<A, B, C>(multiplication:multiplication:result:)((uint64_t)v19, (uint64_t)v15, v32, a5, a6, v27, v33, v34, v23, v26, v28);
  (*(void (**)(char *, uint64_t))(v25 + 8))(v15, a6);
  (*(void (**)(char *, uint64_t))(v21 + 8))(v19, a5);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(v29 + 16))(a5, v29);
  uint64_t *v35 = result;
  return result;
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v13;
  uint64_t v14;
  char *v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;
  char *v19;
  uint64_t v20;
  uint64_t v21;
  void (*v22)(char *, uint64_t, uint64_t);
  double v23;
  uint64_t v24;
  uint64_t v25;
  double v26;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t result;
  void v31[2];
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t *v35;

  uint64_t v33 = a7;
  uint64_t v34 = a8;
  uint64_t v35 = a2;
  uint64_t v32 = a1;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v13 = TupleTypeMetadata2 - 8;
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v15 = (char *)v31 - v14;
  uint64_t v16 = swift_getTupleTypeMetadata2();
  uint64_t v17 = MEMORY[0x1F4188790](v16 - 8);
  uint64_t v19 = (char *)v31 - v18;
  uint64_t v20 = *(int *)(v17 + 56);
  uint64_t v21 = *(void *)(a5 - 8);
  uint64_t v22 = *(void (**)(char *, uint64_t, uint64_t))(v21 + 16);
  v31[1] = a3;
  v22((char *)v31 - v18, a3, a5);
  float v23 = *(double *)(a3 + v20);
  *(double *)&v19[v20] = v23;
  uint64_t v24 = *(int *)(v13 + 56);
  uint64_t v25 = *(void *)(a6 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v25 + 16))(v15, a4, a6);
  float v26 = *(double *)(a4 + v24);
  *(double *)&v15[v24] = v26;
  uint64_t v27 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v28 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v29 = v33;
  static vDSP.add<A, B, C>(multiplication:multiplication:result:)((uint64_t)v19, (uint64_t)v15, v32, a5, a6, v27, v33, v34, v23, v26, v28);
  (*(void (**)(char *, uint64_t))(v25 + 8))(v15, a6);
  (*(void (**)(char *, uint64_t))(v21 + 8))(v19, a5);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(v29 + 16))(a5, v29);
  uint64_t *v35 = result;
  return result;
}

uint64_t static vDSP.add<A, B, C>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, float a10, uint64_t a11)
{
  uint64_t v82 = a8;
  uint64_t v83 = a2;
  uint64_t v89 = a3;
  uint64_t v90 = a6;
  uint64_t v87 = a7;
  uint64_t v88 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  unint64_t v80 = *(void **)(TupleTypeMetadata2 - 8);
  uint64_t v16 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  unint64_t v76 = (char *)v71 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = MEMORY[0x1F4188790](v16);
  BOOL v79 = (char *)v71 - v19;
  uint64_t v20 = MEMORY[0x1F4188790](v18);
  unint64_t v77 = (char *)v71 - v21;
  MEMORY[0x1F4188790](v20);
  float v23 = (char *)v71 - v22;
  uint64_t v24 = swift_getTupleTypeMetadata2();
  uint64_t v91 = *(void *)(v24 - 8);
  uint64_t v25 = MEMORY[0x1F4188790](v24);
  int v75 = (char *)v71 - ((v26 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v27 = MEMORY[0x1F4188790](v25);
  uint64_t v29 = (char *)v71 - v28;
  MEMORY[0x1F4188790](v27);
  uint64_t v31 = (char *)v71 - v30;
  uint64_t v85 = a4;
  uint64_t v32 = *(void *)(a4 - 8);
  uint64_t v33 = *(void (**)(char *, uint64_t, uint64_t))(v32 + 16);
  uint64_t v34 = v32 + 16;
  v33((char *)v71 - v30, a1, a4);
  *(float *)&v31[*(int *)(v24 + 48)] = a9;
  uint64_t v86 = a5;
  uint64_t v35 = *(void *)(a5 - 8);
  uint64_t v36 = *(void (**)(char *, uint64_t, uint64_t))(v35 + 16);
  uint64_t v37 = v35 + 16;
  v36(v23, v83, a5);
  uint64_t v38 = TupleTypeMetadata2;
  *(float *)&v23[*(int *)(TupleTypeMetadata2 + 48)] = a10;
  uint64_t v83 = (*(uint64_t (**)(uint64_t))(*(void *)(v88 + 8) + 16))(v90);
  uint64_t v84 = v24;
  uint64_t v39 = *(int *)(v24 + 48);
  unint64_t v81 = v31;
  uint64_t v40 = v85;
  int v74 = (void (*)(char *, char *, uint64_t))v33;
  uint64_t v73 = v34;
  v33(v29, (uint64_t)v31, v85);
  uint64_t v41 = v29;
  uint64_t v42 = v77;
  *(float *)&v29[v39] = a9;
  uint64_t v43 = *(int *)(v38 + 48);
  v71[0] = v23;
  uint64_t v44 = v86;
  v36(v42, (uint64_t)v23, v86);
  *(float *)&v42[v43] = a10;
  uint64_t v45 = v40;
  uint64_t v46 = v83;
  uint64_t v47 = (*(uint64_t (**)(uint64_t))(v87 + 16))(v45);
  uint64_t v48 = v38;
  uint64_t v49 = *(int *)(v38 + 48);
  uint64_t v50 = v79;
  uint64_t v72 = v36;
  v71[1] = v37;
  v36(v79, (uint64_t)v42, v44);
  *(float *)&v50[v49] = a10;
  BOOL v51 = v47 != v46 || (*(uint64_t (**)(uint64_t))(v82 + 16))(v44) != v46;
  uint64_t v52 = (void (*)(char *, uint64_t))v80[1];
  v52(v50, v48);
  v52(v42, v48);
  uint64_t v54 = v91 + 8;
  uint64_t v53 = *(uint64_t (**)(char *, uint64_t))(v91 + 8);
  uint64_t v55 = v41;
  uint64_t v56 = v84;
  uint64_t result = v53(v55, v84);
  if (v51)
  {
    __break(1u);
  }
  else
  {
    uint64_t v58 = *(int *)(v56 + 48);
    uint64_t v59 = v75;
    uint64_t v91 = v54;
    uint64_t v60 = (void (*)(char *, uint64_t))v53;
    uint64_t v61 = v85;
    v74(v75, v81, v85);
    *(float *)&v59[v58] = a9;
    uint64_t v62 = *(int *)(v48 + 48);
    uint64_t v63 = v76;
    uint64_t v64 = (void (*)(uint64_t, uint64_t))v52;
    uint64_t v65 = v71[0];
    uint64_t v66 = v86;
    uint64_t v67 = ((uint64_t (*)(char *, void, uint64_t))v72)(v76, v71[0], v86);
    unint64_t v80 = v71;
    *(float *)&v63[v62] = a10;
    MEMORY[0x1F4188790](v67);
    v71[-10] = v61;
    v71[-9] = v66;
    uint64_t v69 = v87;
    uint64_t v68 = v88;
    v71[-8] = v90;
    v71[-7] = v69;
    v71[-6] = v82;
    v71[-5] = v68;
    v71[-4] = v59;
    v71[-3] = v63;
    v71[-2] = v83;
    (*(void (**)(uint64_t (*)(uint64_t)))(v68 + 16))(partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:));
    v64(v65, v48);
    uint64_t v70 = v84;
    v60(v81, v84);
    v60(v59, v70);
    return ((uint64_t (*)(char *, uint64_t))v64)(v63, v48);
  }
  return result;
}

uint64_t closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11)
{
  uint64_t v33 = a8;
  uint64_t v34 = a4;
  uint64_t v31 = a7;
  uint64_t v28 = a2;
  uint64_t v32 = a1;
  uint64_t v36 = a9;
  uint64_t v29 = a10;
  uint64_t v30 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v35 = *(void *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v16 = (char *)&v27 - v15;
  uint64_t v17 = swift_getTupleTypeMetadata2();
  uint64_t v18 = *(void *)(v17 - 8);
  uint64_t v19 = MEMORY[0x1F4188790](v17);
  uint64_t v21 = (char *)&v27 - v20;
  uint64_t v22 = *(int *)(v19 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a6 - 8) + 16))((char *)&v27 - v20, a3, a6);
  *(_DWORD *)&v21[v22] = *(_DWORD *)(a3 + v22);
  uint64_t v23 = *(int *)(TupleTypeMetadata2 + 48);
  uint64_t v24 = TupleTypeMetadata2;
  uint64_t v25 = v28;
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a5 - 8) + 16))(v16, v28, a5);
  *(_DWORD *)&v16[v23] = *(_DWORD *)(v25 + v23);
  uint64_t v38 = a5;
  uint64_t v39 = a6;
  uint64_t v40 = v31;
  uint64_t v41 = v33;
  uint64_t v42 = v29;
  uint64_t v43 = v30;
  uint64_t v44 = v21;
  uint64_t v45 = v16;
  uint64_t v46 = v32;
  uint64_t v47 = v34;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t, uint64_t))(v33 + 24))(partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:), v37, MEMORY[0x1E4FBC848] + 8, a5, v33);
  (*(void (**)(char *, uint64_t))(v18 + 8))(v21, v17);
  return (*(uint64_t (**)(char *, uint64_t))(v35 + 8))(v16, v24);
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  char *v16;
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  uint64_t v20;
  char *v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v27;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  char v37[16];
  uint64_t v38;
  uint64_t v39;
  uint64_t v40;
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  char *v44;
  char *v45;
  uint64_t v46;
  uint64_t v47;

  uint64_t v33 = a8;
  uint64_t v34 = a4;
  uint64_t v31 = a7;
  uint64_t v28 = a2;
  uint64_t v32 = a1;
  uint64_t v36 = a9;
  uint64_t v29 = a10;
  uint64_t v30 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v35 = *(void *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v16 = (char *)&v27 - v15;
  uint64_t v17 = swift_getTupleTypeMetadata2();
  uint64_t v18 = *(void *)(v17 - 8);
  uint64_t v19 = MEMORY[0x1F4188790](v17);
  uint64_t v21 = (char *)&v27 - v20;
  uint64_t v22 = *(int *)(v19 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a6 - 8) + 16))((char *)&v27 - v20, a3, a6);
  *(void *)&v21[v22] = *(void *)(a3 + v22);
  uint64_t v23 = *(int *)(TupleTypeMetadata2 + 48);
  uint64_t v24 = TupleTypeMetadata2;
  uint64_t v25 = v28;
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a5 - 8) + 16))(v16, v28, a5);
  *(void *)&v16[v23] = *(void *)(v25 + v23);
  uint64_t v38 = a5;
  uint64_t v39 = a6;
  uint64_t v40 = v31;
  uint64_t v41 = v33;
  uint64_t v42 = v29;
  uint64_t v43 = v30;
  uint64_t v44 = v21;
  uint64_t v45 = v16;
  uint64_t v46 = v32;
  uint64_t v47 = v34;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t, uint64_t))(v33 + 24))(partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:), v37, MEMORY[0x1E4FBC848] + 8, a5, v33);
  (*(void (**)(char *, uint64_t))(v18 + 8))(v21, v17);
  return (*(uint64_t (**)(char *, uint64_t))(v35 + 8))(v16, v24);
}

uint64_t closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13)
{
  uint64_t v36 = a5;
  uint64_t v37 = a6;
  uint64_t v29 = a3;
  uint64_t v34 = a1;
  uint64_t v35 = a2;
  uint64_t v39 = a9;
  uint64_t v33 = a13;
  uint64_t v32 = a12;
  uint64_t v31 = a11;
  uint64_t v30 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v38 = *(void *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v18 = (char *)&v28 - v17;
  uint64_t v19 = swift_getTupleTypeMetadata2();
  uint64_t v20 = *(void *)(v19 - 8);
  uint64_t v21 = MEMORY[0x1F4188790](v19);
  uint64_t v23 = (char *)&v28 - v22;
  uint64_t v24 = *(int *)(v21 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a7 - 8) + 16))((char *)&v28 - v22, a4, a7);
  *(_DWORD *)&v23[v24] = *(_DWORD *)(a4 + v24);
  uint64_t v25 = *(int *)(TupleTypeMetadata2 + 48);
  uint64_t v26 = v29;
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a8 - 8) + 16))(v18, v29, a8);
  *(_DWORD *)&v18[v25] = *(_DWORD *)(v26 + v25);
  uint64_t v41 = a7;
  uint64_t v42 = a8;
  uint64_t v43 = v30;
  uint64_t v44 = v31;
  uint64_t v45 = v32;
  uint64_t v46 = v33;
  uint64_t v47 = v23;
  uint64_t v48 = v18;
  uint64_t v49 = v34;
  uint64_t v50 = v35;
  uint64_t v51 = v36;
  uint64_t v52 = v37;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t, uint64_t))(v32 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:), v40, MEMORY[0x1E4FBC848] + 8, a8, v32);
  (*(void (**)(char *, uint64_t))(v20 + 8))(v23, v19);
  return (*(uint64_t (**)(char *, uint64_t))(v38 + 8))(v18, TupleTypeMetadata2);
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;
  char *v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  uint64_t v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;
  uint64_t v38;
  uint64_t v39;
  char v40[16];
  uint64_t v41;
  uint64_t v42;
  uint64_t v43;
  uint64_t v44;
  uint64_t v45;
  uint64_t v46;
  char *v47;
  char *v48;
  uint64_t v49;
  uint64_t v50;
  uint64_t v51;
  uint64_t v52;

  uint64_t v36 = a5;
  uint64_t v37 = a6;
  uint64_t v29 = a3;
  uint64_t v34 = a1;
  uint64_t v35 = a2;
  uint64_t v39 = a9;
  uint64_t v33 = a13;
  uint64_t v32 = a12;
  uint64_t v31 = a11;
  uint64_t v30 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v38 = *(void *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v18 = (char *)&v28 - v17;
  uint64_t v19 = swift_getTupleTypeMetadata2();
  uint64_t v20 = *(void *)(v19 - 8);
  uint64_t v21 = MEMORY[0x1F4188790](v19);
  uint64_t v23 = (char *)&v28 - v22;
  uint64_t v24 = *(int *)(v21 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a7 - 8) + 16))((char *)&v28 - v22, a4, a7);
  *(void *)&v23[v24] = *(void *)(a4 + v24);
  uint64_t v25 = *(int *)(TupleTypeMetadata2 + 48);
  uint64_t v26 = v29;
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a8 - 8) + 16))(v18, v29, a8);
  *(void *)&v18[v25] = *(void *)(v26 + v25);
  uint64_t v41 = a7;
  uint64_t v42 = a8;
  uint64_t v43 = v30;
  uint64_t v44 = v31;
  uint64_t v45 = v32;
  uint64_t v46 = v33;
  uint64_t v47 = v23;
  uint64_t v48 = v18;
  uint64_t v49 = v34;
  uint64_t v50 = v35;
  uint64_t v51 = v36;
  uint64_t v52 = v37;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t, uint64_t))(v32 + 24))(partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:), v40, MEMORY[0x1E4FBC848] + 8, a8, v32);
  (*(void (**)(char *, uint64_t))(v20 + 8))(v23, v19);
  return (*(uint64_t (**)(char *, uint64_t))(v38 + 8))(v18, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)@<X0>(const float *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, const float *a5@<X4>, uint64_t a6@<X5>, float **a7@<X6>, vDSP_Length a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, long long a12, uint64_t a13)
{
  vDSP_Length v30 = a8;
  uint64_t v28 = a6;
  uint64_t v29 = a7;
  uint64_t v26 = a2;
  uint64_t v27 = a5;
  uint64_t v24 = a9;
  uint64_t v25 = a1;
  long long v23 = a12;
  v22[1] = a13;
  uint64_t v32 = *MEMORY[0x1E4F143B8];
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v16 = *(void *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v18 = (char *)v22 - v17;
  int v19 = *(_DWORD *)(a3 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  uint64_t v20 = *(int *)(TupleTypeMetadata2 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a11 - 8) + 16))(v18, a4, a11);
  *(_DWORD *)&v18[v20] = *(_DWORD *)(a4 + v20);
  int v31 = v19;
  closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)((const float *)&v31, (uint64_t)v18, v27, v28, v25, v26, v29, v30);
  return (*(uint64_t (**)(char *, uint64_t))(v16 + 8))(v18, TupleTypeMetadata2);
}

void closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(const float *a1, uint64_t a2, const float *a3, uint64_t a4, const float *a5, uint64_t a6, float **a7, vDSP_Length a8)
{
  uint64_t v14 = *MEMORY[0x1E4F143B8];
  float __D = *(float *)(a2 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  if (!a3) {
    goto LABEL_7;
  }
  if (!a5)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (!*a7) {
    goto LABEL_9;
  }
  if ((a8 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  vDSP_vsmsma(a3, 1, a1, a5, 1, &__D, *a7, 1, a8);
}

uint64_t static vDSP.add<A, B>(multiplication:multiplication:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7, double a8)
{
  uint64_t v44 = a6;
  uint64_t v45 = a5;
  uint64_t v38 = a2;
  uint64_t v40 = a1;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v43 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v13 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v41 = (char *)v37 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1F4188790](v13);
  uint64_t v16 = (char *)v37 - v15;
  uint64_t v17 = swift_getTupleTypeMetadata2();
  uint64_t v42 = *(void *)(v17 - 8);
  uint64_t v18 = MEMORY[0x1F4188790](v17);
  uint64_t v20 = (char *)v37 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v21 = MEMORY[0x1F4188790](v18);
  long long v23 = (char *)v37 - v22;
  uint64_t v24 = *(int *)(v21 + 48);
  uint64_t v39 = v21;
  uint64_t v25 = *(void *)(a3 - 8);
  v37[0] = *(void *)(v25 + 16);
  v37[1] = v25 + 16;
  ((void (*)(char *, uint64_t, uint64_t))v37[0])((char *)v37 - v22, v40, a3);
  *(double *)&v23[v24] = a7;
  uint64_t v26 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a4 - 8) + 16);
  v26(v16, v38, a4);
  *(double *)&v16[*(int *)(TupleTypeMetadata2 + 48)] = a8;
  uint64_t v40 = (*(uint64_t (**)(uint64_t))(v45 + 16))(a3);
  uint64_t v27 = *(int *)(v17 + 48);
  ((void (*)(char *, char *, uint64_t))v37[0])(v20, v23, a3);
  *(double *)&v20[v27] = a7;
  uint64_t v28 = TupleTypeMetadata2;
  uint64_t v29 = *(int *)(TupleTypeMetadata2 + 48);
  vDSP_Length v30 = v41;
  v26(v41, (uint64_t)v16, a4);
  *(double *)&v30[v29] = a8;
  uint64_t v46 = a3;
  uint64_t v47 = a4;
  uint64_t v48 = v45;
  uint64_t v49 = v44;
  uint64_t v50 = v20;
  uint64_t v51 = v30;
  uint64_t v31 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v40, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.add<A, B>(multiplication:multiplication:));
  uint64_t v32 = *(void (**)(char *, uint64_t))(v43 + 8);
  v32(v16, v28);
  uint64_t v33 = *(void (**)(char *, uint64_t))(v42 + 8);
  uint64_t v34 = v23;
  uint64_t v35 = v39;
  v33(v34, v39);
  v33(v20, v35);
  v32(v30, v28);
  return v31;
}

uint64_t static vDSP.add<A, B, C>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, double a10, uint64_t a11)
{
  uint64_t v82 = a8;
  uint64_t v83 = a2;
  uint64_t v89 = a3;
  uint64_t v90 = a6;
  uint64_t v87 = a7;
  uint64_t v88 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  unint64_t v80 = *(void **)(TupleTypeMetadata2 - 8);
  uint64_t v16 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  unint64_t v76 = (char *)v71 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = MEMORY[0x1F4188790](v16);
  BOOL v79 = (char *)v71 - v19;
  uint64_t v20 = MEMORY[0x1F4188790](v18);
  unint64_t v77 = (char *)v71 - v21;
  MEMORY[0x1F4188790](v20);
  long long v23 = (char *)v71 - v22;
  uint64_t v24 = swift_getTupleTypeMetadata2();
  uint64_t v91 = *(void *)(v24 - 8);
  uint64_t v25 = MEMORY[0x1F4188790](v24);
  int v75 = (char *)v71 - ((v26 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v27 = MEMORY[0x1F4188790](v25);
  uint64_t v29 = (char *)v71 - v28;
  MEMORY[0x1F4188790](v27);
  uint64_t v31 = (char *)v71 - v30;
  uint64_t v85 = a4;
  uint64_t v32 = *(void *)(a4 - 8);
  uint64_t v33 = *(void (**)(char *, uint64_t, uint64_t))(v32 + 16);
  uint64_t v34 = v32 + 16;
  v33((char *)v71 - v30, a1, a4);
  *(double *)&v31[*(int *)(v24 + 48)] = a9;
  uint64_t v86 = a5;
  uint64_t v35 = *(void *)(a5 - 8);
  uint64_t v36 = *(void (**)(char *, uint64_t, uint64_t))(v35 + 16);
  uint64_t v37 = v35 + 16;
  v36(v23, v83, a5);
  uint64_t v38 = TupleTypeMetadata2;
  *(double *)&v23[*(int *)(TupleTypeMetadata2 + 48)] = a10;
  uint64_t v83 = (*(uint64_t (**)(uint64_t))(*(void *)(v88 + 8) + 16))(v90);
  uint64_t v84 = v24;
  uint64_t v39 = *(int *)(v24 + 48);
  unint64_t v81 = v31;
  uint64_t v40 = v85;
  int v74 = (void (*)(char *, char *, uint64_t))v33;
  uint64_t v73 = v34;
  v33(v29, (uint64_t)v31, v85);
  uint64_t v41 = v29;
  uint64_t v42 = v77;
  *(double *)&v29[v39] = a9;
  uint64_t v43 = *(int *)(v38 + 48);
  v71[0] = v23;
  uint64_t v44 = v86;
  v36(v42, (uint64_t)v23, v86);
  *(double *)&v42[v43] = a10;
  uint64_t v45 = v40;
  uint64_t v46 = v83;
  uint64_t v47 = (*(uint64_t (**)(uint64_t))(v87 + 16))(v45);
  uint64_t v48 = v38;
  uint64_t v49 = *(int *)(v38 + 48);
  uint64_t v50 = v79;
  uint64_t v72 = v36;
  v71[1] = v37;
  v36(v79, (uint64_t)v42, v44);
  *(double *)&v50[v49] = a10;
  BOOL v51 = v47 != v46 || (*(uint64_t (**)(uint64_t))(v82 + 16))(v44) != v46;
  uint64_t v52 = (void (*)(char *, uint64_t))v80[1];
  v52(v50, v48);
  v52(v42, v48);
  uint64_t v54 = v91 + 8;
  uint64_t v53 = *(uint64_t (**)(char *, uint64_t))(v91 + 8);
  uint64_t v55 = v41;
  uint64_t v56 = v84;
  uint64_t result = v53(v55, v84);
  if (v51)
  {
    __break(1u);
  }
  else
  {
    uint64_t v58 = *(int *)(v56 + 48);
    uint64_t v59 = v75;
    uint64_t v91 = v54;
    uint64_t v60 = (void (*)(char *, uint64_t))v53;
    uint64_t v61 = v85;
    v74(v75, v81, v85);
    *(double *)&v59[v58] = a9;
    uint64_t v62 = *(int *)(v48 + 48);
    uint64_t v63 = v76;
    uint64_t v64 = (void (*)(uint64_t, uint64_t))v52;
    uint64_t v65 = v71[0];
    uint64_t v66 = v86;
    uint64_t v67 = ((uint64_t (*)(char *, void, uint64_t))v72)(v76, v71[0], v86);
    unint64_t v80 = v71;
    *(double *)&v63[v62] = a10;
    MEMORY[0x1F4188790](v67);
    v71[-10] = v61;
    v71[-9] = v66;
    uint64_t v69 = v87;
    uint64_t v68 = v88;
    v71[-8] = v90;
    v71[-7] = v69;
    v71[-6] = v82;
    v71[-5] = v68;
    v71[-4] = v59;
    v71[-3] = v63;
    v71[-2] = v83;
    (*(void (**)(uint64_t (*)(uint64_t)))(v68 + 16))(partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:));
    v64(v65, v48);
    uint64_t v70 = v84;
    v60(v81, v84);
    v60(v59, v70);
    return ((uint64_t (*)(char *, uint64_t))v64)(v63, v48);
  }
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)@<X0>(const double *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, const double *a5@<X4>, uint64_t a6@<X5>, double **a7@<X6>, vDSP_Length a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, long long a12, uint64_t a13)
{
  vDSP_Length v30 = a8;
  uint64_t v28 = a6;
  uint64_t v29 = a7;
  uint64_t v26 = a2;
  uint64_t v27 = a5;
  uint64_t v24 = a9;
  uint64_t v25 = a1;
  long long v23 = a12;
  v22[1] = a13;
  v31[1] = *(double *)MEMORY[0x1E4F143B8];
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v16 = *(void *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v18 = (char *)v22 - v17;
  double v19 = *(double *)(a3 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  uint64_t v20 = *(int *)(TupleTypeMetadata2 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a11 - 8) + 16))(v18, a4, a11);
  *(void *)&v18[v20] = *(void *)(a4 + v20);
  v31[0] = v19;
  closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(v31, (uint64_t)v18, v27, v28, v25, v26, v29, v30);
  return (*(uint64_t (**)(char *, uint64_t))(v16 + 8))(v18, TupleTypeMetadata2);
}

void closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(const double *a1, uint64_t a2, const double *a3, uint64_t a4, const double *a5, uint64_t a6, double **a7, vDSP_Length a8)
{
  __D[1] = *(double *)MEMORY[0x1E4F143B8];
  __D[0] = *(double *)(a2 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  if (!a3) {
    goto LABEL_7;
  }
  if (!a5)
  {
LABEL_8:
    __break(1u);
LABEL_9:
    __break(1u);
  }
  if (!*a7) {
    goto LABEL_9;
  }
  if ((a8 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  vDSP_vsmsmaD(a3, 1, a1, a5, 1, __D, *a7, 1, a8);
}

uint64_t static vDSP.add<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, unint64_t a10, unint64_t a11, uint64_t a12)
{
  return static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, __PAIR128__(a11, a10), a12, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, __PAIR128__(a11, a10), a12, (uint64_t)partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(char **a1, void (*a2)(char *, char *, char *), uint64_t a3, void (*a4)(char *, char *, char *), uint64_t a5, char *a6, char *a7, char *a8, char *a9, char *a10, uint64_t a11, uint64_t a12, uint64_t a13, char *a14, uint64_t a15)
{
  return static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, (char *)partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, (char *)partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, long long a10, uint64_t a11, uint64_t *a12, unint64_t *a13, void (*a14)(char *, char *, char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, void, void, uint64_t, uint64_t))
{
  uint64_t v65 = a2;
  uint64_t v55 = a8;
  uint64_t v56 = a4;
  uint64_t v66 = a9;
  long long v62 = a10;
  uint64_t v63 = a1;
  uint64_t v64 = a14;
  uint64_t v60 = a13;
  uint64_t v61 = a11;
  uint64_t v59 = a12;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v19 = TupleTypeMetadata2 - 8;
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v21 = (char *)&v50 - v20;
  uint64_t v22 = swift_getTupleTypeMetadata2();
  uint64_t v23 = MEMORY[0x1F4188790](v22 - 8);
  uint64_t v25 = (char *)&v50 - v24;
  uint64_t v26 = *(int *)(v23 + 56);
  uint64_t v27 = &v25[v26];
  uint64_t v28 = a3 + v26;
  uint64_t v54 = a5;
  uint64_t v57 = *(void *)(a5 - 8);
  uint64_t v58 = a3;
  uint64_t v29 = *(void (**)(char *, uint64_t, uint64_t))(v57 + 16);
  uint64_t v50 = v25;
  v29(v25, a3, a5);
  uint64_t v30 = *(void *)(a6 - 8);
  uint64_t v52 = a6;
  uint64_t v53 = v30;
  uint64_t v31 = v27;
  (*(void (**)(char *, uint64_t, uint64_t))(v30 + 16))(v27, v28, a6);
  uint64_t v32 = *(int *)(v19 + 56);
  uint64_t v33 = v21;
  uint64_t v34 = &v21[v32];
  uint64_t v35 = v56 + v32;
  uint64_t v51 = a7;
  uint64_t v36 = *(void *)(a7 - 8);
  (*(void (**)(char *))(v36 + 16))(v21);
  uint64_t v37 = v55;
  uint64_t v38 = *(void *)(v55 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v38 + 16))(v34, v35, v55);
  uint64_t v39 = v59;
  uint64_t v40 = __swift_instantiateConcreteTypeFromMangledName(v59);
  uint64_t v49 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(v60, v39);
  uint64_t v48 = v40;
  uint64_t v41 = v25;
  uint64_t v42 = v31;
  uint64_t v43 = v31;
  uint64_t v44 = v54;
  uint64_t v46 = v51;
  uint64_t v45 = v52;
  v64(v41, v43, v33, v34, v63, v54, v52, v51, v37, v48, v66, v62, *((void *)&v62 + 1), v61, v49);
  (*(void (**)(char *, uint64_t))(v38 + 8))(v34, v37);
  (*(void (**)(char *, uint64_t))(v36 + 8))(v33, v46);
  (*(void (**)(char *, uint64_t))(v53 + 8))(v42, v45);
  (*(void (**)(char *, uint64_t))(v57 + 8))(v50, v44);
  uint64_t result = (*(uint64_t (**)(uint64_t))(v66 + 16))(v44);
  *uint64_t v65 = result;
  return result;
}

uint64_t static vDSP.multiply<A, B, C>(addition:addition:)(uint64_t a1, char *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10)
{
  return static vDSP.multiply<A, B, C>(addition:addition:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:addition:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.multiply<A, B, C>(addition:addition:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:addition:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.multiply<A, B, C, D>(addition:addition:result:)(uint64_t a1, uint64_t a2, char *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13)
{
  return static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, (char *)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:));
}

{
  return static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, (char *)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:));
}

uint64_t static vDSP.multiply<A, B, C>(addition:addition:)(uint64_t a1, char *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t (*a12)(uint64_t, uint64_t, char *))
{
  uint64_t v65 = a8;
  uint64_t v56 = a3;
  uint64_t v57 = a4;
  uint64_t v54 = a2;
  uint64_t v62 = a11;
  uint64_t v63 = a12;
  uint64_t v60 = a9;
  uint64_t v61 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v49 = TupleTypeMetadata2;
  uint64_t v59 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v17 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v55 = (char *)&v48 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1F4188790](v17);
  uint64_t v20 = (char *)&v48 - v19;
  uint64_t v21 = swift_getTupleTypeMetadata2();
  uint64_t v58 = *(void *)(v21 - 8);
  uint64_t v22 = MEMORY[0x1F4188790](v21);
  uint64_t v64 = (char *)&v48 - ((v23 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v24 = MEMORY[0x1F4188790](v22);
  uint64_t v26 = (char *)&v48 - v25;
  uint64_t v27 = *(int *)(v24 + 48);
  uint64_t v53 = v24;
  uint64_t v28 = (char *)&v48 + v27 - v25;
  uint64_t v50 = v28;
  uint64_t v29 = *(void *)(a5 - 8);
  uint64_t v51 = *(void (**)(void))(v29 + 16);
  uint64_t v52 = v29 + 16;
  uint64_t v30 = a1;
  uint64_t v31 = a5;
  ((void (*)(char *, uint64_t, uint64_t))v51)((char *)&v48 - v25, v30, a5);
  uint64_t v32 = a6;
  uint64_t v48 = a6;
  uint64_t v33 = *(void (**)(char *, char *, uint64_t))(*(void *)(a6 - 8) + 16);
  v33(v28, v54, v32);
  uint64_t v54 = &v20[*(int *)(TupleTypeMetadata2 + 48)];
  uint64_t v34 = v54;
  uint64_t v35 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a7 - 8) + 16);
  v35(v20, v56, a7);
  v35(v34, v57, a7);
  uint64_t v57 = (*(uint64_t (**)(uint64_t))(v65 + 16))(a5);
  uint64_t v36 = &v64[*(int *)(v21 + 48)];
  v51();
  uint64_t v37 = v48;
  v33(v36, v50, v48);
  uint64_t v38 = v49;
  uint64_t v39 = v55;
  uint64_t v40 = &v55[*(int *)(v49 + 48)];
  v35(v55, (uint64_t)v20, a7);
  v35(v40, (uint64_t)v54, a7);
  uint64_t v67 = v31;
  uint64_t v68 = v37;
  uint64_t v41 = v64;
  uint64_t v69 = a7;
  uint64_t v70 = v65;
  uint64_t v71 = v60;
  uint64_t v72 = v61;
  uint64_t v73 = v64;
  int v74 = v39;
  uint64_t v42 = v63(v57, v62, v66);
  uint64_t v43 = *(void (**)(char *, uint64_t))(v59 + 8);
  v43(v20, v38);
  uint64_t v44 = *(void (**)(char *, uint64_t))(v58 + 8);
  uint64_t v45 = v26;
  uint64_t v46 = v53;
  v44(v45, v53);
  v44(v41, v46);
  v43(v39, v38);
  return v42;
}

uint64_t closure #1 in static vDSP.multiply<A, B, C>(addition:addition:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t *a11, unint64_t *a12, void (*a13)(char *, char *, char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v60 = a2;
  uint64_t v61 = a8;
  uint64_t v51 = a4;
  uint64_t v58 = a1;
  uint64_t v59 = a13;
  uint64_t v56 = a10;
  uint64_t v57 = a9;
  uint64_t v54 = a11;
  uint64_t v55 = a12;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v18 = TupleTypeMetadata2 - 8;
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v20 = (char *)&v46 - v19;
  uint64_t v21 = swift_getTupleTypeMetadata2();
  uint64_t v22 = MEMORY[0x1F4188790](v21 - 8);
  uint64_t v24 = (char *)&v46 - v23;
  uint64_t v25 = *(int *)(v22 + 56);
  uint64_t v26 = &v24[v25];
  uint64_t v47 = v24;
  uint64_t v48 = &v24[v25];
  uint64_t v27 = a3 + v25;
  uint64_t v50 = a5;
  uint64_t v52 = *(void *)(a5 - 8);
  uint64_t v53 = a3;
  (*(void (**)(char *, uint64_t, uint64_t))(v52 + 16))(v24, a3, a5);
  uint64_t v49 = a6;
  uint64_t v28 = *(void *)(a6 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v28 + 16))(v26, v27, a6);
  uint64_t v29 = *(int *)(v18 + 56);
  uint64_t v30 = v20;
  uint64_t v31 = &v20[v29];
  uint64_t v32 = v51 + v29;
  uint64_t v33 = *(void *)(a7 - 8);
  uint64_t v34 = *(void (**)(char *))(v33 + 16);
  v34(v30);
  ((void (*)(char *, uint64_t, uint64_t))v34)(v31, v32, a7);
  uint64_t v35 = v54;
  uint64_t v36 = __swift_instantiateConcreteTypeFromMangledName(v54);
  uint64_t v45 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(v55, v35);
  uint64_t v37 = v24;
  uint64_t v38 = v48;
  uint64_t v39 = v30;
  uint64_t v40 = v30;
  uint64_t v41 = v49;
  uint64_t v42 = v50;
  v59(v37, v48, v39, v31, v58, v50, v49, a7, v36, v61, v57, v56, v45);
  uint64_t v43 = *(void (**)(char *, uint64_t))(v33 + 8);
  v43(v31, a7);
  v43(v40, a7);
  (*(void (**)(char *, uint64_t))(v28 + 8))(v38, v41);
  (*(void (**)(char *, uint64_t))(v52 + 8))(v47, v42);
  uint64_t result = (*(uint64_t (**)(uint64_t))(v61 + 16))(v42);
  *uint64_t v60 = result;
  return result;
}

uint64_t static vDSP.multiply<A, B, C, D>(addition:addition:result:)(uint64_t a1, uint64_t a2, char *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, char *a14)
{
  uint64_t v113 = a5;
  unint64_t v114 = a3;
  uint64_t v118 = a4;
  uint64_t v119 = a2;
  uint64_t v112 = a13;
  uint64_t v110 = a12;
  uint64_t v104 = a11;
  uint64_t v109 = a10;
  uint64_t v111 = a9;
  uint64_t v115 = a8;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v123 = TupleTypeMetadata2;
  uint64_t v117 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v18 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v102 = (char *)&v92 - ((v19 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v20 = MEMORY[0x1F4188790](v18);
  unint64_t v105 = (char *)&v92 - v21;
  uint64_t v22 = MEMORY[0x1F4188790](v20);
  unint64_t v95 = (char *)&v92 - v23;
  uint64_t v24 = MEMORY[0x1F4188790](v22);
  unint64_t v124 = (char *)&v92 - v25;
  MEMORY[0x1F4188790](v24);
  uint64_t v27 = (char *)&v92 - v26;
  uint64_t v125 = swift_getTupleTypeMetadata2();
  uint64_t v116 = *(void *)(v125 - 8);
  uint64_t v28 = MEMORY[0x1F4188790](v125);
  unint64_t v94 = (char *)&v92 - ((v29 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v30 = MEMORY[0x1F4188790](v28);
  unint64_t v99 = (char *)&v92 - v31;
  uint64_t v32 = MEMORY[0x1F4188790](v30);
  uint64_t v34 = (char *)&v92 - v33;
  uint64_t v35 = MEMORY[0x1F4188790](v32);
  uint64_t v37 = (char *)&v92 - v36;
  unint64_t v107 = (char *)&v92 + *(int *)(v35 + 48) - v36;
  uint64_t v38 = v107;
  uint64_t v39 = *(void *)(a6 - 8);
  uint64_t v40 = *(void (**)(char *, uint64_t, uint64_t))(v39 + 16);
  uint64_t v122 = v39 + 16;
  uint64_t v41 = a1;
  uint64_t v42 = a7;
  v40((char *)&v92 - v36, v41, a6);
  unint64_t v106 = (void (*)(char *, char *, uint64_t))v40;
  uint64_t v43 = *(void *)(a7 - 8);
  uint64_t v44 = *(void (**)(char *, char *, uint64_t))(v43 + 16);
  uint64_t v120 = v43 + 16;
  unint64_t v121 = v44;
  v44(v38, (char *)v119, a7);
  uint64_t v45 = *(int *)(TupleTypeMetadata2 + 48);
  uint64_t v46 = v27;
  unint64_t v108 = v27;
  uint64_t v47 = &v27[v45];
  uint64_t v48 = v115;
  uint64_t v49 = *(void (**)(void))(*(void *)(v115 - 8) + 16);
  uint64_t v119 = *(void *)(v115 - 8) + 16;
  ((void (*)(char *, char *, uint64_t))v49)(v46, v114, v115);
  ((void (*)(char *, uint64_t, uint64_t))v49)(v47, v118, v48);
  uint64_t v118 = (*(uint64_t (**)(uint64_t))(*(void *)(v112 + 8) + 16))(v111);
  unint64_t v98 = &v34[*(int *)(v125 + 48)];
  uint64_t v50 = v98;
  unsigned __int8 v103 = v37;
  v40(v34, (uint64_t)v37, a6);
  uint64_t v51 = v99;
  v121(v50, v107, v42);
  uint64_t v52 = &v124[*(int *)(v123 + 48)];
  v49();
  unint64_t v114 = v52;
  long long v93 = v47;
  unint64_t v100 = v49;
  ((void (*)(char *, char *, uint64_t))v49)(v52, v47, v48);
  uint64_t v53 = v118;
  uint64_t v54 = (*(uint64_t (**)(uint64_t))(v109 + 16))(a6);
  uint64_t v55 = v125;
  uint64_t v56 = &v51[*(int *)(v125 + 48)];
  unsigned int v101 = v34;
  uint64_t v96 = a6;
  v106(v51, v34, a6);
  v121(v56, v98, v42);
  uint64_t v97 = v42;
  BOOL v57 = v54 == v53 && (*(uint64_t (**)(uint64_t))(v104 + 16))(v42) == v53;
  uint64_t v58 = v116 + 8;
  unint64_t v99 = *(char **)(v116 + 8);
  ((void (*)(char *, uint64_t))v99)(v51, v55);
  uint64_t v59 = v123;
  uint64_t v60 = v105;
  uint64_t v61 = &v105[*(int *)(v123 + 48)];
  uint64_t v62 = v115;
  uint64_t v63 = v100;
  ((void (*)(char *, char *, uint64_t))v100)(v105, v124, v115);
  ((void (*)(char *, char *, uint64_t))v63)(v61, v114, v62);
  uint64_t v64 = v110;
  LODWORD(v116) = v57 && (*(uint64_t (**)(uint64_t, uint64_t))(v110 + 16))(v62, v110) == v53;
  uint64_t v65 = v117 + 8;
  unint64_t v105 = *(char **)(v117 + 8);
  ((void (*)(char *, uint64_t))v105)(v60, v59);
  uint64_t v66 = v102;
  uint64_t v67 = &v102[*(int *)(v59 + 48)];
  uint64_t v68 = v124;
  ((void (*)(char *, char *, uint64_t))v63)(v102, v124, v62);
  ((void (*)(char *, char *, uint64_t))v63)(v67, v114, v62);
  if (v116)
  {
    uint64_t v69 = (*(uint64_t (**)(uint64_t, uint64_t))(v64 + 16))(v62, v64);
    uint64_t v70 = (void (*)(char *, uint64_t))v105;
    ((void (*)(char *, uint64_t))v105)(v66, v59);
    v70(v68, v59);
    uint64_t v71 = v125;
    uint64_t v72 = (void (*)(char *, uint64_t))v99;
    uint64_t result = ((uint64_t (*)(char *, uint64_t))v99)(v101, v125);
    if (v69 == v118)
    {
      uint64_t v74 = v62;
      unint64_t v124 = a14;
      uint64_t v75 = *(int *)(v71 + 48);
      uint64_t v116 = v58;
      unint64_t v76 = v94;
      unint64_t v77 = &v94[v75];
      uint64_t v78 = v59;
      uint64_t v79 = v96;
      v106(v94, v103, v96);
      uint64_t v80 = v97;
      v121(v77, v107, v97);
      unint64_t v81 = v95;
      uint64_t v82 = &v95[*(int *)(v78 + 48)];
      uint64_t v117 = v65;
      uint64_t v83 = v108;
      ((void (*)(char *, char *, uint64_t))v63)(v95, v108, v74);
      uint64_t v84 = ((uint64_t (*)(char *, char *, uint64_t))v63)(v82, v93, v74);
      MEMORY[0x1F4188790](v84);
      *(&v92 - 12) = v79;
      *(&v92 - 11) = v80;
      uint64_t v85 = v111;
      uint64_t v86 = v112;
      *(&v92 - 10) = v74;
      *(&v92 - 9) = v85;
      uint64_t v87 = v104;
      *(&v92 - 8) = v109;
      *(&v92 - 7) = v87;
      *(&v92 - 6) = v110;
      *(&v92 - 5) = v86;
      *(&v92 - 4) = (uint64_t)v76;
      *(&v92 - 3) = (uint64_t)v81;
      *(&v92 - 2) = v118;
      (*(void (**)(char *))(v86 + 16))(v124);
      uint64_t v88 = v123;
      uint64_t v89 = (uint64_t (*)(char *, uint64_t))v105;
      ((void (*)(char *, uint64_t))v105)(v83, v123);
      uint64_t v90 = v125;
      v72(v103, v125);
      v72(v76, v90);
      return v89(v81, v88);
    }
  }
  else
  {
    uint64_t v91 = (void (*)(char *, uint64_t))v105;
    ((void (*)(char *, uint64_t))v105)(v66, v59);
    v91(v68, v59);
    uint64_t result = ((uint64_t (*)(char *, uint64_t))v99)(v101, v125);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, long long a11, uint64_t a12, uint64_t a13)
{
  uint64_t v41 = a8;
  uint64_t v42 = a4;
  uint64_t v40 = a1;
  uint64_t v45 = a9;
  uint64_t v44 = a13;
  uint64_t v39 = a12;
  long long v38 = a11;
  uint64_t v37 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v34 = TupleTypeMetadata2;
  uint64_t v43 = *(void *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v20 = (char *)v33 - v19;
  uint64_t v35 = swift_getTupleTypeMetadata2();
  uint64_t v36 = *(void *)(v35 - 8);
  uint64_t v21 = MEMORY[0x1F4188790](v35);
  uint64_t v23 = (char *)v33 - v22;
  uint64_t v24 = *(int *)(v21 + 48);
  uint64_t v25 = &v23[v24];
  uint64_t v26 = a2;
  v33[1] = a2;
  uint64_t v27 = a2 + v24;
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a5 - 8) + 16))(v23, v26, a5);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a6 - 8) + 16))(v25, v27, a6);
  uint64_t v28 = *(int *)(TupleTypeMetadata2 + 48);
  uint64_t v29 = &v20[v28];
  uint64_t v30 = a3 + v28;
  uint64_t v31 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a7 - 8) + 16);
  v31(v20, a3, a7);
  v31(v29, v30, a7);
  uint64_t v47 = a5;
  uint64_t v48 = a6;
  uint64_t v49 = a7;
  uint64_t v50 = v41;
  uint64_t v51 = v37;
  long long v52 = v38;
  uint64_t v53 = v39;
  uint64_t v54 = v23;
  uint64_t v55 = v20;
  uint64_t v56 = v40;
  uint64_t v57 = v42;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(v37 + 24))(v44, v46, MEMORY[0x1E4FBC848] + 8, a5, v37);
  (*(void (**)(char *, uint64_t))(v36 + 8))(v23, v35);
  return (*(uint64_t (**)(char *, uint64_t))(v43 + 8))(v20, v34);
}

uint64_t closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16)
{
  uint64_t v38 = a5;
  uint64_t v39 = a6;
  uint64_t v29 = a4;
  uint64_t v36 = a1;
  uint64_t v37 = a2;
  uint64_t v41 = a9;
  uint64_t v40 = a16;
  uint64_t v35 = a15;
  uint64_t v34 = a14;
  uint64_t v33 = a13;
  uint64_t v32 = a12;
  uint64_t v31 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v30 = TupleTypeMetadata2;
  uint64_t v20 = *(void *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v22 = (char *)&v28 - v21;
  uint64_t v28 = a3 + *(int *)(swift_getTupleTypeMetadata2() + 48);
  uint64_t v23 = *(int *)(TupleTypeMetadata2 + 48);
  uint64_t v24 = &v22[v23];
  uint64_t v25 = v29 + v23;
  uint64_t v26 = *(void (**)(char *))(*(void *)(a10 - 8) + 16);
  v26(v22);
  ((void (*)(char *, uint64_t, uint64_t))v26)(v24, v25, a10);
  uint64_t v43 = a7;
  uint64_t v44 = a8;
  uint64_t v45 = a10;
  uint64_t v46 = v31;
  uint64_t v47 = v32;
  uint64_t v48 = v33;
  uint64_t v49 = v34;
  uint64_t v50 = v35;
  uint64_t v51 = v22;
  uint64_t v52 = v36;
  uint64_t v53 = v37;
  uint64_t v54 = v38;
  uint64_t v55 = v39;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(v33 + 24))(v40, v42, MEMORY[0x1E4FBC848] + 8, a8, v33);
  return (*(uint64_t (**)(char *, uint64_t))(v20 + 8))(v22, v30);
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, long long a12, uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16)
{
  uint64_t v32 = a8;
  uint64_t v37 = a6;
  uint64_t v38 = a7;
  uint64_t v36 = a5;
  uint64_t v34 = a4;
  uint64_t v35 = a2;
  uint64_t v33 = a1;
  uint64_t v40 = a9;
  uint64_t v39 = a16;
  uint64_t v31 = a15;
  uint64_t v30 = a13;
  long long v29 = a12;
  uint64_t v28 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v18 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v19 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v21 = (char *)&v27 - v20;
  uint64_t v22 = *(int *)(v19 + 48);
  uint64_t v23 = &v21[v22];
  uint64_t v27 = a3;
  uint64_t v24 = a3 + v22;
  uint64_t v25 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a11 - 8) + 16);
  v25(v21, a3, a11);
  v25(v23, v24, a11);
  uint64_t v42 = v32;
  uint64_t v43 = v28;
  uint64_t v44 = a11;
  long long v45 = v29;
  uint64_t v46 = v30;
  uint64_t v47 = a14;
  uint64_t v48 = v31;
  uint64_t v49 = v21;
  uint64_t v50 = v34;
  uint64_t v51 = v36;
  uint64_t v52 = v33;
  uint64_t v53 = v35;
  uint64_t v54 = v37;
  uint64_t v55 = v38;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(v47 + 24))(v39, v41, MEMORY[0x1E4FBC848] + 8, a11, v47);
  return (*(uint64_t (**)(char *, uint64_t))(v18 + 8))(v21, TupleTypeMetadata2);
}

uint64_t static vDSP.subtract<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, uint64_t a2, void (*a3)(char *, uint64_t, uint64_t), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  return static vDSP.subtract<A, B, C, D>(multiplication:multiplication:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:multiplication:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.subtract<A, B, C, D>(multiplication:multiplication:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, (uint64_t)partial apply for closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:multiplication:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(void (*a1)(char *, char *, char *), uint64_t a2, void (*a3)(char *, char *, uint64_t), uint64_t a4, uint64_t a5, char *a6, uint64_t a7, char *a8, char *a9, char *a10, uint64_t a11, char *a12, uint64_t a13, uint64_t a14, uint64_t a15)
{
  return static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, (char *)partial apply for closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, (char *)partial apply for closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t static vDSP.subtract<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, uint64_t a2, void (*a3)(char *, uint64_t, uint64_t), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t (*a14)(uint64_t, uint64_t, char *))
{
  uint64_t v64 = a6;
  uint64_t v67 = a4;
  uint64_t v66 = a3;
  uint64_t v65 = a2;
  uint64_t v73 = a13;
  uint64_t v74 = a14;
  uint64_t v70 = a9;
  uint64_t v71 = a10;
  uint64_t v72 = a12;
  uint64_t v76 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v57 = TupleTypeMetadata2;
  uint64_t v69 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v19 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v75 = (char *)&v53 - ((v20 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1F4188790](v19);
  uint64_t v22 = (char *)&v53 - v21;
  uint64_t v23 = swift_getTupleTypeMetadata2();
  uint64_t v68 = *(void *)(v23 - 8);
  uint64_t v24 = MEMORY[0x1F4188790](v23);
  uint64_t v63 = (char *)&v53 - ((v25 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v26 = MEMORY[0x1F4188790](v24);
  uint64_t v28 = (char *)&v53 - v27;
  uint64_t v29 = *(int *)(v26 + 48);
  uint64_t v62 = v26;
  uint64_t v30 = (char *)&v53 + v29 - v27;
  uint64_t v59 = v30;
  uint64_t v31 = *(void *)(a7 - 8);
  uint64_t v60 = *(void (**)(char *, uint64_t, uint64_t))(v31 + 16);
  uint64_t v61 = v31 + 16;
  uint64_t v32 = a1;
  uint64_t v33 = a7;
  v60((char *)&v53 - v27, v32, a7);
  uint64_t v34 = a8;
  uint64_t v58 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a8 - 8) + 16);
  v58(v30, v65, a8);
  uint64_t v35 = *(int *)(TupleTypeMetadata2 + 48);
  uint64_t v36 = v22;
  uint64_t v56 = v22;
  uint64_t v37 = &v22[v35];
  uint64_t v54 = &v22[v35];
  uint64_t v38 = a5;
  uint64_t v39 = *(void *)(a5 - 8);
  uint64_t v55 = *(void (**)(char *, char *, uint64_t))(v39 + 16);
  uint64_t v65 = v39 + 16;
  v55(v36, (char *)v66, a5);
  uint64_t v40 = v64;
  uint64_t v66 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(v64 - 8) + 16);
  v66(v37, v67, v64);
  uint64_t v41 = *(uint64_t (**)(uint64_t))(v76 + 16);
  uint64_t v53 = v33;
  uint64_t v67 = v41(v33);
  uint64_t v42 = v63;
  uint64_t v43 = &v63[*(int *)(v23 + 48)];
  v60(v63, (uint64_t)v28, v33);
  v58(v43, (uint64_t)v59, v34);
  uint64_t v44 = v57;
  long long v45 = v75;
  uint64_t v46 = &v75[*(int *)(v57 + 48)];
  uint64_t v47 = v56;
  v55(v75, v56, v38);
  v66(v46, (uint64_t)v54, v40);
  uint64_t v78 = v38;
  uint64_t v79 = v40;
  uint64_t v80 = v53;
  uint64_t v81 = v34;
  uint64_t v82 = v70;
  uint64_t v83 = v71;
  uint64_t v84 = v76;
  uint64_t v85 = v72;
  uint64_t v86 = v42;
  uint64_t v87 = v45;
  uint64_t v48 = v74(v67, v73, v77);
  uint64_t v49 = *(void (**)(char *, uint64_t))(v69 + 8);
  v49(v47, v44);
  uint64_t v50 = *(void (**)(char *, uint64_t))(v68 + 8);
  uint64_t v51 = v62;
  v50(v28, v62);
  v50(v42, v51);
  v49(v75, v44);
  return v48;
}

uint64_t closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t *a13, unint64_t *a14, void (*a15)(char *, char *, char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  uint64_t v66 = a2;
  uint64_t v55 = a6;
  uint64_t v56 = a4;
  uint64_t v64 = a1;
  uint64_t v65 = a15;
  uint64_t v62 = a10;
  uint64_t v63 = a9;
  uint64_t v67 = a11;
  uint64_t v60 = a14;
  uint64_t v61 = a12;
  uint64_t v58 = a13;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v20 = TupleTypeMetadata2 - 8;
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v22 = (char *)&v49 - v21;
  uint64_t v23 = swift_getTupleTypeMetadata2();
  uint64_t v24 = MEMORY[0x1F4188790](v23 - 8);
  uint64_t v26 = (char *)&v49 - v25;
  uint64_t v27 = *(int *)(v24 + 56);
  uint64_t v28 = &v26[v27];
  uint64_t v51 = &v26[v27];
  uint64_t v53 = v26;
  uint64_t v54 = a7;
  uint64_t v59 = a3;
  uint64_t v29 = a3 + v27;
  uint64_t v57 = *(void *)(a7 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v57 + 16))(v26, a3, a7);
  uint64_t v52 = *(void *)(a8 - 8);
  uint64_t v30 = *(void (**)(char *, uint64_t, uint64_t))(v52 + 16);
  uint64_t v31 = v28;
  uint64_t v32 = a8;
  uint64_t v50 = a8;
  v30(v31, v29, a8);
  uint64_t v33 = *(int *)(v20 + 56);
  uint64_t v34 = &v22[v33];
  uint64_t v35 = v56 + v33;
  uint64_t v36 = a5;
  uint64_t v37 = *(void *)(a5 - 8);
  (*(void (**)(char *))(v37 + 16))(v22);
  uint64_t v38 = v55;
  uint64_t v39 = *(void *)(v55 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v39 + 16))(v34, v35, v55);
  uint64_t v40 = v58;
  uint64_t v41 = __swift_instantiateConcreteTypeFromMangledName(v58);
  uint64_t v42 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(v60, v40);
  uint64_t v48 = v41;
  uint64_t v47 = v32;
  uint64_t v44 = v53;
  uint64_t v43 = v54;
  long long v45 = v51;
  v65(v53, v51, v22, v34, v64, v36, v38, v54, v47, v48, v63, v62, v67, v61, v42);
  (*(void (**)(char *, uint64_t))(v39 + 8))(v34, v38);
  (*(void (**)(char *, uint64_t))(v37 + 8))(v22, v36);
  (*(void (**)(char *, uint64_t))(v52 + 8))(v45, v50);
  (*(void (**)(char *, uint64_t))(v57 + 8))(v44, v43);
  uint64_t result = (*(uint64_t (**)(uint64_t))(v67 + 16))(v43);
  *uint64_t v66 = result;
  return result;
}

uint64_t static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(void (*a1)(char *, char *, char *), uint64_t a2, void (*a3)(char *, char *, uint64_t), uint64_t a4, uint64_t a5, char *a6, uint64_t a7, char *a8, char *a9, char *a10, uint64_t a11, char *a12, uint64_t a13, uint64_t a14, uint64_t a15, char *a16)
{
  uint64_t v131 = a7;
  unint64_t v124 = a3;
  uint64_t v125 = a4;
  uint64_t v123 = a2;
  unint64_t v127 = a1;
  uint64_t v116 = a15;
  uint64_t v117 = a5;
  uint64_t v109 = a14;
  uint64_t v114 = a13;
  uint64_t v108 = a11;
  uint64_t v115 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v119 = TupleTypeMetadata2;
  uint64_t v121 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v19 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  unint64_t v106 = (char *)&v97 - ((v20 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v21 = MEMORY[0x1F4188790](v19);
  unint64_t v105 = (char *)&v97 - v22;
  uint64_t v23 = MEMORY[0x1F4188790](v21);
  unint64_t v99 = (char *)&v97 - v24;
  uint64_t v25 = MEMORY[0x1F4188790](v23);
  uint64_t v118 = (char *)&v97 - v26;
  MEMORY[0x1F4188790](v25);
  uint64_t v28 = (char *)&v97 - v27;
  uint64_t v132 = swift_getTupleTypeMetadata2();
  uint64_t v130 = *(void *)(v132 - 8);
  uint64_t v29 = MEMORY[0x1F4188790](v132);
  unint64_t v98 = (char *)&v97 - ((v30 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v31 = MEMORY[0x1F4188790](v29);
  uint64_t v102 = (char *)&v97 - v32;
  uint64_t v33 = MEMORY[0x1F4188790](v31);
  uint64_t v35 = (char *)&v97 - v34;
  uint64_t v36 = MEMORY[0x1F4188790](v33);
  uint64_t v38 = (char *)&v97 - v37;
  uint64_t v39 = (char *)&v97 + *(int *)(v36 + 48) - v37;
  uint64_t v40 = *((void *)a8 - 1);
  unint64_t v128 = *(void (**)(char *, uint64_t))(v40 + 16);
  uint64_t v112 = v40 + 16;
  ((void (*)(char *, void, char *))v128)((char *)&v97 - v37, v127, a8);
  long long v129 = a9;
  uint64_t v41 = *((void *)a9 - 1);
  uint64_t v42 = *(void (**)(char *, char *, char *))(v41 + 16);
  uint64_t v126 = v41 + 16;
  unint64_t v127 = v42;
  v42(v39, (char *)v123, a9);
  uint64_t v43 = *(int *)(TupleTypeMetadata2 + 48);
  uint64_t v113 = v28;
  uint64_t v44 = &v28[v43];
  uint64_t v111 = &v28[v43];
  long long v45 = a6;
  uint64_t v120 = a6;
  uint64_t v46 = *((void *)a6 - 1);
  uint64_t v47 = *(void (**)(char *, char *, char *))(v46 + 16);
  uint64_t v48 = v46 + 16;
  v47(v28, (char *)v124, v45);
  uint64_t v49 = *(void (**)(char *, uint64_t))(*(void *)(v131 - 8) + 16);
  uint64_t v50 = *(void *)(v131 - 8) + 16;
  v49(v44, v125);
  uint64_t v122 = (*(uint64_t (**)(char *))(*(void *)(v116 + 8) + 16))(v115);
  uint64_t v51 = v132;
  uint64_t v110 = &v35[*(int *)(v132 + 48)];
  uint64_t v52 = v110;
  unint64_t v107 = v38;
  ((void (*)(char *, char *, char *))v128)(v35, v38, a8);
  uint64_t v97 = v39;
  v127(v52, v39, v129);
  uint64_t v53 = &v118[*(int *)(v119 + 48)];
  unsigned __int8 v103 = v47;
  uint64_t v125 = v48;
  ((void (*)(void))v47)();
  unsigned int v101 = v53;
  uint64_t v54 = v102;
  uint64_t v123 = v50;
  unint64_t v124 = (void (*)(char *, char *, uint64_t))v49;
  ((void (*)(char *, char *, uint64_t))v49)(v53, v111, v131);
  uint64_t v55 = (*(uint64_t (**)(char *))(v114 + 16))(a8);
  uint64_t v56 = &v54[*(int *)(v51 + 48)];
  uint64_t v104 = v35;
  uint64_t v57 = v35;
  uint64_t v58 = v122;
  unint64_t v100 = a8;
  ((void (*)(char *, char *, char *))v128)(v54, v57, a8);
  uint64_t v59 = v129;
  v127(v56, v110, v129);
  BOOL v60 = v55 == v58 && (*(uint64_t (**)(char *, uint64_t))(v109 + 16))(v59, v109) == v58;
  uint64_t v61 = *(char **)(v130 + 8);
  v130 += 8;
  uint64_t v110 = v61;
  ((void (*)(char *, uint64_t))v61)(v54, v132);
  uint64_t v63 = v118;
  uint64_t v62 = v119;
  uint64_t v64 = v105;
  uint64_t v65 = &v105[*(int *)(v119 + 48)];
  uint64_t v66 = v120;
  uint64_t v67 = v103;
  v103(v105, v118, v120);
  uint64_t v68 = v101;
  v124(v65, v101, v131);
  if (v60) {
    BOOL v60 = (*(uint64_t (**)(char *))(v108 + 16))(v66) == v58;
  }
  uint64_t v69 = v64;
  uint64_t v70 = v106;
  uint64_t v72 = v121 + 8;
  uint64_t v71 = *(void (**)(char *, uint64_t))(v121 + 8);
  v71(v69, v62);
  uint64_t v73 = &v70[*(int *)(v62 + 48)];
  v67(v70, v63, v66);
  uint64_t v74 = v68;
  uint64_t v75 = v131;
  v124(v73, v74, v131);
  if (v60)
  {
    uint64_t v76 = (uint64_t (*)(uint64_t))*((void *)a12 + 2);
    unint64_t v106 = a12;
    uint64_t v77 = v76(v75);
    v71(v70, v62);
    v71(v63, v62);
    uint64_t result = ((uint64_t (*)(char *, uint64_t))v110)(v104, v132);
    if (v77 == v122)
    {
      uint64_t v118 = a16;
      uint64_t v121 = v72;
      uint64_t v79 = v98;
      uint64_t v80 = &v98[*(int *)(v132 + 48)];
      uint64_t v81 = v100;
      ((void (*)(char *, char *, char *))v128)(v98, v107, v100);
      unint64_t v128 = v71;
      uint64_t v82 = v62;
      uint64_t v83 = v129;
      v127(v80, v97, v129);
      uint64_t v84 = *(int *)(v62 + 48);
      uint64_t v85 = v99;
      uint64_t v86 = &v99[v84];
      uint64_t v87 = v113;
      v67(v99, v113, v66);
      uint64_t v88 = ((uint64_t (*)(char *, char *, uint64_t))v124)(v86, v111, v75);
      MEMORY[0x1F4188790](v88);
      *(&v97 - 14) = v66;
      *(&v97 - 13) = v89;
      *(&v97 - 12) = v81;
      *(&v97 - 11) = v83;
      uint64_t v90 = v116;
      uint64_t v91 = (char *)v108;
      *(&v97 - 10) = v115;
      *(&v97 - 9) = v91;
      uint64_t v92 = (char *)v114;
      *(&v97 - 8) = v106;
      *(&v97 - 7) = v92;
      *(&v97 - 6) = (char *)v109;
      *(&v97 - 5) = (char *)v90;
      *(&v97 - 4) = v79;
      *(&v97 - 3) = v85;
      *(&v97 - 2) = (char *)v122;
      (*(void (**)(char *))(v90 + 16))(v118);
      long long v93 = v87;
      unint64_t v94 = (uint64_t (*)(char *, uint64_t))v128;
      v128(v93, v82);
      uint64_t v95 = v132;
      uint64_t v96 = v110;
      ((void (*)(char *, uint64_t))v110)(v107, v132);
      ((void (*)(char *, uint64_t))v96)(v79, v95);
      return v94(v85, v82);
    }
  }
  else
  {
    v71(v70, v62);
    v71(v63, v62);
    uint64_t result = ((uint64_t (*)(char *, uint64_t))v110)(v104, v132);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, long long a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15)
{
  uint64_t v43 = a4;
  uint64_t v36 = a3;
  uint64_t v42 = a1;
  uint64_t v46 = a9;
  uint64_t v45 = a15;
  uint64_t v41 = a14;
  uint64_t v40 = a13;
  uint64_t v39 = a12;
  uint64_t v38 = a11;
  long long v37 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v44 = *(void *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v21 = (char *)v34 - v20;
  uint64_t v35 = swift_getTupleTypeMetadata2();
  uint64_t v22 = *(void *)(v35 - 8);
  uint64_t v23 = MEMORY[0x1F4188790](v35);
  uint64_t v25 = (char *)v34 - v24;
  uint64_t v26 = *(int *)(v23 + 48);
  uint64_t v27 = &v25[v26];
  uint64_t v28 = a2;
  v34[1] = a2;
  uint64_t v29 = a2 + v26;
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a7 - 8) + 16))(v25, v28, a7);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a8 - 8) + 16))(v27, v29, a8);
  uint64_t v30 = *(int *)(TupleTypeMetadata2 + 48);
  uint64_t v31 = &v21[v30];
  uint64_t v32 = v36 + v30;
  (*(void (**)(char *))(*(void *)(a5 - 8) + 16))(v21);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a6 - 8) + 16))(v31, v32, a6);
  uint64_t v48 = a5;
  uint64_t v49 = a6;
  uint64_t v50 = a7;
  uint64_t v51 = a8;
  long long v52 = v37;
  uint64_t v53 = v38;
  uint64_t v54 = v39;
  uint64_t v55 = v40;
  uint64_t v56 = v41;
  uint64_t v57 = v25;
  uint64_t v58 = v21;
  uint64_t v59 = v42;
  uint64_t v60 = v43;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(v39 + 24))(v45, v47, MEMORY[0x1E4FBC848] + 8, a7, v39);
  (*(void (**)(char *, uint64_t))(v22 + 8))(v25, v35);
  return (*(uint64_t (**)(char *, uint64_t))(v44 + 8))(v21, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, long long a12, uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16, uint64_t a17)
{
  uint64_t v37 = a5;
  uint64_t v38 = a6;
  uint64_t v36 = a2;
  uint64_t v35 = a1;
  uint64_t v40 = a9;
  uint64_t v39 = a17;
  uint64_t v34 = a16;
  uint64_t v33 = a15;
  uint64_t v32 = a14;
  uint64_t v31 = a13;
  long long v30 = a12;
  v29[0] = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v22 = *(void *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v24 = (char *)v29 - v23;
  v29[1] = a3 + *(int *)(swift_getTupleTypeMetadata2() + 48);
  uint64_t v25 = *(int *)(TupleTypeMetadata2 + 48);
  uint64_t v26 = &v24[v25];
  uint64_t v27 = a4 + v25;
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a7 - 8) + 16))(v24, a4, a7);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a8 - 8) + 16))(v26, v27, a8);
  uint64_t v42 = a7;
  uint64_t v43 = a8;
  uint64_t v44 = v29[0];
  uint64_t v45 = a11;
  long long v46 = v30;
  uint64_t v47 = v31;
  uint64_t v48 = v32;
  uint64_t v49 = v33;
  uint64_t v50 = v34;
  uint64_t v51 = v24;
  uint64_t v52 = v35;
  uint64_t v53 = v36;
  uint64_t v54 = v37;
  uint64_t v55 = v38;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(v33 + 24))(v39, v41, MEMORY[0x1E4FBC848] + 8, a11, v33);
  return (*(uint64_t (**)(char *, uint64_t))(v22 + 8))(v24, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, long long a11, uint64_t a12, uint64_t a13, long long a14, uint64_t a15, uint64_t a16, uint64_t a17)
{
  uint64_t v38 = a7;
  uint64_t v37 = a6;
  uint64_t v36 = a5;
  uint64_t v34 = a4;
  uint64_t v35 = a2;
  uint64_t v33 = a1;
  uint64_t v40 = a9;
  uint64_t v39 = a17;
  uint64_t v32 = a16;
  uint64_t v31 = a15;
  long long v30 = a14;
  uint64_t v29 = a12;
  long long v28 = a11;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v20 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v21 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v23 = (char *)&v28 - v22;
  uint64_t v24 = *(int *)(v21 + 48);
  uint64_t v25 = &v23[v24];
  uint64_t v26 = a3 + v24;
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a8 - 8) + 16))(v23, a3, a8);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a10 - 8) + 16))(v25, v26, a10);
  uint64_t v42 = a8;
  uint64_t v43 = a10;
  long long v44 = v28;
  uint64_t v45 = v29;
  uint64_t v46 = a13;
  long long v47 = v30;
  uint64_t v48 = v31;
  uint64_t v49 = v32;
  uint64_t v50 = v23;
  uint64_t v51 = v34;
  uint64_t v52 = v36;
  uint64_t v53 = v33;
  uint64_t v54 = v35;
  uint64_t v55 = v37;
  uint64_t v56 = v38;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(v46 + 24))(v39, v41, MEMORY[0x1E4FBC848] + 8, a8, v46);
  return (*(uint64_t (**)(char *, uint64_t))(v20 + 8))(v23, TupleTypeMetadata2);
}

uint64_t static vDSP.multiply<A, B, C, D>(subtraction:subtraction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, unint64_t a10, unint64_t a11, uint64_t a12)
{
  return static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, __PAIR128__(a11, a10), a12, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:subtraction:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, __PAIR128__(a11, a10), a12, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:subtraction:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:)(char **a1, void (*a2)(char *, char *, char *), uint64_t a3, void (*a4)(char *, char *, char *), uint64_t a5, char *a6, char *a7, char *a8, char *a9, char *a10, uint64_t a11, uint64_t a12, uint64_t a13, char *a14, uint64_t a15)
{
  return static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, (char *)partial apply for closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:));
}

{
  return static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, (char *)partial apply for closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:));
}

uint64_t static vDSP.multiply<A, B, C, D>(addition:subtraction:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, unint64_t a10, unint64_t a11, uint64_t a12)
{
  return static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, __PAIR128__(a11, a10), a12, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:subtraction:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, __PAIR128__(a11, a10), a12, (uint64_t)partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:subtraction:), (uint64_t (*)(uint64_t, uint64_t, char *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(char **a1, void (*a2)(char *, char *, char *), uint64_t a3, void (*a4)(char *, char *, char *), uint64_t a5, char *a6, char *a7, char *a8, char *a9, char *a10, uint64_t a11, uint64_t a12, uint64_t a13, char *a14, uint64_t a15)
{
  return static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, (char *)partial apply for closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:));
}

{
  return static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, (char *)partial apply for closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:));
}

uint64_t static vDSP.add<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, long long a10, uint64_t a11, uint64_t a12, uint64_t (*a13)(uint64_t, uint64_t, char *))
{
  uint64_t v68 = a8;
  uint64_t v67 = a7;
  uint64_t v72 = a4;
  uint64_t v70 = a3;
  uint64_t v69 = a2;
  uint64_t v77 = a12;
  uint64_t v78 = a13;
  uint64_t v76 = a11;
  long long v75 = a10;
  uint64_t v79 = a9;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v61 = TupleTypeMetadata2;
  uint64_t v74 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v17 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v71 = (char *)&v55 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1F4188790](v17);
  uint64_t v20 = (char *)&v55 - v19;
  uint64_t v21 = swift_getTupleTypeMetadata2();
  uint64_t v73 = *(void *)(v21 - 8);
  uint64_t v22 = MEMORY[0x1F4188790](v21);
  uint64_t v24 = (char *)&v55 - ((v23 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v25 = MEMORY[0x1F4188790](v22);
  uint64_t v27 = (char *)&v55 - v26;
  uint64_t v28 = *(int *)(v25 + 48);
  uint64_t v66 = v25;
  uint64_t v29 = (char *)&v55 + v28 - v26;
  uint64_t v63 = v29;
  uint64_t v30 = *(void *)(a5 - 8);
  uint64_t v64 = *(void (**)(char *, uint64_t, uint64_t))(v30 + 16);
  uint64_t v65 = v30 + 16;
  uint64_t v31 = a1;
  uint64_t v32 = a5;
  v64((char *)&v55 - v26, v31, a5);
  uint64_t v60 = a6;
  uint64_t v62 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a6 - 8) + 16);
  v62(v29, v69, a6);
  uint64_t v33 = *(int *)(TupleTypeMetadata2 + 48);
  uint64_t v59 = v20;
  uint64_t v34 = &v20[v33];
  uint64_t v57 = &v20[v33];
  uint64_t v35 = v67;
  uint64_t v36 = *(void *)(v67 - 8);
  uint64_t v58 = *(void (**)(char *, uint64_t, uint64_t))(v36 + 16);
  uint64_t v69 = v36 + 16;
  v58(v20, v70, v67);
  uint64_t v37 = v68;
  uint64_t v38 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(v68 - 8) + 16);
  v38(v34, v72, v68);
  uint64_t v39 = *(uint64_t (**)(uint64_t))(v79 + 16);
  uint64_t v55 = v32;
  uint64_t v72 = v39(v32);
  uint64_t v40 = *(int *)(v21 + 48);
  uint64_t v56 = v24;
  uint64_t v41 = &v24[v40];
  v64(v24, (uint64_t)v27, v32);
  uint64_t v42 = v60;
  v62(v41, (uint64_t)v63, v60);
  uint64_t v43 = v61;
  long long v44 = v71;
  uint64_t v45 = &v71[*(int *)(v61 + 48)];
  uint64_t v46 = v59;
  v58(v71, (uint64_t)v59, v35);
  v38(v45, (uint64_t)v57, v37);
  uint64_t v81 = v55;
  uint64_t v82 = v42;
  uint64_t v83 = v35;
  uint64_t v84 = v37;
  uint64_t v85 = v79;
  long long v86 = v75;
  long long v47 = v56;
  uint64_t v87 = v76;
  uint64_t v88 = v56;
  uint64_t v48 = v44;
  uint64_t v89 = v44;
  uint64_t v49 = v78(v72, v77, v80);
  uint64_t v50 = *(void (**)(char *, uint64_t))(v74 + 8);
  uint64_t v51 = v43;
  v50(v46, v43);
  uint64_t v52 = *(void (**)(char *, uint64_t))(v73 + 8);
  uint64_t v53 = v66;
  v52(v27, v66);
  v52(v47, v53);
  v50(v48, v51);
  return v49;
}

uint64_t static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(char **a1, void (*a2)(char *, char *, char *), uint64_t a3, void (*a4)(char *, char *, char *), uint64_t a5, char *a6, char *a7, char *a8, char *a9, char *a10, uint64_t a11, uint64_t a12, uint64_t a13, char *a14, uint64_t a15, char *a16)
{
  long long v136 = a8;
  uint64_t v131 = a3;
  uint64_t v132 = a4;
  uint64_t v130 = a2;
  long long v133 = a1;
  uint64_t v124 = a15;
  uint64_t v125 = a5;
  uint64_t v113 = a13;
  uint64_t v112 = a12;
  uint64_t v121 = a11;
  uint64_t v123 = a10;
  long long v137 = a9;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v138 = TupleTypeMetadata2;
  uint64_t v127 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v19 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v110 = (char *)&v103 - ((v20 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v21 = MEMORY[0x1F4188790](v19);
  uint64_t v114 = (char *)&v103 - v22;
  uint64_t v23 = MEMORY[0x1F4188790](v21);
  unint64_t v105 = (char *)&v103 - v24;
  uint64_t v25 = MEMORY[0x1F4188790](v23);
  long long v135 = (char *)&v103 - v26;
  MEMORY[0x1F4188790](v25);
  uint64_t v28 = (char *)&v103 - v27;
  uint64_t v106 = swift_getTupleTypeMetadata2();
  uint64_t v126 = *(void *)(v106 - 8);
  uint64_t v29 = MEMORY[0x1F4188790](v106);
  uint64_t v104 = (char *)&v103 - ((v30 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v31 = MEMORY[0x1F4188790](v29);
  uint64_t v108 = (char *)&v103 - v32;
  uint64_t v33 = MEMORY[0x1F4188790](v31);
  uint64_t v35 = (char *)&v103 - v34;
  uint64_t v36 = MEMORY[0x1F4188790](v33);
  uint64_t v38 = (char **)((char *)&v103 - v37);
  uint64_t v39 = (char *)&v103 + *(int *)(v36 + 48) - v37;
  uint64_t v40 = *((void *)a6 - 1);
  uint64_t v119 = *(void (**)(char *, char **, char *))(v40 + 16);
  uint64_t v118 = v40 + 16;
  uint64_t v122 = a6;
  v119((char *)&v103 - v37, v133, a6);
  long long v134 = a7;
  uint64_t v41 = *((void *)a7 - 1);
  uint64_t v116 = *(void (**)(char *, char *, char *))(v41 + 16);
  long long v133 = (char **)(v41 + 16);
  v116(v39, (char *)v130, a7);
  uint64_t v42 = *(int *)(TupleTypeMetadata2 + 48);
  uint64_t v43 = v28;
  uint64_t v120 = v28;
  long long v44 = &v28[v42];
  uint64_t v117 = v44;
  uint64_t v45 = *(void (**)(char *, uint64_t))(*((void *)v136 - 1) + 16);
  uint64_t v46 = *((void *)v136 - 1) + 16;
  v45(v43, v131);
  long long v47 = *(void (**)(char *, char *, char *))(*((void *)v137 - 1) + 16);
  uint64_t v48 = *((void *)v137 - 1) + 16;
  ((void (*)(char *, void (*)(char *, char *, char *)))v47)(v44, v132);
  uint64_t v128 = (*(uint64_t (**)(char *))(*(void *)(v124 + 8) + 16))(v123);
  uint64_t v49 = v106;
  uint64_t v115 = &v35[*(int *)(v106 + 48)];
  uint64_t v50 = v115;
  uint64_t v111 = v38;
  uint64_t v51 = a6;
  uint64_t v52 = (void (*)(char *, char *, char *))v119;
  v119(v35, v38, v51);
  uint64_t v53 = v50;
  unsigned __int8 v103 = v39;
  uint64_t v54 = v116;
  v116(v53, v39, v134);
  uint64_t v55 = &v135[*(int *)(v138 + 48)];
  uint64_t v131 = v46;
  uint64_t v132 = (void (*)(char *, char *, char *))v45;
  ((void (*)(void))v45)();
  unint64_t v107 = v55;
  uint64_t v56 = v49;
  uint64_t v129 = v48;
  uint64_t v130 = v47;
  v47(v55, v117, v137);
  uint64_t v57 = v108;
  uint64_t v58 = v122;
  uint64_t v59 = (*(uint64_t (**)(char *))(v121 + 16))(v122);
  uint64_t v60 = &v57[*(int *)(v49 + 48)];
  uint64_t v109 = v35;
  v52(v57, v35, v58);
  uint64_t v61 = v134;
  v54(v60, v115, v134);
  BOOL v62 = v59 == v128;
  uint64_t v63 = v128;
  BOOL v100 = v62 && (*(uint64_t (**)(char *))(v112 + 16))(v61) == v63;
  uint64_t v64 = v126 + 8;
  uint64_t v115 = *(char **)(v126 + 8);
  ((void (*)(char *, uint64_t))v115)(v57, v56);
  uint64_t v65 = v114;
  uint64_t v66 = &v114[*(int *)(v138 + 48)];
  uint64_t v67 = v136;
  v132(v114, v135, v136);
  uint64_t v68 = v107;
  v130(v66, v107, v137);
  uint64_t v69 = v110;
  uint64_t v70 = v56;
  if (v100) {
    BOOL v100 = (*(uint64_t (**)(char *))(v113 + 16))(v67) == v63;
  }
  uint64_t v71 = v127 + 8;
  uint64_t v72 = v65;
  uint64_t v73 = v138;
  uint64_t v114 = *(char **)(v127 + 8);
  ((void (*)(char *, uint64_t))v114)(v72, v138);
  uint64_t v74 = &v69[*(int *)(v73 + 48)];
  long long v75 = v135;
  uint64_t v76 = v67;
  v132(v69, v135, v67);
  uint64_t v77 = v137;
  v130(v74, v68, v137);
  if (v100)
  {
    uint64_t v78 = (uint64_t (*)(char *))*((void *)a14 + 2);
    uint64_t v108 = a14;
    uint64_t v79 = v78(v77);
    uint64_t v80 = v138;
    uint64_t v81 = (void (*)(char *, uint64_t))v114;
    ((void (*)(char *, uint64_t))v114)(v69, v138);
    v81(v75, v80);
    uint64_t result = ((uint64_t (*)(char *, uint64_t))v115)(v109, v70);
    if (v79 == v128)
    {
      long long v135 = a16;
      uint64_t v83 = *(int *)(v70 + 48);
      uint64_t v126 = v64;
      uint64_t v127 = v71;
      uint64_t v84 = v77;
      uint64_t v85 = v104;
      long long v86 = &v104[v83];
      uint64_t v87 = v138;
      uint64_t v88 = v122;
      v119(v104, v111, v122);
      uint64_t v89 = v76;
      uint64_t v90 = v134;
      v116(v86, v103, v134);
      uint64_t v91 = v105;
      uint64_t v92 = &v105[*(int *)(v87 + 48)];
      long long v93 = v120;
      v132(v105, v120, v89);
      uint64_t v94 = ((uint64_t (*)(char *, char *, char *))v130)(v92, v117, v84);
      long long v133 = &v103;
      MEMORY[0x1F4188790](v94);
      *(&v103 - 14) = v88;
      *(&v103 - 13) = v90;
      *(&v103 - 12) = v89;
      *(&v103 - 11) = v84;
      uint64_t v95 = v124;
      uint64_t v96 = (char *)v121;
      *(&v103 - 10) = v123;
      *(&v103 - 9) = v96;
      uint64_t v97 = (char *)v113;
      *(&v103 - 8) = (char *)v112;
      *(&v103 - 7) = v97;
      *(&v103 - 6) = v108;
      *(&v103 - 5) = (char *)v95;
      *(&v103 - 4) = v85;
      *(&v103 - 3) = v91;
      *(&v103 - 2) = (char *)v128;
      (*(void (**)(char *))(v95 + 16))(v135);
      unint64_t v98 = (uint64_t (*)(char *, uint64_t))v114;
      ((void (*)(char *, uint64_t))v114)(v93, v87);
      unint64_t v99 = v115;
      ((void (*)(char **, uint64_t))v115)(v111, v70);
      ((void (*)(char *, uint64_t))v99)(v85, v70);
      return v98(v91, v87);
    }
  }
  else
  {
    uint64_t v101 = v138;
    uint64_t v102 = (void (*)(char *, uint64_t))v114;
    ((void (*)(char *, uint64_t))v114)(v69, v138);
    v102(v75, v101);
    uint64_t result = ((uint64_t (*)(char *, uint64_t))v115)(v109, v70);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, long long a12, uint64_t a13, uint64_t a14, uint64_t a15)
{
  uint64_t v43 = a4;
  uint64_t v36 = a3;
  uint64_t v42 = a1;
  uint64_t v46 = a9;
  uint64_t v45 = a15;
  uint64_t v41 = a14;
  uint64_t v40 = a13;
  long long v39 = a12;
  uint64_t v38 = a11;
  uint64_t v37 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v44 = *(void *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v21 = (char *)v34 - v20;
  uint64_t v35 = swift_getTupleTypeMetadata2();
  uint64_t v22 = *(void *)(v35 - 8);
  uint64_t v23 = MEMORY[0x1F4188790](v35);
  uint64_t v25 = (char *)v34 - v24;
  uint64_t v26 = *(int *)(v23 + 48);
  uint64_t v27 = &v25[v26];
  uint64_t v28 = a2;
  v34[1] = a2;
  uint64_t v29 = a2 + v26;
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a5 - 8) + 16))(v25, v28, a5);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a6 - 8) + 16))(v27, v29, a6);
  uint64_t v30 = *(int *)(TupleTypeMetadata2 + 48);
  uint64_t v31 = &v21[v30];
  uint64_t v32 = v36 + v30;
  (*(void (**)(char *))(*(void *)(a7 - 8) + 16))(v21);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a8 - 8) + 16))(v31, v32, a8);
  uint64_t v48 = a5;
  uint64_t v49 = a6;
  uint64_t v50 = a7;
  uint64_t v51 = a8;
  uint64_t v52 = v37;
  uint64_t v53 = v38;
  long long v54 = v39;
  uint64_t v55 = v40;
  uint64_t v56 = v41;
  uint64_t v57 = v25;
  uint64_t v58 = v21;
  uint64_t v59 = v42;
  uint64_t v60 = v43;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(v38 + 24))(v45, v47, MEMORY[0x1E4FBC848] + 8, a5, v38);
  (*(void (**)(char *, uint64_t))(v22 + 8))(v25, v35);
  return (*(uint64_t (**)(char *, uint64_t))(v44 + 8))(v21, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, long long a15, uint64_t a16, uint64_t a17)
{
  uint64_t v30 = a7;
  uint64_t v39 = a5;
  uint64_t v40 = a6;
  uint64_t v38 = a2;
  uint64_t v37 = a1;
  uint64_t v42 = a9;
  uint64_t v41 = a17;
  uint64_t v36 = a16;
  long long v35 = a15;
  uint64_t v34 = a14;
  uint64_t v33 = a13;
  uint64_t v32 = a12;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v21 = *(void *)(TupleTypeMetadata2 - 8);
  MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v23 = (char *)&v29 - v22;
  uint64_t v24 = a8;
  uint64_t v31 = a3 + *(int *)(swift_getTupleTypeMetadata2() + 48);
  uint64_t v25 = *(int *)(TupleTypeMetadata2 + 48);
  uint64_t v26 = &v23[v25];
  uint64_t v27 = a4 + v25;
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a10 - 8) + 16))(v23, a4, a10);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a11 - 8) + 16))(v26, v27, a11);
  uint64_t v44 = v30;
  uint64_t v45 = v24;
  uint64_t v46 = a10;
  uint64_t v47 = a11;
  uint64_t v48 = v32;
  uint64_t v49 = v33;
  uint64_t v50 = v34;
  long long v51 = v35;
  uint64_t v52 = v36;
  uint64_t v53 = v23;
  uint64_t v54 = v37;
  uint64_t v55 = v38;
  uint64_t v56 = v39;
  uint64_t v57 = v40;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(v34 + 24))(v41, v43, MEMORY[0x1E4FBC848] + 8, v24, v34);
  return (*(uint64_t (**)(char *, uint64_t))(v21 + 8))(v23, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11, uint64_t a12, long long a13, uint64_t a14, uint64_t a15, uint64_t a16, uint64_t a17, uint64_t a18)
{
  uint64_t v34 = a8;
  uint64_t v40 = a7;
  uint64_t v39 = a6;
  uint64_t v38 = a5;
  uint64_t v36 = a4;
  uint64_t v37 = a2;
  uint64_t v35 = a1;
  uint64_t v42 = a9;
  uint64_t v41 = a18;
  uint64_t v33 = a17;
  uint64_t v32 = a16;
  uint64_t v31 = a14;
  long long v30 = a13;
  uint64_t v29 = a10;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v20 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v21 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v23 = (char *)&v28 - v22;
  uint64_t v24 = *(int *)(v21 + 48);
  uint64_t v25 = &v23[v24];
  uint64_t v26 = a3 + v24;
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a11 - 8) + 16))(v23, a3, a11);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a12 - 8) + 16))(v25, v26, a12);
  uint64_t v44 = v34;
  uint64_t v45 = v29;
  uint64_t v46 = a11;
  uint64_t v47 = a12;
  long long v48 = v30;
  uint64_t v49 = v31;
  uint64_t v50 = a15;
  uint64_t v51 = v32;
  uint64_t v52 = v33;
  uint64_t v53 = v23;
  uint64_t v54 = v36;
  uint64_t v55 = v38;
  uint64_t v56 = v35;
  uint64_t v57 = v37;
  uint64_t v58 = v39;
  uint64_t v59 = v40;
  (*(void (**)(uint64_t, char *, uint64_t, uint64_t, uint64_t))(v50 + 24))(v41, v43, MEMORY[0x1E4FBC848] + 8, a11, v50);
  return (*(uint64_t (**)(char *, uint64_t))(v20 + 8))(v23, TupleTypeMetadata2);
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, void *a9, uint64_t a10, uint64_t (*a11)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, void, uint64_t, uint64_t))
{
  if (!a3)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (!a5)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (!a7)
  {
LABEL_11:
    __break(1u);
    goto LABEL_12;
  }
  if (!result)
  {
LABEL_12:
    __break(1u);
    goto LABEL_13;
  }
  if (*a9)
  {
    if ((a10 & 0x8000000000000000) == 0) {
      return a11(a3, 1, a5, 1, a7, 1, result, 1, *a9, 1, a10);
    }
    __break(1u);
    goto LABEL_9;
  }
LABEL_13:
  __break(1u);
  return result;
}

uint64_t static vDSP.add<A>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, float a4, float a5)
{
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v11 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v12 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v14 = &v25[-((v13 + 15) & 0xFFFFFFFFFFFFFFF0)];
  uint64_t v15 = MEMORY[0x1F4188790](v12);
  uint64_t v17 = &v25[-v16];
  uint64_t v18 = *(int *)(v15 + 48);
  uint64_t v19 = *(void (**)(unsigned char *, uint64_t, uint64_t))(*(void *)(a2 - 8) + 16);
  v19(&v25[-v16], a1, a2);
  *(float *)&v17[v18] = a4;
  uint64_t v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  uint64_t v21 = *(int *)(TupleTypeMetadata2 + 48);
  v19(v14, (uint64_t)v17, a2);
  *(float *)&v14[v21] = a4;
  uint64_t v26 = a2;
  uint64_t v27 = a3;
  uint64_t v28 = v14;
  float v29 = a5;
  uint64_t v22 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v20, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.add<A>(multiplication:_:));
  uint64_t v23 = *(void (**)(unsigned char *, uint64_t))(v11 + 8);
  v23(v17, TupleTypeMetadata2);
  v23(v14, TupleTypeMetadata2);
  return v22;
}

uint64_t closure #1 in static vDSP.add<A>(multiplication:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, float a6)
{
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v13 = MEMORY[0x1F4188790](TupleTypeMetadata2 - 8);
  uint64_t v15 = (char *)&v22 - v14;
  uint64_t v16 = *(int *)(v13 + 56);
  uint64_t v17 = *(void *)(a4 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v17 + 16))((char *)&v22 - v14, a3, a4);
  float v18 = *(float *)(a3 + v16);
  *(float *)&v15[v16] = v18;
  uint64_t v19 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v20 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.add<A, B>(multiplication:_:result:)((uint64_t)v15, a1, a4, v19, a5, v20, v18, a6);
  (*(void (**)(char *, uint64_t))(v17 + 8))(v15, a4);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.add<A, B>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7, float a8)
{
  uint64_t v40 = a1;
  uint64_t v41 = a4;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v15 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v16 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v36 = (char *)&v35 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = MEMORY[0x1F4188790](v16);
  uint64_t v20 = (char *)&v35 - v19;
  MEMORY[0x1F4188790](v18);
  uint64_t v22 = (char *)&v35 - v21;
  uint64_t v23 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a3 - 8) + 16);
  v23((char *)&v35 - v21, v40, a3);
  *(float *)&v22[*(int *)(TupleTypeMetadata2 + 48)] = a7;
  uint64_t v39 = a2;
  uint64_t v40 = a6;
  uint64_t v24 = (*(uint64_t (**)(uint64_t))(*(void *)(a6 + 8) + 16))(v41);
  uint64_t v25 = *(int *)(TupleTypeMetadata2 + 48);
  v23(v20, (uint64_t)v22, a3);
  *(float *)&v20[v25] = a7;
  uint64_t v26 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  uint64_t v38 = a5;
  uint64_t v27 = v26(a3, a5);
  uint64_t v28 = *(uint64_t (**)(char *, uint64_t))(v15 + 8);
  uint64_t v37 = v15 + 8;
  uint64_t result = v28(v20, TupleTypeMetadata2);
  if (v27 == v24)
  {
    uint64_t v30 = *(int *)(TupleTypeMetadata2 + 48);
    uint64_t v31 = v36;
    uint64_t v32 = ((uint64_t (*)(char *, char *, uint64_t))v23)(v36, v22, a3);
    *(float *)&v31[v30] = a7;
    MEMORY[0x1F4188790](v32);
    uint64_t v34 = v40;
    uint64_t v33 = v41;
    *(&v35 - 8) = a3;
    *(&v35 - 7) = v33;
    *(&v35 - 6) = v38;
    *(&v35 - 5) = v34;
    *(&v35 - 4) = (uint64_t)v31;
    *((float *)&v35 - 6) = a8;
    *(&v35 - 2) = v24;
    (*(void (**)(uint64_t (*)@<X0>(uint64_t@<X0>, uint64_t@<X8>)))(v34 + 16))(partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:result:));
    v28(v22, TupleTypeMetadata2);
    return v28(v31, TupleTypeMetadata2);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in static vDSP.add<A, B>(multiplication:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X8>, float a9@<S0>)
{
  uint64_t v24 = a8;
  uint64_t v22 = a7;
  uint64_t v23 = a3;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v16 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v17 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v19 = (char *)&v22 - v18;
  uint64_t v20 = *(int *)(v17 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a4 - 8) + 16))((char *)&v22 - v18, a2, a4);
  *(_DWORD *)&v19[v20] = *(_DWORD *)(a2 + v20);
  uint64_t v26 = a4;
  uint64_t v27 = a5;
  uint64_t v28 = a6;
  uint64_t v29 = v22;
  uint64_t v30 = v19;
  float v31 = a9;
  uint64_t v32 = a1;
  uint64_t v33 = v23;
  (*(void (**)(void (*)(const float *, uint64_t), char *, uint64_t, uint64_t, uint64_t))(a6 + 24))(partial apply for closure #1 in closure #1 in static vDSP.add<A, B>(multiplication:_:result:), v25, MEMORY[0x1E4FBC848] + 8, a4, a6);
  return (*(uint64_t (**)(char *, uint64_t))(v16 + 8))(v19, TupleTypeMetadata2);
}

void closure #1 in closure #1 in static vDSP.add<A, B>(multiplication:_:result:)(const float *a1, float a2, uint64_t a3, uint64_t a4, float **a5, vDSP_Length a6)
{
  uint64_t v13 = *MEMORY[0x1E4F143B8];
  float v10 = *(float *)(a4 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  float v11 = a2;
  float __B = v10;
  if (!a1) {
    goto LABEL_6;
  }
  if (!*a5) {
    goto LABEL_7;
  }
  if ((a6 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  vDSP_vsmsa(a1, 1, &__B, &v11, *a5, 1, a6);
}

uint64_t static vDSP.add<A>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, double a4, double a5)
{
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v11 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v12 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v14 = &v25[-((v13 + 15) & 0xFFFFFFFFFFFFFFF0)];
  uint64_t v15 = MEMORY[0x1F4188790](v12);
  uint64_t v17 = &v25[-v16];
  uint64_t v18 = *(int *)(v15 + 48);
  uint64_t v19 = *(void (**)(unsigned char *, uint64_t, uint64_t))(*(void *)(a2 - 8) + 16);
  v19(&v25[-v16], a1, a2);
  *(double *)&v17[v18] = a4;
  uint64_t v20 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  uint64_t v21 = *(int *)(TupleTypeMetadata2 + 48);
  v19(v14, (uint64_t)v17, a2);
  *(double *)&v14[v21] = a4;
  uint64_t v26 = a2;
  uint64_t v27 = a3;
  uint64_t v28 = v14;
  double v29 = a5;
  uint64_t v22 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v20, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.add<A>(multiplication:_:));
  uint64_t v23 = *(void (**)(unsigned char *, uint64_t))(v11 + 8);
  v23(v17, TupleTypeMetadata2);
  v23(v14, TupleTypeMetadata2);
  return v22;
}

uint64_t closure #1 in static vDSP.add<A>(multiplication:_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, double a6)
{
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v13 = MEMORY[0x1F4188790](TupleTypeMetadata2 - 8);
  uint64_t v15 = (char *)&v22 - v14;
  uint64_t v16 = *(int *)(v13 + 56);
  uint64_t v17 = *(void *)(a4 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v17 + 16))((char *)&v22 - v14, a3, a4);
  double v18 = *(double *)(a3 + v16);
  *(double *)&v15[v16] = v18;
  uint64_t v19 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v20 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.add<A, B>(multiplication:_:result:)((uint64_t)v15, a1, a4, v19, a5, v20, v18, a6);
  (*(void (**)(char *, uint64_t))(v17 + 8))(v15, a4);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a4, a5);
  *a2 = result;
  return result;
}

uint64_t static vDSP.add<A, B>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7, double a8)
{
  uint64_t v40 = a1;
  uint64_t v41 = a4;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v15 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v16 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v36 = (char *)&v35 - ((v17 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v18 = MEMORY[0x1F4188790](v16);
  uint64_t v20 = (char *)&v35 - v19;
  MEMORY[0x1F4188790](v18);
  uint64_t v22 = (char *)&v35 - v21;
  uint64_t v23 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a3 - 8) + 16);
  v23((char *)&v35 - v21, v40, a3);
  *(double *)&v22[*(int *)(TupleTypeMetadata2 + 48)] = a7;
  uint64_t v39 = a2;
  uint64_t v40 = a6;
  uint64_t v24 = (*(uint64_t (**)(uint64_t))(*(void *)(a6 + 8) + 16))(v41);
  uint64_t v25 = *(int *)(TupleTypeMetadata2 + 48);
  v23(v20, (uint64_t)v22, a3);
  *(double *)&v20[v25] = a7;
  uint64_t v26 = *(uint64_t (**)(uint64_t, uint64_t))(a5 + 16);
  uint64_t v38 = a5;
  uint64_t v27 = v26(a3, a5);
  uint64_t v28 = *(uint64_t (**)(char *, uint64_t))(v15 + 8);
  uint64_t v37 = v15 + 8;
  uint64_t result = v28(v20, TupleTypeMetadata2);
  if (v27 == v24)
  {
    uint64_t v30 = *(int *)(TupleTypeMetadata2 + 48);
    float v31 = v36;
    uint64_t v32 = ((uint64_t (*)(char *, char *, uint64_t))v23)(v36, v22, a3);
    *(double *)&v31[v30] = a7;
    MEMORY[0x1F4188790](v32);
    uint64_t v34 = v40;
    uint64_t v33 = v41;
    *(&v35 - 8) = a3;
    *(&v35 - 7) = v33;
    *(&v35 - 6) = v38;
    *(&v35 - 5) = v34;
    *(&v35 - 4) = (uint64_t)v31;
    *((double *)&v35 - 3) = a8;
    *(&v35 - 2) = v24;
    (*(void (**)(uint64_t (*)@<X0>(uint64_t@<X0>, uint64_t@<X8>)))(v34 + 16))(partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:result:));
    v28(v22, TupleTypeMetadata2);
    return v28(v31, TupleTypeMetadata2);
  }
  else
  {
    __break(1u);
  }
  return result;
}

uint64_t closure #1 in static vDSP.add<A, B>(multiplication:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X8>, double a9@<D0>)
{
  uint64_t v24 = a8;
  uint64_t v22 = a7;
  uint64_t v23 = a3;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v16 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v17 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v19 = (char *)&v22 - v18;
  uint64_t v20 = *(int *)(v17 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a4 - 8) + 16))((char *)&v22 - v18, a2, a4);
  *(void *)&v19[v20] = *(void *)(a2 + v20);
  uint64_t v26 = a4;
  uint64_t v27 = a5;
  uint64_t v28 = a6;
  uint64_t v29 = v22;
  uint64_t v30 = v19;
  double v31 = a9;
  uint64_t v32 = a1;
  uint64_t v33 = v23;
  (*(void (**)(void (*)(const double *, uint64_t), char *, uint64_t, uint64_t, uint64_t))(a6 + 24))(partial apply for closure #1 in closure #1 in static vDSP.add<A, B>(multiplication:_:result:), v25, MEMORY[0x1E4FBC848] + 8, a4, a6);
  return (*(uint64_t (**)(char *, uint64_t))(v16 + 8))(v19, TupleTypeMetadata2);
}

void closure #1 in closure #1 in static vDSP.add<A, B>(multiplication:_:result:)(const double *a1, double a2, uint64_t a3, uint64_t a4, double **a5, vDSP_Length a6)
{
  __B[1] = *(double *)MEMORY[0x1E4F143B8];
  double v10 = *(double *)(a4 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  double __C = a2;
  __B[0] = v10;
  if (!a1) {
    goto LABEL_6;
  }
  if (!*a5) {
    goto LABEL_7;
  }
  if ((a6 & 0x8000000000000000) != 0)
  {
    __break(1u);
LABEL_6:
    __break(1u);
LABEL_7:
    __break(1u);
  }
  vDSP_vsmsaD(a1, 1, __B, &__C, *a5, 1, a6);
}

uint64_t static vDSP.subtract<A, B>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, float a7)
{
  uint64_t v28 = a6;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v27 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v14 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v16 = (char *)&v25 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1F4188790](v14);
  uint64_t v18 = (char *)&v25 - v17;
  uint64_t v19 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a4 - 8) + 16);
  v19((char *)&v25 - v17, a1, a4);
  *(float *)&v18[*(int *)(TupleTypeMetadata2 + 48)] = a7;
  uint64_t v20 = a5;
  uint64_t v26 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  uint64_t v21 = *(int *)(TupleTypeMetadata2 + 48);
  v19(v16, (uint64_t)v18, a4);
  *(float *)&v16[v21] = a7;
  uint64_t v29 = a3;
  uint64_t v30 = a4;
  uint64_t v31 = v20;
  uint64_t v32 = v28;
  uint64_t v33 = v16;
  uint64_t v34 = a2;
  uint64_t v22 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v26, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.subtract<A, B>(multiplication:_:));
  uint64_t v23 = *(void (**)(char *, uint64_t))(v27 + 8);
  v23(v18, TupleTypeMetadata2);
  v23(v16, TupleTypeMetadata2);
  return v22;
}

uint64_t closure #1 in static vDSP.subtract<A, B>(multiplication:_:)(char *a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v26 = a2;
  uint64_t v25 = a8;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v15 = MEMORY[0x1F4188790](TupleTypeMetadata2 - 8);
  uint64_t v17 = (char *)&v24 - v16;
  uint64_t v18 = *(int *)(v15 + 56);
  uint64_t v19 = *(void *)(a6 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v19 + 16))((char *)&v24 - v16, a3, a6);
  float v20 = *(float *)(a3 + v18);
  *(float *)&v17[v18] = v20;
  uint64_t v21 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v22 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  static vDSP.subtract<A, B, C>(multiplication:_:result:)(v17, a4, a1, a5, a6, v21, a7, v25, v20, v22);
  (*(void (**)(char *, uint64_t))(v19 + 8))(v17, a6);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  *uint64_t v26 = result;
  return result;
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  uint64_t v16;
  char *v17;
  uint64_t v18;
  uint64_t v19;
  double v20;
  uint64_t v21;
  uint64_t v22;
  uint64_t result;
  uint64_t v24;
  uint64_t v25;
  uint64_t *v26;

  uint64_t v26 = a2;
  uint64_t v25 = a8;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v15 = MEMORY[0x1F4188790](TupleTypeMetadata2 - 8);
  uint64_t v17 = (char *)&v24 - v16;
  uint64_t v18 = *(int *)(v15 + 56);
  uint64_t v19 = *(void *)(a6 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v19 + 16))((char *)&v24 - v16, a3, a6);
  float v20 = *(double *)(a3 + v18);
  *(double *)&v17[v18] = v20;
  uint64_t v21 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v22 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  static vDSP.subtract<A, B, C>(multiplication:_:result:)(v17, a4, a1, a5, a6, v21, a7, v25, v20, v22);
  (*(void (**)(char *, uint64_t))(v19 + 8))(v17, a6);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t))(a7 + 16))(a5, a7);
  *uint64_t v26 = result;
  return result;
}

uint64_t static vDSP.subtract<A, B, C>(multiplication:_:result:)(char *a1, uint64_t a2, char *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, uint64_t a10)
{
  uint64_t v55 = a8;
  uint64_t v56 = a6;
  uint64_t v51 = a7;
  uint64_t v52 = a1;
  uint64_t v53 = a4;
  uint64_t v54 = a2;
  uint64_t v50 = *(void *)(a4 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v14 = (char *)v46 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v16 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v17 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  long long v48 = (char *)v46 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v19 = MEMORY[0x1F4188790](v17);
  uint64_t v21 = (char *)v46 - v20;
  MEMORY[0x1F4188790](v19);
  uint64_t v23 = (char *)v46 - v22;
  uint64_t v24 = *(void *)(a5 - 8);
  uint64_t v25 = *(uint64_t (**)(char *, char *, uint64_t))(v24 + 16);
  uint64_t v26 = v24 + 16;
  v25((char *)v46 - v22, v52, a5);
  *(float *)&v23[*(int *)(TupleTypeMetadata2 + 48)] = a9;
  uint64_t v49 = a10;
  uint64_t v27 = *(uint64_t (**)(uint64_t))(*(void *)(a10 + 8) + 16);
  uint64_t v52 = a3;
  uint64_t v28 = v27(v56);
  uint64_t v29 = *(int *)(TupleTypeMetadata2 + 48);
  v46[2] = v26;
  uint64_t v47 = v25;
  v25(v21, v23, a5);
  *(float *)&v21[v29] = a9;
  uint64_t v30 = (*(uint64_t (**)(uint64_t))(v55 + 16))(a5);
  uint64_t v33 = *(uint64_t (**)(void, void))(v16 + 8);
  uint64_t v32 = v16 + 8;
  uint64_t v31 = v33;
  uint64_t result = v33(v21, TupleTypeMetadata2);
  if (v30 == v28)
  {
    v46[0] = v31;
    v46[1] = v32;
    uint64_t v35 = v50;
    uint64_t v36 = v53;
    (*(void (**)(char *, uint64_t, uint64_t))(v50 + 16))(v14, v54, v53);
    uint64_t v37 = v51;
    uint64_t v38 = (*(uint64_t (**)(uint64_t, uint64_t))(v51 + 16))(v36, v51);
    uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v35 + 8))(v14, v36);
    if (v38 == v28)
    {
      uint64_t v39 = *(int *)(TupleTypeMetadata2 + 48);
      uint64_t v40 = v48;
      uint64_t v41 = v47(v48, v23, a5);
      *(float *)&v40[v39] = a9;
      MEMORY[0x1F4188790](v41);
      v46[-10] = v53;
      v46[-9] = a5;
      uint64_t v42 = v55;
      v46[-8] = v56;
      v46[-7] = v37;
      uint64_t v43 = v49;
      v46[-6] = v42;
      v46[-5] = v43;
      uint64_t v44 = v54;
      v46[-4] = v40;
      v46[-3] = v44;
      v46[-2] = v28;
      (*(void (**)(uint64_t (*)(uint64_t)))(v43 + 16))(partial apply for closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:));
      uint64_t v45 = (uint64_t (*)(char *, uint64_t))v46[0];
      ((void (*)(char *, uint64_t))v46[0])(v23, TupleTypeMetadata2);
      return v45(v40, TupleTypeMetadata2);
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X3>, uint64_t a5@<X4>, uint64_t a6@<X5>, uint64_t a7@<X6>, uint64_t a8@<X7>, uint64_t a9@<X8>, uint64_t a10, uint64_t a11)
{
  uint64_t v24 = a8;
  uint64_t v25 = a4;
  uint64_t v21 = a7;
  uint64_t v22 = a1;
  uint64_t v23 = a3;
  uint64_t v26 = a9;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v15 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v16 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v18 = (char *)&v21 - v17;
  uint64_t v19 = *(int *)(v16 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a6 - 8) + 16))((char *)&v21 - v17, a2, a6);
  *(_DWORD *)&v18[v19] = *(_DWORD *)(a2 + v19);
  uint64_t v28 = a5;
  uint64_t v29 = a6;
  uint64_t v30 = v21;
  uint64_t v31 = v24;
  uint64_t v32 = a10;
  uint64_t v33 = a11;
  uint64_t v34 = v18;
  uint64_t v35 = v23;
  uint64_t v36 = v22;
  uint64_t v37 = v25;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t, uint64_t))(v32 + 24))(partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:), v27, MEMORY[0x1E4FBC848] + 8, a6, v32);
  return (*(uint64_t (**)(char *, uint64_t))(v15 + 8))(v18, TupleTypeMetadata2);
}

{
  uint64_t TupleTypeMetadata2;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  char *v18;
  uint64_t v19;
  uint64_t v21;
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;
  uint64_t v25;
  uint64_t v26;
  char v27[16];
  uint64_t v28;
  uint64_t v29;
  uint64_t v30;
  uint64_t v31;
  uint64_t v32;
  uint64_t v33;
  char *v34;
  uint64_t v35;
  uint64_t v36;
  uint64_t v37;

  uint64_t v24 = a8;
  uint64_t v25 = a4;
  uint64_t v21 = a7;
  uint64_t v22 = a1;
  uint64_t v23 = a3;
  uint64_t v26 = a9;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v15 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v16 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v18 = (char *)&v21 - v17;
  uint64_t v19 = *(int *)(v16 + 48);
  (*(void (**)(char *, uint64_t, uint64_t))(*(void *)(a6 - 8) + 16))((char *)&v21 - v17, a2, a6);
  *(void *)&v18[v19] = *(void *)(a2 + v19);
  uint64_t v28 = a5;
  uint64_t v29 = a6;
  uint64_t v30 = v21;
  uint64_t v31 = v24;
  uint64_t v32 = a10;
  uint64_t v33 = a11;
  uint64_t v34 = v18;
  uint64_t v35 = v23;
  uint64_t v36 = v22;
  uint64_t v37 = v25;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), char *, uint64_t, uint64_t, uint64_t))(v32 + 24))(partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:), v27, MEMORY[0x1E4FBC848] + 8, a6, v32);
  return (*(uint64_t (**)(char *, uint64_t))(v15 + 8))(v18, TupleTypeMetadata2);
}

uint64_t static vDSP.subtract<A, B>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, double a7)
{
  uint64_t v28 = a6;
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v27 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v14 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  uint64_t v16 = (char *)&v25 - ((v15 + 15) & 0xFFFFFFFFFFFFFFF0);
  MEMORY[0x1F4188790](v14);
  uint64_t v18 = (char *)&v25 - v17;
  uint64_t v19 = *(void (**)(char *, uint64_t, uint64_t))(*(void *)(a4 - 8) + 16);
  v19((char *)&v25 - v17, a1, a4);
  *(double *)&v18[*(int *)(TupleTypeMetadata2 + 48)] = a7;
  uint64_t v20 = a5;
  uint64_t v26 = (*(uint64_t (**)(uint64_t, uint64_t))(a5 + 16))(a3, a5);
  uint64_t v21 = *(int *)(TupleTypeMetadata2 + 48);
  v19(v16, (uint64_t)v18, a4);
  *(double *)&v16[v21] = a7;
  uint64_t v29 = a3;
  uint64_t v30 = a4;
  uint64_t v31 = v20;
  uint64_t v32 = v28;
  uint64_t v33 = v16;
  uint64_t v34 = a2;
  uint64_t v22 = specialized Array.init(_unsafeUninitializedCapacity:initializingWith:)(v26, (uint64_t (*)(void *, uint64_t *))partial apply for closure #1 in static vDSP.subtract<A, B>(multiplication:_:));
  uint64_t v23 = *(void (**)(char *, uint64_t))(v27 + 8);
  v23(v18, TupleTypeMetadata2);
  v23(v16, TupleTypeMetadata2);
  return v22;
}

uint64_t static vDSP.subtract<A, B, C>(multiplication:_:result:)(char *a1, uint64_t a2, char *a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, double a9, uint64_t a10)
{
  uint64_t v55 = a8;
  uint64_t v56 = a6;
  uint64_t v51 = a7;
  uint64_t v52 = a1;
  uint64_t v53 = a4;
  uint64_t v54 = a2;
  uint64_t v50 = *(void *)(a4 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v14 = (char *)v46 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  TupleTypeMetadata2 = swift_getTupleTypeMetadata2();
  uint64_t v16 = *(void *)(TupleTypeMetadata2 - 8);
  uint64_t v17 = MEMORY[0x1F4188790](TupleTypeMetadata2);
  long long v48 = (char *)v46 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v19 = MEMORY[0x1F4188790](v17);
  uint64_t v21 = (char *)v46 - v20;
  MEMORY[0x1F4188790](v19);
  uint64_t v23 = (char *)v46 - v22;
  uint64_t v24 = *(void *)(a5 - 8);
  uint64_t v25 = *(uint64_t (**)(char *, char *, uint64_t))(v24 + 16);
  uint64_t v26 = v24 + 16;
  v25((char *)v46 - v22, v52, a5);
  *(double *)&v23[*(int *)(TupleTypeMetadata2 + 48)] = a9;
  uint64_t v49 = a10;
  uint64_t v27 = *(uint64_t (**)(uint64_t))(*(void *)(a10 + 8) + 16);
  uint64_t v52 = a3;
  uint64_t v28 = v27(v56);
  uint64_t v29 = *(int *)(TupleTypeMetadata2 + 48);
  v46[2] = v26;
  uint64_t v47 = v25;
  v25(v21, v23, a5);
  *(double *)&v21[v29] = a9;
  uint64_t v30 = (*(uint64_t (**)(uint64_t))(v55 + 16))(a5);
  uint64_t v33 = *(uint64_t (**)(void, void))(v16 + 8);
  uint64_t v32 = v16 + 8;
  uint64_t v31 = v33;
  uint64_t result = v33(v21, TupleTypeMetadata2);
  if (v30 == v28)
  {
    v46[0] = v31;
    v46[1] = v32;
    uint64_t v35 = v50;
    uint64_t v36 = v53;
    (*(void (**)(char *, uint64_t, uint64_t))(v50 + 16))(v14, v54, v53);
    uint64_t v37 = v51;
    uint64_t v38 = (*(uint64_t (**)(uint64_t, uint64_t))(v51 + 16))(v36, v51);
    uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v35 + 8))(v14, v36);
    if (v38 == v28)
    {
      uint64_t v39 = *(int *)(TupleTypeMetadata2 + 48);
      uint64_t v40 = v48;
      uint64_t v41 = v47(v48, v23, a5);
      *(double *)&v40[v39] = a9;
      MEMORY[0x1F4188790](v41);
      v46[-10] = v53;
      v46[-9] = a5;
      uint64_t v42 = v55;
      v46[-8] = v56;
      v46[-7] = v37;
      uint64_t v43 = v49;
      v46[-6] = v42;
      v46[-5] = v43;
      uint64_t v44 = v54;
      v46[-4] = v40;
      v46[-3] = v44;
      v46[-2] = v28;
      (*(void (**)(uint64_t (*)(uint64_t)))(v43 + 16))(partial apply for closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:));
      uint64_t v45 = (uint64_t (*)(char *, uint64_t))v46[0];
      ((void (*)(char *, uint64_t))v46[0])(v23, TupleTypeMetadata2);
      return v45(v40, TupleTypeMetadata2);
    }
  }
  else
  {
    __break(1u);
  }
  __break(1u);
  return result;
}

uint64_t closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, void *a6, uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  if (!a3)
  {
LABEL_7:
    __break(1u);
    goto LABEL_8;
  }
  if (!result)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  if (*a6)
  {
    if ((a7 & 0x8000000000000000) == 0) {
      return a8(a3, 1, a5, result, 1);
    }
    __break(1u);
    goto LABEL_7;
  }
LABEL_9:
  __break(1u);
  return result;
}

uint64_t static vDSP.normalize<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return static vDSP.normalize<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.normalize<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

{
  return static vDSP.normalize<A>(_:)(a1, a2, a3, (uint64_t)partial apply for closure #1 in static vDSP.normalize<A>(_:), (uint64_t (*)(uint64_t, uint64_t, void *))specialized Array.init(_unsafeUninitializedCapacity:initializingWith:));
}

uint64_t closure #1 in static vDSP.normalize<A>(_:)(uint64_t a1, uint64_t *a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  uint64_t v10 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  uint64_t v11 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>);
  double v12 = static vDSP.normalize<A, B>(_:result:)(a3, a1, a4, v10, a5, v11);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t, double))(a5 + 16))(a4, a5, v12);
  *a2 = result;
  return result;
}

{
  uint64_t v10;
  uint64_t v11;
  float v12;
  uint64_t result;

  uint64_t v10 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  uint64_t v11 = lazy protocol witness table accessor for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>(&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, &demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>);
  double v12 = static vDSP.normalize<A, B>(_:result:)(a3, a1, a4, v10, a5, v11);
  uint64_t result = (*(uint64_t (**)(uint64_t, uint64_t, float))(a5 + 16))(a4, a5, v12);
  *a2 = result;
  return result;
}

double static vDSP.normalize<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v18 = *MEMORY[0x1E4F143B8];
  uint64_t v7 = 0;
  double v8 = 0.0;
  uint64_t v10 = a3;
  uint64_t v11 = a4;
  uint64_t v12 = a5;
  uint64_t v13 = a6;
  uint64_t v14 = a2;
  uint64_t v15 = &v8;
  uint64_t v16 = &v7;
  uint64_t v17 = a1;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), unsigned char *, uint64_t, uint64_t))(a5 + 24))(partial apply for closure #1 in static vDSP.normalize<A, B>(_:result:), v9, MEMORY[0x1E4FBC848] + 8, a3);
  return v8;
}

void closure #1 in closure #1 in static vDSP.normalize<A, B>(_:result:)(double **a1, const double *a2, uint64_t a3, double *a4, double *a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  if (!a2)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  float v9 = *a1;
  if (*a1)
  {
    vDSP_Length v13 = (*(uint64_t (**)(uint64_t))(a9 + 16))(a7);
    if ((v13 & 0x8000000000000000) == 0)
    {
      vDSP_normalizeD(a2, 1, v9, 1, a4, a5, v13);
      return;
    }
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
}

double static vDSP.standardDeviation<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v12 = *MEMORY[0x1E4F143B8];
  double v4 = 0.0;
  uint64_t v5 = 0;
  uint64_t v7 = a2;
  uint64_t v8 = a3;
  float v9 = &v5;
  uint64_t v10 = &v4;
  uint64_t v11 = a1;
  (*(void (**)(void (*)(const double *, uint64_t), unsigned char *, uint64_t, uint64_t))(*(void *)(a3 + 8)
                                                                                                  + 24))(partial apply for closure #1 in static vDSP.standardDeviation<A>(_:), v6, MEMORY[0x1E4FBC848] + 8, a2);
  return v4;
}

void closure #1 in static vDSP.standardDeviation<A>(_:)(const double *a1, uint64_t a2, double *a3, double *a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  if (a1)
  {
    vDSP_Length v10 = (*(uint64_t (**)(uint64_t))(*(void *)(a7 + 8) + 16))(a6);
    if ((v10 & 0x8000000000000000) == 0)
    {
      vDSP_normalizeD(a1, 1, 0, 1, a3, a4, v10);
      return;
    }
    __break(1u);
  }
  __break(1u);
}

uint64_t static vDSP.normalize<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(uint64_t, uint64_t, void *))
{
  uint64_t v10 = (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 16))(a2, a3);
  _OWORD v12[2] = a2;
  v12[3] = a3;
  v12[4] = a1;
  return a5(v10, a4, v12);
}

float static vDSP.normalize<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v17 = *MEMORY[0x1E4F143B8];
  uint64_t v7 = 0;
  uint64_t v9 = a3;
  uint64_t v10 = a4;
  uint64_t v11 = a5;
  uint64_t v12 = a6;
  uint64_t v13 = a2;
  uint64_t v14 = (char *)&v7 + 4;
  uint64_t v15 = &v7;
  uint64_t v16 = a1;
  (*(void (**)(uint64_t (*)(uint64_t, uint64_t), unsigned char *, uint64_t, uint64_t))(a5 + 24))(partial apply for closure #1 in static vDSP.normalize<A, B>(_:result:), v8, MEMORY[0x1E4FBC848] + 8, a3);
  return *((float *)&v7 + 1);
}

void closure #1 in closure #1 in static vDSP.normalize<A, B>(_:result:)(float **a1, const float *a2, uint64_t a3, float *a4, float *a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9)
{
  if (!a2)
  {
LABEL_6:
    __break(1u);
    goto LABEL_7;
  }
  uint64_t v9 = *a1;
  if (*a1)
  {
    vDSP_Length v13 = (*(uint64_t (**)(uint64_t))(a9 + 16))(a7);
    if ((v13 & 0x8000000000000000) == 0)
    {
      vDSP_normalize(a2, 1, v9, 1, a4, a5, v13);
      return;
    }
    __break(1u);
    goto LABEL_6;
  }
LABEL_7:
  __break(1u);
}

float static vDSP.standardDeviation<A>(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v11 = *MEMORY[0x1E4F143B8];
  uint64_t v4 = 0;
  uint64_t v6 = a2;
  uint64_t v7 = a3;
  uint64_t v8 = (char *)&v4 + 4;
  uint64_t v9 = &v4;
  uint64_t v10 = a1;
  (*(void (**)(void (*)(const float *, uint64_t), unsigned char *, uint64_t, uint64_t))(*(void *)(a3 + 8)
                                                                                                 + 24))(partial apply for closure #1 in static vDSP.standardDeviation<A>(_:), v5, MEMORY[0x1E4FBC848] + 8, a2);
  return *(float *)&v4;
}

void closure #1 in static vDSP.standardDeviation<A>(_:)(const float *a1, uint64_t a2, float *a3, float *a4, uint64_t a5, uint64_t a6, uint64_t a7)
{
  if (a1)
  {
    vDSP_Length v10 = (*(uint64_t (**)(uint64_t))(*(void *)(a7 + 8) + 16))(a6);
    if ((v10 & 0x8000000000000000) == 0)
    {
      vDSP_normalize(a1, 1, 0, 1, a3, a4, v10);
      return;
    }
    __break(1u);
  }
  __break(1u);
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.add<A, B, C>(_:_:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B>(_:_:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.add<A, B, C>(_:_:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B>(_:_:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.subtract<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.subtract<A, B, C>(_:_:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.subtract<A, B>(_:_:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.subtract<A, B, C>(_:_:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.subtract<A, B>(_:_:));
}

uint64_t partial apply for closure #1 in static vDSP.subtract<A, B, C>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A>(_:_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.add<A>(_:_:)(a1, a2, *(void *)(v2 + 40), *(void *)(v2 + 16), *(void *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float))static vDSP.multiply<A, B>(_:_:result:), *(float *)(v2 + 32));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.add<A>(_:_:)(a1, a2, *(void *)(v2 + 40), *(void *)(v2 + 16), *(void *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double))static vDSP.multiply<A, B>(_:_:result:), *(double *)(v2 + 32));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C>(_:_:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B>(_:_:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C>(_:_:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B>(_:_:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.divide<A>(_:_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.divide<A>(_:_:)(a1, a2, *(void *)(v2 + 32), *(void *)(v2 + 16), *(void *)(v2 + 24), *(float *)(v2 + 40));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.divide<A>(_:_:)(a1, a2, *(void *)(v2 + 32), *(void *)(v2 + 16), *(void *)(v2 + 24), *(double *)(v2 + 40));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.add<A>(_:_:)(a1, a2, *(void *)(v2 + 40), *(void *)(v2 + 16), *(void *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float))static vDSP.divide<A, B>(_:_:result:), *(float *)(v2 + 32));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.add<A>(_:_:)(a1, a2, *(void *)(v2 + 40), *(void *)(v2 + 16), *(void *)(v2 + 24), (void (*)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double))static vDSP.divide<A, B>(_:_:result:), *(double *)(v2 + 32));
}

uint64_t partial apply for closure #1 in static vDSP.divide<A, B>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convert<A, B>(power:toDecibels:zeroReference:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.divide<A, B>(_:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.divide<A, B, C>(_:_:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B>(_:_:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.divide<A, B, C>(_:_:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B>(_:_:));
}

uint64_t partial apply for closure #1 in static vDSP.divide<A, B, C>(_:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.convolve<A, B, C>(_:withKernel:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))
{
  return a6(a1, a2, v6[6], v6[7], v6[2], v6[3], v6[4], v6[5], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:));
}

{
  return partial apply for closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:));
}

uint64_t partial apply for closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1, uint64_t a2)
{
  uint64_t v3 = *(void *)(v2 + 32);
  uint64_t v4 = *(void *)(v2 + 72);
  v6[1] = *(_OWORD *)(v2 + 16);
  uint64_t v7 = v3;
  long long v8 = *(_OWORD *)(v2 + 40);
  long long v9 = *(_OWORD *)(v2 + 56);
  uint64_t v10 = v4;
  long long v11 = *(_OWORD *)(v2 + 88);
  uint64_t v12 = a1;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, void))(v4 + 16))(a2, v6, MEMORY[0x1E4FBC848] + 8, v8);
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(a1, a2, (void (*)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, uint64_t))static vDSP.multiply<A, B, C>(addition:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(a1, a2, (void (*)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, uint64_t))static vDSP.multiply<A, B, C>(addition:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:), a2);
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:), a2);
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D>(addition:_:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.multiply<A, B, C>(addition:_:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D>(addition:_:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.multiply<A, B, C>(addition:_:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, uint64_t))
{
  return a3(a1, v3[10], v3[11], v3[12], v3[2], v3[3], v3[4], v3[5], v3[6], v3[7], v3[8], v3[9], a2);
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B>(subtraction:_:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(a1, a2, (void (*)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, uint64_t))static vDSP.multiply<A, B, C>(subtraction:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(a1, a2, (void (*)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, uint64_t))static vDSP.multiply<A, B, C>(subtraction:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:), a2);
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:), a2);
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  return closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, *(void *)(v3 + 64), *(void *)(v3 + 80), *(void *)(v3 + 16), *(void *)(v3 + 24), *(void *)(v3 + 32), *(void *)(v3 + 40), *(void *)(v3 + 48), a3, *(float *)(v3 + 72), *(void *)(v3 + 56), a2);
}

{
  uint64_t v3;

  return closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, *(void *)(v3 + 64), *(void *)(v3 + 80), *(void *)(v3 + 16), *(void *)(v3 + 24), *(void *)(v3 + 32), *(void *)(v3 + 40), *(void *)(v3 + 48), a3, *(double *)(v3 + 72), *(void *)(v3 + 56), a2);
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D>(subtraction:_:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.multiply<A, B, C>(addition:_:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D>(subtraction:_:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.multiply<A, B, C>(addition:_:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(uint64_t a1, uint64_t *a2)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(a1, a2, (void (*)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, uint64_t))static vDSP.add<A, B, C>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(a1, a2, (void (*)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, uint64_t))static vDSP.add<A, B, C>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t *a2, void (*a3)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, float, uint64_t))
{
  return closure #1 in static vDSP.multiply<A, B>(addition:_:)(a1, a2, *(void *)(v3 + 48), *(void *)(v3 + 16), *(void *)(v3 + 24), *(void *)(v3 + 32), *(void *)(v3 + 40), a3, *(float *)(v3 + 56));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:), a2);
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:), a2);
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B>(addition:_:)(uint64_t a1, uint64_t *a2, void (*a3)(char *, char *, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, double, uint64_t))
{
  return closure #1 in static vDSP.multiply<A, B>(addition:_:)(a1, a2, *(void *)(v3 + 48), *(void *)(v3 + 16), *(void *)(v3 + 24), *(void *)(v3 + 32), *(void *)(v3 + 40), a3, *(double *)(v3 + 56));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void))closure #1 in static vDSP.add<A, B>(multiplication:_:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void))closure #1 in static vDSP.add<A, B>(multiplication:_:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void))closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void))closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.add<A, B, C, D>(multiplication:_:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.multiply<A, B, C>(addition:_:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.add<A, B, C, D>(multiplication:_:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.multiply<A, B, C>(addition:_:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.subtract<A, B, C, D>(multiplication:_:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.subtract<A, B, C, D>(multiplication:_:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:));
}

uint64_t partial apply for closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B>(multiplication:multiplication:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void))closure #1 in static vDSP.add<A, B>(multiplication:multiplication:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void))closure #1 in static vDSP.add<A, B>(multiplication:multiplication:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void))closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void))closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))
{
  return a3(a1, v3[12], v3[13], v3[14], v3[2], v3[3], v3[4], v3[5], v3[6], v3[7], v3[8], v3[9], v3[10], v3[11], a2);
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:addition:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D>(addition:addition:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.multiply<A, B, C>(addition:addition:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D>(addition:addition:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.multiply<A, B, C>(addition:addition:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:));
}

{
  return partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C>(addition:_:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))
{
  return a6(a1, a2, v6[8], v6[9], v6[2], v6[3], v6[4], v6[5], v6[6], v6[7], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:multiplication:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:multiplication:));
}

uint64_t partial apply for closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:subtraction:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D>(addition:subtraction:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Float>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Float> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(a1, a2, (uint64_t)&demangling cache variable for type metadata for UnsafeMutableBufferPointer<Double>, (uint64_t)&lazy protocol witness table cache variable for type UnsafeMutableBufferPointer<Double> and conformance UnsafeMutableBufferPointer<A>, (uint64_t)static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:));
}

uint64_t partial apply for closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:), (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C, D>(multiplication:multiplication:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, uint64_t, uint64_t, uint64_t))
{
  return a6(a1, a2, v6[10], v6[11], v6[2], v6[3], v6[4], v6[5], v6[6], v6[7], v6[8], v6[9], a3, a4, a5);
}

uint64_t partial apply for closure #1 in static vDSP.add<A>(multiplication:_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.add<A>(multiplication:_:)(a1, a2, *(void *)(v2 + 32), *(void *)(v2 + 16), *(void *)(v2 + 24), *(float *)(v2 + 40));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.add<A>(multiplication:_:)(a1, a2, *(void *)(v2 + 32), *(void *)(v2 + 16), *(void *)(v2 + 24), *(double *)(v2 + 40));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  return closure #1 in static vDSP.add<A, B>(multiplication:_:result:)(a1, *(void *)(v2 + 48), *(void *)(v2 + 64), *(void *)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), a2, *(float *)(v2 + 56));
}

{
  uint64_t v2;

  return closure #1 in static vDSP.add<A, B>(multiplication:_:result:)(a1, *(void *)(v2 + 48), *(void *)(v2 + 64), *(void *)(v2 + 16), *(void *)(v2 + 24), *(void *)(v2 + 32), *(void *)(v2 + 40), a2, *(double *)(v2 + 56));
}

uint64_t partial apply for closure #1 in static vDSP.subtract<A, B>(multiplication:_:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void))closure #1 in static vDSP.subtract<A, B>(multiplication:_:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void))closure #1 in static vDSP.subtract<A, B>(multiplication:_:));
}

uint64_t partial apply for closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)(uint64_t a1)
{
  return partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void))closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, (uint64_t (*)(uint64_t, void, void, void, void, void, void, void, void, void))closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B>(multiplication:_:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, void, void, void, void, void, void))
{
  return a3(a1, a2, v3[6], v3[7], v3[2], v3[3], v3[4], v3[5]);
}

uint64_t partial apply for closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t (*a2)(uint64_t, void, void, void, void, void, void, void, void, void))
{
  return a2(a1, v2[8], v2[9], v2[10], v2[2], v2[3], v2[4], v2[5], v2[6], v2[7]);
}

uint64_t partial apply for closure #1 in static vDSP.normalize<A>(_:)(uint64_t a1, uint64_t *a2)
{
  return closure #1 in static vDSP.normalize<A>(_:)(a1, a2, v2[4], v2[2], v2[3]);
}

{
  uint64_t *v2;

  return closure #1 in static vDSP.normalize<A>(_:)(a1, a2, v2[4], v2[2], v2[3]);
}

uint64_t partial apply for closure #1 in static vDSP.normalize<A, B>(_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in static vDSP.normalize<A, B>(_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.normalize<A, B>(_:result:));
}

{
  return partial apply for closure #1 in static vDSP.normalize<A, B>(_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in static vDSP.normalize<A, B>(_:result:));
}

void partial apply for closure #1 in static vDSP.standardDeviation<A>(_:)(const double *a1, uint64_t a2)
{
  closure #1 in static vDSP.standardDeviation<A>(_:)(a1, a2, *(double **)(v2 + 32), *(double **)(v2 + 40), *(void *)(v2 + 48), *(void *)(v2 + 16), *(void *)(v2 + 24));
}

uint64_t partial apply for closure #1 in static vDSP.normalize<A, B>(_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 40);
  uint64_t v5 = *(void *)(v3 + 72);
  void v7[2] = *(void *)(v3 + 16);
  long long v8 = *(_OWORD *)(v3 + 24);
  uint64_t v9 = v4;
  uint64_t v10 = a1;
  uint64_t v11 = a2;
  long long v12 = *(_OWORD *)(v3 + 56);
  uint64_t v13 = v5;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, void))(v4 + 16))(a3, v7, MEMORY[0x1E4FBC848] + 8, v8);
}

void partial apply for closure #1 in static vDSP.standardDeviation<A>(_:)(const float *a1, uint64_t a2)
{
  closure #1 in static vDSP.standardDeviation<A>(_:)(a1, a2, *(float **)(v2 + 32), *(float **)(v2 + 40), *(void *)(v2 + 48), *(void *)(v2 + 16), *(void *)(v2 + 24));
}

void partial apply for closure #1 in closure #1 in static vDSP.normalize<A, B>(_:result:)(float **a1)
{
  closure #1 in closure #1 in static vDSP.normalize<A, B>(_:result:)(a1, *(const float **)(v1 + 48), *(void *)(v1 + 56), *(float **)(v1 + 64), *(float **)(v1 + 72), *(void *)(v1 + 80), *(void *)(v1 + 16), *(void *)(v1 + 24), *(void *)(v1 + 32));
}

void partial apply for closure #1 in closure #1 in static vDSP.normalize<A, B>(_:result:)(double **a1)
{
  closure #1 in closure #1 in static vDSP.normalize<A, B>(_:result:)(a1, *(const double **)(v1 + 48), *(void *)(v1 + 56), *(double **)(v1 + 64), *(double **)(v1 + 72), *(void *)(v1 + 80), *(void *)(v1 + 16), *(void *)(v1 + 24), *(void *)(v1 + 32));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  uint64_t v5 = v2[2];
  uint64_t v6 = v2[5];
  uint64_t v7 = v2[8];
  uint64_t v8 = v2[10];
  uint64_t v9 = v2[11];
  uint64_t v17 = *(void *)(v7 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  uint64_t v12 = a1;
  uint64_t v13 = a2;
  uint64_t v14 = &v17;
  uint64_t v15 = v8;
  uint64_t v16 = v9;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t), unsigned char *, uint64_t, uint64_t, uint64_t))(v6 + 24))(partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:), v11, MEMORY[0x1E4FBC848] + 8, v5, v6);
}

{
  void *v2;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  unsigned char v11[16];
  uint64_t v12;
  uint64_t v13;
  int *v14;
  uint64_t v15;
  uint64_t v16;
  int v17;

  uint64_t v5 = v2[2];
  uint64_t v6 = v2[5];
  uint64_t v7 = v2[8];
  uint64_t v8 = v2[10];
  uint64_t v9 = v2[11];
  uint64_t v17 = *(_DWORD *)(v7 + *(int *)(swift_getTupleTypeMetadata2() + 48));
  uint64_t v12 = a1;
  uint64_t v13 = a2;
  uint64_t v14 = &v17;
  uint64_t v15 = v8;
  uint64_t v16 = v9;
  return (*(uint64_t (**)(uint64_t (*)(uint64_t, uint64_t), unsigned char *, uint64_t, uint64_t, uint64_t))(v6 + 24))(partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:), v11, MEMORY[0x1E4FBC848] + 8, v5, v6);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)(a1, a2, MEMORY[0x1E4F16E08]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)(a1, a2, MEMORY[0x1E4F16E00]);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(multiplication:_:result:)(a1, a2, *(void *)(v3 + 16), *(void *)(v3 + 24), *(void *)(v3 + 32), *(void **)(v3 + 40), *(void *)(v3 + 48), a3);
}

void partial apply for closure #1 in closure #1 in static vDSP.add<A, B>(multiplication:_:result:)(const double *a1, uint64_t a2)
{
  closure #1 in closure #1 in static vDSP.add<A, B>(multiplication:_:result:)(a1, *(double *)(v2 + 56), a2, *(void *)(v2 + 48), *(double ***)(v2 + 64), *(void *)(v2 + 72));
}

void partial apply for closure #1 in closure #1 in static vDSP.add<A, B>(multiplication:_:result:)(const float *a1, uint64_t a2)
{
  closure #1 in closure #1 in static vDSP.add<A, B>(multiplication:_:result:)(a1, *(float *)(v2 + 56), a2, *(void *)(v2 + 48), *(float ***)(v2 + 64), *(void *)(v2 + 72));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E4F16AE8]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E4F16AE0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))
{
  return a4(a1, a2, v4[12], v4[13], v4[14], v4[15], v4[2], v4[3], v4[4], v4[5], v4[6], v4[7], v4[8], v4[9], v4[10], v4[11], a3);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))
{
  return a4(a1, a2, v4[12], v4[13], v4[14], v4[15], v4[16], v4[2], v4[3], v4[4], v4[5], v4[6], v4[7], v4[8], v4[9], v4[10], v4[11], a3);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, void, uint64_t, uint64_t))
{
  return closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, *(void *)(v3 + 16), *(void *)(v3 + 24), *(void *)(v3 + 32), *(void *)(v3 + 40), *(void *)(v3 + 48), *(void *)(v3 + 56), *(void **)(v3 + 64), *(void *)(v3 + 72), a3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(subtraction:subtraction:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E4F16DE0]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E4F16DD8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E4F16D20]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E4F16D18]);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(v3 + 24);
  uint64_t v6 = *(void *)(v3 + 64);
  long long v12 = *(_OWORD *)(v3 + 104);
  uint64_t v8 = *(void *)(v3 + 120);
  uint64_t v7 = *(void *)(v3 + 128);
  uint64_t v10 = *(void *)(v3 + 136);
  uint64_t v9 = *(void *)(v3 + 144);
  swift_getTupleTypeMetadata2();
  long long v16 = v12;
  uint64_t v17 = v8;
  uint64_t v18 = v7;
  uint64_t v19 = a1;
  uint64_t v20 = a2;
  uint64_t v21 = v10;
  uint64_t v22 = v9;
  return (*(uint64_t (**)(uint64_t, unsigned char *, uint64_t, uint64_t, uint64_t))(v6 + 24))(a3, v15, MEMORY[0x1E4FBC848] + 8, v5, v6);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:), a3);
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:), a3);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:), a3);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:), a3);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E4F16A98]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E4F16A90]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X8>)
{
  return closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, v4[10], v4[11], v4[12], v4[13], v4[2], v4[3], a4, v4[4], v4[5], v4[6], v4[7], v4[8], v4[9], a3);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, uint64_t a4@<X8>)
{
  return closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(a1, a2, *(void *)(v4 + 80), *(void *)(v4 + 88), *(void *)(v4 + 96), *(void *)(v4 + 104), *(void *)(v4 + 112), *(void *)(v4 + 16), a4, *(void *)(v4 + 24), *(void *)(v4 + 32), *(_OWORD *)(v4 + 40), *(void *)(v4 + 56), *(void *)(v4 + 64), *(void *)(v4 + 72), a3);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:addition:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(v3 + 32);
  uint64_t v6 = *(void *)(v3 + 64);
  long long v12 = *(_OWORD *)(v3 + 88);
  uint64_t v8 = *(void *)(v3 + 104);
  uint64_t v7 = *(void *)(v3 + 112);
  uint64_t v10 = *(void *)(v3 + 120);
  uint64_t v9 = *(void *)(v3 + 128);
  swift_getTupleTypeMetadata2();
  long long v16 = v12;
  uint64_t v17 = v8;
  uint64_t v18 = v7;
  uint64_t v19 = a1;
  uint64_t v20 = a2;
  uint64_t v21 = v10;
  uint64_t v22 = v9;
  return (*(uint64_t (**)(uint64_t, unsigned char *, uint64_t, uint64_t, uint64_t))(v6 + 24))(a3, v15, MEMORY[0x1E4FBC848] + 8, v5, v6);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:), (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void, void, void, void, uint64_t))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D, E>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E4F16D10]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(a1, a2, MEMORY[0x1E4F16D08]);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D, E>(addition:subtraction:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(v3 + 40);
  uint64_t v6 = *(void *)(v3 + 80);
  long long v12 = *(_OWORD *)(v3 + 104);
  uint64_t v8 = *(void *)(v3 + 120);
  uint64_t v7 = *(void *)(v3 + 128);
  uint64_t v10 = *(void *)(v3 + 136);
  uint64_t v9 = *(void *)(v3 + 144);
  swift_getTupleTypeMetadata2();
  long long v16 = v12;
  uint64_t v17 = v8;
  uint64_t v18 = v7;
  uint64_t v19 = a1;
  uint64_t v20 = a2;
  uint64_t v21 = v10;
  uint64_t v22 = v9;
  return (*(uint64_t (**)(uint64_t, unsigned char *, uint64_t, uint64_t, uint64_t))(v6 + 24))(a3, v15, MEMORY[0x1E4FBC848] + 8, v5, v6);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void))closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void))closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void, void))
{
  return a3(a1, a2, v3[8], v3[9], v3[10], v3[11], v3[12], v3[13], v3[2], v3[3], v3[4], v3[5], v3[6], v3[7]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, MEMORY[0x1E4F16D40]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, MEMORY[0x1E4F16D38]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v6 = *(void *)(v3 + 16);
  uint64_t v5 = *(void *)(v3 + 24);
  uint64_t v7 = *(void *)(v3 + 88);
  uint64_t v9 = *(void *)(v3 + 96);
  uint64_t v8 = *(void *)(v3 + 104);
  long long v14 = *(_OWORD *)(v3 + 32);
  uint64_t v10 = *(void *)(v3 + 32);
  swift_getTupleTypeMetadata2();
  long long v11 = *(_OWORD *)(v3 + 48);
  long long v12 = *(_OWORD *)(v3 + 64);
  long long v20 = v14;
  long long v21 = v11;
  uint64_t v18 = v6;
  uint64_t v19 = v5;
  long long v22 = v12;
  uint64_t v23 = v7;
  uint64_t v24 = a1;
  uint64_t v25 = a2;
  uint64_t v26 = v9;
  uint64_t v27 = v8;
  return (*(uint64_t (**)(uint64_t, unsigned char *, uint64_t, uint64_t))(v12 + 24))(a3, v17, MEMORY[0x1E4FBC848] + 8, v10);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 16);
  uint64_t v5 = *(void *)(v3 + 48);
  uint64_t v6 = *(void *)(v3 + 104);
  v8[1] = *(_OWORD *)(v3 + 88);
  uint64_t v9 = a1;
  uint64_t v10 = a2;
  uint64_t v11 = v6;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E4FBC848] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t))
{
  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(a1, a2, *(void *)(v3 + 16), *(void *)(v3 + 24), *(void *)(v3 + 32), *(void *)(v3 + 40), *(void **)(v3 + 48), *(void *)(v3 + 56), a3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, MEMORY[0x1E4F16CE0]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, MEMORY[0x1E4F16CD8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void))closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void))closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, (uint64_t (*)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void))closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, MEMORY[0x1E4F16D30]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, MEMORY[0x1E4F16D28]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:multiplication:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void))
{
  return a3(a1, a2, v3[8], v3[9], v3[10], v3[11], v3[2], v3[3], v3[4], v3[5], v3[6], v3[7]);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, void, void, void, void, void, void, void, void, void, void, void))
{
  return a3(a1, a2, v3[8], v3[9], v3[10], v3[11], v3[12], v3[2], v3[3], v3[4], v3[5], v3[6], v3[7]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(subtraction:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, MEMORY[0x1E4F16DD0]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, MEMORY[0x1E4F16DC8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(subtraction:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, MEMORY[0x1E4F16DF0]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, MEMORY[0x1E4F16DE8]);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, void *))
{
  return closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, *(double *)(v3 + 16), a2, *(void *)(v3 + 24), *(void *)(v3 + 32), *(void **)(v3 + 40), *(void *)(v3 + 48), a3);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, uint64_t, uint64_t, float *))
{
  return closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(a1, *(float *)(v3 + 16), a2, *(void *)(v3 + 24), *(void *)(v3 + 32), *(void **)(v3 + 40), *(void *)(v3 + 48), a3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C, D>(addition:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, MEMORY[0x1E4F16AD8]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C, D>(multiplication:_:result:)(a1, a2, MEMORY[0x1E4F16AD0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v5 = *(void *)(v3 + 16);
  uint64_t v6 = *(void *)(v3 + 72);
  uint64_t v7 = *(void *)(v3 + 88);
  uint64_t v8 = *(void *)(v3 + 96);
  uint64_t v9 = *(void *)(v3 + 104);
  long long v12 = *(_OWORD *)(v3 + 24);
  uint64_t v10 = *(void *)(v3 + 24);
  swift_getTupleTypeMetadata2();
  uint64_t v16 = v5;
  long long v17 = v12;
  long long v18 = *(_OWORD *)(v3 + 40);
  long long v19 = *(_OWORD *)(v3 + 56);
  uint64_t v20 = v6;
  uint64_t v21 = v7;
  uint64_t v22 = a1;
  uint64_t v23 = a2;
  uint64_t v24 = v8;
  uint64_t v25 = v9;
  return (*(uint64_t (**)(uint64_t, unsigned char *, uint64_t, uint64_t))(v19 + 24))(a3, v15, MEMORY[0x1E4FBC848] + 8, v10);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C, D>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 32);
  uint64_t v5 = *(void *)(v3 + 64);
  uint64_t v6 = *(void *)(v3 + 104);
  v8[1] = *(_OWORD *)(v3 + 88);
  uint64_t v9 = a1;
  uint64_t v10 = a2;
  uint64_t v11 = v6;
  return (*(uint64_t (**)(uint64_t, _OWORD *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E4FBC848] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v6 = v3[3];
  uint64_t v7 = v3[6];
  uint64_t v8 = v3[9];
  uint64_t v10 = v3[10];
  uint64_t v9 = v3[11];
  swift_getTupleTypeMetadata2();
  uint64_t v14 = v8;
  uint64_t v15 = a1;
  uint64_t v16 = a2;
  uint64_t v17 = v10;
  uint64_t v18 = v9;
  return (*(uint64_t (**)(uint64_t, unsigned char *, uint64_t, uint64_t, uint64_t))(v7 + 24))(a3, v13, MEMORY[0x1E4FBC848] + 8, v6, v7);
}

{
  uint64_t v3;
  uint64_t v6;
  uint64_t v7;
  int v8;
  uint64_t v9;
  uint64_t v10;
  unsigned char v13[16];
  int v14;
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  uint64_t v18;

  uint64_t v6 = *(void *)(v3 + 24);
  uint64_t v7 = *(void *)(v3 + 48);
  uint64_t v8 = *(_DWORD *)(v3 + 72);
  uint64_t v10 = *(void *)(v3 + 80);
  uint64_t v9 = *(void *)(v3 + 88);
  swift_getTupleTypeMetadata2();
  uint64_t v14 = v8;
  uint64_t v15 = a1;
  uint64_t v16 = a2;
  uint64_t v17 = v10;
  uint64_t v18 = v9;
  return (*(uint64_t (**)(uint64_t, unsigned char *, uint64_t, uint64_t, uint64_t))(v7 + 24))(a3, v13, MEMORY[0x1E4FBC848] + 8, v6, v7);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(addition:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, MEMORY[0x1E4F16AF8]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(multiplication:_:result:)(a1, a2, MEMORY[0x1E4F16AF0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1)
{
  return partial apply for closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:));
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, a2, MEMORY[0x1E4F16AC8]);
}

{
  return partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, a2, MEMORY[0x1E4F16AC0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1, uint64_t a2)
{
  long long v3 = v2[2];
  long long v7 = v2[1];
  long long v8 = v3;
  long long v4 = v2[4];
  long long v9 = v2[3];
  long long v10 = v4;
  long long v11 = *(_OWORD *)((char *)v2 + 88);
  uint64_t v12 = a1;
  return (*(uint64_t (**)(uint64_t, uint64_t *, uint64_t, void))(v9 + 24))(a2, &v6, MEMORY[0x1E4FBC848] + 8, v7);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v4 = *(void *)(v3 + 24);
  uint64_t v5 = *(void *)(v3 + 56);
  uint64_t v6 = *(void *)(v3 + 104);
  _OWORD v8[2] = a1;
  void v8[3] = a2;
  long long v9 = *(_OWORD *)(v3 + 88);
  uint64_t v10 = v6;
  return (*(uint64_t (**)(uint64_t, void *, uint64_t, uint64_t))(v5 + 24))(a3, v8, MEMORY[0x1E4FBC848] + 8, v4);
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(uint64_t a1, uint64_t a2, uint64_t (*a3)(uint64_t))
{
  return closure #1 in closure #1 in closure #1 in closure #1 in static vDSP.addSubtract<A, B, C, D>(_:_:addResult:subtractResult:)(a1, a2, *(void *)(v3 + 16), *(void *)(v3 + 24), *(void **)(v3 + 32), *(void **)(v3 + 40), *(void *)(v3 + 48), a3);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16B48]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.divide<A, B, C>(_:_:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16B40]);
}

void partial apply for closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(const double *a1, int a2)
{
  closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(a1, a2, *(double ***)(v2 + 24), *(void *)(v2 + 32), *(double *)(v2 + 16));
}

void partial apply for closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(const float *a1, int a2)
{
  closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(a1, a2, *(float ***)(v2 + 24), *(void *)(v2 + 32), *(float *)(v2 + 16));
}

{
  uint64_t v2;

  closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(a1, a2, *(float ***)(v2 + 24), *(void *)(v2 + 32), *(float *)(v2 + 16));
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.divide<A, B>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:)(a1, a2, *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16DF8]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.multiply<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16D50]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16D48]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.multiply<A, B>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:)(a1, a2, *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16E18]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:)(a1, a2, *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16E10]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.subtract<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.subtract<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16E60]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16E58]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:));
}

{
  return partial apply for closure #1 in closure #1 in static vDSP.compress<A, B, C>(_:gatingVector:result:)(a1, a2, (uint64_t)partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:));
}

uint64_t partial apply for closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16AB8]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in closure #1 in static vDSP.add<A, B, C>(_:_:result:)(a1, a2, *(void *)(v2 + 16), *(void *)(v2 + 24), *(void **)(v2 + 32), *(void *)(v2 + 40), MEMORY[0x1E4F16AB0]);
}

uint64_t partial apply for closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:)(uint64_t a1, uint64_t a2)
{
  return closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:)(a1, a2, *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16DC0]);
}

{
  uint64_t v2;

  return closure #1 in closure #1 in static vDSP.add<A, B>(_:_:result:)(a1, a2, *(void **)(v2 + 24), *(void *)(v2 + 32), MEMORY[0x1E4F16DB8]);
}

uint64_t BNNS.ReductionFunction.bnnsReduceFunction.getter()
{
  if (*((unsigned char *)v0 + 4) == 1) {
    return dword_1D2137C00[*v0];
  }
  else {
    return 8;
  }
}

uint64_t static BNNS.applyReduction(_:input:output:weights:filterParameters:)(unsigned int *a1, _OWORD *a2, long long *a3, uint64_t a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v33 = *MEMORY[0x1E4F143B8];
  uint64_t v11 = *a1;
  int v12 = *((unsigned __int8 *)a1 + 4);
  BNNSNDArrayDescriptor.shape.getter((uint64_t)&v27);
  outlined init with take of BNNS.Shape((uint64_t)&v27, (uint64_t)v31);
  unsigned int v13 = _s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v31);
  if (v13 > 0x12) {
    uint64_t v14 = 8;
  }
  else {
    uint64_t v14 = qword_1D2137B68[v13];
  }
  BNNSNDArrayDescriptor.shape.getter((uint64_t)v26);
  outlined init with take of BNNS.Shape((uint64_t)v26, (uint64_t)v32);
  unsigned int v15 = _s10Accelerate4BNNSO5ShapeOWOg((uint64_t)v32);
  if (v15 > 0x12) {
    uint64_t v16 = 8;
  }
  else {
    uint64_t v16 = qword_1D2137B68[v15];
  }
  if (v14 != v16) {
    __break(1u);
  }
  float v17 = *(float *)&v11;
  if (v12) {
    float v17 = 0.0;
  }
  if (a7 == 1)
  {
    LOBYTE(v27) = v12;
    unint64_t v18 = v11 | ((unint64_t)v12 << 32);
    long long v19 = 0;
  }
  else
  {
    int v27 = a5;
    uint64_t v28 = a6;
    uint64_t v29 = a7;
    uint64_t v30 = a8;
    v26[0] = v12;
    unint64_t v18 = v11 | ((unint64_t)v12 << 32);
    long long v19 = &v27;
  }
  uint64_t result = closure #1 in static BNNS.applyReduction(_:input:output:weights:filterParameters:)((uint64_t)v19, a2, a3, a4, v18, v17);
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    unsigned char *v21 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t closure #1 in static BNNS.applyReduction(_:input:output:weights:filterParameters:)(uint64_t a1, _OWORD *a2, long long *a3, uint64_t a4, uint64_t a5, float a6)
{
  uint64_t v66 = *MEMORY[0x1E4F143B8];
  outlined init with take of BNNSNDArrayDescriptor?(a4, (uint64_t)v63);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v63, (uint64_t)v65);
  outlined init with take of BNNSNDArrayDescriptor?(a4, (uint64_t)v64);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v64) != 1)
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v65, (uint64_t)__src);
    uint64_t v35 = *(void *)&__src[0];
    long long v42 = *(_OWORD *)((char *)&__src[1] + 8);
    long long v43 = *(_OWORD *)((char *)__src + 8);
    long long v40 = *(_OWORD *)((char *)&__src[3] + 8);
    long long v41 = *(_OWORD *)((char *)&__src[2] + 8);
    long long v38 = *(_OWORD *)((char *)&__src[5] + 8);
    long long v39 = *(_OWORD *)((char *)&__src[4] + 8);
    long long v36 = *(_OWORD *)((char *)&__src[7] + 8);
    long long v37 = *(_OWORD *)((char *)&__src[6] + 8);
    uint64_t v13 = *((void *)&__src[8] + 1);
    uint64_t v14 = *((void *)&__src[9] + 1);
    uint64_t v34 = *(void *)&__src[9];
    int v15 = __src[10];
    uint64_t v16 = *(void *)((char *)&__src[10] + 4);
    int v6 = HIDWORD(__src[10]);
    if ((a5 & 0x100000000) != 0) {
      goto LABEL_3;
    }
LABEL_5:
    unsigned int v17 = 8;
    goto LABEL_6;
  }
  uint64_t v13 = 0;
  uint64_t v14 = 0;
  int v15 = 0;
  long long v42 = 0u;
  long long v43 = 0u;
  uint64_t v16 = 0;
  long long v40 = 0u;
  long long v41 = 0u;
  long long v38 = 0u;
  long long v39 = 0u;
  long long v36 = 0u;
  long long v37 = 0u;
  uint64_t v34 = 0;
  uint64_t v35 = 0;
  if ((a5 & 0x100000000) == 0) {
    goto LABEL_5;
  }
LABEL_3:
  unsigned int v17 = dword_1D2137C00[(int)a5];
LABEL_6:
  long long v18 = a2[9];
  __src[8] = a2[8];
  __src[9] = v18;
  long long v19 = a2[5];
  __src[4] = a2[4];
  __src[5] = v19;
  long long v20 = a2[7];
  __src[6] = a2[6];
  __src[7] = v20;
  long long v21 = a2[1];
  __src[0] = *a2;
  __src[1] = v21;
  long long v22 = a2[3];
  __src[2] = a2[2];
  __src[3] = v22;
  long long v23 = a3[8];
  long long v24 = a3[9];
  long long v25 = a3[6];
  __src[18] = a3[7];
  __src[19] = v23;
  long long v26 = a3[10];
  __src[20] = v24;
  __src[21] = v26;
  long long v27 = a3[4];
  long long v28 = a3[5];
  long long v29 = a3[2];
  __src[14] = a3[3];
  __src[15] = v27;
  long long v30 = a2[10];
  __src[16] = v28;
  __src[17] = v25;
  long long v31 = *a3;
  long long v32 = a3[1];
  __src[10] = v30;
  __src[11] = v31;
  __src[12] = v32;
  __src[13] = v29;
  memcpy(__dst, __src, sizeof(__dst));
  uint64_t v46 = v35;
  long long v47 = v43;
  long long v48 = v42;
  long long v49 = v41;
  long long v50 = v40;
  long long v51 = v39;
  long long v52 = v38;
  long long v53 = v37;
  long long v54 = v36;
  uint64_t v55 = v13;
  uint64_t v56 = v34;
  uint64_t v57 = v14;
  int v58 = v15;
  uint64_t v59 = v16;
  int v60 = v6;
  unsigned int v61 = v17;
  float v62 = a6;
  return MEMORY[0x1D25FFF40](__dst, a1);
}

uint64_t BNNS.ReductionLayer.__allocating_init(function:input:output:weights:filterParameters:)(int *a1, _OWORD *a2, long long *a3, uint64_t a4, int a5, uint64_t a6, uint64_t a7, uint64_t a8)
{
  uint64_t v81 = *MEMORY[0x1E4F143B8];
  outlined init with take of BNNSNDArrayDescriptor?(a4, (uint64_t)v78);
  outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v78, (uint64_t)v80);
  uint64_t v14 = *a1;
  int v15 = *((unsigned __int8 *)a1 + 4);
  if (*((unsigned char *)a1 + 4)) {
    float v16 = 0.0;
  }
  else {
    float v16 = *(float *)a1;
  }
  outlined init with take of BNNSNDArrayDescriptor?(a4, (uint64_t)v79);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)v79) == 1)
  {
    uint64_t v17 = 0;
    uint64_t v18 = 0;
    int v19 = 0;
    long long v53 = 0u;
    long long v54 = 0u;
    uint64_t v20 = 0;
    long long v51 = 0u;
    long long v52 = 0u;
    long long v49 = 0u;
    long long v50 = 0u;
    long long v47 = 0u;
    long long v48 = 0u;
    uint64_t v45 = 0;
    uint64_t v46 = 0;
    if (!v15)
    {
LABEL_6:
      unsigned int v21 = 8;
      goto LABEL_9;
    }
  }
  else
  {
    outlined init with take of BNNSNDArrayDescriptor?((uint64_t)v80, (uint64_t)__src);
    uint64_t v46 = *(void *)&__src[0];
    long long v53 = *(_OWORD *)((char *)&__src[1] + 8);
    long long v54 = *(_OWORD *)((char *)__src + 8);
    long long v51 = *(_OWORD *)((char *)&__src[3] + 8);
    long long v52 = *(_OWORD *)((char *)&__src[2] + 8);
    long long v49 = *(_OWORD *)((char *)&__src[5] + 8);
    long long v50 = *(_OWORD *)((char *)&__src[4] + 8);
    long long v47 = *(_OWORD *)((char *)&__src[7] + 8);
    long long v48 = *(_OWORD *)((char *)&__src[6] + 8);
    uint64_t v17 = *((void *)&__src[8] + 1);
    uint64_t v18 = *((void *)&__src[9] + 1);
    uint64_t v45 = *(void *)&__src[9];
    uint64_t v20 = *(void *)((char *)&__src[10] + 4);
    int v19 = __src[10];
    int v8 = HIDWORD(__src[10]);
    if (!v15) {
      goto LABEL_6;
    }
  }
  unsigned int v21 = dword_1D2137C00[v14];
LABEL_9:
  long long v22 = a2[9];
  __src[8] = a2[8];
  __src[9] = v22;
  long long v23 = a2[5];
  __src[4] = a2[4];
  __src[5] = v23;
  long long v24 = a2[7];
  __src[6] = a2[6];
  __src[7] = v24;
  long long v25 = a2[1];
  __src[0] = *a2;
  __src[1] = v25;
  long long v26 = a2[3];
  __src[2] = a2[2];
  __src[3] = v26;
  long long v27 = a3[8];
  long long v28 = a3[9];
  long long v29 = a3[6];
  __src[18] = a3[7];
  __src[19] = v27;
  long long v30 = a3[10];
  __src[20] = v28;
  __src[21] = v30;
  long long v31 = a3[4];
  long long v32 = a3[5];
  long long v33 = a3[2];
  __src[14] = a3[3];
  __src[15] = v31;
  long long v34 = a2[10];
  __src[16] = v32;
  __src[17] = v29;
  long long v35 = *a3;
  long long v36 = a3[1];
  __src[10] = v34;
  __src[11] = v35;
  __src[12] = v36;
  __src[13] = v33;
  memcpy(__dst, __src, sizeof(__dst));
  uint64_t v61 = v46;
  long long v62 = v54;
  long long v63 = v53;
  long long v64 = v52;
  long long v65 = v51;
  long long v66 = v50;
  long long v67 = v49;
  long long v68 = v48;
  long long v69 = v47;
  uint64_t v70 = v17;
  uint64_t v71 = v45;
  uint64_t v72 = v18;
  int v73 = v19;
  uint64_t v74 = v20;
  int v75 = v8;
  unsigned int v76 = v21;
  float v77 = v16;
  if (a7 == 1)
  {
    long long v37 = 0;
  }
  else
  {
    int v55 = a5;
    uint64_t v56 = a6;
    uint64_t v57 = a7;
    uint64_t v58 = a8;
    long long v37 = &v55;
  }
  uint64_t v38 = MEMORY[0x1D2600070](__dst, v37);
  type metadata accessor for BNNS.ReductionLayer();
  uint64_t v39 = swift_allocObject();
  uint64_t v40 = v39;
  if (v38)
  {
    *(void *)(v39 + 16) = v38;
  }
  else
  {
    type metadata accessor for BNNS.Layer();
    swift_deallocPartialClassInstance();
    return 0;
  }
  return v40;
}

uint64_t BNNS.ReductionLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:)(size_t a1, uint64_t a2, uint64_t a3, _OWORD *a4, _OWORD *a5, uint64_t a6)
{
  uint64_t v7 = v6;
  uint64_t v29 = *MEMORY[0x1E4F143B8];
  long long v13 = a4[9];
  *(_OWORD *)&v27.stride[7] = a4[8];
  *(_OWORD *)&v27.data_type = v13;
  *(_OWORD *)&v27.table_data_type = a4[10];
  long long v14 = a4[5];
  *(_OWORD *)&v27.size[7] = a4[4];
  *(_OWORD *)&v27.stride[1] = v14;
  long long v15 = a4[7];
  *(_OWORD *)&v27.stride[3] = a4[6];
  *(_OWORD *)&v27.stride[5] = v15;
  long long v16 = a4[1];
  *(_OWORD *)&v27.vImage_Flags flags = *a4;
  *(_OWORD *)&v27.size[1] = v16;
  long long v17 = a4[3];
  *(_OWORD *)&v27.size[3] = a4[2];
  *(_OWORD *)&v27.size[5] = v17;
  long long v18 = a5[9];
  *(_OWORD *)&v26.stride[7] = a5[8];
  *(_OWORD *)&v26.data_type = v18;
  *(_OWORD *)&v26.table_data_type = a5[10];
  long long v19 = a5[5];
  *(_OWORD *)&v26.size[7] = a5[4];
  *(_OWORD *)&v26.stride[1] = v19;
  long long v20 = a5[7];
  *(_OWORD *)&v26.stride[3] = a5[6];
  *(_OWORD *)&v26.stride[5] = v20;
  long long v21 = a5[1];
  *(_OWORD *)&v26.vImage_Flags flags = *a5;
  *(_OWORD *)&v26.size[1] = v21;
  long long v22 = a5[3];
  *(_OWORD *)&v26.size[3] = a5[2];
  *(_OWORD *)&v26.size[5] = v22;
  outlined init with take of BNNSNDArrayDescriptor?(a6, (uint64_t)&v28);
  if (_sSo21BNNSNDArrayDescriptoraSgWOg((uint64_t)&v28) == 1)
  {
    uint64_t result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(0, v7, a1, a2, &v26, (uint64_t)a5, a3, &v27, (uint64_t)a4, 0);
  }
  else
  {
    BNNSNDArrayDescriptor v25 = v28;
    uint64_t result = closure #1 in closure #1 in closure #1 in closure #1 in static BNNS.layerApplyBackward(_:batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:generatingBiasGradient:)(0, v7, a1, a2, &v26, (uint64_t)a5, a3, &v27, (uint64_t)a4, &v25);
  }
  if (result)
  {
    lazy protocol witness table accessor for type BNNS.Error and conformance BNNS.Error();
    swift_allocError();
    *long long v24 = 0;
    return swift_willThrow();
  }
  return result;
}

uint64_t type metadata accessor for BNNS.ReductionLayer()
{
  return self;
}

uint64_t BNNS.ReductionLayer.deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));
  return v0;
}

uint64_t BNNS.ReductionLayer.__deallocating_deinit()
{
  BNNSFilterDestroy(*(void **)(v0 + 16));

  return swift_deallocClassInstance();
}

uint64_t method lookup function for BNNS.ReductionLayer(uint64_t a1, uint64_t a2)
{
  return MEMORY[0x1F4186708](a1, a2, &nominal type descriptor for BNNS.ReductionLayer);
}

uint64_t dispatch thunk of BNNS.ReductionLayer.applyBackward(batchSize:input:output:outputGradient:generatingInputGradient:generatingWeightsGradient:)(uint64_t a1, uint64_t *a2, uint64_t *a3, uint64_t *a4, uint64_t *a5, uint64_t a6)
{
  uint64_t v7 = a2[17];
  int v8 = *((_DWORD *)a2 + 36);
  uint64_t v9 = a2[19];
  int v10 = *((_DWORD *)a2 + 40);
  uint64_t v11 = a3[17];
  int v12 = *((_DWORD *)a3 + 36);
  uint64_t v13 = a3[19];
  int v14 = *((_DWORD *)a3 + 40);
  uint64_t v15 = a4[17];
  int v16 = *((_DWORD *)a4 + 36);
  uint64_t v17 = a4[19];
  int v18 = *((_DWORD *)a4 + 40);
  uint64_t v19 = a5[17];
  int v20 = *((_DWORD *)a5 + 36);
  uint64_t v21 = a5[19];
  int v22 = *((_DWORD *)a5 + 40);
  char v23 = *(unsigned char *)(a6 + 176);
  long long v24 = *(uint64_t (**)(uint64_t, uint64_t *, uint64_t *, uint64_t *, uint64_t *, _OWORD *))(*(void *)v6 + 112);
  long long v85 = *(_OWORD *)(a2 + 11);
  long long v86 = *(_OWORD *)(a2 + 13);
  long long v87 = *(_OWORD *)(a2 + 15);
  uint64_t v92 = *(uint64_t *)((char *)a2 + 164);
  uint64_t v25 = *a2;
  long long v80 = *(_OWORD *)(a2 + 1);
  long long v81 = *(_OWORD *)(a2 + 3);
  long long v82 = *(_OWORD *)(a2 + 5);
  long long v83 = *(_OWORD *)(a2 + 7);
  long long v84 = *(_OWORD *)(a2 + 9);
  long long v71 = *(_OWORD *)(a3 + 11);
  long long v72 = *(_OWORD *)(a3 + 13);
  long long v73 = *(_OWORD *)(a3 + 15);
  uint64_t v78 = *(uint64_t *)((char *)a3 + 164);
  long long v66 = *(_OWORD *)(a3 + 1);
  long long v67 = *(_OWORD *)(a3 + 3);
  long long v68 = *(_OWORD *)(a3 + 5);
  long long v69 = *(_OWORD *)(a3 + 7);
  long long v70 = *(_OWORD *)(a3 + 9);
  long long v57 = *(_OWORD *)(a4 + 11);
  long long v58 = *(_OWORD *)(a4 + 13);
  long long v59 = *(_OWORD *)(a4 + 15);
  uint64_t v64 = *(uint64_t *)((char *)a4 + 164);
  long long v52 = *(_OWORD *)(a4 + 1);
  long long v53 = *(_OWORD *)(a4 + 3);
  long long v54 = *(_OWORD *)(a4 + 5);
  long long v55 = *(_OWORD *)(a4 + 7);
  long long v56 = *(_OWORD *)(a4 + 9);
  long long v38 = *(_OWORD *)(a5 + 1);
  long long v39 = *(_OWORD *)(a5 + 3);
  long long v40 = *(_OWORD *)(a5 + 5);
  long long v41 = *(_OWORD *)(a5 + 7);
  long long v42 = *(_OWORD *)(a5 + 9);
  long long v43 = *(_OWORD *)(a5 + 11);
  long long v44 = *(_OWORD *)(a5 + 13);
  long long v45 = *(_OWORD *)(a5 + 15);
  uint64_t v50 = *(uint64_t *)((char *)a5 + 164);
  uint64_t v26 = *a3;
  uint64_t v79 = v25;
  uint64_t v27 = *a4;
  uint64_t v65 = v26;
  uint64_t v28 = *a5;
  uint64_t v51 = v27;
  uint64_t v37 = v28;
  long long v29 = *(_OWORD *)(a6 + 16);
  v35[0] = *(_OWORD *)a6;
  v35[1] = v29;
  long long v30 = *(_OWORD *)(a6 + 48);
  _OWORD v35[2] = *(_OWORD *)(a6 + 32);
  v35[3] = v30;
  long long v31 = *(_OWORD *)(a6 + 80);
  void v35[4] = *(_OWORD *)(a6 + 64);
  v35[5] = v31;
  long long v32 = *(_OWORD *)(a6 + 112);
  v35[6] = *(_OWORD *)(a6 + 96);
  v35[7] = v32;
  long long v33 = *(_OWORD *)(a6 + 144);
  v35[8] = *(_OWORD *)(a6 + 128);
  v35[9] = v33;
  v35[10] = *(_OWORD *)(a6 + 160);
  uint64_t v88 = v7;
  int v89 = v8;
  uint64_t v90 = v9;
  int v91 = v10;
  uint64_t v74 = v11;
  int v75 = v12;
  uint64_t v76 = v13;
  int v77 = v14;
  uint64_t v60 = v15;
  int v61 = v16;
  uint64_t v62 = v17;
  int v63 = v18;
  uint64_t v46 = v19;
  int v47 = v20;
  uint64_t v48 = v21;
  int v49 = v22;
  char v36 = v23;
  return v24(a1, &v79, &v65, &v51, &v37, v35);
}

uint64_t getEnumTagSinglePayload for BNNS.ReductionFunction(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 5)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for BNNS.ReductionFunction(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(unsigned char *)(result + 4) = 0;
    *(_DWORD *)uint64_t result = a2 - 1;
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 5) = v3;
  return result;
}

uint64_t getEnumTag for BNNS.ReductionFunction(uint64_t a1)
{
  if (*(unsigned char *)(a1 + 4)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t destructiveInjectEnumTag for BNNS.ReductionFunction(uint64_t result, int a2)
{
  if (a2)
  {
    *(_DWORD *)uint64_t result = a2 - 1;
    *(unsigned char *)(result + 4) = 1;
  }
  else
  {
    *(unsigned char *)(result + 4) = 0;
  }
  return result;
}

ValueMetadata *type metadata accessor for BNNS.ReductionFunction()
{
  return &type metadata for BNNS.ReductionFunction;
}

vImage_Error vImage_Buffer.copy(destinationBuffer:pixelSize:flags:)(vImage_Buffer *dest, size_t pixelSize, vImage_Flags *a3, void *a4, vImagePixelCount a5, vImagePixelCount a6, size_t a7)
{
  uint64_t v15 = *MEMORY[0x1E4F143B8];
  if ((a6 & 0x8000000000000000) != 0) {
    __break(1u);
  }
  if (a6)
  {
    vImage_Flags v7 = *a3;
    src.char data = a4;
    src.vImagePixelCount height = a5;
    src.vImagePixelCount width = a6;
    src.size_t rowBytes = a7;
    vImage_Error result = vImageCopyBuffer(&src, dest, pixelSize, v7);
    if (!result) {
      return result;
    }
    uint64_t v9 = result;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    uint64_t v11 = v10;
    vImage.Error.init(rawValue:)(v9, (char *)&src);
    char data = (char)src.data;
    if (LOBYTE(src.data) == 20) {
      char data = 11;
    }
    *uint64_t v11 = data;
  }
  else
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    *uint64_t v13 = 8;
  }
  return swift_willThrow();
}

uint64_t vImage_Buffer.size.getter(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4)
{
  v5[4] = *MEMORY[0x1E4F143B8];
  v5[0] = a1;
  v5[1] = a2;
  double v5[2] = a3;
  _OWORD v5[3] = a4;
  return MEMORY[0x1D2600EF0](v5);
}

uint64_t vImage_Buffer.init(size:bitsPerPixel:)(double a1, double a2)
{
  uint64_t result = specialized vImage_Buffer.init(size:bitsPerPixel:)(a1, a2);
  if (v2) {
    return v4;
  }
  return result;
}

uint64_t vImage_Buffer.init(width:height:bitsPerPixel:)(uint64_t a1, uint64_t a2)
{
  uint64_t result = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(a1, a2);
  if (v2) {
    return v4;
  }
  return result;
}

uint64_t static vImage_Buffer.preferredAlignmentAndRowBytes(width:height:bitsPerPixel:)(uint64_t a1, uint64_t a2)
{
  uint64_t v10 = *MEMORY[0x1E4F143B8];
  if (a1 < 0 || a2 < 0)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    *char v3 = 8;
  }
  else
  {
    memset(v9, 0, sizeof(v9));
    uint64_t v2 = MEMORY[0x1D2600F00](v9);
    if ((v2 & 0x8000000000000000) == 0) {
      return v2;
    }
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    uint64_t v5 = v4;
    vImage.Error.init(rawValue:)(v2, &v8);
    char v6 = v8;
    if (v8 == 20) {
      char v6 = 11;
    }
    *uint64_t v5 = v6;
  }
  swift_willThrow();
  return v2;
}

void vImage_Buffer.init(cgImage:flags:)(void *a1, unsigned int *a2)
{
  uint64_t v17 = *MEMORY[0x1E4F143B8];
  uint64_t v2 = *a2;
  memset(v13, 0, sizeof(v13));
  char v3 = a1;
  specialized vImage_CGImageFormat.init(cgImage:)(v3, v14);
  outlined init with take of vImage_CGImageFormat?((uint64_t)v14, (uint64_t)v15);
  if (*((void *)&v15[0] + 1) == 1)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    *uint64_t v4 = 13;
LABEL_7:
    swift_willThrow();

    return;
  }
  v11[0] = v15[0];
  v11[1] = v15[1];
  uint64_t v12 = v16;
  uint64_t v5 = MEMORY[0x1D2600F20](v13, v11, 0, v3, v2);
  if (v5)
  {
    uint64_t v6 = v5;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    char v8 = v7;
    vImage.Error.init(rawValue:)(v6, &v10);
    char v9 = v10;
    if (v10 == 20) {
      char v9 = 11;
    }
    *char v8 = v9;
    goto LABEL_7;
  }
}

void vImage_Buffer.init(cgImage:format:flags:)(void *a1, uint64_t a2, unsigned int *a3)
{
  uint64_t v15 = *MEMORY[0x1E4F143B8];
  uint64_t v4 = *a3;
  memset(v14, 0, sizeof(v14));
  long long v5 = *(_OWORD *)(a2 + 16);
  v12[0] = *(_OWORD *)a2;
  v12[1] = v5;
  uint64_t v13 = *(void *)(a2 + 32);
  uint64_t v6 = MEMORY[0x1D2600F20](v14, v12, 0, a1, v4);
  if (v6)
  {
    uint64_t v7 = v6;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    char v9 = v8;
    vImage.Error.init(rawValue:)(v7, &v11);
    char v10 = v11;
    if (v11 == 20) {
      char v10 = 11;
    }
    char *v9 = v10;
    swift_willThrow();
  }
  else
  {
  }
}

void *vImage_Buffer.createCGImage(format:flags:)(uint64_t a1, unsigned int *a2, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6)
{
  uint64_t v19 = *MEMORY[0x1E4F143B8];
  uint64_t v6 = *a2;
  long long v7 = *(_OWORD *)(a1 + 16);
  v17[0] = *(_OWORD *)a1;
  v17[1] = v7;
  uint64_t v18 = *(void *)(a1 + 32);
  uint64_t v15 = 0;
  v16[0] = a3;
  v16[1] = a4;
  v16[2] = a5;
  void v16[3] = a6;
  uint64_t v8 = MEMORY[0x1D2601390](v16, v17, 0, 0, v6, &v15);
  if (!v8) {
    __break(1u);
  }
  char v9 = (void *)v8;
  uint64_t v10 = v15;
  if (v15)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    uint64_t v12 = v11;
    vImage.Error.init(rawValue:)(v10, (char *)v16);
    char v13 = v16[0];
    if (LOBYTE(v16[0]) == 20) {
      char v13 = 11;
    }
    *uint64_t v12 = v13;
    swift_willThrow();
  }
  return v9;
}

uint64_t specialized vImage_Buffer.init(width:height:bitsPerPixel:)(uint64_t a1, uint64_t a2)
{
  uint64_t v11 = *MEMORY[0x1E4F143B8];
  if (a1 < 0 || a2 < 0)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    *uint64_t v4 = 8;
  }
  else
  {
    memset(v10, 0, sizeof(v10));
    uint64_t v2 = MEMORY[0x1D2600F00](v10);
    if ((v2 & 0x8000000000000000) == 0) {
      return *(void *)&v10[0];
    }
    uint64_t v5 = v2;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    long long v7 = v6;
    vImage.Error.init(rawValue:)(v5, &v9);
    char v8 = v9;
    if (v9 == 20) {
      char v8 = 11;
    }
    *long long v7 = v8;
  }
  return swift_willThrow();
}

uint64_t specialized vImage_Buffer.init(size:bitsPerPixel:)(double a1, double a2)
{
  uint64_t v13 = *MEMORY[0x1E4F143B8];
  if (!specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&v10, a1)
    || v11 == 1
    || v10 < 1
    || !specialized static FixedWidthInteger._convert<A>(from:)((uint64_t)&v10, a2)
    || v11 == 1
    || v10 < 1)
  {
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    *uint64_t v5 = 8;
  }
  else
  {
    memset(v12, 0, sizeof(v12));
    uint64_t v3 = MEMORY[0x1D2600F00](v12);
    if ((v3 & 0x8000000000000000) == 0) {
      return *(void *)&v12[0];
    }
    uint64_t v6 = v3;
    lazy protocol witness table accessor for type vImage.Error and conformance vImage.Error();
    swift_allocError();
    char v8 = v7;
    vImage.Error.init(rawValue:)(v6, (char *)&v10);
    char v9 = v10;
    if (v10 == 20) {
      char v9 = 11;
    }
    *char v8 = v9;
  }
  return swift_willThrow();
}

uint64_t outlined init with take of vImage_CGImageFormat?(uint64_t a1, uint64_t a2)
{
  uint64_t v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for vImage_CGImageFormat?);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(void *)(v4 - 8) + 32))(a2, a1, v4);
  return a2;
}

uint64_t vImage_PerpsectiveTransform.Interpolation.vImageWarpInterpolation.getter()
{
  return *v0;
}

BOOL static vImage_PerpsectiveTransform.Interpolation.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return ((*a1 ^ *a2) & 1) == 0;
}

void vImage_PerpsectiveTransform.Interpolation.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int vImage_PerpsectiveTransform.Interpolation.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

void __swiftcall vImage_PerpsectiveTransform.init(source:destination:)(__C::vImage_PerpsectiveTransform_optional *__return_ptr retstr, Swift::tuple_CGPoint_CGPoint_CGPoint_CGPoint *source, Swift::tuple_CGPoint_CGPoint_CGPoint_CGPoint *destination)
{
  uint64_t v36 = *MEMORY[0x1E4F143B8];
  float v12 = v3;
  memset(v35, 0, 36);
  float v13 = v4;
  *(float *)&v35[34] = v12;
  *(float *)&v35[35] = v13;
  float v14 = v5;
  float v15 = v6;
  *(float *)&v35[36] = v14;
  *(float *)&v35[37] = v15;
  float v16 = v7;
  float v17 = v8;
  *(float *)&v35[38] = v16;
  *(float *)&v35[39] = v17;
  float v18 = v9;
  float v19 = v10;
  *(float *)&v35[40] = v18;
  *(float *)&v35[41] = v19;
  float v20 = v37;
  float v21 = v38;
  *(float *)&v35[18] = v20;
  *(float *)&v35[19] = v21;
  float v22 = v39;
  float v23 = v40;
  *(float *)&v35[20] = v22;
  *(float *)&v35[21] = v23;
  float v24 = v41;
  float v25 = v42;
  *(float *)&v35[22] = v24;
  *(float *)&v35[23] = v25;
  float v26 = v43;
  float v27 = v44;
  *(float *)&v35[24] = v26;
  *(float *)&v35[25] = v27;
  vImage_Error PerspectiveWarp = vImageGetPerspectiveWarp((const float (*)[2])&v35[34], (const float (*)[2])&v35[18], (vImage_PerpsectiveTransform *)v35, 0);
  v29.i64[0] = 0;
  v30.i64[0] = PerspectiveWarp;
  int8x16_t v31 = (int8x16_t)vdupq_lane_s64(vceqq_s64(v30, v29).i64[0], 0);
  int8x16_t v32 = vandq_s8(*(int8x16_t *)&v35[4], v31);
  int8x16_t v33 = vandq_s8(*(int8x16_t *)v35, v31);
  Swift::Float v34 = *(float *)&v35[8];
  if (PerspectiveWarp) {
    Swift::Float v34 = 0.0;
  }
  *(int8x16_t *)&retstr->value.a = v33;
  *(int8x16_t *)&retstr->value.tx = v32;
  retstr->value.v = v34;
  retstr->is_nil = PerspectiveWarp != 0;
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance vImage_PerpsectiveTransform(uint64_t a1, uint64_t a2)
{
  long long v2 = *(_OWORD *)(a1 + 16);
  v5[0] = *(_OWORD *)a1;
  v5[1] = v2;
  int v6 = *(_DWORD *)(a1 + 32);
  long long v3 = *(_OWORD *)(a2 + 16);
  v7[0] = *(_OWORD *)a2;
  v7[1] = v3;
  int v8 = *(_DWORD *)(a2 + 32);
  return specialized static vImage_PerpsectiveTransform.== infix(_:_:)((float *)v5, (float *)v7);
}

vImage_Error vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(vImage_Error result, unsigned __int8 *a2, Pixel_8 a3, uint64_t *a4)
{
  uint64_t v16 = *MEMORY[0x1E4F143B8];
  uint64_t v5 = *v4;
  if (!*(void *)(*v4 + 16))
  {
    __break(1u);
LABEL_13:
    __break(1u);
  }
  uint64_t v6 = *a4;
  if (!*(void *)(*a4 + 16)) {
    goto LABEL_13;
  }
  double v7 = *(void **)(v5 + 32);
  int v8 = *(void **)(v6 + 32);
  if (v7)
  {
    if (v8) {
      BOOL v9 = v7 == v8;
    }
    else {
      BOOL v9 = 0;
    }
    if (!v9) {
      goto LABEL_11;
    }
    __break(1u);
  }
  if (v8)
  {
LABEL_11:
    size_t v10 = *(void *)(v5 + 56);
    src.char data = v7;
    *(_OWORD *)&src.vImagePixelCount height = *(_OWORD *)(v5 + 40);
    src.size_t rowBytes = v10;
    size_t v11 = *(void *)(v6 + 56);
    dest.char data = v8;
    *(_OWORD *)&dest.vImagePixelCount height = *(_OWORD *)(v6 + 40);
    dest.size_t rowBytes = v11;
    long long v12 = *(_OWORD *)(result + 16);
    *(_OWORD *)&v13.a = *(_OWORD *)result;
    *(_OWORD *)&v13.tx = v12;
    v13.v = *(float *)(result + 32);
    return vImagePerspectiveWarp_Planar8(&src, &dest, 0, &v13, *a2, a3, 4u);
  }
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(uint64_t a1, unsigned __int8 *a2, unsigned __int16 a3, uint64_t *a4)
{
  double v4 = (uint64_t (*)(uint64_t *, uint64_t *, void, _OWORD *, void, void, uint64_t))MEMORY[0x1E4F17078];

  return vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(a1, a2, a3, a4, v4);
}

{
  uint64_t (*v4)(uint64_t *, uint64_t *, void, _OWORD *, void, void, uint64_t);
  uint64_t vars8;

  double v4 = (uint64_t (*)(uint64_t *, uint64_t *, void, _OWORD *, void, void, uint64_t))MEMORY[0x1E4F17080];

  return vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(a1, a2, a3, a4, v4);
}

uint64_t vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(uint64_t result, unsigned __int8 *a2, unsigned __int16 a3, uint64_t *a4, uint64_t (*a5)(uint64_t *, uint64_t *, void, _OWORD *, void, void, uint64_t))
{
  uint64_t v22 = *MEMORY[0x1E4F143B8];
  uint64_t v6 = *v5;
  if (!*(void *)(*v5 + 16))
  {
    __break(1u);
LABEL_13:
    __break(1u);
  }
  uint64_t v7 = *a4;
  if (!*(void *)(*a4 + 16)) {
    goto LABEL_13;
  }
  uint64_t v8 = *(void *)(v6 + 32);
  uint64_t v9 = *(void *)(v7 + 32);
  if (v8)
  {
    if (v9) {
      BOOL v10 = v8 == v9;
    }
    else {
      BOOL v10 = 0;
    }
    if (!v10) {
      goto LABEL_11;
    }
    __break(1u);
  }
  if (v9)
  {
LABEL_11:
    uint64_t v11 = *(void *)(v6 + 56);
    uint64_t v19 = v8;
    long long v20 = *(_OWORD *)(v6 + 40);
    uint64_t v21 = v11;
    uint64_t v12 = *(void *)(v7 + 56);
    uint64_t v16 = v9;
    long long v17 = *(_OWORD *)(v7 + 40);
    uint64_t v18 = v12;
    long long v13 = *(_OWORD *)(result + 16);
    v14[0] = *(_OWORD *)result;
    v14[1] = v13;
    int v15 = *(_DWORD *)(result + 32);
    return a5(&v19, &v16, 0, v14, *a2, a3, 4);
  }
  __break(1u);
  return result;
}

vImage_Error vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(vImage_Error result, unsigned __int8 *a2, uint8_t a3, uint8_t a4, uint8_t a5, uint8_t a6, uint64_t *a7)
{
  uint64_t v20 = *MEMORY[0x1E4F143B8];
  uint64_t v8 = *v7;
  if (!*(void *)(*v7 + 16))
  {
    __break(1u);
LABEL_13:
    __break(1u);
  }
  uint64_t v9 = *a7;
  if (!*(void *)(*a7 + 16)) {
    goto LABEL_13;
  }
  BOOL v10 = *(void **)(v8 + 32);
  uint64_t v11 = *(void **)(v9 + 32);
  if (v10)
  {
    if (v11) {
      BOOL v12 = v10 == v11;
    }
    else {
      BOOL v12 = 0;
    }
    if (!v12) {
      goto LABEL_11;
    }
    __break(1u);
  }
  if (v11)
  {
LABEL_11:
    size_t v13 = *(void *)(v8 + 56);
    *(_OWORD *)&src.vImagePixelCount height = *(_OWORD *)(v8 + 40);
    src.size_t rowBytes = v13;
    size_t v14 = *(void *)(v9 + 56);
    dest.char data = v11;
    *(_OWORD *)&dest.vImagePixelCount height = *(_OWORD *)(v9 + 40);
    dest.size_t rowBytes = v14;
    src.char data = v10;
    long long v15 = *(_OWORD *)(result + 16);
    *(_OWORD *)&v16.a = *(_OWORD *)result;
    *(_OWORD *)&v16.tx = v15;
    v16.v = *(float *)(result + 32);
    v17[0] = a3;
    v17[1] = a4;
    _OWORD v17[2] = a5;
    v17[3] = a6;
    return vImagePerspectiveWarp_ARGB8888(&src, &dest, 0, &v16, *a2, v17, 4u);
  }
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(uint64_t a1, unsigned __int8 *a2, __int16 a3, __int16 a4, __int16 a5, __int16 a6, uint64_t *a7)
{
  uint64_t v7 = (uint64_t (*)(uint64_t *, uint64_t *, void, _OWORD *, void, _WORD *, uint64_t))MEMORY[0x1E4F17068];

  return vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(a1, a2, a3, a4, a5, a6, a7, v7);
}

{
  uint64_t (*v7)(uint64_t *, uint64_t *, void, _OWORD *, void, _WORD *, uint64_t);
  uint64_t vars8;

  uint64_t v7 = (uint64_t (*)(uint64_t *, uint64_t *, void, _OWORD *, void, _WORD *, uint64_t))MEMORY[0x1E4F17070];

  return vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(a1, a2, a3, a4, a5, a6, a7, v7);
}

uint64_t vImage.PixelBuffer<>.transform(_:interpolation:backgroundColor:destination:)(uint64_t result, unsigned __int8 *a2, __int16 a3, __int16 a4, __int16 a5, __int16 a6, uint64_t *a7, uint64_t (*a8)(uint64_t *, uint64_t *, void, _OWORD *, void, _WORD *, uint64_t))
{
  uint64_t v26 = *MEMORY[0x1E4F143B8];
  uint64_t v9 = *v8;
  if (!*(void *)(*v8 + 16))
  {
    __break(1u);
LABEL_13:
    __break(1u);
  }
  uint64_t v10 = *a7;
  if (!*(void *)(*a7 + 16)) {
    goto LABEL_13;
  }
  uint64_t v11 = *(void *)(v9 + 32);
  uint64_t v12 = *(void *)(v10 + 32);
  if (v11)
  {
    if (v12) {
      BOOL v13 = v11 == v12;
    }
    else {
      BOOL v13 = 0;
    }
    if (!v13) {
      goto LABEL_11;
    }
    __break(1u);
  }
  if (v12)
  {
LABEL_11:
    uint64_t v14 = *(void *)(v9 + 56);
    long long v24 = *(_OWORD *)(v9 + 40);
    uint64_t v25 = v14;
    uint64_t v15 = *(void *)(v10 + 56);
    uint64_t v20 = v12;
    long long v21 = *(_OWORD *)(v10 + 40);
    uint64_t v22 = v15;
    uint64_t v23 = v11;
    long long v16 = *(_OWORD *)(result + 16);
    v17[0] = *(_OWORD *)result;
    v17[1] = v16;
    int v18 = *(_DWORD *)(result + 32);
    v19[0] = a3;
    v19[1] = a4;
    v19[2] = a5;
    v19[3] = a6;
    return a8(&v23, &v20, 0, v17, *a2, v19, 4);
  }
  __break(1u);
  return result;
}

BOOL specialized static vImage_PerpsectiveTransform.== infix(_:_:)(float *a1, float *a2)
{
  return *a1 == *a2
      && a1[1] == a2[1]
      && a1[2] == a2[2]
      && a1[3] == a2[3]
      && a1[4] == a2[4]
      && a1[5] == a2[5]
      && a1[8] == a2[8]
      && a1[6] == a2[6]
      && a1[7] == a2[7];
}

unint64_t lazy protocol witness table accessor for type vImage_PerpsectiveTransform.Interpolation and conformance vImage_PerpsectiveTransform.Interpolation()
{
  unint64_t result = lazy protocol witness table cache variable for type vImage_PerpsectiveTransform.Interpolation and conformance vImage_PerpsectiveTransform.Interpolation;
  if (!lazy protocol witness table cache variable for type vImage_PerpsectiveTransform.Interpolation and conformance vImage_PerpsectiveTransform.Interpolation)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type vImage_PerpsectiveTransform.Interpolation and conformance vImage_PerpsectiveTransform.Interpolation);
  }
  return result;
}

unsigned char *storeEnumTagSinglePayload for vImage_PerpsectiveTransform.Interpolation(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 1 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFF) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFE)
  {
    unsigned int v6 = ((a2 - 255) >> 8) + 1;
    *unint64_t result = a2 + 1;
    switch(v5)
    {
      case 1:
        result[1] = v6;
        break;
      case 2:
        *(_WORD *)(result + 1) = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x1D20EE0E8);
      case 4:
        *(_DWORD *)(result + 1) = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1] = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1) = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1) = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *unint64_t result = a2 + 1;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for vImage_PerpsectiveTransform.Interpolation()
{
  return &type metadata for vImage_PerpsectiveTransform.Interpolation;
}

uint64_t vImage.PixelBuffer<>.init(lumaSource:chromaSource:conversionInfo:)@<X0>(uint64_t *a1@<X0>, uint64_t *a2@<X1>, _OWORD *a3@<X2>, uint64_t *a4@<X8>)
{
  uint64_t v6 = *a1;
  if (!*(void *)(*a1 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  uint64_t v7 = *(void *)(v6 + 48);
  if (v7 < 0)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  uint64_t v4 = *(void *)(v6 + 40);
  if (v4 < 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  if (!v7)
  {
LABEL_10:
    __break(1u);
    goto LABEL_11;
  }
  if (v4)
  {
    uint64_t v9 = *a2;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<vImage.BufferWrapper>);
    uint64_t v10 = swift_allocObject();
    *(_OWORD *)(v10 + 16) = xmmword_1D2135280;
    uint64_t v11 = specialized vImage_Buffer.init(width:height:bitsPerPixel:)(v7, v4);
    uint64_t v13 = v12;
    uint64_t v15 = v14;
    uint64_t v17 = v16;
    type metadata accessor for vImage.BufferReference();
    int v18 = (void *)swift_allocObject();
    v18[2] = v11;
    v18[3] = v13;
    void v18[4] = v15;
    v18[5] = v17;
    *(void *)(v10 + 32) = v11;
    *(void *)(v10 + 40) = v13;
    *(void *)(v10 + 48) = v15;
    *(void *)(v10 + 56) = v17;
    *(void *)(v10 + 64) = v18;
    v21[0] = v6;
    v21[1] = v10;
    uint64_t v20 = v9;
    vImage.PixelBuffer<>.convert(lumaSource:chromaSource:conversionInfo:)((uint64_t)v21, &v20, a3);
    swift_bridgeObjectRelease();
    uint64_t result = swift_bridgeObjectRelease();
    *a4 = v10;
    return result;
  }
LABEL_11:
  __break(1u);

  uint64_t result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

vImage_Error vImage.PixelBuffer<>.convert(lumaSource:chromaSource:conversionInfo:)(uint64_t a1, uint64_t *a2, _OWORD *a3)
{
  uint64_t v27 = *MEMORY[0x1E4F143B8];
  uint64_t v4 = *(void **)a1;
  if (!*(void *)(*(void *)a1 + 16))
  {
    __break(1u);
    goto LABEL_16;
  }
  vImagePixelCount v5 = v4[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_16:
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v6 = *(void **)v3;
  if (!*(void *)(*(void *)v3 + 16))
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  vImagePixelCount v7 = v6[6];
  if ((v7 & 0x8000000000000000) != 0)
  {
LABEL_18:
    __break(1u);
    goto LABEL_19;
  }
  if (v7 < v5)
  {
LABEL_19:
    __break(1u);
    goto LABEL_20;
  }
  vImagePixelCount v8 = v4[5];
  if ((v8 & 0x8000000000000000) != 0)
  {
LABEL_20:
    __break(1u);
    goto LABEL_21;
  }
  vImagePixelCount v9 = v6[5];
  if ((v9 & 0x8000000000000000) != 0)
  {
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  if (v9 < v8)
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v10 = *a2;
  if (!*(void *)(*a2 + 16))
  {
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  vImagePixelCount v11 = *(void *)(v10 + 48);
  if ((v11 & 0x8000000000000000) != 0)
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  if (v7 < v11)
  {
LABEL_25:
    __break(1u);
    goto LABEL_26;
  }
  vImagePixelCount v12 = *(void *)(v10 + 40);
  if ((v12 & 0x8000000000000000) != 0)
  {
LABEL_26:
    __break(1u);
LABEL_27:
    __break(1u);
  }
  if (v9 < v12) {
    goto LABEL_27;
  }
  uint64_t v13 = (void *)v6[4];
  size_t v14 = v6[7];
  dest.char data = v13;
  dest.vImagePixelCount height = v9;
  dest.vImagePixelCount width = v7;
  dest.size_t rowBytes = v14;
  size_t v15 = *(void *)(v10 + 56);
  srcCbCr.char data = *(void **)(v10 + 32);
  srcCbCr.vImagePixelCount height = v12;
  srcCbCr.vImagePixelCount width = v11;
  srcCbCr.size_t rowBytes = v15;
  uint64_t v16 = (void *)v4[4];
  size_t v17 = v4[7];
  srcYp.char data = v16;
  srcYp.vImagePixelCount height = v8;
  srcYp.vImagePixelCount width = v5;
  srcYp.size_t rowBytes = v17;
  long long v18 = a3[5];
  *(_OWORD *)&v23.opaque[64] = a3[4];
  *(_OWORD *)&v23.opaque[80] = v18;
  long long v19 = a3[7];
  *(_OWORD *)&v23.opaque[96] = a3[6];
  *(_OWORD *)&v23.opaque[112] = v19;
  long long v20 = a3[1];
  *(_OWORD *)v23.opaque = *a3;
  *(_OWORD *)&v23.opaque[16] = v20;
  long long v21 = a3[3];
  *(_OWORD *)&v23.opaque[32] = a3[2];
  *(_OWORD *)&v23.opaque[48] = v21;
  return vImageConvert_420Yp8_CbCr8ToARGB8888(&srcYp, &srcCbCr, &dest, &v23, 0, 0xFFu, 0);
}

void vImage.PixelBuffer<>.withCVPixelBuffer(readOnly:body:)(char a1, void (*a2)(void))
{
  pixelBuffer[1] = *(CVPixelBufferRef *)MEMORY[0x1E4F143B8];
  pixelBuffer[0] = 0;
  uint64_t v3 = *(void **)v2;
  if (!*(void *)(*(void *)v2 + 16))
  {
    __break(1u);
    goto LABEL_8;
  }
  size_t v5 = v3[6];
  if ((v5 & 0x8000000000000000) != 0)
  {
LABEL_8:
    __break(1u);
    goto LABEL_9;
  }
  size_t v6 = v3[5];
  if ((v6 & 0x8000000000000000) != 0)
  {
LABEL_9:
    __break(1u);
    goto LABEL_10;
  }
  vImagePixelCount v7 = (void *)v3[4];
  if (!v7)
  {
LABEL_10:
    __break(1u);
LABEL_11:
    __break(1u);
  }
  CVPixelBufferCreateWithBytes(0, v5, v6, 0x42475241u, v7, v3[7], 0, 0, 0, pixelBuffer);
  CVPixelBufferRef v9 = pixelBuffer[0];
  if (!pixelBuffer[0]) {
    goto LABEL_11;
  }
  CVPixelBufferLockFlags v10 = a1 & 1;
  CVPixelBufferLockBaseAddress(pixelBuffer[0], v10);
  vImagePixelCount v11 = v9;
  a2();

  CVPixelBufferUnlockBaseAddress(v11, v10);
}

uint64_t vImage.PixelBuffer<>.rotate(_:backgroundColor:destination:)(unsigned int *a1, __int16 a2, uint64_t *a3)
{
  return specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a3, a2 & 0x1FF, 0, (uint64_t (*)(uint64_t, uint64_t, void, char *, unint64_t, __n128))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UInt8, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, unint64_t, char *, unint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UInt8, @unowned UInt32) -> (@unowned Int), 0, 0, *v3);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UInt8, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, Pixel_8 *a4, vImage_Flags a5, float a6)
{
  return vImageRotate_Planar8(a1, a2, a3, a6, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UInt8, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, uint8_t a3, Pixel_8 *a4, vImage_Flags a5)
{
  return vImageRotate90_Planar8(a1, a2, a3, *a4, a5);
}

uint64_t specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, __int16 a3, char a4, uint64_t (*a5)(uint64_t, uint64_t, void, char *, unint64_t, __n128), uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, unint64_t, char *, unint64_t), uint64_t a8, char a9, uint64_t a10)
{
  uint64_t v22 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a10 + 16))
  {
    __break(1u);
LABEL_12:
    __break(1u);
  }
  if (!*(void *)(a2 + 16)) {
    goto LABEL_12;
  }
  uint64_t v10 = *(void *)(a10 + 32);
  uint64_t v11 = *(void *)(a2 + 32);
  if (v10)
  {
    if (!v11 || v10 != v11) {
      goto LABEL_8;
    }
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  unint64_t v12 = 4;
  if ((a3 & 0x100) != 0) {
    unint64_t v12 = 8;
  }
  unint64_t v15 = v12;
  uint64_t v16 = v11;
  uint64_t v13 = *(void *)(a10 + 56);
  uint64_t v19 = v10;
  long long v20 = *(_OWORD *)(a10 + 40);
  uint64_t v21 = v13;
  uint64_t v14 = *(void *)(a2 + 56);
  __n128 v17 = *(__n128 *)(a2 + 40);
  uint64_t v18 = v14;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)((uint64_t)&v16, result & 0xFFFFFFFFFFLL, a7, v17, a8, (uint64_t)&v19, a3 & 0x1FF, a4, &v15, a9 & 1, a5);
}

uint64_t specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t (*a5)(uint64_t, uint64_t, void, uint64_t *, unint64_t, __n128), uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t), uint64_t a8, char a9, uint64_t a10)
{
  uint64_t v22 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a10 + 16))
  {
    __break(1u);
LABEL_12:
    __break(1u);
  }
  if (!*(void *)(a2 + 16)) {
    goto LABEL_12;
  }
  uint64_t v10 = *(void *)(a10 + 32);
  uint64_t v11 = *(void *)(a2 + 32);
  if (v10)
  {
    if (!v11 || v10 != v11) {
      goto LABEL_8;
    }
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  unint64_t v12 = 4;
  if (!a3) {
    unint64_t v12 = 8;
  }
  unint64_t v15 = v12;
  uint64_t v16 = v11;
  uint64_t v13 = *(void *)(a10 + 56);
  uint64_t v19 = v10;
  long long v20 = *(_OWORD *)(a10 + 40);
  uint64_t v21 = v13;
  uint64_t v14 = *(void *)(a2 + 56);
  __n128 v17 = *(__n128 *)(a2 + 40);
  uint64_t v18 = v14;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)((uint64_t)&v16, result & 0xFFFFFFFFFFLL, a7, v17, a8, (uint64_t)&v19, a3, a4, &v15, a9 & 1, a5);
}

uint64_t specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, unint64_t a3, uint64_t (*a4)(uint64_t, uint64_t, void, float *, unint64_t, float), float a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, unint64_t, float *, unint64_t), uint64_t a8, char a9, uint64_t a10)
{
  uint64_t v22 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a10 + 16))
  {
    __break(1u);
LABEL_12:
    __break(1u);
  }
  if (!*(void *)(a2 + 16)) {
    goto LABEL_12;
  }
  uint64_t v10 = *(void *)(a10 + 32);
  uint64_t v11 = *(void *)(a2 + 32);
  if (v10)
  {
    if (!v11 || v10 != v11) {
      goto LABEL_8;
    }
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  unint64_t v12 = 4;
  if ((a3 & 0x100000000) != 0) {
    unint64_t v12 = 8;
  }
  unint64_t v15 = v12;
  uint64_t v16 = v11;
  uint64_t v13 = *(void *)(a10 + 56);
  uint64_t v19 = v10;
  long long v20 = *(_OWORD *)(a10 + 40);
  uint64_t v21 = v13;
  uint64_t v14 = *(void *)(a2 + 56);
  long long v17 = *(_OWORD *)(a2 + 40);
  uint64_t v18 = v14;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)((uint64_t)&v16, result & 0xFFFFFFFFFFLL, a7, a5, a8, (uint64_t)&v19, a3 | ((HIDWORD(a3) & 1) << 32), &v15, a9 & 1, a4);
}

uint64_t specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, int a3, __int16 a4, uint64_t (*a5)(uint64_t, uint64_t, void, __int16 *, unint64_t, __n128), uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, unint64_t, __int16 *, unint64_t), uint64_t a8, char a9, uint64_t a10)
{
  uint64_t v22 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a10 + 16))
  {
    __break(1u);
LABEL_12:
    __break(1u);
  }
  if (!*(void *)(a2 + 16)) {
    goto LABEL_12;
  }
  uint64_t v10 = *(void *)(a10 + 32);
  uint64_t v11 = *(void *)(a2 + 32);
  if (v10)
  {
    if (!v11 || v10 != v11) {
      goto LABEL_8;
    }
    __break(1u);
  }
  if (!v11)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  unint64_t v12 = 4;
  if ((a3 & 0x10000) != 0) {
    unint64_t v12 = 8;
  }
  unint64_t v15 = v12;
  uint64_t v16 = v11;
  uint64_t v13 = *(void *)(a10 + 56);
  uint64_t v19 = v10;
  long long v20 = *(_OWORD *)(a10 + 40);
  uint64_t v21 = v13;
  uint64_t v14 = *(void *)(a2 + 56);
  __n128 v17 = *(__n128 *)(a2 + 40);
  uint64_t v18 = v14;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)((uint64_t)&v16, result & 0xFFFFFFFFFFLL, a7, v17, a8, (uint64_t)&v19, a3 & 0x1FFFF, a4, &v15, a9 & 1, a5);
}

uint64_t specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t a3, char a4, uint64_t a5, uint64_t (*a6)(uint64_t, uint64_t, void, uint64_t *, unint64_t, __n128), uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t), uint64_t a9, char a10, uint64_t a11)
{
  uint64_t v23 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a11 + 16))
  {
    __break(1u);
LABEL_12:
    __break(1u);
  }
  if (!*(void *)(a2 + 16)) {
    goto LABEL_12;
  }
  uint64_t v11 = *(void *)(a11 + 32);
  uint64_t v12 = *(void *)(a2 + 32);
  if (v11)
  {
    if (!v12 || v11 != v12) {
      goto LABEL_8;
    }
    __break(1u);
  }
  if (!v12)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  unint64_t v13 = 8;
  if ((a4 & 1) == 0) {
    unint64_t v13 = 4;
  }
  unint64_t v16 = v13;
  uint64_t v17 = v12;
  uint64_t v14 = *(void *)(a11 + 56);
  uint64_t v20 = v11;
  long long v21 = *(_OWORD *)(a11 + 40);
  uint64_t v22 = v14;
  uint64_t v15 = *(void *)(a2 + 56);
  __n128 v18 = *(__n128 *)(a2 + 40);
  uint64_t v19 = v15;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)((uint64_t)&v17, result & 0xFFFFFFFFFFLL, a8, v18, a9, (uint64_t)&v20, a3, a4 & 1, a5, &v16, a10 & 1, a6);
}

uint64_t vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(unsigned int *a1, uint64_t *a2, uint64_t a3, uint64_t a4, void (*a5)(uint64_t, uint64_t, void, char *, float), uint64_t a6, void (*a7)(uint64_t, uint64_t, unint64_t, char *), uint64_t a8, char a9, uint64_t a10, uint64_t a11, uint64_t a12)
{
  v30[4] = *MEMORY[0x1E4F143B8];
  uint64_t v24 = *a1;
  uint64_t v23 = *((unsigned __int8 *)a1 + 4);
  uint64_t v14 = *a2;
  uint64_t v15 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v30[0] = v14;
  type metadata accessor for vImage.PixelBuffer();
  uint64_t result = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (v15)
  {
    if (result) {
      BOOL v17 = v15 == result;
    }
    else {
      BOOL v17 = 0;
    }
    if (!v17) {
      goto LABEL_9;
    }
    __break(1u);
  }
  if (!result)
  {
    __break(1u);
    return result;
  }
LABEL_9:
  int v18 = (*(uint64_t (**)(uint64_t, uint64_t, uint64_t))(*(void *)(a12 - 8) + 48))(a3, 1, a12);
  uint64_t v19 = 4;
  if (v18 == 1) {
    uint64_t v19 = 8;
  }
  uint64_t v29 = v19;
  v30[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v30[1] = v20;
  v30[2] = v21;
  v30[3] = v22;
  return closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)((uint64_t)v30, v14, v24 | (v23 << 32), a7, a8, a3, a4, &v29, a9 & 1, a5, a6, *(void *)(a10 + 16), a11, a12);
}

uint64_t vImage.PixelBuffer<>.rotate(_:backgroundColor:destination:)(unsigned int *a1, unint64_t a2, uint64_t *a3)
{
  if ((a2 & 0x100000000) != 0)
  {
    uint64_t v11 = 0;
  }
  else
  {
    char v6 = a2;
    unint64_t v7 = a2 >> 8;
    unint64_t v8 = a2 >> 16;
    unint64_t v9 = a2 >> 24;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    uint64_t v10 = swift_allocObject();
    *(_OWORD *)(v10 + 16) = xmmword_1D2135FC0;
    *(unsigned char *)(v10 + 32) = v6;
    uint64_t v11 = v10 + 32;
    *(unsigned char *)(v10 + 33) = v7;
    *(unsigned char *)(v10 + 34) = v8;
    *(unsigned char *)(v10 + 35) = v9;
  }
  specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a3, v11, (uint64_t)&unk_1F28D7858, (uint64_t (*)(uint64_t, uint64_t, void, uint64_t *, unint64_t, __n128))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<UInt8>, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<UInt8>, @unowned UInt32) -> (@unowned Int), 0, 0, *v3);
  swift_bridgeObjectRelease();

  return swift_bridgeObjectRelease();
}

{
  uint64_t *v3;

  return specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a3, a2 | ((HIDWORD(a2) & 1) << 32), (uint64_t (*)(uint64_t, uint64_t, void, float *, unint64_t, float))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned Float, @unowned UInt32) -> (@unowned Int), 0.0, 0, (uint64_t (*)(uint64_t, uint64_t, unint64_t, float *, unint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned Float, @unowned UInt32) -> (@unowned Int), 0, 0, *v3);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<UInt8>, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, const uint8_t *__attribute__((__org_typedef(Pixel_8888))) *a4, vImage_Flags a5, float a6)
{
  return vImageRotate_ARGB8888(a1, a2, a3, a6, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<UInt8>, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, uint8_t a3, const uint8_t *__attribute__((__org_typedef(Pixel_8888))) *a4, vImage_Flags a5)
{
  return vImageRotate90_ARGB8888(a1, a2, a3, *a4, a5);
}

uint64_t vImage.PixelBuffer<>.rotate(_:backgroundColor:destination:)(unsigned int *a1, unint64_t a2, char a3, uint64_t *a4)
{
  if (a3)
  {
    uint64_t v12 = 0;
  }
  else
  {
    __int16 v7 = a2;
    unint64_t v8 = a2 >> 16;
    unint64_t v9 = HIDWORD(a2);
    unint64_t v10 = HIWORD(a2);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    uint64_t v11 = swift_allocObject();
    *(_OWORD *)(v11 + 16) = xmmword_1D2135FC0;
    *(_WORD *)(v11 + 32) = v7;
    uint64_t v12 = v11 + 32;
    *(_WORD *)(v11 + 34) = v8;
    *(_WORD *)(v11 + 36) = v9;
    *(_WORD *)(v11 + 38) = v10;
  }
  specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a4, v12, (uint64_t)&unk_1F28D7B80, (uint64_t (*)(uint64_t, uint64_t, void, uint64_t *, unint64_t, __n128))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<UInt16>, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<UInt16>, @unowned UInt32) -> (@unowned Int), 0, 0, *v4);
  swift_bridgeObjectRelease();

  return swift_bridgeObjectRelease();
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<UInt16>, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, const uint16_t *__attribute__((__org_typedef(Pixel_ARGB_16U))) *a4, vImage_Flags a5, float a6)
{
  return vImageRotate_ARGB16U(a1, a2, a3, a6, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<UInt16>, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, uint8_t a3, const uint16_t *__attribute__((__org_typedef(Pixel_ARGB_16U))) *a4, vImage_Flags a5)
{
  return vImageRotate90_ARGB16U(a1, a2, a3, *a4, a5);
}

uint64_t vImage.PixelBuffer<>.rotate(_:backgroundColor:destination:)(unsigned int *a1, unint64_t a2, unint64_t a3, char a4, uint64_t *a5)
{
  if (a4)
  {
    uint64_t v12 = 0;
  }
  else
  {
    int v8 = a3;
    int v9 = a2;
    unint64_t v10 = HIDWORD(a2);
    unint64_t v11 = HIDWORD(a3);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
    uint64_t v12 = swift_allocObject();
    *(_OWORD *)(v12 + 16) = xmmword_1D2135FC0;
    *(_DWORD *)(v12 + 32) = v9;
    *(_DWORD *)(v12 + 36) = v10;
    *(_DWORD *)(v12 + 40) = v8;
    *(_DWORD *)(v12 + 44) = v11;
  }
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<Float>);
  uint64_t inited = swift_initStackObject();
  *(_OWORD *)(inited + 16) = xmmword_1D2135280;
  *(_DWORD *)(inited + 32) = 0;
  if (v12) {
    uint64_t v14 = v12 + 32;
  }
  else {
    uint64_t v14 = 0;
  }
  specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a5, v14, inited + 32, (uint64_t (*)(uint64_t, uint64_t, void, uint64_t *, unint64_t, __n128))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<Float>, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<Float>, @unowned UInt32) -> (@unowned Int), 0, 0, *v5);
  swift_bridgeObjectRelease();
  return swift_bridgeObjectRelease();
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<Float>, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, const float *__attribute__((__org_typedef(Pixel_FFFF))) *a4, vImage_Flags a5, float a6)
{
  return vImageRotate_ARGBFFFF(a1, a2, a3, a6, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<Float>, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, uint8_t a3, const float *__attribute__((__org_typedef(Pixel_FFFF))) *a4, vImage_Flags a5)
{
  return vImageRotate90_ARGBFFFF(a1, a2, a3, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned Float, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, Pixel_F *a4, vImage_Flags flags, float a6)
{
  return vImageRotate_PlanarF(a1, a2, a3, a6, *a4, flags);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned Float, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, uint8_t a3, Pixel_F *a4, vImage_Flags flags)
{
  return vImageRotate90_PlanarF(a1, a2, a3, *a4, flags);
}

uint64_t vImage.PixelBuffer<>.rotate(_:backgroundColor:useFloat16Accumulator:destination:)(unsigned int *a1, int a2, char a3, uint64_t *a4)
{
  return specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a4, a2 & 0x1FFFF, 0, (uint64_t (*)(uint64_t, uint64_t, void, __int16 *, unint64_t, __n128))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UInt16, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, unint64_t, __int16 *, unint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UInt16, @unowned UInt32) -> (@unowned Int), 0, a3, *v4);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UInt16, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, const Pixel_16F *a4, vImage_Flags a5, float a6)
{
  return vImageRotate_Planar16F(a1, a2, a3, a6, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UInt16, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, uint8_t a3, const Pixel_16F *a4, vImage_Flags a5)
{
  return vImageRotate90_Planar16F(a1, a2, a3, *a4, a5);
}

uint64_t vImage.PixelBuffer<>.rotate(_:backgroundColor:useFloat16Accumulator:destination:)(unsigned int *a1, unint64_t a2, char a3, uint64_t *a4)
{
  char v8 = BYTE4(a2) & 1;
  if ((a2 & 0x100000000) != 0)
  {
    uint64_t v12 = 0;
  }
  else
  {
    __int16 v9 = a2;
    unint64_t v10 = a2 >> 16;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    uint64_t v11 = swift_allocObject();
    *(_OWORD *)(v11 + 16) = xmmword_1D2135290;
    *(_WORD *)(v11 + 32) = v9;
    uint64_t v12 = v11 + 32;
    *(_WORD *)(v11 + 34) = v10;
  }
  specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a4, v12, v8, (uint64_t)&unk_1F28D7A68, (uint64_t (*)(uint64_t, uint64_t, void, uint64_t *, unint64_t, __n128))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, a3 & 1, *v4);
  swift_bridgeObjectRelease();

  return swift_bridgeObjectRelease();
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, const uint16_t *__attribute__((__org_typedef(Pixel_16F16F))) *a4, vImage_Flags a5, float a6)
{
  return vImageRotate_CbCr16F(a1, a2, a3, a6, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, void *a3, const uint16_t *__attribute__((__org_typedef(Pixel_ARGB_16F))) *a4, vImage_Flags a5, float a6)
{
  return vImageRotate_ARGB16F(a1, a2, a3, a6, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, uint8_t a3, const uint16_t *__attribute__((__org_typedef(Pixel_16F16F))) *a4, vImage_Flags a5)
{
  return vImageRotate90_CbCr16F(a1, a2, a3, *a4, a5);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, uint8_t a3, const uint16_t *__attribute__((__org_typedef(Pixel_ARGB_16F))) *a4, vImage_Flags a5)
{
  return vImageRotate90_ARGB16F(a1, a2, a3, *a4, a5);
}

uint64_t vImage.PixelBuffer<>.rotate(_:backgroundColor:useFloat16Accumulator:destination:)(unsigned int *a1, unint64_t a2, char a3, char a4, uint64_t *a5)
{
  char v9 = a3 & 1;
  if (a3)
  {
    uint64_t v15 = 0;
  }
  else
  {
    __int16 v10 = a2;
    unint64_t v11 = a2 >> 16;
    unint64_t v12 = HIDWORD(a2);
    unint64_t v13 = HIWORD(a2);
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    uint64_t v14 = swift_allocObject();
    *(_OWORD *)(v14 + 16) = xmmword_1D2135FC0;
    *(_WORD *)(v14 + 32) = v10;
    uint64_t v15 = v14 + 32;
    *(_WORD *)(v14 + 34) = v11;
    *(_WORD *)(v14 + 36) = v12;
    *(_WORD *)(v14 + 38) = v13;
  }
  specialized vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(*a1 | ((unint64_t)*((unsigned __int8 *)a1 + 4) << 32), *a5, v15, v9, (uint64_t)&unk_1F28D7AE0, (uint64_t (*)(uint64_t, uint64_t, void, uint64_t *, unint64_t, __n128))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UnsafeMutableRawPointer?, @unowned Float, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt8, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, a4 & 1, *v5);
  swift_bridgeObjectRelease();

  return swift_bridgeObjectRelease();
}

uint64_t closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t a1, uint64_t a2, uint64_t a3, void (*a4)(uint64_t, uint64_t, unint64_t, char *), uint64_t a5, uint64_t a6, uint64_t a7, void *a8, char a9, void (*a10)(uint64_t, uint64_t, void, char *, float), uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14)
{
  void v25[4] = *MEMORY[0x1E4F143B8];
  type metadata accessor for vImage.PixelBuffer();
  v25[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v25[1] = v15;
  _OWORD v25[2] = v16;
  v25[3] = v17;
  return closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)((uint64_t)v25, a3 & 0xFFFFFFFFFFLL, a4, a5, a1, a6, a7, a8, a9, a10, a11, a12, a13, a14);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, unint64_t, char *, unint64_t), __n128 a4, uint64_t a5, uint64_t a6, __int16 a7, char a8, unint64_t *a9, char a10, uint64_t (*a11)(uint64_t, uint64_t, void, char *, unint64_t, __n128))
{
  if (BYTE4(a2) >= 2u)
  {
    if ((a7 & 0x100) == 0) {
      a8 = a7;
    }
    char v14 = a8;
    if ((*a9 & 0x8000000000000000) == 0)
    {
      if (!HIDWORD(*a9)) {
        return a3(a6, result, 0x302010001020300uLL >> (8 * a2), &v14, *a9);
      }
      goto LABEL_21;
    }
    __break(1u);
LABEL_20:
    __break(1u);
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v12 = 4096;
  if ((a10 & 1) == 0) {
    uint64_t v12 = 0;
  }
  unint64_t v13 = *a9 | v12;
  *a9 = v13;
  if (BYTE4(a2) == 1)
  {
    a4.n128_f32[0] = (float)(*(float *)&a2 * 3.1416) / 180.0;
  }
  else
  {
    if (BYTE4(a2))
    {
LABEL_23:
      __break(1u);
      return result;
    }
    a4.n128_u32[0] = a2;
  }
  if ((a7 & 0x100) == 0) {
    a8 = a7;
  }
  char v15 = a8;
  if ((v13 & 0x8000000000000000) != 0) {
    goto LABEL_20;
  }
  if (HIDWORD(v13))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  return a11(a6, result, 0, &v15, v13, a4);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t), __n128 a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, unint64_t *a9, char a10, uint64_t (*a11)(uint64_t, uint64_t, void, uint64_t *, unint64_t, __n128))
{
  if (BYTE4(a2) >= 2u)
  {
    if (a7) {
      uint64_t v13 = a7;
    }
    else {
      uint64_t v13 = a8;
    }
    uint64_t v15 = v13;
    if ((*a9 & 0x8000000000000000) == 0)
    {
      if (!HIDWORD(*a9)) {
        return a3(a6, result, 0x302010001020300uLL >> (8 * a2), &v15, *a9);
      }
      goto LABEL_23;
    }
    __break(1u);
LABEL_22:
    __break(1u);
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v11 = 4096;
  if ((a10 & 1) == 0) {
    uint64_t v11 = 0;
  }
  unint64_t v12 = *a9 | v11;
  *a9 = v12;
  if (BYTE4(a2) == 1)
  {
    a4.n128_f32[0] = (float)(*(float *)&a2 * 3.1416) / 180.0;
  }
  else
  {
    if (BYTE4(a2))
    {
LABEL_25:
      __break(1u);
      return result;
    }
    a4.n128_u32[0] = a2;
  }
  if (a7) {
    uint64_t v14 = a7;
  }
  else {
    uint64_t v14 = a8;
  }
  uint64_t v15 = v14;
  if ((v12 & 0x8000000000000000) != 0) {
    goto LABEL_22;
  }
  if (HIDWORD(v12))
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  return a11(a6, result, 0, &v15, v12, a4);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, unint64_t, float *, unint64_t), float a4, uint64_t a5, uint64_t a6, uint64_t a7, unint64_t *a8, char a9, uint64_t (*a10)(uint64_t, uint64_t, void, float *, unint64_t, float))
{
  if (BYTE4(a2) >= 2u)
  {
    float v13 = a4;
    if ((a7 & 0x100000000) == 0) {
      float v13 = *(float *)&a7;
    }
    float v15 = v13;
    if ((*a8 & 0x8000000000000000) == 0)
    {
      if (!HIDWORD(*a8)) {
        return a3(a6, result, 0x302010001020300uLL >> (8 * a2), &v15, *a8);
      }
      goto LABEL_21;
    }
    __break(1u);
LABEL_20:
    __break(1u);
LABEL_21:
    __break(1u);
    goto LABEL_22;
  }
  uint64_t v10 = 4096;
  if ((a9 & 1) == 0) {
    uint64_t v10 = 0;
  }
  unint64_t v11 = *a8 | v10;
  *a8 = v11;
  if (BYTE4(a2) == 1)
  {
    float v12 = (float)(*(float *)&a2 * 3.1416) / 180.0;
  }
  else
  {
    if (BYTE4(a2))
    {
LABEL_23:
      __break(1u);
      return result;
    }
    float v12 = *(float *)&a2;
  }
  float v14 = a4;
  if ((a7 & 0x100000000) == 0) {
    float v14 = *(float *)&a7;
  }
  float v16 = v14;
  if ((v11 & 0x8000000000000000) != 0) {
    goto LABEL_20;
  }
  if (HIDWORD(v11))
  {
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  return a10(a6, result, 0, &v16, v11, v12);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, unint64_t, __int16 *, unint64_t), __n128 a4, uint64_t a5, uint64_t a6, int a7, __int16 a8, unint64_t *a9, char a10, uint64_t (*a11)(uint64_t, uint64_t, void, __int16 *, unint64_t, __n128))
{
  if (BYTE4(a2) >= 2u)
  {
    if ((a7 & 0x10000) != 0) {
      __int16 v13 = a8;
    }
    else {
      __int16 v13 = a7;
    }
    __int16 v15 = v13;
    if ((*a9 & 0x8000000000000000) == 0)
    {
      if (!HIDWORD(*a9)) {
        return a3(a6, result, 0x302010001020300uLL >> (8 * a2), &v15, *a9);
      }
      goto LABEL_23;
    }
    __break(1u);
LABEL_22:
    __break(1u);
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v11 = 4096;
  if ((a10 & 1) == 0) {
    uint64_t v11 = 0;
  }
  unint64_t v12 = *a9 | v11;
  *a9 = v12;
  if (BYTE4(a2) == 1)
  {
    a4.n128_f32[0] = (float)(*(float *)&a2 * 3.1416) / 180.0;
  }
  else
  {
    if (BYTE4(a2))
    {
LABEL_25:
      __break(1u);
      return result;
    }
    a4.n128_u32[0] = a2;
  }
  if ((a7 & 0x10000) != 0) {
    __int16 v14 = a8;
  }
  else {
    __int16 v14 = a7;
  }
  __int16 v16 = v14;
  if ((v12 & 0x8000000000000000) != 0) {
    goto LABEL_22;
  }
  if (HIDWORD(v12))
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  return a11(a6, result, 0, &v16, v12, a4);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t (*a3)(uint64_t, uint64_t, unint64_t, uint64_t *, unint64_t), __n128 a4, uint64_t a5, uint64_t a6, uint64_t a7, char a8, uint64_t a9, unint64_t *a10, char a11, uint64_t (*a12)(uint64_t, uint64_t, void, uint64_t *, unint64_t, __n128))
{
  if (BYTE4(a2) >= 2u)
  {
    if (a8) {
      uint64_t v14 = a9;
    }
    else {
      uint64_t v14 = a7;
    }
    uint64_t v17 = v14;
    unint64_t v15 = *a10;
    if ((*a10 & 0x8000000000000000) == 0)
    {
      if (!HIDWORD(v15)) {
        return a3(a6, result, 0x302010001020300uLL >> (8 * a2), &v17, v15);
      }
      goto LABEL_23;
    }
    __break(1u);
LABEL_22:
    __break(1u);
LABEL_23:
    __break(1u);
    goto LABEL_24;
  }
  uint64_t v12 = 4096;
  if ((a11 & 1) == 0) {
    uint64_t v12 = 0;
  }
  unint64_t v13 = *a10 | v12;
  *a10 = v13;
  if (BYTE4(a2) == 1)
  {
    a4.n128_f32[0] = (float)(*(float *)&a2 * 3.1416) / 180.0;
  }
  else
  {
    if (BYTE4(a2))
    {
LABEL_25:
      __break(1u);
      return result;
    }
    a4.n128_u32[0] = a2;
  }
  if (a8) {
    uint64_t v16 = a9;
  }
  else {
    uint64_t v16 = a7;
  }
  uint64_t v17 = v16;
  if ((v13 & 0x8000000000000000) != 0) {
    goto LABEL_22;
  }
  if (HIDWORD(v13))
  {
LABEL_24:
    __break(1u);
    goto LABEL_25;
  }
  return a12(a6, result, 0, &v17, v13, a4);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>._rotate<A, B>(_:destination:backgroundColor:nullBackgroundColor:rotateFunc:rotate90Func:useFloat16Accumulator:)(uint64_t a1, uint64_t a2, void (*a3)(uint64_t, uint64_t, unint64_t, char *), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, void *a8, char a9, void (*a10)(uint64_t, uint64_t, void, char *, float), uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14)
{
  uint64_t v51 = a5;
  uint64_t v53 = a6;
  uint64_t v54 = a7;
  v47[1] = a4;
  uint64_t v48 = a3;
  uint64_t v50 = a1;
  uint64_t v52 = a2;
  unsigned int v15 = BYTE4(a2);
  uint64_t v16 = type metadata accessor for Optional();
  uint64_t v17 = MEMORY[0x1F4188790](v16);
  uint64_t v19 = (char *)v47 - ((v18 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v20 = MEMORY[0x1F4188790](v17);
  uint64_t v22 = (char *)v47 - v21;
  uint64_t v23 = *(void *)(a14 - 8);
  uint64_t v24 = MEMORY[0x1F4188790](v20);
  uint64_t v26 = (char *)v47 - ((v25 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v27 = MEMORY[0x1F4188790](v24);
  int v49 = (char *)v47 - v28;
  uint64_t v29 = MEMORY[0x1F4188790](v27);
  int8x16_t v31 = (char *)v47 - v30;
  MEMORY[0x1F4188790](v29);
  long long v35 = (char *)v47 - v34;
  if (v15 >= 2)
  {
    uint64_t v40 = v32;
    uint64_t v41 = v33;
    (*(void (**)(char *, uint64_t, uint64_t))(v33 + 16))(v22, v53, v32);
    (*(void (**)(char *, uint64_t, uint64_t))(v23 + 16))(v31, v54, a14);
    if ((*(unsigned int (**)(char *, uint64_t, uint64_t))(v23 + 48))(v22, 1, a14) == 1)
    {
      (*(void (**)(char *, char *, uint64_t))(v23 + 32))(v35, v31, a14);
      uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v41 + 8))(v22, v40);
    }
    else
    {
      (*(void (**)(char *, uint64_t))(v23 + 8))(v31, a14);
      uint64_t result = (*(uint64_t (**)(char *, char *, uint64_t))(v23 + 32))(v35, v22, a14);
    }
    if ((*a8 & 0x8000000000000000) == 0)
    {
      if (!HIDWORD(*a8))
      {
        v48(v51, v50, 0x302010001020300uLL >> (8 * v52), v35);
        return (*(uint64_t (**)(char *, uint64_t))(v23 + 8))(v35, a14);
      }
      goto LABEL_22;
    }
    __break(1u);
LABEL_21:
    __break(1u);
LABEL_22:
    __break(1u);
    goto LABEL_23;
  }
  uint64_t v36 = v53;
  uint64_t v37 = v54;
  uint64_t v38 = 4096;
  if ((a9 & 1) == 0) {
    uint64_t v38 = 0;
  }
  *a8 |= v38;
  if (v15 == 1) {
    float v39 = (float)(*(float *)&v52 * 3.1416) / 180.0;
  }
  else {
    float v39 = *(float *)&v52;
  }
  uint64_t v43 = v32;
  uint64_t v44 = v33;
  (*(void (**)(char *, uint64_t, uint64_t))(v33 + 16))(v19, v36, v32);
  (*(void (**)(char *, uint64_t, uint64_t))(v23 + 16))(v26, v37, a14);
  int v45 = (*(uint64_t (**)(char *, uint64_t, uint64_t))(v23 + 48))(v19, 1, a14);
  uint64_t v46 = v49;
  if (v45 == 1)
  {
    (*(void (**)(char *, char *, uint64_t))(v23 + 32))(v49, v26, a14);
    uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v44 + 8))(v19, v43);
  }
  else
  {
    (*(void (**)(char *, uint64_t))(v23 + 8))(v26, a14);
    uint64_t result = (*(uint64_t (**)(char *, char *, uint64_t))(v23 + 32))(v46, v19, a14);
  }
  if ((*a8 & 0x8000000000000000) != 0) {
    goto LABEL_21;
  }
  if (!HIDWORD(*a8))
  {
    a10(v51, v50, 0, v46, v39);
    long long v35 = v46;
    return (*(uint64_t (**)(char *, uint64_t))(v23 + 8))(v35, a14);
  }
LABEL_23:
  __break(1u);
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.shear(direction:translate:slope:resamplingFilter:backgroundColor:destination:)(unsigned __int8 *a1, uint64_t a2, unsigned int a3, uint64_t *a4)
{
  char v8 = BYTE2(a3) & 1;
  if ((a3 & 0x10000) != 0)
  {
    uint64_t v12 = 0;
  }
  else
  {
    char v9 = a3;
    unsigned int v10 = a3 >> 8;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    uint64_t v11 = swift_allocObject();
    *(_OWORD *)(v11 + 16) = xmmword_1D2135290;
    *(unsigned char *)(v11 + 32) = v9;
    uint64_t v12 = v11 + 32;
    *(unsigned char *)(v11 + 33) = v10;
  }
  specialized vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*a1, a2, *a4, v12, v8, (uint64_t)&unk_1F28D7748, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Float, @unowned Float, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Float, @unowned Float, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int), 0, 0, *v4);
  swift_bridgeObjectRelease();

  return swift_bridgeObjectRelease();
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Float, @unowned Float, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, const uint8_t *__attribute__((__org_typedef(Pixel_88))) *a6, vImage_Flags a7, float a8, float a9)
{
  return vImageVerticalShear_CbCr8(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShear_CbCr8(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

uint64_t specialized vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, char a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *), uint64_t a8, uint64_t (*a9)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *), uint64_t a10, char a11, uint64_t a12)
{
  uint64_t v25 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a12 + 16))
  {
    __break(1u);
LABEL_15:
    __break(1u);
  }
  if (!*(void *)(a3 + 16)) {
    goto LABEL_15;
  }
  uint64_t v12 = *(void *)(a12 + 32);
  uint64_t v13 = *(void *)(a3 + 32);
  if (v12)
  {
    if (!v13 || v12 != v13) {
      goto LABEL_8;
    }
    __break(1u);
  }
  if (!v13)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  if (a11) {
    uint64_t v14 = 4096;
  }
  else {
    uint64_t v14 = 0;
  }
  uint64_t v15 = 8;
  if ((a5 & 1) == 0) {
    uint64_t v15 = 4;
  }
  uint64_t v18 = v14 | v15;
  uint64_t v19 = v13;
  uint64_t v16 = *(void *)(a12 + 56);
  uint64_t v22 = v12;
  long long v23 = *(_OWORD *)(a12 + 40);
  uint64_t v24 = v16;
  uint64_t v17 = *(void *)(a3 + 56);
  long long v20 = *(_OWORD *)(a3 + 40);
  uint64_t v21 = v17;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)&v19, result & 1, a9, a10, (uint64_t)&v22, a2, a4, a5 & 1, a6, &v18, a7);
}

uint64_t specialized vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t a3, int a4, __int16 a5, uint64_t (*a6)(uint64_t, uint64_t, void, void, uint64_t, __int16 *), uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, void, void, uint64_t, __int16 *), uint64_t a9, char a10, uint64_t a11)
{
  uint64_t v24 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a11 + 16))
  {
    __break(1u);
LABEL_14:
    __break(1u);
  }
  if (!*(void *)(a3 + 16)) {
    goto LABEL_14;
  }
  uint64_t v11 = *(void *)(a11 + 32);
  uint64_t v12 = *(void *)(a3 + 32);
  if (v11)
  {
    if (!v12 || v11 != v12) {
      goto LABEL_8;
    }
    __break(1u);
  }
  if (!v12)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  uint64_t v13 = 4096;
  if ((a10 & 1) == 0) {
    uint64_t v13 = 0;
  }
  uint64_t v14 = 4;
  if ((a4 & 0x10000) != 0) {
    uint64_t v14 = 8;
  }
  uint64_t v17 = v14 | v13;
  uint64_t v18 = v12;
  uint64_t v15 = *(void *)(a11 + 56);
  uint64_t v21 = v11;
  long long v22 = *(_OWORD *)(a11 + 40);
  uint64_t v23 = v15;
  uint64_t v16 = *(void *)(a3 + 56);
  long long v19 = *(_OWORD *)(a3 + 40);
  uint64_t v20 = v16;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)&v18, result & 1, a8, a9, (uint64_t)&v21, a2, a4 & 0x1FFFF, a5, &v17, a6);
}

uint64_t vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(char *a1, uint64_t a2, uint64_t *a3, uint64_t a4, uint64_t a5, void (*a6)(uint64_t, uint64_t, void, void, uint64_t, char *, float, float), uint64_t a7, void (*a8)(uint64_t, uint64_t, void, void, uint64_t, char *, float, float), float a9, float a10, uint64_t a11, char a12, uint64_t a13, uint64_t a14, uint64_t a15)
{
  void v35[4] = *MEMORY[0x1E4F143B8];
  char v28 = *a1;
  uint64_t v17 = *a3;
  uint64_t v18 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v35[0] = v17;
  type metadata accessor for vImage.PixelBuffer();
  uint64_t result = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (v18)
  {
    if (result) {
      BOOL v20 = v18 == result;
    }
    else {
      BOOL v20 = 0;
    }
    if (!v20) {
      goto LABEL_9;
    }
    __break(1u);
  }
  if (!result)
  {
    __break(1u);
    return result;
  }
LABEL_9:
  int v21 = (*(uint64_t (**)(uint64_t, uint64_t, uint64_t))(*(void *)(a15 - 8) + 48))(a4, 1, a15);
  uint64_t v22 = 4;
  if (v21 == 1) {
    uint64_t v22 = 8;
  }
  uint64_t v23 = 4096;
  if ((a12 & 1) == 0) {
    uint64_t v23 = 0;
  }
  uint64_t v34 = v22 | v23;
  v35[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v35[1] = v24;
  _OWORD v35[2] = v25;
  v35[3] = v26;
  return closure #1 in vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)v35, a9, a10, v17, v28, a8, a11, a2, a4, a5, &v34, a6, a7, *(void *)(a13 + 16), a14, a15);
}

uint64_t vImage.PixelBuffer<>.shear(direction:translate:slope:resamplingFilter:backgroundColor:destination:)(unsigned __int8 *a1, uint64_t a2, int a3, uint64_t *a4)
{
  return specialized vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*a1, a2, *a4, a3 & 0x1FFFF, 0, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, __int16 *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Float, @unowned Float, @unowned UnsafeMutableRawPointer, @unowned UInt16, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, __int16 *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Float, @unowned Float, @unowned UnsafeMutableRawPointer, @unowned UInt16, @unowned UInt32) -> (@unowned Int), 0, 0, *v4);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Float, @unowned Float, @unowned UnsafeMutableRawPointer, @unowned UInt16, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, Pixel_16U *a6, vImage_Flags a7, float a8, float a9)
{
  return vImageVerticalShear_Planar16U(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShear_Planar16U(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

uint64_t closure #1 in vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t a1, float a2, float a3, uint64_t a4, char a5, void (*a6)(uint64_t, uint64_t, void, void, uint64_t, char *, float, float), uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, void *a11, void (*a12)(uint64_t, uint64_t, void, void, uint64_t, char *, float, float), uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16)
{
  v29[4] = *MEMORY[0x1E4F143B8];
  int v18 = a5 & 1;
  type metadata accessor for vImage.PixelBuffer();
  v29[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v29[1] = v19;
  void v29[2] = v20;
  v29[3] = v21;
  return closure #1 in closure #1 in vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)v29, v18, a6, a7, a1, a8, a9, a10, a2, a3, a11, a12, a13, a14, a15, a16);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, char a2, uint64_t (*a3)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, char a8, uint64_t a9, void *a10, uint64_t (*a11)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *))
{
  if ((a8 & 1) == 0) {
    a9 = a7;
  }
  if ((a2 & 1) == 0)
  {
    uint64_t v12 = a9;
    if ((*a10 & 0x8000000000000000) != 0)
    {
      __break(1u);
    }
    else if (!HIDWORD(*a10))
    {
      return a11(a5, result, 0, 0, a6, &v12);
    }
    __break(1u);
    goto LABEL_13;
  }
  uint64_t v12 = a9;
  if ((*a10 & 0x8000000000000000) != 0)
  {
LABEL_13:
    __break(1u);
    goto LABEL_14;
  }
  if (HIDWORD(*a10))
  {
LABEL_14:
    __break(1u);
    return result;
  }
  return a3(a5, result, 0, 0, a6, &v12);
}

uint64_t specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, char a2, uint64_t (*a3)(uint64_t, uint64_t, void, void, uint64_t, __int16 *), uint64_t a4, uint64_t a5, uint64_t a6, int a7, __int16 a8, void *a9, uint64_t (*a10)(uint64_t, uint64_t, void, void, uint64_t, __int16 *))
{
  if ((a7 & 0x10000) != 0) {
    __int16 v10 = a8;
  }
  else {
    __int16 v10 = a7;
  }
  if ((a2 & 1) == 0)
  {
    __int16 v12 = v10;
    if ((*a9 & 0x8000000000000000) != 0)
    {
      __break(1u);
    }
    else if (!HIDWORD(*a9))
    {
      return a10(a5, result, 0, 0, a6, &v12);
    }
    __break(1u);
    goto LABEL_14;
  }
  __int16 v11 = v10;
  if ((*a9 & 0x8000000000000000) != 0)
  {
LABEL_14:
    __break(1u);
    goto LABEL_15;
  }
  if (HIDWORD(*a9))
  {
LABEL_15:
    __break(1u);
    return result;
  }
  return a3(a5, result, 0, 0, a6, &v11);
}

uint64_t closure #1 in closure #1 in vImage.PixelBuffer<>._shear<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t a1, int a2, void (*a3)(uint64_t, uint64_t, void, void, uint64_t, char *, float, float), uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, float a9, float a10, void *a11, void (*a12)(uint64_t, uint64_t, void, void, uint64_t, char *, float, float), uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16)
{
  uint64_t v51 = a6;
  uint64_t v53 = a7;
  uint64_t v54 = a8;
  uint64_t v49 = a1;
  uint64_t v50 = a5;
  uint64_t v45 = a4;
  uint64_t v46 = a3;
  int v52 = a2;
  long long v55 = a11;
  uint64_t v18 = type metadata accessor for Optional();
  uint64_t v19 = MEMORY[0x1F4188790](v18);
  uint64_t v21 = (char *)&v45 - ((v20 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v22 = MEMORY[0x1F4188790](v19);
  uint64_t v24 = (char *)&v45 - v23;
  uint64_t v25 = *(void *)(a16 - 8);
  uint64_t v26 = MEMORY[0x1F4188790](v22);
  char v28 = (char *)&v45 - ((v27 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v29 = MEMORY[0x1F4188790](v26);
  int v47 = (char *)&v45 - v30;
  uint64_t v31 = MEMORY[0x1F4188790](v29);
  uint64_t v33 = (char *)&v45 - v32;
  MEMORY[0x1F4188790](v31);
  uint64_t v36 = (char *)&v45 - v35;
  uint64_t v48 = v37;
  uint64_t v38 = *(void (**)(char *, uint64_t, uint64_t))(v37 + 16);
  float v39 = (void (**)(char *, uint64_t, uint64_t))(v25 + 16);
  uint64_t v40 = (unsigned int (**)(char *, uint64_t, uint64_t))(v25 + 48);
  if ((v52 & 1) == 0)
  {
    uint64_t v41 = v34;
    v38(v21, v53, v34);
    (*v39)(v28, v54, a16);
    if ((*v40)(v21, 1, a16) == 1)
    {
      double v42 = v47;
      (*(void (**)(char *, char *, uint64_t))(v25 + 32))(v47, v28, a16);
      uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v48 + 8))(v21, v41);
    }
    else
    {
      (*(void (**)(char *, uint64_t))(v25 + 8))(v28, a16);
      double v42 = v47;
      uint64_t result = (*(uint64_t (**)(char *, char *, uint64_t))(v25 + 32))(v47, v21, a16);
    }
    if ((*v55 & 0x8000000000000000) != 0)
    {
      __break(1u);
    }
    else if (!HIDWORD(*v55))
    {
      a12(v50, v49, 0, 0, v51, v42, a9, a10);
      uint64_t v36 = v42;
      return (*(uint64_t (**)(char *, uint64_t))(v25 + 8))(v36, a16);
    }
    __break(1u);
    goto LABEL_17;
  }
  uint64_t v44 = v34;
  v38(v24, v53, v34);
  (*v39)(v33, v54, a16);
  if ((*v40)(v24, 1, a16) == 1)
  {
    (*(void (**)(char *, char *, uint64_t))(v25 + 32))(v36, v33, a16);
    uint64_t result = (*(uint64_t (**)(char *, uint64_t))(v48 + 8))(v24, v44);
  }
  else
  {
    (*(void (**)(char *, uint64_t))(v25 + 8))(v33, a16);
    uint64_t result = (*(uint64_t (**)(char *, char *, uint64_t))(v25 + 32))(v36, v24, a16);
  }
  if ((*v55 & 0x8000000000000000) != 0)
  {
LABEL_17:
    __break(1u);
    goto LABEL_18;
  }
  if (!HIDWORD(*v55))
  {
    v46(v50, v49, 0, 0, v51, v36, a9, a10);
    return (*(uint64_t (**)(char *, uint64_t))(v25 + 8))(v36, a16);
  }
LABEL_18:
  __break(1u);
  return result;
}

uint64_t vImage.PixelBuffer<>.shear<A>(direction:translate:slope:resamplingFilter:backgroundColor:destination:)(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4, int a5, uint64_t *a6, uint64_t a7)
{
  uint64_t v21 = a6;
  uint64_t v23 = a4;
  uint64_t v12 = *(void *)(a7 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v14 = (char *)&v20 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  if ((v15 & 0x100000000) != 0)
  {
    uint64_t v16 = 0;
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    uint64_t v16 = swift_allocObject();
    *(_OWORD *)(v16 + 16) = xmmword_1D2135290;
    *(_DWORD *)(v16 + 32) = a5;
  }
  uint64_t v17 = *(void (**)(char *, uint64_t, uint64_t))(v12 + 16);
  v17(v14, a2, a7);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  v17(v14, a3, a7);
  BinaryFloatingPoint.init<A>(_:)();
  if (v16) {
    uint64_t v18 = v16 + 32;
  }
  else {
    uint64_t v18 = 0;
  }
  specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*a1, v23, *v21, v18, v16 == 0, (uint64_t)&unk_1F28D7B30, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, 0, *v22);
  return swift_bridgeObjectRelease();
}

{
  uint64_t v11;
  uint64_t v12;
  char *v13;
  void (*v14)(char *);
  uint64_t v16;
  uint64_t *v17;
  uint64_t v18;

  uint64_t v18 = a4;
  HIDWORD(v16) = a5;
  __int16 v11 = *(void *)(a7 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v13 = (char *)&v16 - ((v12 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v14 = *(void (**)(char *))(v11 + 16);
  v14(v13);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  ((void (*)(char *, uint64_t, uint64_t))v14)(v13, a3, a7);
  BinaryFloatingPoint.init<A>(_:)();
  return specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*a1, v18, *a6, WORD2(v16) & 0x1FF, 0, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, char *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UInt8, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, char *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UInt8, @unowned UInt32) -> (@unowned Int), 0, 0, *v17);
}

{
  uint64_t *v7;
  uint64_t *v8;
  uint64_t v13;
  uint64_t v14;
  char *v15;
  uint64_t v16;
  uint64_t v17;
  void (*v18)(char *, uint64_t, uint64_t);
  uint64_t v19;
  uint64_t v21;
  uint64_t *v22;
  uint64_t v23;
  uint64_t v24;
  unsigned __int8 *v25;
  uint64_t *v26;

  char v8 = v7;
  uint64_t v25 = a1;
  uint64_t v26 = a6;
  uint64_t v24 = a3;
  uint64_t v13 = *(void *)(a7 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v15 = (char *)&v21 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0);
  if ((v16 & 0x100000000) != 0)
  {
    uint64_t v17 = 0;
  }
  else
  {
    uint64_t v22 = v8;
    uint64_t v23 = a4;
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt8>);
    uint64_t v17 = swift_allocObject();
    *(_OWORD *)(v17 + 16) = xmmword_1D2135FC0;
    *(_DWORD *)(v17 + 32) = a5;
    char v8 = v22;
    a4 = v23;
  }
  uint64_t v18 = *(void (**)(char *, uint64_t, uint64_t))(v13 + 16);
  v18(v15, a2, a7);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  v18(v15, v24, a7);
  BinaryFloatingPoint.init<A>(_:)();
  if (v17) {
    uint64_t v19 = v17 + 32;
  }
  else {
    uint64_t v19 = 0;
  }
  specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*v25, a4, *v26, v19, v17 == 0, (uint64_t)&unk_1F28D7830, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt8>?, @unowned UInt32) -> (@unowned Int), 0, 0, *v8);
  return swift_bridgeObjectRelease();
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, const uint16_t *__attribute__((__org_typedef(Pixel_16U16U))) *a6, vImage_Flags a7, double a8, double a9)
{
  return vImageVerticalShearD_CbCr16U(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShearD_CbCr16U(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, const uint16_t *__attribute__((__org_typedef(Pixel_16F16F))) *a6, vImage_Flags a7, double a8, double a9)
{
  return vImageVerticalShearD_CbCr16F(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShearD_CbCr16F(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, const uint16_t *__attribute__((__org_typedef(Pixel_ARGB_16F))) *a6, vImage_Flags a7, double a8, double a9)
{
  return vImageVerticalShearD_ARGB16F(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShearD_ARGB16F(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, const uint16_t *__attribute__((__org_typedef(Pixel_ARGB_16U))) *a6, vImage_Flags a7, double a8, double a9)
{
  return vImageVerticalShearD_ARGB16U(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShearD_ARGB16U(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

uint64_t specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t a3, __int16 a4, char a5, uint64_t (*a6)(uint64_t, uint64_t, void, void, uint64_t, char *), uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, void, void, uint64_t, char *), uint64_t a9, char a10, uint64_t a11)
{
  uint64_t v24 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a11 + 16))
  {
    __break(1u);
LABEL_14:
    __break(1u);
  }
  if (!*(void *)(a3 + 16)) {
    goto LABEL_14;
  }
  uint64_t v11 = *(void *)(a11 + 32);
  uint64_t v12 = *(void *)(a3 + 32);
  if (v11)
  {
    if (!v12 || v11 != v12) {
      goto LABEL_8;
    }
    __break(1u);
  }
  if (!v12)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  uint64_t v13 = 4096;
  if ((a10 & 1) == 0) {
    uint64_t v13 = 0;
  }
  uint64_t v14 = 4;
  if ((a4 & 0x100) != 0) {
    uint64_t v14 = 8;
  }
  uint64_t v17 = v14 | v13;
  uint64_t v18 = v12;
  uint64_t v15 = *(void *)(a11 + 56);
  uint64_t v21 = v11;
  long long v22 = *(_OWORD *)(a11 + 40);
  uint64_t v23 = v15;
  uint64_t v16 = *(void *)(a3 + 56);
  long long v19 = *(_OWORD *)(a3 + 40);
  uint64_t v20 = v16;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)&v18, result & 1, a8, a9, (uint64_t)&v21, a2, a4 & 0x1FF, a5, &v17, a6);
}

uint64_t specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t a3, int a4, __int16 a5, uint64_t (*a6)(uint64_t, uint64_t, void, void, uint64_t, __int16 *), uint64_t a7, uint64_t (*a8)(uint64_t, uint64_t, void, void, uint64_t, __int16 *), uint64_t a9, char a10, uint64_t a11)
{
  uint64_t v24 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a11 + 16))
  {
    __break(1u);
LABEL_14:
    __break(1u);
  }
  if (!*(void *)(a3 + 16)) {
    goto LABEL_14;
  }
  uint64_t v11 = *(void *)(a11 + 32);
  uint64_t v12 = *(void *)(a3 + 32);
  if (v11)
  {
    if (!v12 || v11 != v12) {
      goto LABEL_8;
    }
    __break(1u);
  }
  if (!v12)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  uint64_t v13 = 4096;
  if ((a10 & 1) == 0) {
    uint64_t v13 = 0;
  }
  uint64_t v14 = 4;
  if ((a4 & 0x10000) != 0) {
    uint64_t v14 = 8;
  }
  uint64_t v17 = v14 | v13;
  uint64_t v18 = v12;
  uint64_t v15 = *(void *)(a11 + 56);
  uint64_t v21 = v11;
  long long v22 = *(_OWORD *)(a11 + 40);
  uint64_t v23 = v15;
  uint64_t v16 = *(void *)(a3 + 56);
  long long v19 = *(_OWORD *)(a3 + 40);
  uint64_t v20 = v16;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)&v18, result & 1, a8, a9, (uint64_t)&v21, a2, a4 & 0x1FFFF, a5, &v17, a6);
}

uint64_t specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t a3, uint64_t a4, char a5, uint64_t a6, uint64_t (*a7)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *), uint64_t a8, uint64_t (*a9)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *), uint64_t a10, char a11, uint64_t a12)
{
  uint64_t v25 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a12 + 16))
  {
    __break(1u);
LABEL_15:
    __break(1u);
  }
  if (!*(void *)(a3 + 16)) {
    goto LABEL_15;
  }
  uint64_t v12 = *(void *)(a12 + 32);
  uint64_t v13 = *(void *)(a3 + 32);
  if (v12)
  {
    if (!v13 || v12 != v13) {
      goto LABEL_8;
    }
    __break(1u);
  }
  if (!v13)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  if (a11) {
    uint64_t v14 = 4096;
  }
  else {
    uint64_t v14 = 0;
  }
  uint64_t v15 = 8;
  if ((a5 & 1) == 0) {
    uint64_t v15 = 4;
  }
  uint64_t v18 = v14 | v15;
  uint64_t v19 = v13;
  uint64_t v16 = *(void *)(a12 + 56);
  uint64_t v22 = v12;
  long long v23 = *(_OWORD *)(a12 + 40);
  uint64_t v24 = v16;
  uint64_t v17 = *(void *)(a3 + 56);
  long long v20 = *(_OWORD *)(a3 + 40);
  uint64_t v21 = v17;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)&v19, result & 1, a9, a10, (uint64_t)&v22, a2, a4, a5 & 1, a6, &v18, a7);
}

uint64_t specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(uint64_t result, uint64_t a2, uint64_t a3, unint64_t a4, uint64_t (*a5)(uint64_t, uint64_t, void, void, uint64_t, float *), double a6, double a7, float a8, uint64_t a9, uint64_t (*a10)(uint64_t, uint64_t, void, void, uint64_t, float *), uint64_t a11, char a12, uint64_t a13)
{
  uint64_t v26 = *MEMORY[0x1E4F143B8];
  if (!*(void *)(a13 + 16))
  {
    __break(1u);
LABEL_14:
    __break(1u);
  }
  if (!*(void *)(a3 + 16)) {
    goto LABEL_14;
  }
  uint64_t v13 = *(void *)(a13 + 32);
  uint64_t v14 = *(void *)(a3 + 32);
  if (v13)
  {
    if (!v14 || v13 != v14) {
      goto LABEL_8;
    }
    __break(1u);
  }
  if (!v14)
  {
    __break(1u);
    return result;
  }
LABEL_8:
  uint64_t v15 = 4096;
  if ((a12 & 1) == 0) {
    uint64_t v15 = 0;
  }
  uint64_t v16 = 4;
  if ((a4 & 0x100000000) != 0) {
    uint64_t v16 = 8;
  }
  uint64_t v19 = v16 | v15;
  uint64_t v20 = v14;
  uint64_t v17 = *(void *)(a13 + 56);
  uint64_t v23 = v13;
  long long v24 = *(_OWORD *)(a13 + 40);
  uint64_t v25 = v17;
  uint64_t v18 = *(void *)(a3 + 56);
  long long v21 = *(_OWORD *)(a3 + 40);
  uint64_t v22 = v18;
  return specialized closure #1 in closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)&v20, result & 1, a10, a6, a7, a8, a11, (uint64_t)&v23, a2, a4 | ((HIDWORD(a4) & 1) << 32), &v19, a5);
}

uint64_t vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(char *a1, uint64_t a2, uint64_t *a3, uint64_t a4, uint64_t a5, void (*a6)(uint64_t, uint64_t, void, void, uint64_t, char *, double, double), uint64_t a7, void (*a8)(uint64_t, uint64_t, void, void, uint64_t, char *, double, double), double a9, double a10, uint64_t a11, char a12, uint64_t a13, uint64_t a14, uint64_t a15)
{
  void v35[4] = *MEMORY[0x1E4F143B8];
  char v28 = *a1;
  uint64_t v17 = *a3;
  uint64_t v18 = vImage.PixelBuffer<>.vImageBuffer.getter();
  v35[0] = v17;
  type metadata accessor for vImage.PixelBuffer();
  uint64_t result = vImage.PixelBuffer<>.vImageBuffer.getter();
  if (v18)
  {
    if (result) {
      BOOL v20 = v18 == result;
    }
    else {
      BOOL v20 = 0;
    }
    if (!v20) {
      goto LABEL_9;
    }
    __break(1u);
  }
  if (!result)
  {
    __break(1u);
    return result;
  }
LABEL_9:
  int v21 = (*(uint64_t (**)(uint64_t, uint64_t, uint64_t))(*(void *)(a15 - 8) + 48))(a4, 1, a15);
  uint64_t v22 = 4;
  if (v21 == 1) {
    uint64_t v22 = 8;
  }
  uint64_t v23 = 4096;
  if ((a12 & 1) == 0) {
    uint64_t v23 = 0;
  }
  uint64_t v34 = v22 | v23;
  v35[0] = vImage.PixelBuffer<>.vImageBuffer.getter();
  v35[1] = v24;
  _OWORD v35[2] = v25;
  v35[3] = v26;
  return closure #1 in vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)((uint64_t)v35, a9, a10, v17, v28, a8, a11, a2, a4, a5, &v34, a6, a7, *(void *)(a13 + 16), a14, a15);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UInt8, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, Pixel_8 *a6, vImage_Flags a7, double a8, double a9)
{
  return vImageVerticalShearD_Planar8(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShearD_Planar8(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

uint64_t vImage.PixelBuffer<>.shear<A>(direction:translate:slope:resamplingFilter:backgroundColor:useFloat16Accumulator:destination:)(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4, int a5, int a6, uint64_t *a7, uint64_t a8)
{
  int v20 = a6;
  int v18 = a5;
  uint64_t v21 = a4;
  uint64_t v12 = *(void *)(a8 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v14 = (char *)&v17 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v15 = *(void (**)(char *))(v12 + 16);
  v15(v14);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  ((void (*)(char *, uint64_t, uint64_t))v15)(v14, a3, a8);
  BinaryFloatingPoint.init<A>(_:)();
  return specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*a1, v21, *a7, v18 & 0x1FFFF, 0, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, __int16 *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UInt16, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, __int16 *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UInt16, @unowned UInt32) -> (@unowned Int), 0, v20, *v19);
}

vImage_Error specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UInt16, @unowned UInt32) -> (@unowned Int)(const vImage_Buffer *a1, const vImage_Buffer *a2, vImagePixelCount a3, vImagePixelCount a4, void *a5, const Pixel_16F *a6, vImage_Flags a7, double a8, double a9)
{
  return vImageVerticalShearD_Planar16F(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

{
  return vImageHorizontalShearD_Planar16F(a1, a2, a3, a4, a8, a9, a5, *a6, a7);
}

uint64_t vImage.PixelBuffer<>.shear<A>(direction:translate:slope:resamplingFilter:backgroundColor:useFloat16Accumulator:destination:)(unsigned __int8 *a1, uint64_t a2, uint64_t a3, uint64_t a4, int a5, int a6, uint64_t a7, uint64_t a8)
{
  v21[0] = a7;
  int v22 = a6;
  uint64_t v23 = a4;
  uint64_t v13 = *(void *)(a8 - 8);
  MEMORY[0x1F4188790](a1);
  uint64_t v15 = (char *)v21 - ((v14 + 15) & 0xFFFFFFFFFFFFFFF0);
  if ((v16 & 0x100000000) != 0)
  {
    uint64_t v17 = 0;
  }
  else
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<UInt16>);
    uint64_t v17 = swift_allocObject();
    *(_OWORD *)(v17 + 16) = xmmword_1D2135290;
    *(_DWORD *)(v17 + 32) = a5;
  }
  int v18 = *(void (**)(char *, uint64_t, uint64_t))(v13 + 16);
  v18(v15, a2, a8);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  v18(v15, a3, a8);
  BinaryFloatingPoint.init<A>(_:)();
  if (v17) {
    uint64_t v19 = v17 + 32;
  }
  else {
    uint64_t v19 = 0;
  }
  specialized vImage.PixelBuffer<>._shearD<A, B>(direction:translate:slope:resamplingFilter:destination:backgroundColor:nullBackgroundColor:verticalShearFunc:horizontalShearFunc:useFloat16Accumulator:)(*a1, v23, *(void *)v21[0], v19, v17 == 0, (uint64_t)&unk_1F28D7A40, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, (uint64_t (*)(uint64_t, uint64_t, void, void, uint64_t, uint64_t *))specialized thunk for @callee_guaranteed (@unowned UnsafePointer<vImage_Buffer>, @unowned UnsafePointer<vImage_Buffer>, @unowned UInt, @unowned UInt, @unowned Double, @unowned Double, @unowned UnsafeMutableRawPointer, @unowned UnsafePointer<UInt16>?, @unowned UInt32) -> (@unowned Int), 0, v22 & 1, *(void *)v21[1]);
  return swift_bridgeObjectRelease();
}