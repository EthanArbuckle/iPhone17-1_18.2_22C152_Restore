uint64_t x.getter()
{
  return 1;
}

uint64_t y.getter()
{
  return 2;
}

uint64_t z.getter()
{
  return 4;
}

void __swiftcall SPRotation3D.init(axis:angle:)(SPRotation3D *__return_ptr retstr, SPRotationAxis3D *axis, SPAngle angle)
{
  v8.x = angle.radians;
  v8.y = v3;
  v8.z = v4.f64[0];
  v6.radians = v5;
  SPRotation3DMake(v6, &v8, &v7, v4);
}

void SPRotation3DMake(SPAngle a1@<0:D0>, SPRotationAxis3D *a2@<X0>, float64x2_t *a3@<X8>, float64x2_t a4@<Q2>)
{
  a4.f64[0] = a2->z;
  int64x2_t v5 = vceqzq_f64(*(float64x2_t *)&a2->x);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v5, 1), vandq_s8((int8x16_t)vceqzq_f64(a4), (int8x16_t)v5)).u64[0] & 0x8000000000000000) != 0)
  {
    long long v11 = xmmword_228C1F7A0;
    float64x2_t v10 = 0uLL;
  }
  else
  {
    float64x2_t v6 = *(float64x2_t *)&a2->vector.f64[2];
    float64x2_t v7 = vmulq_f64(v6, v6);
    v7.f64[0] = 1.0 / sqrt(v7.f64[0] + vaddvq_f64(vmulq_f64(*(float64x2_t *)&a2->x, *(float64x2_t *)&a2->x)));
    float64x2_t v12 = vmulq_n_f64(*(float64x2_t *)&a2->x, v7.f64[0]);
    float64x2_t v13 = vmulq_f64(v6, v7);
    __double2 v9 = __sincos_stret(a1.radians * 0.5);
    v8.f64[0] = v9.__sinval;
    float64x2_t v10 = vmulq_n_f64(v12, v9.__sinval);
    *(void *)&long long v11 = *(_OWORD *)&vmulq_f64(v8, v13);
    *((void *)&v11 + 1) = *(void *)&v9.__cosval;
  }
  *a3 = v10;
  a3[1] = (float64x2_t)v11;
}

void __swiftcall SPRotation3D.init(quaternion:)(SPRotation3D *__return_ptr retstr, simd_quatf *quaternion)
{
  *(float64x2_t *)&v5.vector.f64[2] = vcvtq_f64_f32(*(float32x2_t *)v2.f32);
  *(float64x2_t *)v5.vector.f64 = vcvt_hight_f64_f32(v2);
  v4[0] = *(_OWORD *)&v5.vector.f64[2];
  v4[1] = *(_OWORD *)v5.vector.f64;
  SPRotation3DMakeWithQuaternion(v5, (uint64_t)v4, &v3);
}

void __swiftcall SPRotation3D.init(quaternion:)(SPRotation3D *__return_ptr retstr, simd_quatd *quaternion)
{
  simd_quatd v4 = v2;
  SPRotation3DMakeWithQuaternion(v2, (uint64_t)&v4, &v3);
}

__n128 SPRotation3DMakeWithQuaternion@<Q0>(simd_quatd a1@<0:Q0, 16:Q1>, uint64_t a2@<X0>, _OWORD *a3@<X8>)
{
  __n128 result = *(__n128 *)a2;
  long long v4 = *(_OWORD *)(a2 + 16);
  *a3 = *(_OWORD *)a2;
  a3[1] = v4;
  return result;
}

void __swiftcall SPRotation3D.init(eye:target:up:)(SPRotation3D *__return_ptr retstr, SPPoint3D *eye, SPPoint3D *target, SPVector3D *up)
{
  v15.x = v4;
  v15.y = v5;
  v15.z = v6;
  v14.x = v7;
  v14.y = v8;
  v14.z = v9;
  v13.x = v10;
  v13.y = v11;
  v13.z = v16;
  SPRotation3DMakeLookAt(&v15, &v14, &v13, &v12);
}

void SPRotation3DMakeLookAt(SPPoint3D *a1@<X0>, SPPoint3D *a2@<X1>, SPVector3D *a3@<X2>, float64x2_t *a4@<X8>)
{
  float64x2_t v5 = vsubq_f64(*(float64x2_t *)&a2->x, *(float64x2_t *)&a1->x);
  float64x2_t v6 = vsubq_f64(*(float64x2_t *)&a2->vector.f64[2], *(float64x2_t *)&a1->vector.f64[2]);
  float64x2_t v7 = vmulq_f64(v5, v5);
  v7.f64[0] = 1.0 / sqrt(vmulq_f64(v6, v6).f64[0] + vaddvq_f64(v7));
  float64x2_t v8 = vmulq_n_f64(v5, v7.f64[0]);
  float64x2_t v9 = vmulq_f64(v6, v7);
  float64x2_t v10 = *(float64x2_t *)&a3->vector.f64[2];
  float64x2_t v11 = vmulq_f64(v10, v10);
  v11.f64[0] = 1.0 / sqrt(v11.f64[0] + vaddvq_f64(vmulq_f64(*(float64x2_t *)&a3->x, *(float64x2_t *)&a3->x)));
  float64x2_t v12 = vmulq_n_f64(*(float64x2_t *)&a3->x, v11.f64[0]);
  float64x2_t v13 = vmulq_f64(v10, v11);
  v11.f64[0] = v9.f64[0];
  v11.f64[1] = v8.f64[0];
  float64x2_t v14 = vmlaq_laneq_f64(vmulq_laneq_f64(vnegq_f64(v9), v12, 1), v13, v8, 1);
  v13.f64[1] = v12.f64[0];
  float64x2_t v15 = vmlaq_f64(vmulq_f64(v13, vnegq_f64(v8)), v12, v11);
  float64x2_t v16 = vmulq_f64(v15, v15);
  double v17 = vmulq_f64(v14, v14).f64[0];
  v14.f64[1] = v15.f64[0];
  v16.f64[0] = 1.0 / sqrt(v16.f64[1] + v17 + v16.f64[0]);
  float64x2_t v18 = vmulq_n_f64(v14, v16.f64[0]);
  float64x2_t v19 = vmulq_laneq_f64(v16, v15, 1);
  float64x2_t v20 = vnegq_f64(v18);
  float64x2_t v21 = vnegq_f64(v19);
  v19.f64[1] = v18.f64[0];
  float64x2_t v22 = vmlaq_f64(vmulq_f64(v11, v20), v8, v19);
  float64x2_t v23 = vmlaq_laneq_f64(vmulq_laneq_f64(v21, v8, 1), v9, v18, 1);
  float64x2_t v24 = vmulq_f64(v22, v22);
  v18.f64[0] = vmulq_f64(v23, v23).f64[0];
  v23.f64[1] = v22.f64[0];
  v24.f64[0] = 1.0 / sqrt(v24.f64[1] + v18.f64[0] + v24.f64[0]);
  v29[0] = v20;
  v29[1] = v21;
  v29[2] = vmulq_n_f64(v23, v24.f64[0]);
  v29[3] = vmulq_laneq_f64(v24, v22, 1);
  v29[4] = v8;
  v29[5] = v9;
  simd_quaternion((uint64_t)v29, (uint64_t)&v30);
  double v25 = vaddvq_f64(vaddq_f64(vmulq_f64(v30, v30), vmulq_f64(v31, v31)));
  if (v25 == 0.0)
  {
    float64x2_t v26 = (float64x2_t)xmmword_228C1F7A0;
    float64x2_t v27 = 0uLL;
  }
  else
  {
    double v28 = 1.0 / sqrt(v25);
    float64x2_t v26 = vmulq_n_f64(v31, v28);
    float64x2_t v27 = vmulq_n_f64(v30, v28);
  }
  *a4 = v27;
  a4[1] = v26;
}

uint64_t (*SPSize3D.simd.modify(void *a1))()
{
  long long v3 = malloc(0x28uLL);
  *a1 = v3;
  v3[4] = v1;
  long long v4 = v1[1];
  *(_OWORD *)long long v3 = *v1;
  *((_OWORD *)v3 + 1) = v4;
  return SPSize3D.simd.modify;
}

uint64_t (*SPPoint3D.simd.modify(void *a1))()
{
  long long v3 = malloc(0x28uLL);
  *a1 = v3;
  v3[4] = v1;
  long long v4 = v1[1];
  *(_OWORD *)long long v3 = *v1;
  *((_OWORD *)v3 + 1) = v4;
  return SPSize3D.simd.modify;
}

__n128 key path setter for SPRotationAxis3D.simd : SPRotationAxis3D(uint64_t a1, _OWORD *a2)
{
  __n128 result = *(__n128 *)a1;
  long long v3 = *(_OWORD *)(a1 + 16);
  *a2 = *(_OWORD *)a1;
  a2[1] = v3;
  return result;
}

void SPRotationAxis3D.simd.setter(__n128 a1, __n128 a2)
{
  *simd_quatd v2 = a1;
  v2[1] = a2;
}

uint64_t (*SPRotationAxis3D.simd.modify(void *a1))()
{
  long long v3 = malloc(0x28uLL);
  *a1 = v3;
  v3[4] = v1;
  long long v4 = v1[1];
  *(_OWORD *)long long v3 = *v1;
  *((_OWORD *)v3 + 1) = v4;
  return SPSize3D.simd.modify;
}

void SPSize3D.simd.modify(void **a1)
{
  v1 = *a1;
  simd_quatd v2 = (_OWORD *)v1[4];
  long long v3 = *((_OWORD *)v1 + 1);
  *simd_quatd v2 = *(_OWORD *)v1;
  v2[1] = v3;
  free(v1);
}

Swift::Bool __swiftcall SPSize3D.containsAny(of:)(Swift::OpaquePointer of)
{
  unint64_t v4 = *((void *)of._rawValue + 2);
  if (v4 >> 31)
  {
    __break(1u);
  }
  else
  {
    v6.width = v1;
    v6.height = v2;
    v6.depth = v3;
    LOBYTE(of._rawValue) = SPSize3DContainsAnyPoint(&v6, (const SPPoint3D *)of._rawValue + 1, v4);
  }
  return (Swift::Bool)of._rawValue;
}

BOOL SPSize3DContainsAnyPoint(SPSize3D *a1, const SPPoint3D *a2, int a3)
{
  if (a3 < 1)
  {
    return 0;
  }
  else
  {
    uint64_t v3 = 0;
    long long v4 = *(_OWORD *)&a1->vector.f64[2];
    float64x2_t v5 = vaddq_f64(vminnmq_f64(*(float64x2_t *)&a1->width, (float64x2_t)0), (float64x2_t)0);
    float64x2_t v6 = vaddq_f64(vminnmq_f64((float64x2_t)(unint64_t)v4, (float64x2_t)0), (float64x2_t)0);
    float64x2_t v7 = vmaxnmq_f64((float64x2_t)(unint64_t)v4, (float64x2_t)0);
    float64x2_t v8 = vaddq_f64(vmaxnmq_f64(*(float64x2_t *)&a1->width, (float64x2_t)0), (float64x2_t)0);
    float64x2_t v9 = vaddq_f64(v7, (float64x2_t)0);
    BOOL v10 = 1;
    do
    {
      float64x2_t v11 = (float64x2_t *)&a2[v3];
      int64x2_t v12 = (int64x2_t)vandq_s8((int8x16_t)vcgeq_f64(v8, *v11), (int8x16_t)vcgeq_f64(*v11, v5));
      if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v12, 1), vandq_s8(vandq_s8((int8x16_t)vcgeq_f64(v9, v11[1]), (int8x16_t)vcgeq_f64(v11[1], v6)), (int8x16_t)v12)).u64[0] & 0x8000000000000000) != 0)break; {
      BOOL v10 = ++v3 < (unint64_t)a3;
      }
    }
    while (a3 != v3);
  }
  return v10;
}

Swift::Bool __swiftcall SPRect3D.containsAny(of:)(Swift::OpaquePointer of)
{
  unint64_t v2 = *((void *)of._rawValue + 2);
  if (v2 >> 31)
  {
    __break(1u);
  }
  else
  {
    long long v4 = *(_OWORD *)(v1 + 16);
    long long v3 = *(_OWORD *)(v1 + 32);
    double v5 = *(double *)(v1 + 48);
    double v6 = *(double *)(v1 + 56);
    *(_OWORD *)&v8.origin.x = *(_OWORD *)v1;
    *(_OWORD *)&v8.origin.vector.f64[2] = v4;
    v8.size.depth = v5;
    v8.size.vector.f64[3] = v6;
    *(_OWORD *)&v8.size.width = v3;
    LOBYTE(of._rawValue) = SPRect3DContainsAnyPoint(&v8, (const SPPoint3D *)of._rawValue + 1, v2);
  }
  return (Swift::Bool)of._rawValue;
}

BOOL SPRect3DContainsAnyPoint(SPRect3D *a1, const SPPoint3D *a2, int a3)
{
  if (a3 < 1)
  {
    return 0;
  }
  else
  {
    uint64_t v3 = 0;
    float64x2_t v4 = *(float64x2_t *)&a1->size.width;
    long long v5 = *(_OWORD *)&a1->size.vector.f64[2];
    float64x2_t v6 = *(float64x2_t *)&a1->origin.vector.f64[2];
    float64x2_t v7 = vaddq_f64(*(float64x2_t *)&a1->origin.x, vminnmq_f64(v4, (float64x2_t)0));
    float64x2_t v8 = vaddq_f64(v6, vminnmq_f64((float64x2_t)(unint64_t)v5, (float64x2_t)0));
    float64x2_t v9 = vaddq_f64(*(float64x2_t *)&a1->origin.x, vmaxnmq_f64(v4, (float64x2_t)0));
    float64x2_t v10 = vaddq_f64(v6, vmaxnmq_f64((float64x2_t)(unint64_t)v5, (float64x2_t)0));
    BOOL v11 = 1;
    do
    {
      int64x2_t v12 = (float64x2_t *)&a2[v3];
      int64x2_t v13 = (int64x2_t)vandq_s8((int8x16_t)vcgeq_f64(v9, *v12), (int8x16_t)vcgeq_f64(*v12, v7));
      if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v13, 1), vandq_s8(vandq_s8((int8x16_t)vcgeq_f64(v10, v12[1]), (int8x16_t)vcgeq_f64(v12[1], v8)), (int8x16_t)v13)).u64[0] & 0x8000000000000000) != 0)break; {
      BOOL v11 = ++v3 < (unint64_t)a3;
      }
    }
    while (a3 != v3);
  }
  return v11;
}

Swift::Double __swiftcall SPRect3D.distance(to:)(SPRect3D *to)
{
  long long v2 = *(_OWORD *)v1;
  double v3 = v1[3];
  long long v4 = *(_OWORD *)&to->origin.x;
  double z = to->origin.z;
  double v6 = to->origin.vector.f64[3];
  v9.double z = v1[2];
  v9.vector.f64[3] = v3;
  *(_OWORD *)&v9.x = v2;
  v8.double z = z;
  v8.vector.f64[3] = v6;
  *(_OWORD *)&v8.x = v4;
  return SPPoint3DDistanceToPoint(&v9, &v8);
}

double SPPoint3DDistanceToPoint(SPPoint3D *a1, SPPoint3D *a2)
{
  float64x2_t v2 = vsubq_f64(*(float64x2_t *)&a1->x, *(float64x2_t *)&a2->x);
  float64x2_t v3 = vsubq_f64(*(float64x2_t *)&a1->vector.f64[2], *(float64x2_t *)&a2->vector.f64[2]);
  return sqrt(vmulq_f64(v3, v3).f64[0] + vaddvq_f64(vmulq_f64(v2, v2)));
}

void __swiftcall SPRect3D.rotation(to:)(SPRotation3D *__return_ptr retstr, SPRect3D *to)
{
  long long v4 = *(_OWORD *)v2;
  double v5 = v2[3];
  long long v6 = *(_OWORD *)&to->origin.x;
  double z = to->origin.z;
  double v8 = to->origin.vector.f64[3];
  v11.double z = v2[2];
  v11.vector.f64[3] = v5;
  *(_OWORD *)&v11.x = v4;
  v10.double z = z;
  v10.vector.f64[3] = v8;
  *(_OWORD *)&v10.x = v6;
  SPPoint3DRotationToPoint((float64x2_t *)&v11, &v10, &v9, v3);
}

float64x2_t *SPPoint3DRotationToPoint@<X0>(float64x2_t *result@<X0>, SPPoint3D *a2@<X1>, float64x2_t *a3@<X8>, float64x2_t a4@<Q5>)
{
  float64x2_t v4 = result[1];
  float64x2_t v5 = vmulq_f64(v4, v4);
  v5.f64[0] = 1.0 / sqrt(v5.f64[0] + vaddvq_f64(vmulq_f64(*result, *result)));
  float64x2_t v6 = vmulq_f64(v4, v5);
  float64x2_t v7 = vmulq_n_f64(*result, v5.f64[0]);
  float64x2_t v8 = (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v7, 8uLL);
  float64x2_t v9 = *(float64x2_t *)&a2->vector.f64[2];
  float64x2_t v10 = vmulq_f64(v9, v9);
  v10.f64[0] = 1.0 / sqrt(v10.f64[0] + vaddvq_f64(vmulq_f64(*(float64x2_t *)&a2->x, *(float64x2_t *)&a2->x)));
  float64x2_t v11 = vmulq_f64(v9, v10);
  float64x2_t v12 = vmulq_n_f64(*(float64x2_t *)&a2->x, v10.f64[0]);
  if (vmulq_f64(v6, v11).f64[0] + vaddvq_f64(vmulq_f64(v7, v12)) >= 0.0)
  {
    float64x2_t v47 = vaddq_f64(v7, v12);
    float64x2_t v48 = vaddq_f64(v6, v11);
    a4.f64[0] = 1.0 / sqrt(vmulq_f64(v48, v48).f64[0] + vaddvq_f64(vmulq_f64(v47, v47)));
    float64x2_t v49 = vmulq_n_f64(v47, a4.f64[0]);
    float64x2_t v50 = vmulq_f64(v48, a4);
    v51.f64[0] = v6.f64[0];
    v51.f64[1] = v7.f64[0];
    v7.f64[1] = v8.f64[0];
    *(void *)&v46.f64[0] = *(_OWORD *)&vmlaq_f64(vmulq_laneq_f64(vnegq_f64(v6), v49, 1), v50, v8);
    v52.f64[0] = v50.f64[0];
    v52.f64[1] = v49.f64[0];
    int64x2_t v53 = (int64x2_t)vmlaq_f64(vmulq_f64(v52, vnegq_f64(v7)), v49, v51);
    *(void *)&v46.f64[1] = v53.i64[0];
    *(void *)&v45.f64[0] = vdupq_laneq_s64(v53, 1).u64[0];
    v45.f64[1] = vmulq_f64(v6, v50).f64[0] + vaddvq_f64(vmulq_f64(v7, v49));
  }
  else
  {
    v13.f64[0] = v7.f64[0];
    v13.f64[1] = v8.f64[0];
    float64x2_t v14 = vmulq_f64(v13, v13);
    v14.f64[0] = 1.0 / sqrt(vmulq_f64(v6, v6).f64[0] + vaddvq_f64(v14));
    float64x2_t v15 = vmulq_n_f64(v13, v14.f64[0]);
    v16.f64[0] = v12.f64[0];
    *(void *)&v16.f64[1] = vextq_s8((int8x16_t)v12, (int8x16_t)v12, 8uLL).u64[0];
    float64x2_t v17 = vmulq_f64(v6, v14);
    a4.f64[0] = 1.0 / sqrt(vmulq_f64(v11, v11).f64[0] + vaddvq_f64(vmulq_f64(v16, v16)));
    float64x2_t v18 = vmulq_n_f64(v16, a4.f64[0]);
    float64x2_t v19 = vmulq_f64(v11, a4);
    float64x2_t v20 = vaddq_f64(v15, v18);
    float64x2_t v21 = vaddq_f64(v17, v19);
    float64x2_t v22 = vmulq_f64(v20, v20);
    v22.f64[0] = vmulq_f64(v21, v21).f64[0] + vaddvq_f64(v22);
    if (v22.f64[0] <= 4.93038066e-32)
    {
      v54.f64[0] = v7.f64[0];
      v54.f64[1] = v8.f64[0];
      float64x2_t v55 = vabsq_f64(v54);
      float64x2_t v56 = vabsq_f64(v6);
      if (v55.f64[0] > v55.f64[1] || v55.f64[0] > v56.f64[0])
      {
        if (v55.f64[1] <= v56.f64[0])
        {
          v69.f64[0] = v7.f64[0];
          v69.f64[1] = v8.f64[0];
          float64x2_t v70 = vmulq_f64(v69, (float64x2_t)vdupq_n_s64(0x8000000000000000));
          float64x2_t v59 = vnegq_f64(v6);
          v6.f64[1] = v7.f64[0];
          float64x2_t v60 = vmlaq_f64(v70, (float64x2_t)xmmword_228C1F7A0, v6);
          _Q1 = 0uLL;
        }
        else
        {
          v56.f64[0] = -0.0;
          v58.f64[0] = v7.f64[0];
          v58.f64[1] = v8.f64[0];
          float64x2_t v59 = vmulq_f64(v6, v56);
          v6.f64[1] = v7.f64[0];
          float64x2_t v60 = vmlaq_f64(vmulq_f64(v58, (float64x2_t)xmmword_228C1F7B0), (float64x2_t)0, v6);
          __asm { FMOV            V1.2D, #1.0 }
        }
        float64x2_t v71 = vmlaq_f64(v59, _Q1, v8);
        float64x2_t v45 = vmulq_f64(v60, v60);
        double v72 = vmulq_f64(v71, v71).f64[0];
        v71.f64[1] = v60.f64[0];
        v45.f64[0] = 1.0 / sqrt(v45.f64[1] + v72 + v45.f64[0]);
        float64x2_t v46 = vmulq_n_f64(v71, v45.f64[0]);
        *(void *)&v45.f64[0] = *(_OWORD *)&vmulq_laneq_f64(v45, v60, 1);
      }
      else
      {
        v66.f64[0] = v6.f64[0];
        v66.f64[1] = v7.f64[0];
        v7.f64[1] = v8.f64[0];
        v12.f64[0] = -0.0;
        float64x2_t v67 = vmlaq_f64(vmulq_f64(v7, (float64x2_t)xmmword_228C1F7C0), (float64x2_t)xmmword_228C1F7D0, v66);
        float64x2_t v68 = vmlaq_f64(vmulq_f64(v6, v12), (float64x2_t)0, v8);
        float64x2_t v45 = vmulq_f64(v67, v67);
        v66.f64[0] = vmulq_f64(v68, v68).f64[0];
        v68.f64[1] = v67.f64[0];
        v45.f64[0] = 1.0 / sqrt(v45.f64[1] + v66.f64[0] + v45.f64[0]);
        float64x2_t v46 = vmulq_n_f64(v68, v45.f64[0]);
        *(void *)&v45.f64[0] = *(_OWORD *)&vmulq_laneq_f64(v45, v67, 1);
      }
      v45.f64[1] = 0.0;
    }
    else
    {
      v22.f64[0] = 1.0 / sqrt(v22.f64[0]);
      *(void *)&v20.f64[1] = vextq_s8((int8x16_t)v20, (int8x16_t)v20, 8uLL).u64[0];
      float64x2_t v23 = vmulq_f64(v21, v22);
      float64x2_t v24 = vmulq_n_f64(v20, v22.f64[0]);
      float64x2_t v25 = vaddq_f64(v7, v24);
      float64x2_t v26 = vaddq_f64(v6, v23);
      float64x2_t v27 = vmulq_f64(v26, v26);
      v27.f64[0] = 1.0 / sqrt(v27.f64[0] + vaddvq_f64(vmulq_f64(v25, v25)));
      float64x2_t v28 = vmulq_n_f64(v25, v27.f64[0]);
      float64x2_t v29 = vmulq_f64(v26, v27);
      v27.f64[0] = v6.f64[0];
      v27.f64[1] = v7.f64[0];
      v7.f64[1] = v8.f64[0];
      *(void *)&double v30 = *(_OWORD *)&vmlaq_f64(vmulq_laneq_f64(vnegq_f64(v6), v28, 1), v29, v8);
      double v31 = vmulq_f64(v6, v29).f64[0];
      v29.f64[1] = v28.f64[0];
      float64x2_t v32 = vmlaq_f64(vmulq_f64(v29, vnegq_f64(v7)), v28, v27);
      double v33 = v31 + vaddvq_f64(vmulq_f64(v7, v28));
      float64x2_t v34 = vaddq_f64(v12, v24);
      float64x2_t v35 = vaddq_f64(v11, v23);
      float64x2_t v36 = vmulq_f64(v35, v35);
      v36.f64[0] = 1.0 / sqrt(v36.f64[0] + vaddvq_f64(vmulq_f64(v34, v34)));
      float64x2_t v37 = vmulq_n_f64(v34, v36.f64[0]);
      float64x2_t v38 = vmulq_f64(v35, v36);
      float64x2_t v39 = vnegq_f64(v23);
      v28.f64[0] = vmulq_f64(v23, v38).f64[0];
      v23.f64[1] = v24.f64[0];
      int8x16_t v40 = (int8x16_t)vmlaq_laneq_f64(vmulq_laneq_f64(v39, v37, 1), v38, v24, 1);
      v38.f64[1] = v37.f64[0];
      int64x2_t v41 = (int64x2_t)vmlaq_f64(vmulq_f64(v38, vnegq_f64(v24)), v37, v23);
      *(void *)&v23.f64[0] = v40.i64[0];
      *(void *)&v23.f64[1] = v41.i64[0];
      v37.f64[0] = vaddvq_f64(vmulq_f64(v24, v37));
      *(void *)&v24.f64[0] = vdupq_laneq_s64(v41, 1).u64[0];
      v24.f64[1] = v28.f64[0] + v37.f64[0];
      float64x2_t v42 = vnegq_f64(v23);
      float64x2_t v43 = (float64x2_t)vextq_s8((int8x16_t)v24, (int8x16_t)vnegq_f64(v24), 8uLL);
      float64x2_t v44 = vmlaq_laneq_f64(vmulq_n_f64(v23, v33), (float64x2_t)vextq_s8((int8x16_t)v42, v40, 8uLL), v32, 1);
      float64x2_t v45 = vaddq_f64(vmlaq_laneq_f64(vmulq_n_f64(v24, v33), v43, v32, 1), vmlaq_n_f64(vmulq_n_f64(v42, v32.f64[0]), (float64x2_t)vzip1q_s64(v41, (int64x2_t)v42), v30));
      float64x2_t v46 = vaddq_f64(v44, vmlaq_n_f64(vmulq_n_f64(v24, v32.f64[0]), v43, v30));
    }
  }
  *a3 = v46;
  a3[1] = v45;
  return result;
}

void __swiftcall SPPoint3D.translated(by:)(SPPoint3D *__return_ptr retstr, SPSize3D *by)
{
  double v8 = v7;
  double v9 = v6;
  double v10 = v5;
  v13.width = v2;
  v13.height = v3;
  v13.depth = v4;
  SPVector3DMakeWithSize(&v13, (uint64_t)&v12);
  v13.width = v10;
  v13.height = v9;
  v13.depth = v8;
  SPPoint3DTranslate((SPPoint3D *)&v13, &v12, (uint64_t)&v11);
}

double SPVector3DMakeWithSize@<D0>(SPSize3D *a1@<X0>, uint64_t a2@<X8>)
{
  double result = a1->depth;
  *(_OWORD *)a2 = *(_OWORD *)&a1->width;
  *(double *)(a2 + 16) = result;
  return result;
}

{
  double result;

  double result = a1->depth;
  *(_OWORD *)a2 = *(_OWORD *)&a1->width;
  *(double *)(a2 + 16) = result;
  return result;
}

float64x2_t SPPoint3DTranslate@<Q0>(SPPoint3D *a1@<X0>, SPVector3D *a2@<X1>, uint64_t a3@<X8>)
{
  uint64_t v3 = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a1->vector.f64[2], *(float64x2_t *)&a2->vector.f64[2]);
  float64x2_t result = vaddq_f64(*(float64x2_t *)&a1->x, *(float64x2_t *)&a2->x);
  *(float64x2_t *)a3 = result;
  *(void *)(a3 + 16) = v3;
  return result;
}

void __swiftcall SPRect3D.translated(by:)(SPRect3D *__return_ptr retstr, SPSize3D *by)
{
  SPRect3D.translated(by:)((void (*)(_OWORD *__return_ptr, SPSize3D *, unsigned char *))SPRect3DTranslate, (uint64_t)retstr, v2, v3, v4);
}

float64x2_t SPRect3DTranslate@<Q0>(SPRect3D *a1@<X0>, SPVector3D *a2@<X1>, uint64_t a3@<X8>)
{
  uint64_t v3 = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a1->origin.vector.f64[2], *(float64x2_t *)&a2->vector.f64[2]);
  float64x2_t result = vaddq_f64(*(float64x2_t *)&a1->origin.x, *(float64x2_t *)&a2->x);
  long long v5 = *(_OWORD *)&a1->size.vector.f64[2];
  *(_OWORD *)(a3 + 32) = *(_OWORD *)&a1->size.width;
  *(_OWORD *)(a3 + 48) = v5;
  *(float64x2_t *)a3 = result;
  *(void *)(a3 + 16) = v3;
  return result;
}

{
  uint64_t v3;
  float64x2_t result;
  long long v5;

  uint64_t v3 = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a1->origin.vector.f64[2], *(float64x2_t *)&a2->vector.f64[2]);
  float64x2_t result = vaddq_f64(*(float64x2_t *)&a1->origin.x, *(float64x2_t *)&a2->x);
  long long v5 = *(_OWORD *)&a1->size.vector.f64[2];
  *(_OWORD *)(a3 + 32) = *(_OWORD *)&a1->size.width;
  *(_OWORD *)(a3 + 48) = v5;
  *(float64x2_t *)a3 = result;
  *(void *)(a3 + 16) = v3;
  return result;
}

{
  uint64_t v3;
  float64x2_t result;
  long long v5;

  uint64_t v3 = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a1->origin.vector.f64[2], *(float64x2_t *)&a2->vector.f64[2]);
  float64x2_t result = vaddq_f64(*(float64x2_t *)&a1->origin.x, *(float64x2_t *)&a2->x);
  long long v5 = *(_OWORD *)&a1->size.vector.f64[2];
  *(_OWORD *)(a3 + 32) = *(_OWORD *)&a1->size.width;
  *(_OWORD *)(a3 + 48) = v5;
  *(float64x2_t *)a3 = result;
  *(void *)(a3 + 16) = v3;
  return result;
}

void __swiftcall SPRay3D.translated(by:)(SPRay3D *__return_ptr retstr, SPSize3D *by)
{
  SPRect3D.translated(by:)((void (*)(_OWORD *__return_ptr, SPSize3D *, unsigned char *))SPRay3DTranslate, (uint64_t)retstr, v2, v3, v4);
}

double SPRect3D.translated(by:)@<D0>(void (*a1)(_OWORD *__return_ptr, SPSize3D *, unsigned char *)@<X0>, uint64_t a2@<X8>, double a3@<D0>, double a4@<D1>, double a5@<D2>)
{
  long long v15 = v5[1];
  long long v16 = *v5;
  long long v17 = v5[2];
  uint64_t v9 = *((void *)v5 + 6);
  uint64_t v8 = *((void *)v5 + 7);
  v22.width = a3;
  v22.height = a4;
  v22.depth = a5;
  SPVector3DMakeWithSize(&v22, (uint64_t)v21);
  *(_OWORD *)&v22.width = v16;
  *(_OWORD *)&v22.vector.f64[2] = v15;
  uint64_t v24 = v9;
  uint64_t v25 = v8;
  long long v23 = v17;
  a1(v18, &v22, v21);
  double result = *(double *)v18;
  long long v11 = v18[1];
  long long v12 = v18[2];
  uint64_t v13 = v19;
  uint64_t v14 = v20;
  *(_OWORD *)a2 = v18[0];
  *(_OWORD *)(a2 + 16) = v11;
  *(void *)(a2 + 48) = v13;
  *(void *)(a2 + 56) = v14;
  *(_OWORD *)(a2 + 32) = v12;
  return result;
}

float64x2_t *SPRay3DTranslate@<X0>(float64x2_t *result@<X0>, SPVector3D *a2@<X1>, float64x2_t *a3@<X8>)
{
  *(void *)&float64_t v3 = *(_OWORD *)&vaddq_f64(result[1], *(float64x2_t *)&a2->vector.f64[2]);
  float64x2_t v4 = vaddq_f64(*result, *(float64x2_t *)&a2->x);
  float64x2_t v5 = result[2];
  float64x2_t v6 = result[3];
  int64x2_t v7 = vceqzq_f64(v5);
  a3[2] = 0u;
  a3[3] = 0u;
  *a3 = v4;
  a3[1] = 0u;
  a3[1].f64[0] = v3;
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v7, 1), vandq_s8((int8x16_t)vceqzq_f64(v6), (int8x16_t)v7)).u64[0] & 0x8000000000000000) != 0)
  {
    a3[2] = v5;
    a3[3] = v6;
  }
  else
  {
    float64x2_t v8 = vmulq_f64(v6, v6);
    v8.f64[0] = 1.0 / sqrt(v8.f64[0] + vaddvq_f64(vmulq_f64(v5, v5)));
    a3[2] = vmulq_n_f64(v5, v8.f64[0]);
    *(void *)&a3[3].f64[0] = *(_OWORD *)&vmulq_f64(v6, v8);
  }
  return result;
}

{
  float64_t v3;
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  int64x2_t v7;
  float64x2_t v8;

  *(void *)&float64_t v3 = *(_OWORD *)&vaddq_f64(result[1], *(float64x2_t *)&a2->vector.f64[2]);
  float64x2_t v4 = vaddq_f64(*result, *(float64x2_t *)&a2->x);
  float64x2_t v5 = result[2];
  float64x2_t v6 = result[3];
  int64x2_t v7 = vceqzq_f64(v5);
  a3[2] = 0u;
  a3[3] = 0u;
  *a3 = v4;
  a3[1] = 0u;
  a3[1].f64[0] = v3;
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v7, 1), vandq_s8((int8x16_t)vceqzq_f64(v6), (int8x16_t)v7)).u64[0] & 0x8000000000000000) != 0)
  {
    a3[2] = v5;
    a3[3] = v6;
  }
  else
  {
    float64x2_t v8 = vmulq_f64(v6, v6);
    v8.f64[0] = 1.0 / sqrt(v8.f64[0] + vaddvq_f64(vmulq_f64(v5, v5)));
    a3[2] = vmulq_n_f64(v5, v8.f64[0]);
    *(void *)&a3[3].f64[0] = *(_OWORD *)&vmulq_f64(v6, v8);
  }
  return result;
}

{
  float64_t v3;
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  int64x2_t v7;
  float64x2_t v8;

  *(void *)&float64_t v3 = *(_OWORD *)&vaddq_f64(result[1], *(float64x2_t *)&a2->vector.f64[2]);
  float64x2_t v4 = vaddq_f64(*result, *(float64x2_t *)&a2->x);
  float64x2_t v5 = result[2];
  float64x2_t v6 = result[3];
  int64x2_t v7 = vceqzq_f64(v5);
  a3[2] = 0u;
  a3[3] = 0u;
  *a3 = v4;
  a3[1] = 0u;
  a3[1].f64[0] = v3;
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v7, 1), vandq_s8((int8x16_t)vceqzq_f64(v6), (int8x16_t)v7)).u64[0] & 0x8000000000000000) != 0)
  {
    a3[2] = v5;
    a3[3] = v6;
  }
  else
  {
    float64x2_t v8 = vmulq_f64(v6, v6);
    v8.f64[0] = 1.0 / sqrt(v8.f64[0] + vaddvq_f64(vmulq_f64(v5, v5)));
    a3[2] = vmulq_n_f64(v5, v8.f64[0]);
    *(void *)&a3[3].f64[0] = *(_OWORD *)&vmulq_f64(v6, v8);
  }
  return result;
}

void __swiftcall SPPose3D.translated(by:)(SPPose3D *__return_ptr retstr, SPSize3D *by)
{
  long long v13 = *((_OWORD *)v2 + 1);
  long long v14 = *(_OWORD *)v2;
  long long v15 = *((_OWORD *)v2 + 2);
  double v8 = v2[6];
  double v7 = v2[7];
  v20.position.x = v3;
  v20.position.y = v4;
  v20.position.double z = v5;
  SPVector3DMakeWithSize((SPSize3D *)&v20, (uint64_t)&v19);
  *(_OWORD *)&v20.position.x = v14;
  *(_OWORD *)&v20.position.vector.f64[2] = v13;
  v20.rotation.vector.f64[2] = v8;
  v20.rotation.vector.f64[3] = v7;
  *(_OWORD *)v20.rotation.vector.f64 = v15;
  SPPose3DTranslate(&v20, &v19, (uint64_t)v16);
  long long v9 = v16[1];
  long long v10 = v16[2];
  double v11 = v17;
  double v12 = v18;
  *(_OWORD *)&retstr->position.x = v16[0];
  *(_OWORD *)&retstr->position.vector.f64[2] = v9;
  retstr->rotation.vector.f64[2] = v11;
  retstr->rotation.vector.f64[3] = v12;
  *(_OWORD *)retstr->rotation.vector.f64 = v10;
}

float64x2_t SPPose3DTranslate@<Q0>(SPPose3D *a1@<X0>, SPVector3D *a2@<X1>, uint64_t a3@<X8>)
{
  *(void *)(a3 + 24) = 0;
  uint64_t v3 = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a1->position.vector.f64[2], *(float64x2_t *)&a2->vector.f64[2]);
  float64x2_t result = vaddq_f64(*(float64x2_t *)&a1->position.x, *(float64x2_t *)&a2->x);
  long long v5 = *(_OWORD *)a1->rotation.vector.f64;
  long long v6 = *(_OWORD *)&a1->rotation.quaternion.vector.f64[2];
  *(float64x2_t *)a3 = result;
  *(void *)(a3 + 16) = v3;
  *(_OWORD *)(a3 + 32) = v5;
  *(_OWORD *)(a3 + 48) = v6;
  return result;
}

void __swiftcall SPPose3D.init(matrix:)(SPPose3D_optional *__return_ptr retstr, simd_double4x4 *matrix)
{
  long long v3 = *(_OWORD *)&matrix->columns[0].f64[2];
  long long v4 = *(_OWORD *)matrix->columns[1].f64;
  long long v5 = *(_OWORD *)&matrix->columns[1].f64[2];
  long long v6 = *(_OWORD *)matrix->columns[2].f64;
  long long v7 = *(_OWORD *)&matrix->columns[2].f64[2];
  long long v8 = *(_OWORD *)matrix->columns[3].f64;
  long long v9 = *(_OWORD *)&matrix->columns[3].f64[2];
  float64x2_t v23 = *(float64x2_t *)matrix->columns[0].f64;
  long long v24 = v3;
  long long v25 = v4;
  long long v26 = v5;
  long long v27 = v6;
  long long v28 = v7;
  long long v29 = v8;
  long long v30 = v9;
  SPPose3DMakeWith4x4Matrix((uint64_t)&v23, &v19);
  double v10 = *((double *)&v20 + 1);
  double v11 = *(double *)&v20;
  double v12 = v19.f64[1];
  double v13 = v19.f64[0];
  double v14 = *((double *)&v22 + 1);
  double v15 = *(double *)&v22;
  double v16 = *((double *)&v21 + 1);
  double v17 = *(double *)&v21;
  long long v24 = v20;
  float64x2_t v23 = v19;
  long long v26 = v22;
  long long v25 = v21;
  int v18 = SPPose3DIsValid(&v23);
  if (!v18)
  {
    double v13 = 0.0;
    double v12 = 0.0;
    double v11 = 0.0;
    double v10 = 0.0;
    double v17 = 0.0;
    double v16 = 0.0;
    double v15 = 0.0;
    double v14 = 0.0;
  }
  retstr->value.position.x = v13;
  retstr->value.position.y = v12;
  retstr->value.position.double z = v11;
  retstr->value.position.vector.f64[3] = v10;
  retstr->value.rotation.vector.f64[0] = v17;
  retstr->value.rotation.vector.f64[1] = v16;
  retstr->value.rotation.vector.f64[2] = v15;
  retstr->value.rotation.vector.f64[3] = v14;
  retstr->is_nil = v18 ^ 1;
}

void SPPose3DMakeWith4x4Matrix(uint64_t a1@<X0>, _OWORD *a2@<X8>)
{
  float64x2_t v4 = *(float64x2_t *)a1;
  float64x2_t v3 = *(float64x2_t *)(a1 + 16);
  float64x2_t v6 = *(float64x2_t *)(a1 + 32);
  float64x2_t v5 = *(float64x2_t *)(a1 + 48);
  float64x2_t v7 = *(float64x2_t *)(a1 + 64);
  float64x2_t v8 = *(float64x2_t *)(a1 + 80);
  long long v46 = *(_OWORD *)(a1 + 96);
  long long v47 = *(_OWORD *)(a1 + 112);
  v9.f64[0] = *(float64_t *)(a1 + 80);
  v9.f64[1] = *(float64_t *)(a1 + 64);
  v10.f64[0] = *(float64_t *)(a1 + 48);
  v10.f64[1] = *(float64_t *)(a1 + 32);
  float64x2_t v11 = vmulq_f64(*(float64x2_t *)a1, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v8, 8uLL), vnegq_f64(v10)), v9, (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v5, 8uLL)));
  float64x2_t v13 = vmulq_f64(v3, vmlaq_laneq_f64(vmulq_f64(v7, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v6, 1))), v6, v7, 1));
  BOOL v12 = v13.f64[0] + vaddvq_f64(v11) < 0.0;
  v13.f64[0] = -1.0;
  if (!v12) {
    v13.f64[0] = 1.0;
  }
  v14.f64[0] = sqrt(vmulq_f64(v3, v3).f64[0] + vaddvq_f64(vmulq_f64(v4, v4)));
  float64x2_t v15 = vmulq_f64(v8, v8);
  v15.f64[0] = sqrt(v15.f64[0] + vaddvq_f64(vmulq_f64(v7, v7)));
  v14.f64[1] = sqrt(vmulq_f64(v5, v5).f64[0] + vaddvq_f64(vmulq_f64(v6, v6)));
  float64x2_t v16 = vmulq_n_f64(v14, v13.f64[0]);
  float64x2_t v17 = vmulq_f64(v15, v13);
  float64x2_t v18 = vdivq_f64(v4, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v16.f64[0], 0));
  float64x2_t v19 = vdivq_f64(v3, v16);
  float64x2_t v20 = (float64x2_t)vdupq_laneq_s64((int64x2_t)v16, 1);
  float64x2_t v21 = vdivq_f64(v6, v20);
  float64x2_t v22 = vdivq_f64(v5, v20);
  float64x2_t v23 = vdivq_f64(v7, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v17.f64[0], 0));
  float64x2_t v24 = vdivq_f64(v8, v17);
  float64x2_t v25 = vmulq_f64(v22, v19);
  float64x2_t v26 = vmulq_f64(v19, v19);
  v26.f64[0] = v26.f64[0] + vaddvq_f64(vmulq_f64(v18, v18));
  v25.f64[0] = (v25.f64[0] + vaddvq_f64(vmulq_f64(v21, v18))) / v26.f64[0];
  float64x2_t v27 = vsubq_f64(v21, vmulq_n_f64(v18, v25.f64[0]));
  float64x2_t v28 = vsubq_f64(v22, vmulq_f64(v19, v25));
  float64x2_t v29 = vmulq_f64(v24, v19);
  v29.f64[0] = (v29.f64[0] + vaddvq_f64(vmulq_f64(v23, v18))) / v26.f64[0];
  float64x2_t v30 = vmulq_n_f64(v18, v29.f64[0]);
  float64x2_t v31 = vsubq_f64(v24, vmulq_f64(v19, v29));
  float64x2_t v32 = vsubq_f64(v23, v30);
  float64x2_t v33 = vmulq_f64(v23, v27);
  v33.f64[0] = vmulq_f64(v24, v28).f64[0] + vaddvq_f64(v33);
  float64x2_t v34 = vmulq_f64(v28, v28);
  v34.f64[0] = v34.f64[0] + vaddvq_f64(vmulq_f64(v27, v27));
  v33.f64[0] = v33.f64[0] / v34.f64[0];
  float64x2_t v35 = vmulq_f64(v28, v33);
  float64x2_t v36 = vsubq_f64(v32, vmulq_n_f64(v27, v33.f64[0]));
  float64x2_t v37 = vsubq_f64(v31, v35);
  v26.f64[0] = 1.0 / sqrt(v26.f64[0]);
  v34.f64[0] = 1.0 / sqrt(v34.f64[0]);
  v11.f64[0] = 1.0 / sqrt(vmulq_f64(v37, v37).f64[0] + vaddvq_f64(vmulq_f64(v36, v36)));
  float64x2_t v49 = 0u;
  float64x2_t v50 = 0u;
  v48[0] = vmulq_n_f64(v18, v26.f64[0]);
  v48[1] = vmulq_f64(v19, v26);
  v48[2] = vmulq_n_f64(v27, v34.f64[0]);
  v48[3] = vmulq_f64(v28, v34);
  v48[4] = vmulq_n_f64(v36, v11.f64[0]);
  v48[5] = vmulq_f64(v37, v11);
  simd_quaternion((uint64_t)v48, (uint64_t)&v49);
  int64x2_t v38 = (int64x2_t)vornq_s8(vmvnq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v49), (int8x16_t)vcgezq_f64(v49))), vorrq_s8((int8x16_t)vcltzq_f64(v50), (int8x16_t)vcgezq_f64(v50)));
  unint64_t v39 = vorrq_s8((int8x16_t)v38, (int8x16_t)vdupq_laneq_s64(v38, 1)).u64[0];
  float64x2_t v40 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if ((v39 & 0x8000000000000000) != 0)
  {
    float64x2_t v43 = v40;
    float64x2_t v42 = v40;
  }
  else
  {
    double v41 = vaddvq_f64(vaddq_f64(vmulq_f64(v49, v49), vmulq_f64(v50, v50)));
    if (v41 == 0.0)
    {
      float64x2_t v42 = (float64x2_t)xmmword_228C1F7A0;
      float64x2_t v43 = 0uLL;
    }
    else
    {
      double v44 = 1.0 / sqrt(v41);
      float64x2_t v42 = vmulq_n_f64(v50, v44);
      float64x2_t v43 = vmulq_n_f64(v49, v44);
    }
  }
  int64x2_t v45 = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v43), (int8x16_t)vcgezq_f64(v43)), (int8x16_t)vceqq_f64(vabsq_f64(v43), v40)), vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v42), (int8x16_t)vcgezq_f64(v42)), (int8x16_t)vceqq_f64(vabsq_f64(v42), v40)));
  if ((vandq_s8((int8x16_t)v45, (int8x16_t)vdupq_laneq_s64(v45, 1)).u64[0] & 0x8000000000000000) != 0)
  {
    *a2 = v46;
    a2[1] = (unint64_t)v47;
    a2[2] = v43;
  }
  else
  {
    *a2 = SPPose3DInvalid;
    a2[1] = unk_228C1FB10;
    float64x2_t v42 = (float64x2_t)unk_228C1FB30;
    a2[2] = xmmword_228C1FB20;
  }
  a2[3] = v42;
}

{
  float64x2_t v3;
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  BOOL v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  float64x2_t v17;
  float64x2_t v18;
  float64x2_t v19;
  float64x2_t v20;
  float64x2_t v21;
  float64x2_t v22;
  float64x2_t v23;
  float64x2_t v24;
  float64x2_t v25;
  float64x2_t v26;
  float64x2_t v27;
  float64x2_t v28;
  float64x2_t v29;
  float64x2_t v30;
  float64x2_t v31;
  float64x2_t v32;
  float64x2_t v33;
  float64x2_t v34;
  float64x2_t v35;
  float64x2_t v36;
  float64x2_t v37;
  int64x2_t v38;
  unint64_t v39;
  float64x2_t v40;
  double v41;
  float64x2_t v42;
  float64x2_t v43;
  double v44;
  int64x2_t v45;
  long long v46;
  long long v47;
  _OWORD v48[6];
  float64x2_t v49;
  float64x2_t v50;

  float64x2_t v4 = *(float64x2_t *)a1;
  float64x2_t v3 = *(float64x2_t *)(a1 + 16);
  float64x2_t v6 = *(float64x2_t *)(a1 + 32);
  float64x2_t v5 = *(float64x2_t *)(a1 + 48);
  float64x2_t v7 = *(float64x2_t *)(a1 + 64);
  float64x2_t v8 = *(float64x2_t *)(a1 + 80);
  long long v46 = *(_OWORD *)(a1 + 96);
  long long v47 = *(_OWORD *)(a1 + 112);
  v9.f64[0] = *(float64_t *)(a1 + 80);
  v9.f64[1] = *(float64_t *)(a1 + 64);
  v10.f64[0] = *(float64_t *)(a1 + 48);
  v10.f64[1] = *(float64_t *)(a1 + 32);
  float64x2_t v11 = vmulq_f64(*(float64x2_t *)a1, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v8, 8uLL), vnegq_f64(v10)), v9, (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v5, 8uLL)));
  float64x2_t v13 = vmulq_f64(v3, vmlaq_laneq_f64(vmulq_f64(v7, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v6, 1))), v6, v7, 1));
  BOOL v12 = v13.f64[0] + vaddvq_f64(v11) < 0.0;
  v13.f64[0] = -1.0;
  if (!v12) {
    v13.f64[0] = 1.0;
  }
  v14.f64[0] = sqrt(vmulq_f64(v3, v3).f64[0] + vaddvq_f64(vmulq_f64(v4, v4)));
  float64x2_t v15 = vmulq_f64(v8, v8);
  v15.f64[0] = sqrt(v15.f64[0] + vaddvq_f64(vmulq_f64(v7, v7)));
  v14.f64[1] = sqrt(vmulq_f64(v5, v5).f64[0] + vaddvq_f64(vmulq_f64(v6, v6)));
  float64x2_t v16 = vmulq_n_f64(v14, v13.f64[0]);
  float64x2_t v17 = vmulq_f64(v15, v13);
  float64x2_t v18 = vdivq_f64(v4, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v16.f64[0], 0));
  float64x2_t v19 = vdivq_f64(v3, v16);
  float64x2_t v20 = (float64x2_t)vdupq_laneq_s64((int64x2_t)v16, 1);
  float64x2_t v21 = vdivq_f64(v6, v20);
  float64x2_t v22 = vdivq_f64(v5, v20);
  float64x2_t v23 = vdivq_f64(v7, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v17.f64[0], 0));
  float64x2_t v24 = vdivq_f64(v8, v17);
  float64x2_t v25 = vmulq_f64(v22, v19);
  float64x2_t v26 = vmulq_f64(v19, v19);
  v26.f64[0] = v26.f64[0] + vaddvq_f64(vmulq_f64(v18, v18));
  v25.f64[0] = (v25.f64[0] + vaddvq_f64(vmulq_f64(v21, v18))) / v26.f64[0];
  float64x2_t v27 = vsubq_f64(v21, vmulq_n_f64(v18, v25.f64[0]));
  float64x2_t v28 = vsubq_f64(v22, vmulq_f64(v19, v25));
  float64x2_t v29 = vmulq_f64(v24, v19);
  v29.f64[0] = (v29.f64[0] + vaddvq_f64(vmulq_f64(v23, v18))) / v26.f64[0];
  float64x2_t v30 = vmulq_n_f64(v18, v29.f64[0]);
  float64x2_t v31 = vsubq_f64(v24, vmulq_f64(v19, v29));
  float64x2_t v32 = vsubq_f64(v23, v30);
  float64x2_t v33 = vmulq_f64(v23, v27);
  v33.f64[0] = vmulq_f64(v24, v28).f64[0] + vaddvq_f64(v33);
  float64x2_t v34 = vmulq_f64(v28, v28);
  v34.f64[0] = v34.f64[0] + vaddvq_f64(vmulq_f64(v27, v27));
  v33.f64[0] = v33.f64[0] / v34.f64[0];
  float64x2_t v35 = vmulq_f64(v28, v33);
  float64x2_t v36 = vsubq_f64(v32, vmulq_n_f64(v27, v33.f64[0]));
  float64x2_t v37 = vsubq_f64(v31, v35);
  v26.f64[0] = 1.0 / sqrt(v26.f64[0]);
  v34.f64[0] = 1.0 / sqrt(v34.f64[0]);
  v11.f64[0] = 1.0 / sqrt(vmulq_f64(v37, v37).f64[0] + vaddvq_f64(vmulq_f64(v36, v36)));
  float64x2_t v49 = 0u;
  float64x2_t v50 = 0u;
  v48[0] = vmulq_n_f64(v18, v26.f64[0]);
  v48[1] = vmulq_f64(v19, v26);
  v48[2] = vmulq_n_f64(v27, v34.f64[0]);
  v48[3] = vmulq_f64(v28, v34);
  v48[4] = vmulq_n_f64(v36, v11.f64[0]);
  v48[5] = vmulq_f64(v37, v11);
  simd_quaternion((uint64_t)v48, (uint64_t)&v49);
  int64x2_t v38 = (int64x2_t)vornq_s8(vmvnq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v49), (int8x16_t)vcgezq_f64(v49))), vorrq_s8((int8x16_t)vcltzq_f64(v50), (int8x16_t)vcgezq_f64(v50)));
  unint64_t v39 = vorrq_s8((int8x16_t)v38, (int8x16_t)vdupq_laneq_s64(v38, 1)).u64[0];
  float64x2_t v40 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if ((v39 & 0x8000000000000000) != 0)
  {
    float64x2_t v43 = v40;
    float64x2_t v42 = v40;
  }
  else
  {
    double v41 = vaddvq_f64(vaddq_f64(vmulq_f64(v49, v49), vmulq_f64(v50, v50)));
    if (v41 == 0.0)
    {
      float64x2_t v42 = (float64x2_t)xmmword_228C1F7A0;
      float64x2_t v43 = 0uLL;
    }
    else
    {
      double v44 = 1.0 / sqrt(v41);
      float64x2_t v42 = vmulq_n_f64(v50, v44);
      float64x2_t v43 = vmulq_n_f64(v49, v44);
    }
  }
  int64x2_t v45 = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v43), (int8x16_t)vcgezq_f64(v43)), (int8x16_t)vceqq_f64(vabsq_f64(v43), v40)), vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v42), (int8x16_t)vcgezq_f64(v42)), (int8x16_t)vceqq_f64(vabsq_f64(v42), v40)));
  if ((vandq_s8((int8x16_t)v45, (int8x16_t)vdupq_laneq_s64(v45, 1)).u64[0] & 0x8000000000000000) != 0)
  {
    *a2 = v46;
    a2[1] = (unint64_t)v47;
    a2[2] = v43;
  }
  else
  {
    *a2 = SPPose3DInvalid_0;
    a2[1] = unk_228C205B0;
    float64x2_t v42 = (float64x2_t)unk_228C205D0;
    a2[2] = xmmword_228C205C0;
  }
  a2[3] = v42;
}

unint64_t SPPose3DIsValid(float64x2_t *a1)
{
  float64x2_t v1 = a1[3];
  int8x16_t v2 = vorrq_s8((int8x16_t)vcltzq_f64(v1), (int8x16_t)vcgezq_f64(v1));
  float64x2_t v3 = vabsq_f64(v1);
  float64x2_t v4 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  int64x2_t v5 = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(a1[2]), (int8x16_t)vcgezq_f64(a1[2])), (int8x16_t)vceqq_f64(vabsq_f64(a1[2]), v4)), vbicq_s8(v2, (int8x16_t)vceqq_f64(v3, v4)));
  float64x2_t v6 = (float64x2_t)vdupq_laneq_s64(v5, 1);
  if ((vandq_s8((int8x16_t)v5, (int8x16_t)v6).u64[0] & 0x8000000000000000) == 0) {
    return 0;
  }
  v6.f64[0] = a1[1].f64[0];
  float64x2_t v8 = (float64x2_t)vcltzq_f64(*a1);
  int8x16_t v9 = vorrq_s8((int8x16_t)v8, (int8x16_t)vcgezq_f64(*a1));
  v8.f64[0] = INFINITY;
  int64x2_t v10 = (int64x2_t)vbicq_s8(v9, (int8x16_t)vceqq_f64(vabsq_f64(*a1), v4));
  return vandq_s8((int8x16_t)vdupq_laneq_s64(v10, 1), vandq_s8((int8x16_t)vshlq_n_s64((int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v6), (int8x16_t)vcgezq_f64(v6)), (int8x16_t)vceqq_f64(vabsq_f64(v6), v8)), 0x3FuLL), (int8x16_t)v10)).u64[0] >> 63;
}

void __swiftcall SPPose3D.init(matrix:)(SPPose3D_optional *__return_ptr retstr, simd_float4x4 *matrix)
{
  float64x2_t v20 = vcvtq_f64_f32(*(float32x2_t *)v2.f32);
  float64x2_t v21 = vcvt_hight_f64_f32(v2);
  float64x2_t v22 = vcvtq_f64_f32(*(float32x2_t *)v3.f32);
  float64x2_t v23 = vcvt_hight_f64_f32(v3);
  float64x2_t v24 = vcvtq_f64_f32(*(float32x2_t *)v4.f32);
  float64x2_t v25 = vcvt_hight_f64_f32(v4);
  float64x2_t v26 = vcvtq_f64_f32(*(float32x2_t *)v5.f32);
  float64x2_t v27 = vcvt_hight_f64_f32(v5);
  SPPose3DMakeWith4x4Matrix((uint64_t)&v20, &v16);
  double v7 = v17.f64[1];
  double v8 = v17.f64[0];
  double v9 = v16.f64[1];
  double v10 = v16.f64[0];
  double v11 = v19.f64[1];
  double v12 = v19.f64[0];
  double v13 = v18.f64[1];
  double v14 = v18.f64[0];
  float64x2_t v21 = v17;
  float64x2_t v20 = v16;
  float64x2_t v23 = v19;
  float64x2_t v22 = v18;
  int v15 = SPPose3DIsValid(&v20);
  if (!v15)
  {
    double v10 = 0.0;
    double v9 = 0.0;
    double v8 = 0.0;
    double v7 = 0.0;
    double v14 = 0.0;
    double v13 = 0.0;
    double v12 = 0.0;
    double v11 = 0.0;
  }
  retstr->value.position.x = v10;
  retstr->value.position.y = v9;
  retstr->value.position.double z = v8;
  retstr->value.position.vector.f64[3] = v7;
  retstr->value.rotation.vector.f64[0] = v14;
  retstr->value.rotation.vector.f64[1] = v13;
  retstr->value.rotation.vector.f64[2] = v12;
  retstr->value.rotation.vector.f64[3] = v11;
  retstr->is_nil = v15 ^ 1;
}

void __swiftcall SPAffineTransform3D.init(projectiveTransform:)(SPAffineTransform3D_optional *__return_ptr retstr, SPProjectiveTransform3D *projectiveTransform)
{
  long long v3 = *(_OWORD *)&projectiveTransform->matrix.columns[0].f64[2];
  float64x2_t v4 = *(float64x2_t *)projectiveTransform->matrix.columns[1].f64;
  long long v5 = *(_OWORD *)&projectiveTransform->matrix.columns[1].f64[2];
  long long v6 = *(_OWORD *)projectiveTransform->matrix.columns[2].f64;
  long long v7 = *(_OWORD *)&projectiveTransform->matrix.columns[2].f64[2];
  long long v8 = *(_OWORD *)projectiveTransform->matrix.columns[3].f64;
  long long v9 = *(_OWORD *)&projectiveTransform->matrix.columns[3].f64[2];
  float64x2_t v26 = *(float64x2_t *)projectiveTransform->matrix.columns[0].f64;
  long long v27 = v3;
  float64x2_t v28 = v4;
  long long v29 = v5;
  long long v30 = v6;
  long long v31 = v7;
  long long v32 = v8;
  long long v33 = v9;
  SPAffineTransform3DMakeWithProjective((uint64_t)&v26, &v18);
  long long v16 = v21;
  long long v17 = v19;
  float64x2_t v26 = v18;
  long long v27 = v19;
  float64x2_t v12 = v20;
  float64x2_t v13 = v18;
  float64x2_t v28 = v20;
  long long v29 = v21;
  long long v14 = *(_OWORD *)v25;
  long long v15 = v23;
  long long v30 = v22;
  long long v31 = v23;
  long long v10 = v24;
  long long v11 = v22;
  long long v32 = v24;
  long long v33 = *(_OWORD *)v25;
  if (SPAffineTransform3DIsValid(&v26, *(double *)&v19, *(double *)&v21, *(double *)&v23, v25[0], v18.f64[0], v20))
  {
    float64x2_t v18 = v13;
    long long v19 = v17;
    float64x2_t v20 = v12;
    long long v21 = v16;
    long long v22 = v11;
    long long v23 = v15;
    long long v24 = v10;
    *(_OWORD *)float64x2_t v25 = v14;
    _sSo19SPAffineTransform3DaSgWOi_((uint64_t)&v18);
  }
  else
  {
    _sSo19SPAffineTransform3DaSgWOi0_((uint64_t)&v18);
  }
  outlined init with take of SPAffineTransform3D?((uint64_t)&v18, (uint64_t)&v26, &demangling cache variable for type metadata for SPAffineTransform3D?);
  outlined init with take of SPAffineTransform3D?((uint64_t)&v26, (uint64_t)retstr, &demangling cache variable for type metadata for SPAffineTransform3D?);
}

uint64_t SPAffineTransform3DMakeWithProjective@<X0>(uint64_t result@<X0>, _OWORD *a2@<X8>)
{
  long long v2 = *(_OWORD *)(result + 16);
  long long v3 = *(_OWORD *)(result + 48);
  long long v4 = *(_OWORD *)(result + 80);
  long long v5 = *(_OWORD *)(result + 112);
  double v6 = vabdd_f64(0.0, *(double *)(result + 56));
  BOOL v7 = vabdd_f64(0.0, *(double *)(result + 24)) < 0.0000000149011612 && v6 < 0.0000000149011612;
  double v8 = vabdd_f64(0.0, *((double *)&v4 + 1));
  BOOL v9 = v7 && v8 < 0.0000000149011612;
  double v10 = vabdd_f64(1.0, *((double *)&v5 + 1));
  if (v9 && v10 < 0.0000000149011612)
  {
    long long v12 = *(_OWORD *)(result + 32);
    long long v13 = *(_OWORD *)(result + 64);
    long long v14 = *(_OWORD *)(result + 96);
    *a2 = *(_OWORD *)result;
    a2[1] = v2;
    a2[2] = v12;
    a2[3] = v3;
    a2[4] = v13;
    a2[5] = v4;
    a2[6] = v14;
    a2[7] = v5;
  }
  else
  {
    a2[4] = xmmword_228C1FB80;
    a2[5] = unk_228C1FB90;
    a2[6] = xmmword_228C1FBA0;
    a2[7] = unk_228C1FBB0;
    *a2 = SPAffineTransform3DInvalid;
    a2[1] = unk_228C1FB50;
    a2[2] = xmmword_228C1FB60;
    a2[3] = unk_228C1FB70;
  }
  return result;
}

unint64_t SPAffineTransform3DIsValid(float64x2_t *a1, double a2, double a3, double a4, double a5, double a6, float64x2_t a7)
{
  float64x2_t v7 = a1[1];
  int8x16_t v8 = vorrq_s8((int8x16_t)vcltzq_f64(v7), (int8x16_t)vcgezq_f64(v7));
  float64x2_t v9 = vabsq_f64(v7);
  float64x2_t v10 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  a7.f64[0] = INFINITY;
  int64x2_t v11 = (int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(*a1), (int8x16_t)vcgezq_f64(*a1)), (int8x16_t)vceqq_f64(vabsq_f64(*a1), v10));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v11, 1), vandq_s8(vbicq_s8(v8, (int8x16_t)vceqq_f64(v9, a7)), (int8x16_t)v11)).u64[0] & 0x8000000000000000) == 0)return 0; {
  float64x2_t v13 = a1[2];
  }
  float64x2_t v12 = a1[3];
  int8x16_t v14 = vorrq_s8((int8x16_t)vcltzq_f64(v13), (int8x16_t)vcgezq_f64(v13));
  float64x2_t v15 = (float64x2_t)vcltzq_f64(v12);
  float64x2_t v16 = vabsq_f64(v13);
  int8x16_t v17 = (int8x16_t)vceqq_f64(v16, v10);
  v16.f64[0] = INFINITY;
  int64x2_t v18 = (int64x2_t)vbicq_s8(v14, v17);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v18, 1), vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)v15, (int8x16_t)vcgezq_f64(v12)), (int8x16_t)vceqq_f64(vabsq_f64(v12), v16)), (int8x16_t)v18)).u64[0] & 0x8000000000000000) == 0)return 0; {
  float64x2_t v19 = a1[5];
  }
  int8x16_t v20 = vorrq_s8((int8x16_t)vcltzq_f64(v19), (int8x16_t)vcgezq_f64(v19));
  float64x2_t v21 = vabsq_f64(v19);
  float64x2_t v22 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v15.f64[0] = INFINITY;
  int64x2_t v23 = (int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(a1[4]), (int8x16_t)vcgezq_f64(a1[4])), (int8x16_t)vceqq_f64(vabsq_f64(a1[4]), v22));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v23, 1), vandq_s8(vbicq_s8(v20, (int8x16_t)vceqq_f64(v21, v15)), (int8x16_t)v23)).u64[0] & 0x8000000000000000) == 0)return 0; {
  float64x2_t v25 = a1[6];
  }
  int8x16_t v26 = vorrq_s8((int8x16_t)vcltzq_f64(v25), (int8x16_t)vcgezq_f64(v25));
  float64x2_t v27 = vabsq_f64(v25);
  int8x16_t v28 = (int8x16_t)vceqq_f64(v27, v22);
  v27.f64[0] = INFINITY;
  int64x2_t v29 = (int64x2_t)vbicq_s8(v26, v28);
  return vandq_s8((int8x16_t)vdupq_laneq_s64(v29, 1), vandq_s8((int8x16_t)vshlq_n_s64((int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(a1[7]), (int8x16_t)vcgezq_f64(a1[7])), (int8x16_t)vceqq_f64(vabsq_f64(a1[7]), v27)), 0x3FuLL), (int8x16_t)v29)).u64[0] >> 63;
}

double _sSo19SPAffineTransform3DaSgWOi0_(uint64_t a1)
{
  double result = 0.0;
  *(_OWORD *)(a1 + 96) = 0u;
  *(_OWORD *)(a1 + 112) = 0u;
  *(_OWORD *)(a1 + 64) = 0u;
  *(_OWORD *)(a1 + 80) = 0u;
  *(_OWORD *)(a1 + 32) = 0u;
  *(_OWORD *)(a1 + 48) = 0u;
  *(_OWORD *)a1 = 0u;
  *(_OWORD *)(a1 + 16) = 0u;
  *(unsigned char *)(a1 + 128) = 1;
  return result;
}

uint64_t __swift_instantiateConcreteTypeFromMangledName(uint64_t *a1)
{
  uint64_t result = *a1;
  if (result < 0)
  {
    uint64_t result = swift_getTypeByMangledNameInContext2();
    *a1 = result;
  }
  return result;
}

uint64_t _sSo19SPAffineTransform3DaSgWOi_(uint64_t result)
{
  *(unsigned char *)(result + 128) = 0;
  return result;
}

void __swiftcall SPAffineTransform3D.init(_:)(SPAffineTransform3D_optional *__return_ptr retstr, simd_double4x4 *a2)
{
  long long v3 = *(_OWORD *)&a2->columns[0].f64[2];
  float64x2_t v4 = *(float64x2_t *)a2->columns[1].f64;
  long long v5 = *(_OWORD *)&a2->columns[1].f64[2];
  long long v6 = *(_OWORD *)a2->columns[2].f64;
  long long v7 = *(_OWORD *)&a2->columns[2].f64[2];
  long long v8 = *(_OWORD *)a2->columns[3].f64;
  long long v9 = *(_OWORD *)&a2->columns[3].f64[2];
  float64x2_t v26 = *(float64x2_t *)a2->columns[0].f64;
  long long v27 = v3;
  float64x2_t v28 = v4;
  long long v29 = v5;
  long long v30 = v6;
  long long v31 = v7;
  long long v32 = v8;
  long long v33 = v9;
  SPAffineTransform3DMakeWith4x4Matrix((uint64_t)&v26, &v18);
  long long v16 = v21;
  long long v17 = v19;
  float64x2_t v26 = v18;
  long long v27 = v19;
  float64x2_t v12 = v20;
  float64x2_t v13 = v18;
  float64x2_t v28 = v20;
  long long v29 = v21;
  long long v14 = *(_OWORD *)v25;
  long long v15 = v23;
  long long v30 = v22;
  long long v31 = v23;
  long long v10 = v24;
  long long v11 = v22;
  long long v32 = v24;
  long long v33 = *(_OWORD *)v25;
  if (SPAffineTransform3DIsValid(&v26, *(double *)&v19, *(double *)&v21, *(double *)&v23, v25[0], v18.f64[0], v20))
  {
    float64x2_t v18 = v13;
    long long v19 = v17;
    float64x2_t v20 = v12;
    long long v21 = v16;
    long long v22 = v11;
    long long v23 = v15;
    long long v24 = v10;
    *(_OWORD *)float64x2_t v25 = v14;
    _sSo19SPAffineTransform3DaSgWOi_((uint64_t)&v18);
  }
  else
  {
    _sSo19SPAffineTransform3DaSgWOi0_((uint64_t)&v18);
  }
  outlined init with take of SPAffineTransform3D?((uint64_t)&v18, (uint64_t)&v26, &demangling cache variable for type metadata for SPAffineTransform3D?);
  outlined init with take of SPAffineTransform3D?((uint64_t)&v26, (uint64_t)retstr, &demangling cache variable for type metadata for SPAffineTransform3D?);
}

double SPAffineTransform3DMakeWith4x4Matrix@<D0>(uint64_t a1@<X0>, _OWORD *a2@<X8>)
{
  long long v2 = *(_OWORD *)(a1 + 16);
  if (vabdd_f64(0.0, *(double *)(a1 + 24)) >= 0.0000000149011612
    || (long long v3 = *(_OWORD *)(a1 + 48), vabdd_f64(0.0, *(double *)(a1 + 56)) >= 0.0000000149011612)
    || (long long v4 = *(_OWORD *)(a1 + 80), vabdd_f64(0.0, *(double *)(a1 + 88)) >= 0.0000000149011612)
    || (long long v5 = *(_OWORD *)(a1 + 112), vabdd_f64(1.0, *(double *)(a1 + 120)) >= 0.0000000149011612))
  {
    a2[4] = xmmword_228C1FB80;
    a2[5] = unk_228C1FB90;
    a2[6] = xmmword_228C1FBA0;
    a2[7] = unk_228C1FBB0;
    *a2 = SPAffineTransform3DInvalid;
    a2[1] = unk_228C1FB50;
    *(void *)&long long v2 = 0x7FF0000000000000;
    a2[2] = xmmword_228C1FB60;
    a2[3] = unk_228C1FB70;
  }
  else
  {
    long long v6 = *(_OWORD *)(a1 + 32);
    long long v7 = *(_OWORD *)(a1 + 64);
    long long v8 = *(_OWORD *)(a1 + 96);
    *a2 = *(_OWORD *)a1;
    a2[1] = v2;
    a2[2] = v6;
    a2[3] = v3;
    a2[4] = v7;
    a2[5] = v4;
    a2[6] = v8;
    a2[7] = v5;
  }
  return *(double *)&v2;
}

void __swiftcall SPAffineTransform3D.init(translation:)(SPAffineTransform3D *__return_ptr retstr, SPSize3D *translation)
{
  SPAffineTransform3D.init(translation:)((void (*)(SPSize3D *__return_ptr, unsigned char *, double))SPAffineTransform3DMakeTranslation, retstr, v2, v3, v4);
}

__n128 SPAffineTransform3DMakeTranslation@<Q0>(SPVector3D *a1@<X0>, uint64_t a2@<X8>, __n128 a3@<Q1>)
{
  __n128 result = *(__n128 *)&a1->x;
  a3.n128_u64[0] = *(void *)&a1->z;
  *(_OWORD *)a2 = xmmword_228C1F7D0;
  *(_OWORD *)(a2 + 16) = 0u;
  *(_OWORD *)(a2 + 32) = xmmword_228C1F7A0;
  *(_OWORD *)(a2 + 48) = 0u;
  *(void *)(a2 + 64) = 0;
  *(void *)(a2 + 72) = 0;
  __asm { FMOV            V2.2D, #1.0 }
  *(_OWORD *)(a2 + 80) = _Q2;
  *(__n128 *)(a2 + 96) = result;
  *(__n128 *)(a2 + 112) = a3;
  return result;
}

void __swiftcall SPAffineTransform3D.translated(by:)(SPAffineTransform3D *__return_ptr retstr, SPSize3D *by)
{
  SPAffineTransform3D.translated(by:)((void (*)(_OWORD *__return_ptr, SPSize3D *, unsigned char *))SPAffineTransform3DTranslate, retstr, v2, v3, v4);
}

float64x2_t *SPAffineTransform3DTranslate@<X0>(float64x2_t *result@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>)
{
  float64x2_t v4 = *a2;
  v3.f64[0] = a2[1].f64[0];
  float64x2_t v6 = *result;
  float64x2_t v5 = result[1];
  float64x2_t v8 = result[2];
  float64x2_t v7 = result[3];
  float64x2_t v10 = result[4];
  float64x2_t v9 = result[5];
  float64x2_t v12 = result[6];
  float64x2_t v11 = result[7];
  float64x2_t v13 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v14 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v17 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v18 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, v8), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], *result)), (int8x16_t)vceqq_f64(v16, v10));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v18, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v7), (int8x16_t)vceqq_f64(v13, v5)), (int8x16_t)vceqq_f64(v17, v9)), (int8x16_t)v18)).u64[0] & 0x8000000000000000) != 0)
  {
    __asm { FMOV            V24.2D, #1.0 }
    int64x2_t v24 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], (float64x2_t)xmmword_228C1F7D0)), (int8x16_t)vceqzq_f64(v16));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v24, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqzq_f64(v14), (int8x16_t)vceqzq_f64(v13)), (int8x16_t)vceqq_f64(v17, _Q24)), (int8x16_t)v24)).u64[0] & 0x8000000000000000) != 0)
    {
      float64x2_t v11 = vaddq_f64(v3, v11);
      float64x2_t v12 = vaddq_f64(v4, v12);
      *a3 = v6;
      a3[1] = v5;
      a3[2] = v8;
      a3[3] = v7;
      a3[4] = v10;
      a3[5] = v9;
LABEL_12:
      a3[6] = v12;
      a3[7] = v11;
      return result;
    }
  }
  int64x2_t v25 = vceqzq_f64(v12);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v25, 1), vandq_s8((int8x16_t)vceqzq_f64(v11), (int8x16_t)v25)).u64[0] & 0x8000000000000000) != 0)
  {
    int64x2_t v26 = vceqzq_f64(v4);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v26, 1), vandq_s8((int8x16_t)vceqzq_f64(v3), (int8x16_t)v26)).u64[0] & 0x8000000000000000) != 0)
    {
      uint64_t v38 = 0;
      long long v48 = xmmword_228C1F7D0;
      long long v49 = 0u;
      long long v50 = xmmword_228C1F7A0;
      long long v51 = 0u;
      long long v52 = 0uLL;
      __asm { FMOV            V17.2D, #1.0 }
      long long v53 = _Q17;
      float64x2_t v56 = 0u;
      float64x2_t v57 = 0u;
      float64x2_t v58 = 0u;
      float64x2_t v59 = 0u;
      float64x2_t v60 = 0u;
      float64x2_t v61 = 0u;
      *(void *)&v6.f64[1] = vextq_s8((int8x16_t)v6, (int8x16_t)v6, 8uLL).u64[0];
      *(void *)&v8.f64[1] = vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL).u64[0];
      *(void *)&v10.f64[1] = vextq_s8((int8x16_t)v10, (int8x16_t)v10, 8uLL).u64[0];
      do
      {
        float64x2_t v41 = *(float64x2_t *)((char *)&v48 + v38);
        float64x2_t v40 = *(float64x2_t *)((char *)&v48 + v38 + 16);
        float64x2_t v42 = (float64x2_t *)((char *)&v56 + v38);
        *float64x2_t v42 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v41.f64[0]), v8, v41, 1), v10, v40.f64[0]);
        v42[1] = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v5, v41), v7, v41, 1), v40, v9);
        v38 += 32;
      }
      while (v38 != 96);
      float64x2_t v43 = v57;
      float64x2_t v44 = v58;
      float64x2_t v45 = v59;
      float64x2_t v46 = v60;
      float64x2_t v47 = v61;
      *a3 = v56;
      a3[1] = v43;
      a3[2] = v44;
      a3[3] = v45;
      a3[4] = v46;
      a3[5] = v47;
      goto LABEL_12;
    }
  }
  uint64_t v27 = 0;
  v5.f64[1] = 0.0;
  v7.f64[1] = 0.0;
  v9.f64[1] = 0.0;
  v11.f64[1] = 1.0;
  v3.f64[1] = 1.0;
  long long v48 = xmmword_228C1F7D0;
  long long v49 = 0u;
  long long v50 = xmmword_228C1F7A0;
  long long v51 = 0u;
  long long v52 = 0u;
  long long v53 = xmmword_228C1F7D0;
  float64x2_t v54 = v4;
  float64x2_t v55 = v3;
  float64x2_t v56 = 0u;
  float64x2_t v57 = 0u;
  float64x2_t v58 = 0u;
  float64x2_t v59 = 0u;
  float64x2_t v60 = 0u;
  float64x2_t v61 = 0u;
  float64x2_t v62 = 0u;
  float64x2_t v63 = 0u;
  do
  {
    float64x2_t v29 = *(float64x2_t *)((char *)&v48 + v27);
    float64x2_t v28 = *(float64x2_t *)((char *)&v48 + v27 + 16);
    long long v30 = (float64x2_t *)((char *)&v56 + v27);
    *long long v30 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v29.f64[0]), v8, v29, 1), v10, v28.f64[0]), v12, v28, 1);
    v30[1] = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v5, v29.f64[0]), v7, v29, 1), v9, v28.f64[0]), v11, v28, 1);
    v27 += 32;
  }
  while (v27 != 128);
  float64x2_t v31 = v57;
  float64x2_t v32 = v58;
  float64x2_t v33 = v59;
  float64x2_t v34 = v60;
  float64x2_t v35 = v61;
  float64x2_t v36 = v62;
  float64x2_t v37 = v63;
  *a3 = v56;
  a3[1] = v31;
  a3[2] = v32;
  a3[3] = v33;
  a3[4] = v34;
  a3[5] = v35;
  a3[6] = v36;
  a3[7] = v37;
  return result;
}

{
  float64x2_t v3;
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  float64x2_t v17;
  int64x2_t v18;
  int64x2_t v24;
  int64x2_t v25;
  int64x2_t v26;
  uint64_t v27;
  float64x2_t v28;
  float64x2_t v29;
  float64x2_t *v30;
  float64x2_t v31;
  float64x2_t v32;
  float64x2_t v33;
  float64x2_t v34;
  float64x2_t v35;
  float64x2_t v36;
  float64x2_t v37;
  uint64_t v38;
  float64x2_t v40;
  float64x2_t v41;
  float64x2_t *v42;
  float64x2_t v43;
  float64x2_t v44;
  float64x2_t v45;
  float64x2_t v46;
  float64x2_t v47;
  long long v48;
  long long v49;
  long long v50;
  long long v51;
  long long v52;
  long long v53;
  float64x2_t v54;
  float64x2_t v55;
  float64x2_t v56;
  float64x2_t v57;
  float64x2_t v58;
  float64x2_t v59;
  float64x2_t v60;
  float64x2_t v61;
  float64x2_t v62;
  float64x2_t v63;

  float64x2_t v4 = *a2;
  v3.f64[0] = a2[1].f64[0];
  float64x2_t v6 = *result;
  float64x2_t v5 = result[1];
  float64x2_t v8 = result[2];
  float64x2_t v7 = result[3];
  float64x2_t v10 = result[4];
  float64x2_t v9 = result[5];
  float64x2_t v12 = result[6];
  float64x2_t v11 = result[7];
  float64x2_t v13 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v14 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v17 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v18 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, v8), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], *result)), (int8x16_t)vceqq_f64(v16, v10));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v18, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v7), (int8x16_t)vceqq_f64(v13, v5)), (int8x16_t)vceqq_f64(v17, v9)), (int8x16_t)v18)).u64[0] & 0x8000000000000000) != 0)
  {
    __asm { FMOV            V24.2D, #1.0 }
    int64x2_t v24 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], (float64x2_t)xmmword_228C1F7D0)), (int8x16_t)vceqzq_f64(v16));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v24, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqzq_f64(v14), (int8x16_t)vceqzq_f64(v13)), (int8x16_t)vceqq_f64(v17, _Q24)), (int8x16_t)v24)).u64[0] & 0x8000000000000000) != 0)
    {
      float64x2_t v11 = vaddq_f64(v3, v11);
      float64x2_t v12 = vaddq_f64(v4, v12);
      *a3 = v6;
      a3[1] = v5;
      a3[2] = v8;
      a3[3] = v7;
      a3[4] = v10;
      a3[5] = v9;
LABEL_12:
      a3[6] = v12;
      a3[7] = v11;
      return result;
    }
  }
  int64x2_t v25 = vceqzq_f64(v12);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v25, 1), vandq_s8((int8x16_t)vceqzq_f64(v11), (int8x16_t)v25)).u64[0] & 0x8000000000000000) != 0)
  {
    int64x2_t v26 = vceqzq_f64(v4);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v26, 1), vandq_s8((int8x16_t)vceqzq_f64(v3), (int8x16_t)v26)).u64[0] & 0x8000000000000000) != 0)
    {
      uint64_t v38 = 0;
      long long v48 = xmmword_228C1F7D0;
      long long v49 = 0u;
      long long v50 = xmmword_228C1F7A0;
      long long v51 = 0u;
      long long v52 = 0uLL;
      __asm { FMOV            V17.2D, #1.0 }
      long long v53 = _Q17;
      float64x2_t v56 = 0u;
      float64x2_t v57 = 0u;
      float64x2_t v58 = 0u;
      float64x2_t v59 = 0u;
      float64x2_t v60 = 0u;
      float64x2_t v61 = 0u;
      *(void *)&v6.f64[1] = vextq_s8((int8x16_t)v6, (int8x16_t)v6, 8uLL).u64[0];
      *(void *)&v8.f64[1] = vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL).u64[0];
      *(void *)&v10.f64[1] = vextq_s8((int8x16_t)v10, (int8x16_t)v10, 8uLL).u64[0];
      do
      {
        float64x2_t v41 = *(float64x2_t *)((char *)&v48 + v38);
        float64x2_t v40 = *(float64x2_t *)((char *)&v48 + v38 + 16);
        float64x2_t v42 = (float64x2_t *)((char *)&v56 + v38);
        *float64x2_t v42 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v41.f64[0]), v8, v41, 1), v10, v40.f64[0]);
        v42[1] = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v5, v41), v7, v41, 1), v40, v9);
        v38 += 32;
      }
      while (v38 != 96);
      float64x2_t v43 = v57;
      float64x2_t v44 = v58;
      float64x2_t v45 = v59;
      float64x2_t v46 = v60;
      float64x2_t v47 = v61;
      *a3 = v56;
      a3[1] = v43;
      a3[2] = v44;
      a3[3] = v45;
      a3[4] = v46;
      a3[5] = v47;
      goto LABEL_12;
    }
  }
  uint64_t v27 = 0;
  v5.f64[1] = 0.0;
  v7.f64[1] = 0.0;
  v9.f64[1] = 0.0;
  v11.f64[1] = 1.0;
  v3.f64[1] = 1.0;
  long long v48 = xmmword_228C1F7D0;
  long long v49 = 0u;
  long long v50 = xmmword_228C1F7A0;
  long long v51 = 0u;
  long long v52 = 0u;
  long long v53 = xmmword_228C1F7D0;
  float64x2_t v54 = v4;
  float64x2_t v55 = v3;
  float64x2_t v56 = 0u;
  float64x2_t v57 = 0u;
  float64x2_t v58 = 0u;
  float64x2_t v59 = 0u;
  float64x2_t v60 = 0u;
  float64x2_t v61 = 0u;
  float64x2_t v62 = 0u;
  float64x2_t v63 = 0u;
  do
  {
    float64x2_t v29 = *(float64x2_t *)((char *)&v48 + v27);
    float64x2_t v28 = *(float64x2_t *)((char *)&v48 + v27 + 16);
    long long v30 = (float64x2_t *)((char *)&v56 + v27);
    *long long v30 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v29.f64[0]), v8, v29, 1), v10, v28.f64[0]), v12, v28, 1);
    v30[1] = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v5, v29.f64[0]), v7, v29, 1), v9, v28.f64[0]), v11, v28, 1);
    v27 += 32;
  }
  while (v27 != 128);
  float64x2_t v31 = v57;
  float64x2_t v32 = v58;
  float64x2_t v33 = v59;
  float64x2_t v34 = v60;
  float64x2_t v35 = v61;
  float64x2_t v36 = v62;
  float64x2_t v37 = v63;
  *a3 = v56;
  a3[1] = v31;
  a3[2] = v32;
  a3[3] = v33;
  a3[4] = v34;
  a3[5] = v35;
  a3[6] = v36;
  a3[7] = v37;
  return result;
}

{
  float64x2_t v3;
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  float64x2_t v17;
  int64x2_t v18;
  int64x2_t v24;
  int64x2_t v25;
  int64x2_t v26;
  uint64_t v27;
  float64x2_t v28;
  float64x2_t v29;
  float64x2_t *v30;
  float64x2_t v31;
  float64x2_t v32;
  float64x2_t v33;
  float64x2_t v34;
  float64x2_t v35;
  float64x2_t v36;
  float64x2_t v37;
  uint64_t v38;
  float64x2_t v40;
  float64x2_t v41;
  float64x2_t *v42;
  float64x2_t v43;
  float64x2_t v44;
  float64x2_t v45;
  float64x2_t v46;
  float64x2_t v47;
  long long v48;
  long long v49;
  long long v50;
  long long v51;
  long long v52;
  long long v53;
  float64x2_t v54;
  float64x2_t v55;
  float64x2_t v56;
  float64x2_t v57;
  float64x2_t v58;
  float64x2_t v59;
  float64x2_t v60;
  float64x2_t v61;
  float64x2_t v62;
  float64x2_t v63;

  float64x2_t v4 = *a2;
  v3.f64[0] = a2[1].f64[0];
  float64x2_t v6 = *result;
  float64x2_t v5 = result[1];
  float64x2_t v8 = result[2];
  float64x2_t v7 = result[3];
  float64x2_t v10 = result[4];
  float64x2_t v9 = result[5];
  float64x2_t v12 = result[6];
  float64x2_t v11 = result[7];
  float64x2_t v13 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v14 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v17 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v18 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, v8), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], *result)), (int8x16_t)vceqq_f64(v16, v10));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v18, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v7), (int8x16_t)vceqq_f64(v13, v5)), (int8x16_t)vceqq_f64(v17, v9)), (int8x16_t)v18)).u64[0] & 0x8000000000000000) != 0)
  {
    __asm { FMOV            V24.2D, #1.0 }
    int64x2_t v24 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], (float64x2_t)xmmword_228C1F7D0)), (int8x16_t)vceqzq_f64(v16));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v24, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqzq_f64(v14), (int8x16_t)vceqzq_f64(v13)), (int8x16_t)vceqq_f64(v17, _Q24)), (int8x16_t)v24)).u64[0] & 0x8000000000000000) != 0)
    {
      float64x2_t v11 = vaddq_f64(v3, v11);
      float64x2_t v12 = vaddq_f64(v4, v12);
      *a3 = v6;
      a3[1] = v5;
      a3[2] = v8;
      a3[3] = v7;
      a3[4] = v10;
      a3[5] = v9;
LABEL_12:
      a3[6] = v12;
      a3[7] = v11;
      return result;
    }
  }
  int64x2_t v25 = vceqzq_f64(v12);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v25, 1), vandq_s8((int8x16_t)vceqzq_f64(v11), (int8x16_t)v25)).u64[0] & 0x8000000000000000) != 0)
  {
    int64x2_t v26 = vceqzq_f64(v4);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v26, 1), vandq_s8((int8x16_t)vceqzq_f64(v3), (int8x16_t)v26)).u64[0] & 0x8000000000000000) != 0)
    {
      uint64_t v38 = 0;
      long long v48 = xmmword_228C1F7D0;
      long long v49 = 0u;
      long long v50 = xmmword_228C1F7A0;
      long long v51 = 0u;
      long long v52 = 0uLL;
      __asm { FMOV            V17.2D, #1.0 }
      long long v53 = _Q17;
      float64x2_t v56 = 0u;
      float64x2_t v57 = 0u;
      float64x2_t v58 = 0u;
      float64x2_t v59 = 0u;
      float64x2_t v60 = 0u;
      float64x2_t v61 = 0u;
      *(void *)&v6.f64[1] = vextq_s8((int8x16_t)v6, (int8x16_t)v6, 8uLL).u64[0];
      *(void *)&v8.f64[1] = vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL).u64[0];
      *(void *)&v10.f64[1] = vextq_s8((int8x16_t)v10, (int8x16_t)v10, 8uLL).u64[0];
      do
      {
        float64x2_t v41 = *(float64x2_t *)((char *)&v48 + v38);
        float64x2_t v40 = *(float64x2_t *)((char *)&v48 + v38 + 16);
        float64x2_t v42 = (float64x2_t *)((char *)&v56 + v38);
        *float64x2_t v42 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v41.f64[0]), v8, v41, 1), v10, v40.f64[0]);
        v42[1] = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v5, v41), v7, v41, 1), v40, v9);
        v38 += 32;
      }
      while (v38 != 96);
      float64x2_t v43 = v57;
      float64x2_t v44 = v58;
      float64x2_t v45 = v59;
      float64x2_t v46 = v60;
      float64x2_t v47 = v61;
      *a3 = v56;
      a3[1] = v43;
      a3[2] = v44;
      a3[3] = v45;
      a3[4] = v46;
      a3[5] = v47;
      goto LABEL_12;
    }
  }
  uint64_t v27 = 0;
  v5.f64[1] = 0.0;
  v7.f64[1] = 0.0;
  v9.f64[1] = 0.0;
  v11.f64[1] = 1.0;
  v3.f64[1] = 1.0;
  long long v48 = xmmword_228C1F7D0;
  long long v49 = 0u;
  long long v50 = xmmword_228C1F7A0;
  long long v51 = 0u;
  long long v52 = 0u;
  long long v53 = xmmword_228C1F7D0;
  float64x2_t v54 = v4;
  float64x2_t v55 = v3;
  float64x2_t v56 = 0u;
  float64x2_t v57 = 0u;
  float64x2_t v58 = 0u;
  float64x2_t v59 = 0u;
  float64x2_t v60 = 0u;
  float64x2_t v61 = 0u;
  float64x2_t v62 = 0u;
  float64x2_t v63 = 0u;
  do
  {
    float64x2_t v29 = *(float64x2_t *)((char *)&v48 + v27);
    float64x2_t v28 = *(float64x2_t *)((char *)&v48 + v27 + 16);
    long long v30 = (float64x2_t *)((char *)&v56 + v27);
    *long long v30 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v29.f64[0]), v8, v29, 1), v10, v28.f64[0]), v12, v28, 1);
    v30[1] = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v5, v29.f64[0]), v7, v29, 1), v9, v28.f64[0]), v11, v28, 1);
    v27 += 32;
  }
  while (v27 != 128);
  float64x2_t v31 = v57;
  float64x2_t v32 = v58;
  float64x2_t v33 = v59;
  float64x2_t v34 = v60;
  float64x2_t v35 = v61;
  float64x2_t v36 = v62;
  float64x2_t v37 = v63;
  *a3 = v56;
  a3[1] = v31;
  a3[2] = v32;
  a3[3] = v33;
  a3[4] = v34;
  a3[5] = v35;
  a3[6] = v36;
  a3[7] = v37;
  return result;
}

double key path setter for SPAffineTransform3D.offset : SPAffineTransform3D(double *a1, SPAffineTransform3D *a2)
{
  long long v2 = *(_OWORD *)a1;
  double v3 = a1[3];
  v5.double z = a1[2];
  v5.vector.f64[3] = v3;
  *(_OWORD *)&v5.x = v2;
  *(void *)&double result = SPAffineTransform3DSetTranslation(a2, &v5).n128_u64[0];
  return result;
}

double SPAffineTransform3D.offset.setter(double a1, double a2, double a3)
{
  v5.x = a1;
  v5.y = a2;
  v5.double z = a3;
  *(void *)&double result = SPAffineTransform3DSetTranslation(v3, &v5).n128_u64[0];
  return result;
}

__n128 SPAffineTransform3DSetTranslation(SPAffineTransform3D *a1, SPVector3D *a2)
{
  __n128 result = *(__n128 *)&a2->x;
  long long v3 = *(_OWORD *)&a2->vector.f64[2];
  *(_OWORD *)a1->matrix.columns[3].f64 = *(_OWORD *)&a2->x;
  *(_OWORD *)&a1->matrix.columns[3].f64[2] = v3;
  return result;
}

{
  __n128 result;
  long long v3;

  __n128 result = *(__n128 *)&a2->x;
  long long v3 = *(_OWORD *)&a2->vector.f64[2];
  *(_OWORD *)a1->matrix.columns[3].f64 = *(_OWORD *)&a2->x;
  *(_OWORD *)&a1->matrix.columns[3].f64[2] = v3;
  return result;
}

void (*SPAffineTransform3D.offset.modify(__n128 **a1))(uint64_t **a1, uint64_t a2)
{
  long long v3 = (__n128 *)malloc(0x28uLL);
  *a1 = v3;
  v3[2].n128_u64[0] = (unint64_t)v1;
  __n128 v4 = v1[1];
  __n128 v5 = v1[2];
  __n128 v6 = v1[3];
  __n128 v7 = v1[4];
  __n128 v8 = v1[5];
  __n128 v9 = v1[6];
  __n128 v10 = v1[7];
  v12[0] = *v1;
  v12[1] = v4;
  v12[2] = v5;
  v12[3] = v6;
  v12[4] = v7;
  v12[5] = v8;
  v12[6] = v9;
  v12[7] = v10;
  SPAffineTransform3DGetTranslation(v12, v3);
  return SPAffineTransform3D.offset.modify;
}

void SPAffineTransform3D.offset.modify(uint64_t **a1, uint64_t a2)
{
}

void __swiftcall SPAffineTransform3D.inverted()(SPAffineTransform3D_optional *__return_ptr retstr)
{
  long long v3 = *(_OWORD *)(v1 + 16);
  float64x2_t v4 = *(float64x2_t *)(v1 + 32);
  long long v5 = *(_OWORD *)(v1 + 48);
  long long v6 = *(_OWORD *)(v1 + 64);
  long long v7 = *(_OWORD *)(v1 + 80);
  long long v8 = *(_OWORD *)(v1 + 96);
  long long v9 = *(_OWORD *)(v1 + 112);
  float64x2_t v26 = *(float64x2_t *)v1;
  long long v27 = v3;
  float64x2_t v28 = v4;
  long long v29 = v5;
  long long v30 = v6;
  long long v31 = v7;
  long long v32 = v8;
  long long v33 = v9;
  SPAffineTransform3DInverted((uint64_t)&v26, &v18);
  long long v16 = v21;
  long long v17 = v19;
  float64x2_t v26 = v18;
  long long v27 = v19;
  float64x2_t v12 = v20;
  float64x2_t v13 = v18;
  float64x2_t v28 = v20;
  long long v29 = v21;
  long long v14 = *(_OWORD *)v25;
  long long v15 = v23;
  long long v30 = v22;
  long long v31 = v23;
  long long v10 = v24;
  long long v11 = v22;
  long long v32 = v24;
  long long v33 = *(_OWORD *)v25;
  if (SPAffineTransform3DIsValid(&v26, *(double *)&v19, *(double *)&v21, *(double *)&v23, v25[0], v18.f64[0], v20))
  {
    float64x2_t v18 = v13;
    long long v19 = v17;
    float64x2_t v20 = v12;
    long long v21 = v16;
    long long v22 = v11;
    long long v23 = v15;
    long long v24 = v10;
    *(_OWORD *)int64x2_t v25 = v14;
    _sSo19SPAffineTransform3DaSgWOi_((uint64_t)&v18);
  }
  else
  {
    _sSo19SPAffineTransform3DaSgWOi0_((uint64_t)&v18);
  }
  outlined init with take of SPAffineTransform3D?((uint64_t)&v18, (uint64_t)&v26, &demangling cache variable for type metadata for SPAffineTransform3D?);
  outlined init with take of SPAffineTransform3D?((uint64_t)&v26, (uint64_t)retstr, &demangling cache variable for type metadata for SPAffineTransform3D?);
}

float64_t SPAffineTransform3DInverted@<D0>(uint64_t a1@<X0>, _OWORD *a2@<X8>)
{
  v3.f64[0] = *(float64_t *)(a1 + 80);
  v3.f64[1] = *(float64_t *)(a1 + 64);
  v4.f64[0] = *(float64_t *)(a1 + 48);
  v4.f64[1] = *(float64_t *)(a1 + 32);
  if (vmulq_f64(*(float64x2_t *)(a1 + 16), vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)(a1 + 64), vnegq_f64((float64x2_t)vdupq_laneq_s64(*(int64x2_t *)(a1 + 32), 1))), *(float64x2_t *)(a1 + 32), *(float64x2_t *)(a1 + 64), 1)).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)a1, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8(*(int8x16_t *)(a1 + 64), *(int8x16_t *)(a1 + 80), 8uLL), vnegq_f64(v4)), v3, (float64x2_t)vextq_s8(*(int8x16_t *)(a1 + 32), *(int8x16_t *)(a1 + 48), 8uLL)))) == 0.0)
  {
    a2[4] = xmmword_228C1FB80;
    a2[5] = unk_228C1FB90;
    a2[6] = xmmword_228C1FBA0;
    a2[7] = unk_228C1FBB0;
    *a2 = SPAffineTransform3DInvalid;
    a2[1] = unk_228C1FB50;
    v9.f64[0] = INFINITY;
    a2[2] = xmmword_228C1FB60;
    a2[3] = unk_228C1FB70;
  }
  else
  {
    __invert_d3();
    float64x2_t v7 = *(float64x2_t *)(a1 + 96);
    float64x2_t v6 = *(float64x2_t *)(a1 + 112);
    float64x2_t v8 = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v7, (float64x2_t)0), (float64x2_t)0, v7, 1), v6, (float64x2_t)0);
    float64x2_t v9 = vnegq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v7.f64[0]), (float64x2_t)0, v7, 1), (float64x2_t)0, v6.f64[0]));
    *a2 = 0u;
    a2[1] = 0u;
    a2[2] = 0u;
    a2[3] = 0u;
    a2[4] = 0u;
    a2[5] = 0u;
    a2[6] = v9;
    a2[7] = vnegq_f64(v8);
  }
  return v9.f64[0];
}

{
  float64x2_t v3;
  float64x2_t v4;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;

  v3.f64[0] = *(float64_t *)(a1 + 80);
  v3.f64[1] = *(float64_t *)(a1 + 64);
  v4.f64[0] = *(float64_t *)(a1 + 48);
  v4.f64[1] = *(float64_t *)(a1 + 32);
  if (vmulq_f64(*(float64x2_t *)(a1 + 16), vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)(a1 + 64), vnegq_f64((float64x2_t)vdupq_laneq_s64(*(int64x2_t *)(a1 + 32), 1))), *(float64x2_t *)(a1 + 32), *(float64x2_t *)(a1 + 64), 1)).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)a1, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8(*(int8x16_t *)(a1 + 64), *(int8x16_t *)(a1 + 80), 8uLL), vnegq_f64(v4)), v3, (float64x2_t)vextq_s8(*(int8x16_t *)(a1 + 32), *(int8x16_t *)(a1 + 48), 8uLL)))) == 0.0)
  {
    a2[4] = xmmword_228C22140;
    a2[5] = unk_228C22150;
    a2[6] = xmmword_228C22160;
    a2[7] = unk_228C22170;
    *a2 = SPAffineTransform3DInvalid_0;
    a2[1] = unk_228C22110;
    v9.f64[0] = INFINITY;
    a2[2] = xmmword_228C22120;
    a2[3] = unk_228C22130;
  }
  else
  {
    __invert_d3();
    float64x2_t v7 = *(float64x2_t *)(a1 + 96);
    float64x2_t v6 = *(float64x2_t *)(a1 + 112);
    float64x2_t v8 = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v7, (float64x2_t)0), (float64x2_t)0, v7, 1), v6, (float64x2_t)0);
    float64x2_t v9 = vnegq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v7.f64[0]), (float64x2_t)0, v7, 1), (float64x2_t)0, v6.f64[0]));
    *a2 = 0u;
    a2[1] = 0u;
    a2[2] = 0u;
    a2[3] = 0u;
    a2[4] = 0u;
    a2[5] = 0u;
    a2[6] = v9;
    a2[7] = vnegq_f64(v8);
  }
  return v9.f64[0];
}

void __swiftcall SPAffineTransform3D.init(matrix:)(SPAffineTransform3D *__return_ptr retstr, simd_float4x3 *matrix)
{
  specialized SPAffineTransform3D.init(_:)((float32x2_t *)matrix, v7);
  long long v3 = v7[5];
  *(_OWORD *)retstr->matrix.columns[2].f64 = v7[4];
  *(_OWORD *)&retstr->matrix.columns[2].f64[2] = v3;
  long long v4 = v7[7];
  *(_OWORD *)retstr->matrix.columns[3].f64 = v7[6];
  *(_OWORD *)&retstr->matrix.columns[3].f64[2] = v4;
  long long v5 = v7[1];
  *(_OWORD *)retstr->matrix.columns[0].f64 = v7[0];
  *(_OWORD *)&retstr->matrix.columns[0].f64[2] = v5;
  long long v6 = v7[3];
  *(_OWORD *)retstr->matrix.columns[1].f64 = v7[2];
  *(_OWORD *)&retstr->matrix.columns[1].f64[2] = v6;
}

void __swiftcall SPAffineTransform3D.init(matrix:)(SPAffineTransform3D_optional *__return_ptr retstr, simd_float4x4 *matrix)
{
  specialized SPAffineTransform3D.init(_:)((uint64_t)v7, v2, v3, v4, v5);
  outlined init with take of SPAffineTransform3D?((uint64_t)v7, (uint64_t)retstr, &demangling cache variable for type metadata for SPAffineTransform3D?);
}

void __swiftcall SPAffineTransform3D.init(_:)(SPAffineTransform3D_optional *__return_ptr retstr, simd_float4x4 *a2)
{
  specialized SPAffineTransform3D.init(_:)((uint64_t)v7, v2, v3, v4, v5);
  outlined init with take of SPAffineTransform3D?((uint64_t)v7, (uint64_t)retstr, &demangling cache variable for type metadata for SPAffineTransform3D?);
}

void __swiftcall SPAffineTransform3D.init(matrix:)(SPAffineTransform3D_optional *__return_ptr retstr, simd_double4x4 *matrix)
{
  long long v3 = *(_OWORD *)&matrix->columns[0].f64[2];
  float64x2_t v4 = *(float64x2_t *)matrix->columns[1].f64;
  long long v5 = *(_OWORD *)&matrix->columns[1].f64[2];
  long long v6 = *(_OWORD *)matrix->columns[2].f64;
  long long v7 = *(_OWORD *)&matrix->columns[2].f64[2];
  long long v8 = *(_OWORD *)matrix->columns[3].f64;
  long long v9 = *(_OWORD *)&matrix->columns[3].f64[2];
  float64x2_t v26 = *(float64x2_t *)matrix->columns[0].f64;
  long long v27 = v3;
  float64x2_t v28 = v4;
  long long v29 = v5;
  long long v30 = v6;
  long long v31 = v7;
  long long v32 = v8;
  long long v33 = v9;
  SPAffineTransform3DMakeWith4x4Matrix((uint64_t)&v26, &v18);
  long long v16 = v21;
  long long v17 = v19;
  float64x2_t v26 = v18;
  long long v27 = v19;
  float64x2_t v12 = v20;
  float64x2_t v13 = v18;
  float64x2_t v28 = v20;
  long long v29 = v21;
  long long v14 = *(_OWORD *)v25;
  long long v15 = v23;
  long long v30 = v22;
  long long v31 = v23;
  long long v10 = v24;
  long long v11 = v22;
  long long v32 = v24;
  long long v33 = *(_OWORD *)v25;
  if (SPAffineTransform3DIsValid(&v26, *(double *)&v19, *(double *)&v21, *(double *)&v23, v25[0], v18.f64[0], v20))
  {
    float64x2_t v18 = v13;
    long long v19 = v17;
    float64x2_t v20 = v12;
    long long v21 = v16;
    long long v22 = v11;
    long long v23 = v15;
    long long v24 = v10;
    *(_OWORD *)int64x2_t v25 = v14;
    _sSo19SPAffineTransform3DaSgWOi_((uint64_t)&v18);
  }
  else
  {
    _sSo19SPAffineTransform3DaSgWOi0_((uint64_t)&v18);
  }
  outlined init with take of SPAffineTransform3D?((uint64_t)&v18, (uint64_t)&v26, &demangling cache variable for type metadata for SPAffineTransform3D?);
  outlined init with take of SPAffineTransform3D?((uint64_t)&v26, (uint64_t)retstr, &demangling cache variable for type metadata for SPAffineTransform3D?);
}

void __swiftcall SPProjectiveTransform3D.init(translation:)(SPProjectiveTransform3D *__return_ptr retstr, SPSize3D *translation)
{
  SPAffineTransform3D.init(translation:)((void (*)(SPSize3D *__return_ptr, unsigned char *, double))SPProjectiveTransform3DMakeTranslation, retstr, v2, v3, v4);
}

double SPAffineTransform3D.init(translation:)@<D0>(void (*a1)(SPSize3D *__return_ptr, unsigned char *, double)@<X0>, _OWORD *a2@<X8>, double a3@<D0>, double a4@<D1>, double a5@<D2>)
{
  v16.width = a3;
  v16.height = a4;
  v16.depth = a5;
  double v7 = SPVector3DMakeWithSize(&v16, (uint64_t)v23);
  a1(&v16, v23, v7);
  double result = v16.width;
  long long v9 = *(_OWORD *)&v16.vector.f64[2];
  long long v10 = v17;
  long long v11 = v18;
  long long v12 = v19;
  long long v13 = v20;
  long long v14 = v21;
  long long v15 = v22;
  *a2 = *(_OWORD *)&v16.width;
  a2[1] = v9;
  a2[2] = v10;
  a2[3] = v11;
  a2[4] = v12;
  a2[5] = v13;
  a2[6] = v14;
  a2[7] = v15;
  return result;
}

void __swiftcall SPProjectiveTransform3D.translated(by:)(SPProjectiveTransform3D *__return_ptr retstr, SPSize3D *by)
{
  SPAffineTransform3D.translated(by:)((void (*)(_OWORD *__return_ptr, SPSize3D *, unsigned char *))SPProjectiveTransform3DTranslate, retstr, v2, v3, v4);
}

double SPAffineTransform3D.translated(by:)@<D0>(void (*a1)(_OWORD *__return_ptr, SPSize3D *, unsigned char *)@<X0>, _OWORD *a2@<X8>, double a3@<D0>, double a4@<D1>, double a5@<D2>)
{
  long long v22 = v5[2];
  long long v23 = *v5;
  long long v18 = v5[3];
  long long v19 = v5[1];
  long long v20 = v5[6];
  long long v21 = v5[4];
  long long v16 = v5[7];
  long long v17 = v5[5];
  v26.width = a3;
  v26.height = a4;
  v26.depth = a5;
  SPVector3DMakeWithSize(&v26, (uint64_t)v25);
  *(_OWORD *)&v26.width = v23;
  *(_OWORD *)&v26.vector.f64[2] = v19;
  long long v27 = v22;
  long long v28 = v18;
  long long v29 = v21;
  long long v30 = v17;
  long long v31 = v20;
  long long v32 = v16;
  a1(v24, &v26, v25);
  double result = *(double *)v24;
  long long v9 = v24[1];
  long long v10 = v24[2];
  long long v11 = v24[3];
  long long v12 = v24[4];
  long long v13 = v24[5];
  long long v14 = v24[6];
  long long v15 = v24[7];
  *a2 = v24[0];
  a2[1] = v9;
  a2[2] = v10;
  a2[3] = v11;
  a2[4] = v12;
  a2[5] = v13;
  a2[6] = v14;
  a2[7] = v15;
  return result;
}

double SPAffineTransform3D.offset.getter()
{
  __n128 v1 = v0[1];
  __n128 v2 = v0[2];
  __n128 v3 = v0[3];
  __n128 v4 = v0[4];
  __n128 v5 = v0[5];
  __n128 v6 = v0[6];
  __n128 v7 = v0[7];
  v10[0] = *v0;
  v10[1] = v1;
  v10[2] = v2;
  v10[3] = v3;
  v10[4] = v4;
  v10[5] = v5;
  v10[6] = v6;
  v10[7] = v7;
  SPAffineTransform3DGetTranslation(v10, &v9);
  return v9.n128_f64[0];
}

double key path getter for SPAffineTransform3D.offset : SPAffineTransform3D@<D0>(__n128 *a1@<X0>, __n128 *a2@<X8>)
{
  __n128 v2 = a1[1];
  __n128 v3 = a1[2];
  __n128 v4 = a1[3];
  __n128 v5 = a1[4];
  __n128 v6 = a1[5];
  __n128 v7 = a1[6];
  __n128 v8 = a1[7];
  v10[0] = *a1;
  v10[1] = v2;
  v10[2] = v3;
  v10[3] = v4;
  v10[4] = v5;
  v10[5] = v6;
  v10[6] = v7;
  v10[7] = v8;
  *(void *)&double result = SPAffineTransform3DGetTranslation(v10, a2).n128_u64[0];
  return result;
}

double key path setter for SPProjectiveTransform3D.offset : SPProjectiveTransform3D(double *a1, SPProjectiveTransform3D *a2)
{
  long long v2 = *(_OWORD *)a1;
  double v3 = a1[3];
  v5.double z = a1[2];
  v5.vector.f64[3] = v3;
  *(_OWORD *)&v5.x = v2;
  *(void *)&double result = SPProjectiveTransform3DSetTranslation(a2, &v5).n128_u64[0];
  return result;
}

double SPProjectiveTransform3D.offset.setter(double a1, double a2, double a3)
{
  v5.x = a1;
  v5.y = a2;
  v5.double z = a3;
  *(void *)&double result = SPProjectiveTransform3DSetTranslation(v3, &v5).n128_u64[0];
  return result;
}

void (*SPProjectiveTransform3D.offset.modify(__n128 **a1))(uint64_t **a1, uint64_t a2)
{
  double v3 = (__n128 *)malloc(0x28uLL);
  *a1 = v3;
  v3[2].n128_u64[0] = (unint64_t)v1;
  __n128 v4 = v1[1];
  __n128 v5 = v1[2];
  __n128 v6 = v1[3];
  __n128 v7 = v1[4];
  __n128 v8 = v1[5];
  __n128 v9 = v1[6];
  __n128 v10 = v1[7];
  v12[0] = *v1;
  v12[1] = v4;
  v12[2] = v5;
  v12[3] = v6;
  v12[4] = v7;
  v12[5] = v8;
  v12[6] = v9;
  v12[7] = v10;
  SPAffineTransform3DGetTranslation(v12, v3);
  return SPProjectiveTransform3D.offset.modify;
}

void SPProjectiveTransform3D.offset.modify(uint64_t **a1, uint64_t a2)
{
}

void SPAffineTransform3D.offset.modify(uint64_t **a1, uint64_t a2, void (*a3)(uint64_t, void *))
{
  double v3 = *a1;
  uint64_t v5 = (*a1)[2];
  uint64_t v4 = (*a1)[3];
  uint64_t v7 = **a1;
  uint64_t v6 = (*a1)[1];
  uint64_t v8 = (*a1)[4];
  v9[0] = v7;
  v9[1] = v6;
  v9[2] = v5;
  v9[3] = v4;
  a3(v8, v9);

  free(v3);
}

void __swiftcall SPProjectiveTransform3D.inverted()(SPProjectiveTransform3D_optional *__return_ptr retstr)
{
  long long v3 = *(_OWORD *)(v1 + 16);
  long long v4 = *(_OWORD *)(v1 + 32);
  long long v5 = *(_OWORD *)(v1 + 48);
  long long v6 = *(_OWORD *)(v1 + 64);
  long long v7 = *(_OWORD *)(v1 + 80);
  long long v8 = *(_OWORD *)(v1 + 96);
  long long v9 = *(_OWORD *)(v1 + 112);
  float64x2_t v26 = *(float64x2_t *)v1;
  long long v27 = v3;
  long long v28 = v4;
  long long v29 = v5;
  long long v30 = v6;
  long long v31 = v7;
  long long v32 = v8;
  long long v33 = v9;
  SPProjectiveTransform3DInverted(&v26, &v18);
  long long v16 = v21;
  long long v17 = v19;
  float64x2_t v26 = v18;
  long long v27 = v19;
  long long v12 = v20;
  float64x2_t v13 = v18;
  long long v28 = v20;
  long long v29 = v21;
  long long v14 = v25;
  long long v15 = v23;
  long long v30 = v22;
  long long v31 = v23;
  long long v10 = v24;
  long long v11 = v22;
  long long v32 = v24;
  long long v33 = v25;
  if (SPProjectiveTransform3DIsValid(&v26))
  {
    float64x2_t v18 = v13;
    long long v19 = v17;
    long long v20 = v12;
    long long v21 = v16;
    long long v22 = v11;
    long long v23 = v15;
    long long v24 = v10;
    long long v25 = v14;
    _sSo19SPAffineTransform3DaSgWOi_((uint64_t)&v18);
  }
  else
  {
    _sSo19SPAffineTransform3DaSgWOi0_((uint64_t)&v18);
  }
  outlined init with take of SPAffineTransform3D?((uint64_t)&v18, (uint64_t)&v26, &demangling cache variable for type metadata for SPProjectiveTransform3D?);
  outlined init with take of SPAffineTransform3D?((uint64_t)&v26, (uint64_t)retstr, &demangling cache variable for type metadata for SPProjectiveTransform3D?);
}

void __swiftcall SPProjectiveTransform3D.init(matrix:)(SPProjectiveTransform3D *__return_ptr retstr, simd_float4x4 *matrix)
{
  specialized SPProjectiveTransform3D.init(_:)(v11, v2, v3, v4, v5);
  long long v7 = v11[5];
  *(_OWORD *)retstr->matrix.columns[2].f64 = v11[4];
  *(_OWORD *)&retstr->matrix.columns[2].f64[2] = v7;
  long long v8 = v11[7];
  *(_OWORD *)retstr->matrix.columns[3].f64 = v11[6];
  *(_OWORD *)&retstr->matrix.columns[3].f64[2] = v8;
  long long v9 = v11[1];
  *(_OWORD *)retstr->matrix.columns[0].f64 = v11[0];
  *(_OWORD *)&retstr->matrix.columns[0].f64[2] = v9;
  long long v10 = v11[3];
  *(_OWORD *)retstr->matrix.columns[1].f64 = v11[2];
  *(_OWORD *)&retstr->matrix.columns[1].f64[2] = v10;
}

double SIMD3<>.init(rotationAxis:)(double a1, double a2)
{
  *(float *)&unsigned int v2 = a1;
  *(float *)&unsigned int v3 = a2;
  return COERCE_DOUBLE(__PAIR64__(v3, v2));
}

double SIMD3<>.init(size:)(double a1, double a2)
{
  *(float *)&unsigned int v2 = a1;
  *(float *)&unsigned int v3 = a2;
  return COERCE_DOUBLE(__PAIR64__(v3, v2));
}

void __swiftcall simd_float4x3.init(affineTransform:)(simd_float4x3 *__return_ptr retstr, SPAffineTransform3D *affineTransform)
{
  specialized simd_float4x3.init(_:)((float64x2_t *)affineTransform, v5);
  simd_float3 v3 = (simd_float3)v5[1];
  retstr->columns[0] = (simd_float3)v5[0];
  retstr->columns[1] = v3;
  simd_float3 v4 = (simd_float3)v5[3];
  retstr->columns[2] = (simd_float3)v5[2];
  retstr->columns[3] = v4;
}

void __swiftcall simd_float4x4.init(projectiveTransform:)(simd_float4x4 *__return_ptr retstr, SPProjectiveTransform3D *projectiveTransform)
{
}

void __swiftcall simd_float4x4.init(affineTransform:)(simd_float4x4 *__return_ptr retstr, SPAffineTransform3D *affineTransform)
{
  __n128 v2 = *(__n128 *)&affineTransform->matrix.columns[0].f64[2];
  long long v3 = *(_OWORD *)affineTransform->matrix.columns[1].f64;
  __n128 v4 = *(__n128 *)&affineTransform->matrix.columns[1].f64[2];
  long long v5 = *(_OWORD *)affineTransform->matrix.columns[2].f64;
  __n128 v6 = *(__n128 *)&affineTransform->matrix.columns[2].f64[2];
  long long v7 = *(_OWORD *)affineTransform->matrix.columns[3].f64;
  long long v8 = *(_OWORD *)&affineTransform->matrix.columns[3].f64[2];
  v10[0]  = *(_OWORD *)affineTransform->matrix.columns[0].f64;
  v10[1]  = v2;
  v10[2]  = v3;
  v10[3]  = v4;
  v10[4]  = v5;
  v10[5]  = v6;
  v10[6]  = v7;
  v10[7]  = v8;
  SPAffineTransform3DGet4x4Matrix((uint64_t)v10, (uint64_t)&v9, v2, v4, v6);
}

void __swiftcall simd_float4x4.init(pose:)(simd_float4x4 *__return_ptr retstr, SPPose3D *pose)
{
  long long v2 = *(_OWORD *)&pose->position.vector.f64[2];
  long long v3 = *(_OWORD *)pose->rotation.vector.f64;
  double v4 = pose->rotation.vector.f64[2];
  double v5 = pose->rotation.vector.f64[3];
  *(_OWORD *)&v7.position.x  = *(_OWORD *)&pose->position.x;
  *(_OWORD *)&v7.position.vector.f64[2]  = v2;
  v7.rotation.vector.f64[2]  = v4;
  v7.rotation.vector.f64[3]  = v5;
  *(_OWORD *)v7.rotation.vector.f64  = v3;
  SPPose3DGet4x4Matrix(&v7, (uint64_t)&v6);
}

void __swiftcall SPEulerAngles.init(_:_:_:order:)(SPEulerAngles *__return_ptr retstr, SPAngle _, SPAngle a3, SPAngle a4, SPEulerAngleOrder order)
{
}

double SPRect3D.minX.getter()
{
  return SPRect3D.minX.getter((void (*)(double *__return_ptr, _OWORD *))SPRect3DGetMinimum);
}

double SPRect3D.minY.getter()
{
  return SPRect3D.minY.getter((void (*)(void *__return_ptr, _OWORD *))SPRect3DGetMinimum);
}

double SPRect3D.minZ.getter()
{
  return SPRect3D.minZ.getter((void (*)(void *__return_ptr, _OWORD *))SPRect3DGetMinimum);
}

double SPRect3D.midX.getter()
{
  return SPRect3D.minX.getter((void (*)(double *__return_ptr, _OWORD *))SPRect3DGetCenter);
}

double SPRect3D.midY.getter()
{
  return SPRect3D.minY.getter((void (*)(void *__return_ptr, _OWORD *))SPRect3DGetCenter);
}

double SPRect3D.midZ.getter()
{
  return SPRect3D.minZ.getter((void (*)(void *__return_ptr, _OWORD *))SPRect3DGetCenter);
}

double SPRect3D.maxX.getter()
{
  return SPRect3D.minX.getter((void (*)(double *__return_ptr, _OWORD *))SPRect3DGetMaximum);
}

double SPRect3D.minX.getter(void (*a1)(double *__return_ptr, _OWORD *))
{
  long long v2 = *(_OWORD *)(v1 + 16);
  long long v3 = *(_OWORD *)(v1 + 32);
  uint64_t v4 = *(void *)(v1 + 48);
  uint64_t v5 = *(void *)(v1 + 56);
  v8[0]  = *(_OWORD *)v1;
  v8[1]  = v2;
  uint64_t v9 = v4;
  uint64_t v10 = v5;
  v8[2]  = v3;
  a1(v7, v8);
  return v7[0];
}

double SPRect3D.maxY.getter()
{
  return SPRect3D.minY.getter((void (*)(void *__return_ptr, _OWORD *))SPRect3DGetMaximum);
}

double SPRect3D.minY.getter(void (*a1)(void *__return_ptr, _OWORD *))
{
  long long v2 = *(_OWORD *)(v1 + 16);
  long long v3 = *(_OWORD *)(v1 + 32);
  uint64_t v4 = *(void *)(v1 + 48);
  uint64_t v5 = *(void *)(v1 + 56);
  v8[0]  = *(_OWORD *)v1;
  v8[1]  = v2;
  uint64_t v9 = v4;
  uint64_t v10 = v5;
  v8[2]  = v3;
  a1(v7, v8);
  return *(double *)&v7[1];
}

double SPRect3D.maxZ.getter()
{
  return SPRect3D.minZ.getter((void (*)(void *__return_ptr, _OWORD *))SPRect3DGetMaximum);
}

double SPRect3D.minZ.getter(void (*a1)(void *__return_ptr, _OWORD *))
{
  long long v2 = *(_OWORD *)(v1 + 16);
  long long v3 = *(_OWORD *)(v1 + 32);
  uint64_t v4 = *(void *)(v1 + 48);
  uint64_t v5 = *(void *)(v1 + 56);
  v8[0]  = *(_OWORD *)v1;
  v8[1]  = v2;
  uint64_t v9 = v4;
  uint64_t v10 = v5;
  v8[2]  = v3;
  a1(v7, v8);
  return *(double *)&v7[2];
}

double protocol witness for Translatable3D.translated(by:) in conformance SPPoint3D@<D0>(uint64_t a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>)
{
  long long v9 = *(_OWORD *)v4;
  double v7 = v4[2];
  double v6 = v4[3];
  v11.width  = a2;
  v11.height  = a3;
  v11.depth  = a4;
  SPVector3DMakeWithSize(&v11, (uint64_t)&v10);
  v11.depth  = v7;
  v11.vector.f64[3]  = v6;
  *(_OWORD *)&v11.width  = v9;
  *(void *)&double result = *(_OWORD *)&SPPoint3DTranslate((SPPoint3D *)&v11, &v10, a1);
  return result;
}

{
  double *v4;
  long long v5;
  double v6;
  double result;
  SPVector3D v8;
  SPPoint3D v9;

  uint64_t v5 = *(_OWORD *)v4;
  double v6 = v4[3];
  v9.double z = v4[2];
  v9.vector.f64[3]  = v6;
  *(_OWORD *)&v9.x  = v5;
  v8.x  = a2;
  v8.y  = a3;
  v8.double z = a4;
  *(void *)&double result = *(_OWORD *)&SPPoint3DTranslate(&v9, &v8, a1);
  return result;
}

double protocol witness for Translatable3D.translated(by:) in conformance SPPose3D@<D0>(uint64_t a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>)
{
  long long v13 = *((_OWORD *)v4 + 1);
  long long v14 = *(_OWORD *)v4;
  long long v15 = *((_OWORD *)v4 + 2);
  double v7 = v4[6];
  double v6 = v4[7];
  v20.position.x  = a2;
  v20.position.y  = a3;
  v20.position.double z = a4;
  SPVector3DMakeWithSize((SPSize3D *)&v20, (uint64_t)&v19);
  *(_OWORD *)&v20.position.x  = v14;
  *(_OWORD *)&v20.position.vector.f64[2]  = v13;
  v20.rotation.vector.f64[2]  = v7;
  v20.rotation.vector.f64[3]  = v6;
  *(_OWORD *)v20.rotation.vector.f64  = v15;
  SPPose3DTranslate(&v20, &v19, (uint64_t)v16);
  double result = *(double *)v16;
  long long v9 = v16[1];
  long long v10 = v16[2];
  uint64_t v11 = v17;
  uint64_t v12 = v18;
  *(_OWORD *)a1  = v16[0];
  *(_OWORD *)(a1 + 16)  = v9;
  *(void *)(a1 + 48)  = v11;
  *(void *)(a1 + 56)  = v12;
  *(_OWORD *)(a1 + 32)  = v10;
  return result;
}

{
  uint64_t v4;
  long long v6;
  long long v7;
  double v8;
  double v9;
  double result;
  long long v11;
  long long v12;
  uint64_t v13;
  uint64_t v14;
  _OWORD v15[3];
  uint64_t v16;
  uint64_t v17;
  SPVector3D v18;
  SPPose3D v19;

  double v6 = *(_OWORD *)(v4 + 16);
  double v7 = *(_OWORD *)(v4 + 32);
  long long v8 = *(double *)(v4 + 48);
  long long v9 = *(double *)(v4 + 56);
  *(_OWORD *)&v19.position.x  = *(_OWORD *)v4;
  *(_OWORD *)&v19.position.vector.f64[2]  = v6;
  v19.rotation.vector.f64[2]  = v8;
  v19.rotation.vector.f64[3]  = v9;
  *(_OWORD *)v19.rotation.vector.f64  = v7;
  v18.x  = a2;
  v18.y  = a3;
  v18.double z = a4;
  SPPose3DTranslate(&v19, &v18, (uint64_t)v15);
  double result = *(double *)v15;
  uint64_t v11 = v15[1];
  uint64_t v12 = v15[2];
  long long v13 = v16;
  long long v14 = v17;
  *(_OWORD *)a1  = v15[0];
  *(_OWORD *)(a1 + 16)  = v11;
  *(void *)(a1 + 48)  = v13;
  *(void *)(a1 + 56)  = v14;
  *(_OWORD *)(a1 + 32)  = v12;
  return result;
}

double protocol witness for Translatable3D.translated(by:) in conformance SPProjectiveTransform3D@<D0>(_OWORD *a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>)
{
  return protocol witness for Translatable3D.translated(by:) in conformance SPProjectiveTransform3D((void (*)(_OWORD *__return_ptr, SPSize3D *, unsigned char *))SPProjectiveTransform3DTranslate, a1, a2, a3, a4);
}

{
  return protocol witness for Translatable3D.translated(by:) in conformance SPProjectiveTransform3D((void (*)(_OWORD *__return_ptr, _OWORD *, void *))SPProjectiveTransform3DTranslate, a1, a2, a3, a4);
}

double protocol witness for Translatable3D.translated(by:) in conformance SPRect3D@<D0>(uint64_t a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>)
{
  return protocol witness for Translatable3D.translated(by:) in conformance SPRect3D((void (*)(_OWORD *__return_ptr, SPSize3D *, unsigned char *))SPRect3DTranslate, a1, a2, a3, a4);
}

{
  return protocol witness for Translatable3D.translated(by:) in conformance SPRect3D((void (*)(_OWORD *__return_ptr, _OWORD *, void *))SPRect3DTranslate, a1, a2, a3, a4);
}

BOOL protocol witness for Volumetric.containsAny(of:) in conformance SPRect3D(BOOL result)
{
  unint64_t v2 = *(void *)(result + 16);
  if (v2 >> 31)
  {
    __break(1u);
  }
  else
  {
    long long v4 = *(_OWORD *)(v1 + 16);
    long long v3 = *(_OWORD *)(v1 + 32);
    double v5 = *(double *)(v1 + 48);
    double v6 = *(double *)(v1 + 56);
    *(_OWORD *)&v7.origin.x  = *(_OWORD *)v1;
    *(_OWORD *)&v7.origin.vector.f64[2]  = v4;
    v7.size.depth  = v5;
    v7.size.vector.f64[3]  = v6;
    *(_OWORD *)&v7.size.width  = v3;
    return SPRect3DContainsAnyPoint(&v7, (const SPPoint3D *)(result + 32), v2);
  }
  return result;
}

double protocol witness for Translatable3D.translated(by:) in conformance SPRay3D@<D0>(uint64_t a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>)
{
  return protocol witness for Translatable3D.translated(by:) in conformance SPRect3D((void (*)(_OWORD *__return_ptr, SPSize3D *, unsigned char *))SPRay3DTranslate, a1, a2, a3, a4);
}

double protocol witness for Translatable3D.translated(by:) in conformance SPRect3D@<D0>(void (*a1)(_OWORD *__return_ptr, SPSize3D *, unsigned char *)@<X2>, uint64_t a2@<X8>, double a3@<D0>, double a4@<D1>, double a5@<D2>)
{
  long long v15 = v5[1];
  long long v16 = *v5;
  long long v17 = v5[2];
  uint64_t v9 = *((void *)v5 + 6);
  uint64_t v8 = *((void *)v5 + 7);
  v22.width  = a3;
  v22.height  = a4;
  v22.depth  = a5;
  SPVector3DMakeWithSize(&v22, (uint64_t)v21);
  *(_OWORD *)&v22.width  = v16;
  *(_OWORD *)&v22.vector.f64[2]  = v15;
  uint64_t v24 = v9;
  uint64_t v25 = v8;
  long long v23 = v17;
  a1(v18, &v22, v21);
  double result = *(double *)v18;
  long long v11 = v18[1];
  long long v12 = v18[2];
  uint64_t v13 = v19;
  uint64_t v14 = v20;
  *(_OWORD *)a2  = v18[0];
  *(_OWORD *)(a2 + 16)  = v11;
  *(void *)(a2 + 48)  = v13;
  *(void *)(a2 + 56)  = v14;
  *(_OWORD *)(a2 + 32)  = v12;
  return result;
}

BOOL protocol witness for Volumetric.containsAny(of:) in conformance SPSize3D(BOOL result)
{
  unint64_t v2 = *(void *)(result + 16);
  if (v2 >> 31)
  {
    __break(1u);
  }
  else
  {
    long long v3 = *(_OWORD *)v1;
    double v4 = v1[3];
    v5.depth  = v1[2];
    v5.vector.f64[3]  = v4;
    *(_OWORD *)&v5.width  = v3;
    return SPSize3DContainsAnyPoint(&v5, (const SPPoint3D *)(result + 32), v2);
  }
  return result;
}

double protocol witness for Translatable3D.translated(by:) in conformance SPAffineTransform3D@<D0>(_OWORD *a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>)
{
  return protocol witness for Translatable3D.translated(by:) in conformance SPProjectiveTransform3D((void (*)(_OWORD *__return_ptr, SPSize3D *, unsigned char *))SPAffineTransform3DTranslate, a1, a2, a3, a4);
}

{
  return protocol witness for Translatable3D.translated(by:) in conformance SPAffineTransform3D((void (*)(_OWORD *__return_ptr, _OWORD *, void *))SPAffineTransform3DTranslate, a1, a2, a3, a4);
}

double protocol witness for Translatable3D.translated(by:) in conformance SPProjectiveTransform3D@<D0>(void (*a1)(_OWORD *__return_ptr, SPSize3D *, unsigned char *)@<X2>, _OWORD *a2@<X8>, double a3@<D0>, double a4@<D1>, double a5@<D2>)
{
  long long v22 = v5[2];
  long long v23 = *v5;
  long long v18 = v5[3];
  long long v19 = v5[1];
  long long v20 = v5[6];
  long long v21 = v5[4];
  long long v16 = v5[7];
  long long v17 = v5[5];
  v26.width  = a3;
  v26.height  = a4;
  v26.depth  = a5;
  SPVector3DMakeWithSize(&v26, (uint64_t)v25);
  *(_OWORD *)&v26.width  = v23;
  *(_OWORD *)&v26.vector.f64[2]  = v19;
  long long v27 = v22;
  long long v28 = v18;
  long long v29 = v21;
  long long v30 = v17;
  long long v31 = v20;
  long long v32 = v16;
  a1(v24, &v26, v25);
  double result = *(double *)v24;
  long long v9 = v24[1];
  long long v10 = v24[2];
  long long v11 = v24[3];
  long long v12 = v24[4];
  long long v13 = v24[5];
  long long v14 = v24[6];
  long long v15 = v24[7];
  *a2  = v24[0];
  a2[1]  = v9;
  a2[2]  = v10;
  a2[3]  = v11;
  a2[4]  = v12;
  a2[5]  = v13;
  a2[6]  = v14;
  a2[7]  = v15;
  return result;
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance SPEulerAngleOrder(_DWORD *a1, _DWORD *a2)
{
  return *a1 == *a2;
}

_DWORD *protocol witness for RawRepresentable.init(rawValue:) in conformance SPEulerAngleOrder@<X0>(_DWORD *result@<X0>, uint64_t a2@<X8>)
{
  *(_DWORD *)a2  = *result;
  *(unsigned char *)(a2 + 4)  = 0;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance SPEulerAngleOrder(_DWORD *a1@<X8>)
{
  *a1  = *v1;
}

uint64_t specialized SPAffineTransform3D.init(_:)@<X0>(uint64_t a1@<X8>, float32x4_t a2@<Q0>, float32x4_t a3@<Q1>, float32x4_t a4@<Q2>, float32x4_t a5@<Q3>)
{
  float64x2_t v23 = vcvtq_f64_f32(*(float32x2_t *)a2.f32);
  float64x2_t v24 = vcvt_hight_f64_f32(a2);
  float64x2_t v25 = vcvtq_f64_f32(*(float32x2_t *)a3.f32);
  float64x2_t v26 = vcvt_hight_f64_f32(a3);
  float64x2_t v27 = vcvtq_f64_f32(*(float32x2_t *)a4.f32);
  float64x2_t v28 = vcvt_hight_f64_f32(a4);
  float64x2_t v29 = vcvtq_f64_f32(*(float32x2_t *)a5.f32);
  float64x2_t v30 = vcvt_hight_f64_f32(a5);
  SPAffineTransform3DMakeWith4x4Matrix((uint64_t)&v23, &v15);
  float64x2_t v13 = v18;
  float64x2_t v14 = v16;
  float64x2_t v23 = v15;
  float64x2_t v24 = v16;
  float64x2_t v9 = v17;
  float64x2_t v10 = v15;
  float64x2_t v25 = v17;
  float64x2_t v26 = v18;
  float64x2_t v11 = v22;
  float64x2_t v12 = v20;
  float64x2_t v27 = v19;
  float64x2_t v28 = v20;
  float64x2_t v7 = v21;
  float64x2_t v8 = v19;
  float64x2_t v29 = v21;
  float64x2_t v30 = v22;
  if (SPAffineTransform3DIsValid(&v23, v16.f64[0], v18.f64[0], v20.f64[0], v22.f64[0], v15.f64[0], v17))
  {
    float64x2_t v15 = v10;
    float64x2_t v16 = v14;
    float64x2_t v17 = v9;
    float64x2_t v18 = v13;
    float64x2_t v19 = v8;
    float64x2_t v20 = v12;
    float64x2_t v21 = v7;
    float64x2_t v22 = v11;
    _sSo19SPAffineTransform3DaSgWOi_((uint64_t)&v15);
  }
  else
  {
    _sSo19SPAffineTransform3DaSgWOi0_((uint64_t)&v15);
  }
  outlined init with take of SPAffineTransform3D?((uint64_t)&v15, (uint64_t)&v23, &demangling cache variable for type metadata for SPAffineTransform3D?);
  return outlined init with take of SPAffineTransform3D?((uint64_t)&v23, a1, &demangling cache variable for type metadata for SPAffineTransform3D?);
}

__n128 SPProjectiveTransform3DMakeTranslation@<Q0>(SPVector3D *a1@<X0>, uint64_t a2@<X8>)
{
  __n128 result = *(__n128 *)&a1->x;
  __asm { FMOV            V1.2D, #1.0 }
  *(double *)&_Q1  = a1->z;
  float64x2_t v8 = (_OWORD *)MEMORY[0x263EF8990];
  long long v9 = *(_OWORD *)(MEMORY[0x263EF8990] + 48);
  *(_OWORD *)(a2 + 32)  = *(_OWORD *)(MEMORY[0x263EF8990] + 32);
  *(_OWORD *)(a2 + 48)  = v9;
  long long v10 = v8[5];
  *(_OWORD *)(a2 + 64)  = v8[4];
  *(_OWORD *)(a2 + 80)  = v10;
  long long v11 = v8[1];
  *(_OWORD *)a2  = *v8;
  *(_OWORD *)(a2 + 16)  = v11;
  *(__n128 *)(a2 + 96)  = result;
  *(_OWORD *)(a2 + 112)  = _Q1;
  return result;
}

float64x2_t *SPProjectiveTransform3DTranslate@<X0>(float64x2_t *result@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>)
{
  float64x2_t v3 = *a2;
  __asm { FMOV            V18.2D, #1.0 }
  _Q18.f64[0]  = a2[1].f64[0];
  float64x2_t v9 = *(float64x2_t *)(MEMORY[0x263EF8990] + 16);
  float64x2_t v10 = *(float64x2_t *)(MEMORY[0x263EF8990] + 32);
  float64x2_t v11 = *(float64x2_t *)(MEMORY[0x263EF8990] + 48);
  float64x2_t v12 = *(float64x2_t *)(MEMORY[0x263EF8990] + 64);
  float64x2_t v13 = *(float64x2_t *)(MEMORY[0x263EF8990] + 80);
  float64x2_t v15 = *result;
  float64x2_t v14 = result[1];
  float64x2_t v16 = result[2];
  float64x2_t v17 = result[3];
  float64x2_t v19 = result[4];
  float64x2_t v18 = result[5];
  float64x2_t v21 = result[6];
  float64x2_t v20 = result[7];
  *(void *)&v22.f64[0]  = vdupq_laneq_s64((int64x2_t)v18, 1).u64[0];
  v22.f64[1]  = result[7].f64[1];
  long long v23 = xmmword_228C1F7A0;
  if (vmaxv_u16((uint16x4_t)vmovn_s32((int32x4_t)vmvnq_s8((int8x16_t)vuzp1q_s32((int32x4_t)vceqzq_f64((float64x2_t)vzip2q_s64((int64x2_t)v17, (int64x2_t)v14)), (int32x4_t)vceqq_f64(v22, (float64x2_t)xmmword_228C1F7A0))))))goto LABEL_6; {
  int64x2_t v24 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(v15, (float64x2_t)xmmword_228C1F7D0), (int8x16_t)vceqzq_f64(v14));
  }
  if ((vandq_s8((int8x16_t)v24, (int8x16_t)vdupq_laneq_s64(v24, 1)).u64[0] & 0x8000000000000000) != 0)
  {
    int64x2_t v25 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(v16, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqzq_f64(v17));
    if ((vandq_s8((int8x16_t)v25, (int8x16_t)vdupq_laneq_s64(v25, 1)).u64[0] & 0x8000000000000000) != 0)
    {
      int64x2_t v26 = (int64x2_t)vandq_s8((int8x16_t)vceqzq_f64(v19), (int8x16_t)vceqq_f64(v18, (float64x2_t)xmmword_228C1F7D0));
      if ((vandq_s8((int8x16_t)v26, (int8x16_t)vdupq_laneq_s64(v26, 1)).u64[0] & 0x8000000000000000) != 0
        && v9.f64[1] == 0.0
        && v11.f64[1] == 0.0
        && v13.f64[1] == 0.0)
      {
        int64x2_t v38 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(*MEMORY[0x263EF8990], (float64x2_t)xmmword_228C1F7D0), (int8x16_t)vceqzq_f64(v9));
        if ((vandq_s8((int8x16_t)v38, (int8x16_t)vdupq_laneq_s64(v38, 1)).u64[0] & 0x8000000000000000) != 0)
        {
          int64x2_t v39 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(v10, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqzq_f64(v11));
          if ((vandq_s8((int8x16_t)v39, (int8x16_t)vdupq_laneq_s64(v39, 1)).u64[0] & 0x8000000000000000) != 0)
          {
            int64x2_t v40 = (int64x2_t)vandq_s8((int8x16_t)vceqzq_f64(v12), (int8x16_t)vceqq_f64(v13, (float64x2_t)xmmword_228C1F7D0));
            if ((vandq_s8((int8x16_t)v40, (int8x16_t)vdupq_laneq_s64(v40, 1)).u64[0] & 0x8000000000000000) != 0)
            {
              float64x2_t v32 = vaddq_f64(v3, v21);
              *(void *)&long long v23 = *(_OWORD *)&vaddq_f64(_Q18, v20);
              *((void *)&v23 + 1)  = *(void *)&result[7].f64[1];
              goto LABEL_9;
            }
          }
        }
      }
    }
  }
  int64x2_t v27 = vceqzq_f64(v21);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v27, 1), vandq_s8((int8x16_t)vceqzq_f64(v20), (int8x16_t)v27)).u64[0] & 0x8000000000000000) != 0
    && v9.f64[1] == 0.0
    && v11.f64[1] == 0.0
    && v13.f64[1] == 0.0
    && (int64x2_t v33 = vceqzq_f64(v3),
        (vandq_s8((int8x16_t)vdupq_laneq_s64(v33, 1), vandq_s8((int8x16_t)vceqzq_f64(_Q18), (int8x16_t)v33)).u64[0] & 0x8000000000000000) != 0))
  {
    uint64_t v34 = 0;
    long long v41 = *MEMORY[0x263EF8990];
    float64x2_t v42 = v9;
    float64x2_t v43 = v10;
    float64x2_t v44 = v11;
    float64x2_t v45 = v12;
    float64x2_t v46 = v13;
    float64x2_t v49 = 0u;
    float64x2_t v50 = 0u;
    float64x2_t v51 = 0u;
    float64x2_t v52 = 0u;
    float64x2_t v53 = 0u;
    float64x2_t v54 = 0u;
    *(void *)&v15.f64[1]  = vextq_s8((int8x16_t)v15, (int8x16_t)v15, 8uLL).u64[0];
    *(void *)&v16.f64[1]  = vextq_s8((int8x16_t)v16, (int8x16_t)v16, 8uLL).u64[0];
    *(void *)&v19.f64[1]  = vextq_s8((int8x16_t)v19, (int8x16_t)v19, 8uLL).u64[0];
    do
    {
      float64x2_t v36 = *(float64x2_t *)((char *)&v41 + v34);
      float64x2_t v35 = *(float64x2_t *)((char *)&v41 + v34 + 16);
      float64x2_t v37 = (float64x2_t *)((char *)&v49 + v34);
      *float64x2_t v37 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v15, v36.f64[0]), v16, v36, 1), v19, v35.f64[0]);
      v37[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v14, v36), v17, v36, 1), v35, v18);
      v34 += 32;
    }
    while (v34 != 96);
    float64x2_t v15 = v49;
    float64x2_t v16 = v51;
    float64x2_t v14 = (float64x2_t)*(unint64_t *)&v50.f64[0];
    float64x2_t v19 = v53;
    float64x2_t v17 = (float64x2_t)*(unint64_t *)&v52.f64[0];
    float64x2_t v18 = (float64x2_t)*(unint64_t *)&v54.f64[0];
    float64x2_t v32 = 0uLL;
  }
  else
  {
LABEL_6:
    uint64_t v28 = 0;
    long long v41 = *MEMORY[0x263EF8990];
    float64x2_t v42 = v9;
    float64x2_t v43 = v10;
    float64x2_t v44 = v11;
    float64x2_t v45 = v12;
    float64x2_t v46 = v13;
    float64x2_t v47 = v3;
    float64x2_t v48 = _Q18;
    float64x2_t v49 = 0u;
    float64x2_t v50 = 0u;
    float64x2_t v51 = 0u;
    float64x2_t v52 = 0u;
    float64x2_t v53 = 0u;
    float64x2_t v54 = 0u;
    float64x2_t v55 = 0u;
    long long v56 = 0u;
    do
    {
      float64x2_t v30 = *(float64x2_t *)((char *)&v41 + v28);
      float64x2_t v29 = *(float64x2_t *)((char *)&v41 + v28 + 16);
      long long v31 = (float64x2_t *)((char *)&v49 + v28);
      *long long v31 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v15, v30.f64[0]), v16, v30, 1), v19, v29.f64[0]), v21, v29, 1);
      v31[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v14, v30.f64[0]), v17, v30, 1), v18, v29.f64[0]), v20, v29, 1);
      v28 += 32;
    }
    while (v28 != 128);
    float64x2_t v15 = v49;
    float64x2_t v14 = v50;
    float64x2_t v16 = v51;
    float64x2_t v17 = v52;
    float64x2_t v19 = v53;
    float64x2_t v18 = v54;
    float64x2_t v32 = v55;
    long long v23 = v56;
  }
LABEL_9:
  *a3  = v15;
  a3[1]  = v14;
  a3[2]  = v16;
  a3[3]  = v17;
  a3[4]  = v19;
  a3[5]  = v18;
  a3[6]  = v32;
  a3[7]  = (float64x2_t)v23;
  return result;
}

__n128 SPAffineTransform3DGetTranslation@<Q0>(__n128 *a1@<X0>, __n128 *a2@<X8>)
{
  __n128 result = a1[6];
  unint64_t v3 = a1[7].n128_u64[0];
  *a2  = result;
  a2[1].n128_u64[0]  = v3;
  return result;
}

__n128 SPProjectiveTransform3DSetTranslation(SPProjectiveTransform3D *a1, SPVector3D *a2)
{
  __n128 result = *(__n128 *)&a2->x;
  long long v3 = *(_OWORD *)&a2->vector.f64[2];
  *((void *)&v3 + 1)  = *(void *)&a1->matrix.columns[3].f64[3];
  *(_OWORD *)a1->matrix.columns[3].f64  = *(_OWORD *)&a2->x;
  *(_OWORD *)&a1->matrix.columns[3].f64[2]  = v3;
  return result;
}

{
  __n128 result;
  long long v3;

  __n128 result = *(__n128 *)&a2->x;
  long long v3 = *(_OWORD *)&a2->vector.f64[2];
  *((void *)&v3 + 1)  = *(void *)&a1->matrix.columns[3].f64[3];
  *(_OWORD *)a1->matrix.columns[3].f64  = *(_OWORD *)&a2->x;
  *(_OWORD *)&a1->matrix.columns[3].f64[2]  = v3;
  return result;
}

float64x2_t *SPProjectiveTransform3DInverted@<X0>(float64x2_t *result@<X0>, _OWORD *a2@<X8>)
{
  long long v3 = result;
  float64x2x2_t v21 = vld2q_f64(v3->f64);
  v3 += 2;
  float64x2_t v4 = result[3];
  float64x2_t v5 = result[4];
  float64x2_t v6 = result[5];
  float64x2_t v7 = result[6];
  float64x2_t v8 = result[7];
  float64x2_t v9 = (float64x2_t)vextq_s8((int8x16_t)v4, *(int8x16_t *)v3, 8uLL);
  float64x2_t v10 = (float64x2_t)vextq_s8(*(int8x16_t *)v3, (int8x16_t)v4, 8uLL);
  float64x2_t v11 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)v7, 8uLL);
  float64x2_t v12 = (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v8, 8uLL);
  float64x2_t v13 = (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v5, 8uLL);
  float64x2_t v14 = (float64x2_t)vextq_s8((int8x16_t)v5, (int8x16_t)v6, 8uLL);
  float64x2_t v15 = vnegq_f64(v14);
  float64x2_t v16 = vnegq_f64(v13);
  int64x2_t v17 = (int64x2_t)vmlaq_f64(vmlaq_f64(vmulq_f64(*v3, vmlaq_f64(vmulq_f64(v12, v16), v11, v14)), vmlaq_f64(vmulq_f64(v7, v15), v12, v5), v9), vmlaq_f64(vmulq_f64(v11, vnegq_f64(v5)), v7, v13), v10);
  int64x2_t v18 = (int64x2_t)vmlaq_f64(vmlaq_f64(vmulq_f64(v4, vmlaq_f64(vmulq_f64(v11, v15), v12, v13)), vmlaq_f64(vmulq_f64(v8, v16), v11, v6), v10), vmlaq_f64(vmulq_f64(v12, vnegq_f64(v6)), v8, v14), v9);
  if (vaddvq_f64(vsubq_f64(vmulq_f64(v21.val[0], (float64x2_t)vzip1q_s64(v18, v17)), vmulq_f64(v21.val[1], (float64x2_t)vzip2q_s64(v18, v17)))) == 0.0)
  {
    a2[4]  = xmmword_228C1FC00;
    a2[5]  = unk_228C1FC10;
    a2[6]  = xmmword_228C1FC20;
    a2[7]  = unk_228C1FC30;
    *a2  = SPProjectiveTransform3DInvalid;
    a2[1]  = unk_228C1FBD0;
    long long v19 = xmmword_228C1FBE0;
    long long v20 = unk_228C1FBF0;
  }
  else
  {
    __n128 result = (float64x2_t *)__invert_d4();
    a2[4]  = 0u;
    a2[5]  = 0u;
    a2[6]  = 0u;
    a2[7]  = 0u;
    *a2  = 0u;
    a2[1]  = 0u;
    long long v19 = 0u;
    long long v20 = 0u;
  }
  a2[2]  = v19;
  a2[3]  = v20;
  return result;
}

{
  float64x2_t *v3;
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  int64x2_t v17;
  int64x2_t v18;
  long long v19;
  long long v20;
  float64x2x2_t v21;

  long long v3 = result;
  float64x2x2_t v21 = vld2q_f64(v3->f64);
  v3 += 2;
  float64x2_t v4 = result[3];
  float64x2_t v5 = result[4];
  float64x2_t v6 = result[5];
  float64x2_t v7 = result[6];
  float64x2_t v8 = result[7];
  float64x2_t v9 = (float64x2_t)vextq_s8((int8x16_t)v4, *(int8x16_t *)v3, 8uLL);
  float64x2_t v10 = (float64x2_t)vextq_s8(*(int8x16_t *)v3, (int8x16_t)v4, 8uLL);
  float64x2_t v11 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)v7, 8uLL);
  float64x2_t v12 = (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v8, 8uLL);
  float64x2_t v13 = (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v5, 8uLL);
  float64x2_t v14 = (float64x2_t)vextq_s8((int8x16_t)v5, (int8x16_t)v6, 8uLL);
  float64x2_t v15 = vnegq_f64(v14);
  float64x2_t v16 = vnegq_f64(v13);
  int64x2_t v17 = (int64x2_t)vmlaq_f64(vmlaq_f64(vmulq_f64(*v3, vmlaq_f64(vmulq_f64(v12, v16), v11, v14)), vmlaq_f64(vmulq_f64(v7, v15), v12, v5), v9), vmlaq_f64(vmulq_f64(v11, vnegq_f64(v5)), v7, v13), v10);
  int64x2_t v18 = (int64x2_t)vmlaq_f64(vmlaq_f64(vmulq_f64(v4, vmlaq_f64(vmulq_f64(v11, v15), v12, v13)), vmlaq_f64(vmulq_f64(v8, v16), v11, v6), v10), vmlaq_f64(vmulq_f64(v12, vnegq_f64(v6)), v8, v14), v9);
  if (vaddvq_f64(vsubq_f64(vmulq_f64(v21.val[0], (float64x2_t)vzip1q_s64(v18, v17)), vmulq_f64(v21.val[1], (float64x2_t)vzip2q_s64(v18, v17)))) == 0.0)
  {
    a2[4]  = xmmword_228C20CA0;
    a2[5]  = unk_228C20CB0;
    a2[6]  = xmmword_228C20CC0;
    a2[7]  = unk_228C20CD0;
    *a2  = SPProjectiveTransform3DInvalid_0;
    a2[1]  = unk_228C20C70;
    long long v19 = xmmword_228C20C80;
    long long v20 = unk_228C20C90;
  }
  else
  {
    __n128 result = (float64x2_t *)__invert_d4();
    a2[4]  = 0u;
    a2[5]  = 0u;
    a2[6]  = 0u;
    a2[7]  = 0u;
    *a2  = 0u;
    a2[1]  = 0u;
    long long v19 = 0u;
    long long v20 = 0u;
  }
  a2[2]  = v19;
  a2[3]  = v20;
  return result;
}

unint64_t SPProjectiveTransform3DIsValid(float64x2_t *a1)
{
  float64x2_t v1 = a1[1];
  int8x16_t v2 = vorrq_s8((int8x16_t)vcltzq_f64(v1), (int8x16_t)vcgezq_f64(v1));
  float64x2_t v3 = vabsq_f64(v1);
  float64x2_t v4 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  int64x2_t v5 = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(*a1), (int8x16_t)vcgezq_f64(*a1)), (int8x16_t)vceqq_f64(vabsq_f64(*a1), v4)), vbicq_s8(v2, (int8x16_t)vceqq_f64(v3, v4)));
  if ((vandq_s8((int8x16_t)v5, (int8x16_t)vdupq_laneq_s64(v5, 1)).u64[0] & 0x8000000000000000) == 0) {
    return 0;
  }
  int64x2_t v6 = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(a1[2]), (int8x16_t)vcgezq_f64(a1[2])), (int8x16_t)vceqq_f64(vabsq_f64(a1[2]), v4)), vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(a1[3]), (int8x16_t)vcgezq_f64(a1[3])), (int8x16_t)vceqq_f64(vabsq_f64(a1[3]), v4)));
  if ((vandq_s8((int8x16_t)v6, (int8x16_t)vdupq_laneq_s64(v6, 1)).u64[0] & 0x8000000000000000) == 0) {
    return 0;
  }
  float64x2_t v7 = a1[5];
  int8x16_t v8 = vorrq_s8((int8x16_t)vcltzq_f64(v7), (int8x16_t)vcgezq_f64(v7));
  float64x2_t v9 = vabsq_f64(v7);
  float64x2_t v10 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  int64x2_t v11 = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(a1[4]), (int8x16_t)vcgezq_f64(a1[4])), (int8x16_t)vceqq_f64(vabsq_f64(a1[4]), v10)), vbicq_s8(v8, (int8x16_t)vceqq_f64(v9, v10)));
  if ((vandq_s8((int8x16_t)v11, (int8x16_t)vdupq_laneq_s64(v11, 1)).u64[0] & 0x8000000000000000) == 0) {
    return 0;
  }
  int64x2_t v13 = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(a1[6]), (int8x16_t)vcgezq_f64(a1[6])), (int8x16_t)vceqq_f64(vabsq_f64(a1[6]), v10)), vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(a1[7]), (int8x16_t)vcgezq_f64(a1[7])), (int8x16_t)vceqq_f64(vabsq_f64(a1[7]), v10)));
  return vandq_s8((int8x16_t)v13, (int8x16_t)vdupq_laneq_s64(v13, 1)).u64[0] >> 63;
}

uint64_t outlined init with take of SPAffineTransform3D?(uint64_t a1, uint64_t a2, uint64_t *a3)
{
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName(a3);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(void *)(v5 - 8) + 32))(a2, a1, v5);
  return a2;
}

__n128 SPAffineTransform3DGet4x4Matrix@<Q0>(uint64_t a1@<X0>, uint64_t a2@<X8>, __n128 a3@<Q1>, __n128 a4@<Q3>, __n128 a5@<Q5>)
{
  __n128 result = *(__n128 *)a1;
  a3.n128_u64[0]  = *(void *)(a1 + 16);
  long long v6 = *(_OWORD *)(a1 + 32);
  a4.n128_u64[0]  = *(void *)(a1 + 48);
  long long v7 = *(_OWORD *)(a1 + 64);
  a5.n128_u64[0]  = *(void *)(a1 + 80);
  long long v8 = *(_OWORD *)(a1 + 96);
  __asm { FMOV            V7.2D, #1.0 }
  *(void *)&_Q7  = *(void *)(a1 + 112);
  *(_OWORD *)a2  = *(_OWORD *)a1;
  *(__n128 *)(a2 + 16)  = a3;
  *(_OWORD *)(a2 + 32)  = v6;
  *(__n128 *)(a2 + 48)  = a4;
  *(_OWORD *)(a2 + 64)  = v7;
  *(__n128 *)(a2 + 80)  = a5;
  *(_OWORD *)(a2 + 96)  = v8;
  *(_OWORD *)(a2 + 112)  = _Q7;
  return result;
}

__n128 SPPose3DGet4x4Matrix@<Q0>(SPPose3D *a1@<X0>, uint64_t a2@<X8>)
{
  _Q1  = *(float64x2_t *)a1->rotation.vector.f64;
  _Q0  = *(float64x2_t *)&a1->rotation.quaternion.vector.f64[2];
  _D2  = a1->rotation.vector.f64[1];
  __asm { FMLS            D3, D0, V0.D[0] }
  _D5  = a1->rotation.vector.f64[3];
  __asm { FMLA            D3, D5, V0.D[1] }
  double v12 = vmlad_n_f64(vmuld_lane_f64(_Q0.f64[0], _Q0, 1), _D2, _Q1.f64[0]);
  v13.f64[0]  = vmuld_lane_f64(_D2, _Q0, 1);
  double v14 = vmlad_n_f64(-(_D2 * _D5), _Q0.f64[0], _Q1.f64[0]);
  *((double *)&_Q3 + 1)  = v12 + v12;
  *(_OWORD *)a2  = _Q3;
  *(_OWORD *)(a2 + 16)  = COERCE_UNSIGNED_INT64(v14 + v14);
  *(double *)&_Q3  = vmlad_n_f64(-(_Q0.f64[0] * _D5), _D2, _Q1.f64[0]);
  *(double *)&_Q3  = *(double *)&_Q3 + *(double *)&_Q3;
  __asm
  {
    FMLA            D4, D2, V1.D[1]
    FMLA            D4, D5, V0.D[1]
    FMLS            D4, D1, V1.D[0]
    FMLA            D6, D0, V1.D[1]
  }
  *((void *)&_Q3 + 1)  = _D4;
  *(_OWORD *)(a2 + 32)  = _Q3;
  *(_OWORD *)(a2 + 48)  = COERCE_UNSIGNED_INT64(_D6 + _D6);
  *(double *)&_Q3  = -(_Q1.f64[0] * _D5);
  float64x2_t v19 = (float64x2_t)vzip1q_s64((int64x2_t)_Q1, (int64x2_t)_Q0);
  __asm
  {
    FMLS            D5, D1, V1.D[0]
    FMLS            D5, D2, V1.D[1]
  }
  _Q1.f64[0]  = _Q0.f64[0];
  *(void *)&v13.f64[1]  = _Q3;
  float64x2_t v22 = vmlaq_f64(v13, v19, _Q1);
  *(float64x2_t *)(a2 + 64)  = vaddq_f64(v22, v22);
  *(_OWORD *)(a2 + 80)  = _D5;
  __n128 result = *(__n128 *)&a1->position.x;
  long long v24 = *(_OWORD *)&a1->position.vector.f64[2];
  *((void *)&v24 + 1)  = 1.0;
  *(_OWORD *)(a2 + 96)  = *(_OWORD *)&a1->position.x;
  *(_OWORD *)(a2 + 112)  = v24;
  return result;
}

{
  __n128 result;
  long long v5;
  float64x2_t v6[2];
  simd_quatd v7;

  *(_OWORD *)(a2 + 96)  = 0u;
  *(_OWORD *)(a2 + 112)  = 0u;
  *(_OWORD *)(a2 + 64)  = 0u;
  *(_OWORD *)(a2 + 80)  = 0u;
  *(_OWORD *)(a2 + 32)  = 0u;
  *(_OWORD *)(a2 + 48)  = 0u;
  *(_OWORD *)a2  = 0u;
  *(_OWORD *)(a2 + 16)  = 0u;
  *(_OWORD *)&v7.vector.f64[2]  = *(_OWORD *)&a1->rotation.quaternion.vector.f64[2];
  v6[0]  = *(float64x2_t *)a1->rotation.vector.f64;
  *(float64x2_t *)v7.vector.f64  = v6[0];
  v6[1]  = *(float64x2_t *)&v7.vector.f64[2];
  simd_matrix4x4(v7, v6, a2);
  __n128 result = *(__n128 *)&a1->position.x;
  uint64_t v5 = *(_OWORD *)&a1->position.vector.f64[2];
  *((void *)&v5 + 1)  = *(void *)(a2 + 120);
  *(_OWORD *)(a2 + 96)  = *(_OWORD *)&a1->position.x;
  *(_OWORD *)(a2 + 112)  = v5;
  return result;
}

float64x2_t SPRect3DGetMinimum@<Q0>(SPRect3D *a1@<X0>, uint64_t a2@<X8>)
{
  uint64_t v2 = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a1->origin.vector.f64[2], vminnmq_f64((float64x2_t)*(unint64_t *)&a1->size.depth, (float64x2_t)0));
  float64x2_t result = vaddq_f64(*(float64x2_t *)&a1->origin.x, vminnmq_f64(*(float64x2_t *)&a1->size.width, (float64x2_t)0));
  *(float64x2_t *)a2  = result;
  *(void *)(a2 + 16)  = v2;
  return result;
}

float64_t SPRect3DGetCenter@<D0>(SPRect3D *a1@<X0>, float64x2_t *a2@<X8>)
{
  __asm { FMOV            V4.2D, #0.5 }
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a1->origin.vector.f64[2], vmulq_f64(*(float64x2_t *)&a1->size.vector.f64[2], _Q4));
  *a2  = vaddq_f64(*(float64x2_t *)&a1->origin.x, vmulq_f64(*(float64x2_t *)&a1->size.width, _Q4));
  a2[1].f64[0]  = result;
  return result;
}

float64x2_t SPRect3DGetMaximum@<Q0>(SPRect3D *a1@<X0>, uint64_t a2@<X8>)
{
  uint64_t v2 = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a1->origin.vector.f64[2], vmaxnmq_f64((float64x2_t)*(unint64_t *)&a1->size.depth, (float64x2_t)0));
  float64x2_t result = vaddq_f64(*(float64x2_t *)&a1->origin.x, vmaxnmq_f64(*(float64x2_t *)&a1->size.width, (float64x2_t)0));
  *(float64x2_t *)a2  = result;
  *(void *)(a2 + 16)  = v2;
  return result;
}

void type metadata accessor for SPEulerAngleOrder(uint64_t a1)
{
}

__n128 __swift_memcpy32_16(_OWORD *a1, uint64_t a2)
{
  __n128 result = *(__n128 *)a2;
  long long v3 = *(_OWORD *)(a2 + 16);
  *a1  = *(_OWORD *)a2;
  a1[1]  = v3;
  return result;
}

void type metadata accessor for SPRotationAxis3D(uint64_t a1)
{
}

__n128 __swift_memcpy128_16(uint64_t a1, long long *a2)
{
  long long v2 = *a2;
  long long v3 = a2[1];
  long long v4 = a2[3];
  *(_OWORD *)(a1 + 32)  = a2[2];
  *(_OWORD *)(a1 + 48)  = v4;
  *(_OWORD *)a1  = v2;
  *(_OWORD *)(a1 + 16)  = v3;
  __n128 result = (__n128)a2[4];
  long long v6 = a2[5];
  long long v7 = a2[7];
  *(_OWORD *)(a1 + 96)  = a2[6];
  *(_OWORD *)(a1 + 112)  = v7;
  *(__n128 *)(a1 + 64)  = result;
  *(_OWORD *)(a1 + 80)  = v6;
  return result;
}

uint64_t getEnumTagSinglePayload for simd_double4x3(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 128)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for simd_double4x3(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(void *)(result + 120)  = 0;
    *(_OWORD *)(result + 104)  = 0u;
    *(_OWORD *)(result + 88)  = 0u;
    *(_OWORD *)(result + 72)  = 0u;
    *(_OWORD *)(result + 56)  = 0u;
    *(_OWORD *)(result + 40)  = 0u;
    *(_OWORD *)(result + 24)  = 0u;
    *(_OWORD *)(result + 8)  = 0u;
    *(void *)__n128 result = (a2 - 1);
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 128)  = v3;
  return result;
}

void type metadata accessor for simd_double4x3(uint64_t a1)
{
}

uint64_t initializeBufferWithCopyOfBuffer for SPVector3D(uint64_t *a1, uint64_t *a2)
{
  uint64_t v2 = *a2;
  *a1  = *a2;
  uint64_t v3 = v2 + 16;
  swift_retain();
  return v3;
}

uint64_t getEnumTagSinglePayload for SPVector3D(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 32)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for SPVector3D(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(void *)(result + 16)  = 0;
    *(void *)(result + 24)  = 0;
    *(void *)__n128 result = (a2 - 1);
    *(void *)(result + 8)  = 0;
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 32)  = v3;
  return result;
}

void type metadata accessor for SPVector3D(uint64_t a1)
{
}

__n128 __swift_memcpy64_16(uint64_t a1, uint64_t a2)
{
  __n128 result = *(__n128 *)a2;
  long long v3 = *(_OWORD *)(a2 + 16);
  long long v4 = *(_OWORD *)(a2 + 48);
  *(_OWORD *)(a1 + 32)  = *(_OWORD *)(a2 + 32);
  *(_OWORD *)(a1 + 48)  = v4;
  *(__n128 *)a1  = result;
  *(_OWORD *)(a1 + 16)  = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for SPRay3D(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 64)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for SPRay3D(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(void *)(result + 56)  = 0;
    *(_OWORD *)(result + 40)  = 0u;
    *(_OWORD *)(result + 24)  = 0u;
    *(_OWORD *)(result + 8)  = 0u;
    *(void *)__n128 result = (a2 - 1);
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 64)  = v3;
  return result;
}

void type metadata accessor for SPRay3D(uint64_t a1)
{
}

void type metadata accessor for SPSize3D(uint64_t a1)
{
}

void type metadata accessor for SPRect3D(uint64_t a1)
{
}

void type metadata accessor for simd_double4x4(uint64_t a1)
{
}

void type metadata accessor for SPAngle(uint64_t a1)
{
}

void type metadata accessor for SPSphericalCoordinates3D(uint64_t a1)
{
}

void type metadata accessor for SPPose3D(uint64_t a1)
{
}

void type metadata accessor for simd_quatd(uint64_t a1)
{
}

void type metadata accessor for SPRotation3D(uint64_t a1)
{
}

__n128 __swift_memcpy80_16(uint64_t a1, uint64_t a2)
{
  *(_OWORD *)a1  = *(_OWORD *)a2;
  __n128 result = *(__n128 *)(a2 + 16);
  long long v3 = *(_OWORD *)(a2 + 32);
  long long v4 = *(_OWORD *)(a2 + 64);
  *(_OWORD *)(a1 + 48)  = *(_OWORD *)(a2 + 48);
  *(_OWORD *)(a1 + 64)  = v4;
  *(__n128 *)(a1 + 16)  = result;
  *(_OWORD *)(a1 + 32)  = v3;
  return result;
}

uint64_t getEnumTagSinglePayload for SPScaledPose3D(uint64_t a1, int a2)
{
  if (a2 && *(unsigned char *)(a1 + 80)) {
    return (*(_DWORD *)a1 + 1);
  }
  else {
    return 0;
  }
}

uint64_t storeEnumTagSinglePayload for SPScaledPose3D(uint64_t result, int a2, int a3)
{
  if (a2)
  {
    *(void *)(result + 72)  = 0;
    *(_OWORD *)(result + 56)  = 0u;
    *(_OWORD *)(result + 40)  = 0u;
    *(_OWORD *)(result + 24)  = 0u;
    *(_OWORD *)(result + 8)  = 0u;
    *(void *)__n128 result = (a2 - 1);
    if (!a3) {
      return result;
    }
    char v3 = 1;
  }
  else
  {
    if (!a3) {
      return result;
    }
    char v3 = 0;
  }
  *(unsigned char *)(result + 80)  = v3;
  return result;
}

void type metadata accessor for SPScaledPose3D(uint64_t a1)
{
}

void type metadata accessor for SPPoint3D(uint64_t a1)
{
}

void type metadata accessor for SPProjectiveTransform3D(uint64_t a1)
{
}

void type metadata accessor for SPAffineTransform3D(uint64_t a1)
{
}

void type metadata accessor for SPEulerAngleOrder(uint64_t a1, unint64_t *a2)
{
  if (!*a2)
  {
    ForeignTypeMetadata  = swift_getForeignTypeMetadata();
    if (!v4) {
      atomic_store(ForeignTypeMetadata, a2);
    }
  }
}

__n128 simd_quaternion@<Q0>(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  int64x2_t v3 = *(int64x2_t *)a1;
  __n128 result = *(__n128 *)(a1 + 16);
  float64x2_t v4 = *(float64x2_t *)(a1 + 32);
  float64x2_t v5 = *(float64x2_t *)(a1 + 48);
  double v6 = *(double *)(a1 + 40);
  int64x2_t v7 = *(int64x2_t *)(a1 + 64);
  long long v8 = *(_OWORD *)(a1 + 80);
  double v9 = *(double *)a1 + v6 + *(double *)&v8;
  if (v9 >= 0.0)
  {
    double v27 = sqrt(v9 + 1.0);
    double v28 = v27 + v27;
    double v29 = 1.0 / (v27 + v27);
    *(double *)&long long v33 = v29 * vsubq_f64(v5, (float64x2_t)vdupq_laneq_s64(v7, 1)).f64[0];
    double v34 = (*(double *)v7.i64 - result.n128_f64[0]) * v29;
    result.n128_f64[0]  = v29 * vsubq_f64((float64x2_t)vdupq_laneq_s64(v3, 1), v4).f64[0];
    double v35 = v28 * 0.25;
  }
  else if (*(double *)v3.i64 < v6 || *(double *)v3.i64 < *(double *)&v8)
  {
    double v11 = 1.0 - *(double *)v3.i64;
    BOOL v12 = v6 < *(double *)&v8;
    double v13 = sqrt(1.0 - *(double *)v3.i64 - v6 + *(double *)&v8);
    double v14 = v13 + v13;
    double v15 = 1.0 / v14;
    double v16 = (result.n128_f64[0] + *(double *)v7.i64) * (1.0 / v14);
    double v17 = vaddq_f64(v5, (float64x2_t)vdupq_laneq_s64(v7, 1)).f64[0];
    double v18 = 1.0 / v14 * v17;
    float64x2_t v19 = (float64x2_t)vdupq_laneq_s64(v3, 1);
    double v20 = v14 * 0.25;
    double v21 = v15 * vsubq_f64(v19, v4).f64[0];
    double v22 = sqrt(v6 + v11 - *(double *)&v8);
    double v23 = v22 + v22;
    double v24 = 1.0 / v23 * vaddq_f64(v19, v4).f64[0];
    double v25 = v23 * 0.25;
    double v26 = 1.0 / v23 * v17;
    double v35 = (*(double *)v7.i64 - result.n128_f64[0]) * (1.0 / v23);
    if (v12) {
      *(double *)&long long v33 = v16;
    }
    else {
      *(double *)&long long v33 = v24;
    }
    if (v12) {
      double v34 = v18;
    }
    else {
      double v34 = v25;
    }
    if (v12) {
      result.n128_f64[0]  = v20;
    }
    else {
      result.n128_f64[0]  = v26;
    }
    if (v12) {
      double v35 = v21;
    }
  }
  else
  {
    double v30 = sqrt(*(double *)v3.i64 + 1.0 - v6 - *(double *)&v8);
    double v31 = v30 + v30;
    double v32 = 1.0 / v31;
    *(double *)&long long v33 = v31 * 0.25;
    double v34 = v32 * vaddq_f64((float64x2_t)vdupq_laneq_s64(v3, 1), v4).f64[0];
    result.n128_f64[0]  = (result.n128_f64[0] + *(double *)v7.i64) * v32;
    double v35 = v32 * vsubq_f64(v5, (float64x2_t)vdupq_laneq_s64(v7, 1)).f64[0];
  }
  *((double *)&v33 + 1)  = v34;
  result.n128_f64[1]  = v35;
  *(_OWORD *)a2  = v33;
  *(__n128 *)(a2 + 16)  = result;
  return result;
}

void __swiftcall SPPoint3D.init(x:y:z:)(SPPoint3D *__return_ptr retstr, Swift::Double x, Swift::Double y, Swift::Double z)
{
  SPPoint3DMake(x, y, z, (double *)&v4);
}

void SPPoint3DMake(double a1@<D0>, double a2@<D1>, double a3@<D2>, double *a4@<X8>)
{
  *a4  = a1;
  a4[1]  = a2;
  a4[2]  = a3;
}

double SPPoint3D.init(_:)(__n128 a1, __n128 a2)
{
  v4[0]  = a1;
  v4[1]  = a2;
  SPPoint3DMakeWithVector(v4, &v3);
  return v3.n128_f64[0];
}

__n128 SPPoint3DMakeWithVector@<Q0>(__n128 *a1@<X0>, __n128 *a2@<X8>)
{
  __n128 result = *a1;
  unint64_t v3 = a1[1].n128_u64[0];
  *a2  = *a1;
  a2[1].n128_u64[0]  = v3;
  return result;
}

void __swiftcall SPPoint3D.init(_:)(SPPoint3D *__return_ptr retstr, SPVector3D *a2)
{
  SPPoint3D.init(_:)((void (*)(double *__return_ptr, void *))SPPoint3DMakeWithSize, v2, v3, v4);
}

void __swiftcall SPPoint3D.init(_:)(SPPoint3D *__return_ptr retstr, SPSize3D *a2)
{
  SPPoint3D.init(_:)((void (*)(double *__return_ptr, void *))SPPoint3DMakeWithSize, v2, v3, v4);
}

double SPPoint3D.init(_:)(void (*a1)(double *__return_ptr, void *), double a2, double a3, double a4)
{
  *(double *)double v6 = a2;
  *(double *)&v6[1]  = a3;
  *(double *)&v6[2]  = a4;
  a1(&v5, v6);
  return v5;
}

double SPPoint3DMakeWithSize@<D0>(SPSize3D *a1@<X0>, uint64_t a2@<X8>)
{
  double result = a1->depth;
  *(_OWORD *)a2  = *(_OWORD *)&a1->width;
  *(double *)(a2 + 16)  = result;
  return result;
}

Swift::Void __swiftcall SPPoint3D.clamp(to:)(SPRect3D *to)
{
  long long v2 = *(_OWORD *)&to->origin.x;
  long long v3 = *(_OWORD *)&to->origin.vector.f64[2];
  long long v4 = *(_OWORD *)&to->size.width;
  depth  = to->size.depth;
  double v6 = to->size.vector.f64[3];
  long long v7 = *(_OWORD *)v1;
  double v8 = *(double *)(v1 + 24);
  v10.double z = *(double *)(v1 + 16);
  v10.vector.f64[3]  = v8;
  *(_OWORD *)&v10.x  = v7;
  *(_OWORD *)&v9.origin.x  = v2;
  *(_OWORD *)&v9.origin.vector.f64[2]  = v3;
  v9.size.depth  = depth;
  v9.size.vector.f64[3]  = v6;
  *(_OWORD *)&v9.size.width  = v4;
  SPPoint3DClampToRect(&v10, &v9, v1);
}

float64x2_t SPPoint3DClampToRect@<Q0>(SPPoint3D *a1@<X0>, SPRect3D *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v3 = *(float64x2_t *)&a2->origin.x;
  float64x2_t v4 = *(float64x2_t *)&a2->size.vector.f64[2];
  *(void *)&double v5 = *(_OWORD *)&vabsq_f64(v4);
  *(void *)&a2->origin.double z = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a2->origin.vector.f64[2], vminnmq_f64((float64x2_t)*(unint64_t *)&v4.f64[0], (float64x2_t)0));
  float64x2_t v7 = *(float64x2_t *)&a2->origin.vector.f64[2];
  float64x2_t v6 = *(float64x2_t *)&a2->size.width;
  float64x2_t v8 = vaddq_f64(v3, vminnmq_f64(v6, (float64x2_t)0));
  *(float64x2_t *)&a2->origin.x  = v8;
  float64x2_t v9 = vabsq_f64(v6);
  *(float64x2_t *)&a2->size.width  = v9;
  a2->size.depth  = v5;
  long long v10 = *(_OWORD *)&a1->vector.f64[2];
  unint64_t v11 = *(_OWORD *)&vaddq_f64(v7, *(float64x2_t *)&a2->size.vector.f64[2]);
  float64x2_t result = vminnmq_f64(vmaxnmq_f64(*(float64x2_t *)&a1->x, v8), vaddq_f64(v8, v9));
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = *(_OWORD *)&vminnmq_f64((float64x2_t)(unint64_t)*(_OWORD *)&vmaxnmq_f64((float64x2_t)(unint64_t)v10, (float64x2_t)*(unint64_t *)&v7.f64[0]), (float64x2_t)v11);
  return result;
}

void __swiftcall SPPoint3D.applying(_:)(SPPoint3D *__return_ptr retstr, SPPose3D *a2)
{
  SPPoint3D.applying(_:)((long long *)a2, (void (*)(double *__return_ptr, void *, _OWORD *))SPPoint3DApplyPose, v2, v3, v4);
}

float64x2_t SPPoint3DApplyPose@<Q0>(SPPoint3D *a1@<X0>, SPPose3D *a2@<X1>, float64x2_t *a3@<X8>)
{
  long long v3 = *(_OWORD *)&a1->vector.f64[2];
  float64x2_t v4 = *(float64x2_t *)a2->rotation.vector.f64;
  float64x2_t v5 = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  float64x2_t v6 = vmulq_f64(v5, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v7 = (int8x16_t)vnegq_f64(v4);
  float64x2_t v8 = (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)vnegq_f64(v6), 8uLL);
  float64x2_t v9 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v4, *(float64x2_t *)&a1->x, 1), (float64x2_t)vextq_s8(v7, (int8x16_t)v4, 8uLL), a1->x), v8, *(double *)&v3);
  float64x2_t v10 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v6, *(float64x2_t *)&a1->x, 1), v8, a1->x), (float64x2_t)vextq_s8((int8x16_t)v4, v7, 8uLL), *(double *)&v3);
  float64x2_t v11 = vnegq_f64(v10);
  float64x2_t v12 = (float64x2_t)vextq_s8((int8x16_t)v9, (int8x16_t)vnegq_f64(v9), 8uLL);
  float64x2_t v13 = vmlaq_n_f64(vmulq_laneq_f64(v9, v4, 1), v12, v4.f64[0]);
  float64x2_t v14 = vmlaq_n_f64(vmulq_laneq_f64(v11, v4, 1), (float64x2_t)vextq_s8((int8x16_t)v10, (int8x16_t)v11, 8uLL), v4.f64[0]);
  float64x2_t v15 = vmlaq_n_f64(vmulq_laneq_f64(v9, v5, 1), v12, v5.f64[0]);
  float64x2_t v16 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v10, v5, 1), (float64x2_t)vextq_s8((int8x16_t)v11, (int8x16_t)v10, 8uLL), v5.f64[0]), v13);
  *(void *)&v11.f64[0]  = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a2->position.vector.f64[2], vaddq_f64(v15, v14));
  float64x2_t result = vaddq_f64(*(float64x2_t *)&a2->position.x, v16);
  *a3  = result;
  a3[1].f64[0]  = v11.f64[0];
  return result;
}

{
  long long v3;
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  int8x16_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  float64x2_t result;

  long long v3 = *(_OWORD *)&a1->vector.f64[2];
  float64x2_t v4 = *(float64x2_t *)a2->rotation.vector.f64;
  float64x2_t v5 = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  float64x2_t v6 = vmulq_f64(v5, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v7 = (int8x16_t)vnegq_f64(v4);
  float64x2_t v8 = (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)vnegq_f64(v6), 8uLL);
  float64x2_t v9 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v4, *(float64x2_t *)&a1->x, 1), (float64x2_t)vextq_s8(v7, (int8x16_t)v4, 8uLL), a1->x), v8, *(double *)&v3);
  float64x2_t v10 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v6, *(float64x2_t *)&a1->x, 1), v8, a1->x), (float64x2_t)vextq_s8((int8x16_t)v4, v7, 8uLL), *(double *)&v3);
  float64x2_t v11 = vnegq_f64(v10);
  float64x2_t v12 = (float64x2_t)vextq_s8((int8x16_t)v9, (int8x16_t)vnegq_f64(v9), 8uLL);
  float64x2_t v13 = vmlaq_n_f64(vmulq_laneq_f64(v9, v4, 1), v12, v4.f64[0]);
  float64x2_t v14 = vmlaq_n_f64(vmulq_laneq_f64(v11, v4, 1), (float64x2_t)vextq_s8((int8x16_t)v10, (int8x16_t)v11, 8uLL), v4.f64[0]);
  float64x2_t v15 = vmlaq_n_f64(vmulq_laneq_f64(v9, v5, 1), v12, v5.f64[0]);
  float64x2_t v16 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v10, v5, 1), (float64x2_t)vextq_s8((int8x16_t)v11, (int8x16_t)v10, 8uLL), v5.f64[0]), v13);
  *(void *)&v11.f64[0]  = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a2->position.vector.f64[2], vaddq_f64(v15, v14));
  float64x2_t result = vaddq_f64(*(float64x2_t *)&a2->position.x, v16);
  *a3  = result;
  a3[1].f64[0]  = v11.f64[0];
  return result;
}

{
  long long v3;
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  int8x16_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  float64x2_t result;

  long long v3 = *(_OWORD *)&a1->vector.f64[2];
  float64x2_t v4 = *(float64x2_t *)a2->rotation.vector.f64;
  float64x2_t v5 = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  float64x2_t v6 = vmulq_f64(v5, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v7 = (int8x16_t)vnegq_f64(v4);
  float64x2_t v8 = (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)vnegq_f64(v6), 8uLL);
  float64x2_t v9 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v4, *(float64x2_t *)&a1->x, 1), (float64x2_t)vextq_s8(v7, (int8x16_t)v4, 8uLL), a1->x), v8, *(double *)&v3);
  float64x2_t v10 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v6, *(float64x2_t *)&a1->x, 1), v8, a1->x), (float64x2_t)vextq_s8((int8x16_t)v4, v7, 8uLL), *(double *)&v3);
  float64x2_t v11 = vnegq_f64(v10);
  float64x2_t v12 = (float64x2_t)vextq_s8((int8x16_t)v9, (int8x16_t)vnegq_f64(v9), 8uLL);
  float64x2_t v13 = vmlaq_n_f64(vmulq_laneq_f64(v9, v4, 1), v12, v4.f64[0]);
  float64x2_t v14 = vmlaq_n_f64(vmulq_laneq_f64(v11, v4, 1), (float64x2_t)vextq_s8((int8x16_t)v10, (int8x16_t)v11, 8uLL), v4.f64[0]);
  float64x2_t v15 = vmlaq_n_f64(vmulq_laneq_f64(v9, v5, 1), v12, v5.f64[0]);
  float64x2_t v16 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v10, v5, 1), (float64x2_t)vextq_s8((int8x16_t)v11, (int8x16_t)v10, 8uLL), v5.f64[0]), v13);
  *(void *)&v11.f64[0]  = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a2->position.vector.f64[2], vaddq_f64(v15, v14));
  float64x2_t result = vaddq_f64(*(float64x2_t *)&a2->position.x, v16);
  *a3  = result;
  a3[1].f64[0]  = v11.f64[0];
  return result;
}

void __swiftcall SPPoint3D.unapplying(_:)(SPPoint3D *__return_ptr retstr, SPPose3D *a2)
{
  SPPoint3D.applying(_:)((long long *)a2, (void (*)(double *__return_ptr, void *, _OWORD *))SPPoint3DUnapplyPose, v2, v3, v4);
}

double SPPoint3D.applying(_:)(long long *a1, void (*a2)(double *__return_ptr, void *, _OWORD *), double a3, double a4, double a5)
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  uint64_t v8 = *((void *)a1 + 6);
  uint64_t v9 = *((void *)a1 + 7);
  *(double *)float64x2_t v15 = a3;
  *(double *)&v15[1]  = a4;
  *(double *)&v15[2]  = a5;
  v12[0]  = v5;
  v12[1]  = v6;
  uint64_t v13 = v8;
  uint64_t v14 = v9;
  v12[2]  = v7;
  a2(&v11, v15, v12);
  return v11;
}

{
  long long v5;
  long long v6;
  long long v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  double v12;
  _OWORD v13[3];
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void v17[4];

  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  uint64_t v8 = *((void *)a1 + 6);
  uint64_t v9 = *((void *)a1 + 7);
  float64x2_t v10 = *((void *)a1 + 8);
  *(double *)double v17 = a3;
  *(double *)&v17[1]  = a4;
  *(double *)&v17[2]  = a5;
  v13[0]  = v5;
  v13[1]  = v6;
  uint64_t v14 = v8;
  float64x2_t v15 = v9;
  v13[2]  = v7;
  float64x2_t v16 = v10;
  a2(&v12, v17, v13);
  return v12;
}

float64x2_t SPPoint3DUnapplyPose@<Q0>(SPPoint3D *a1@<X0>, SPPose3D *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v3 = *(float64x2_t *)a2->rotation.vector.f64;
  float64x2_t v4 = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  float64x2_t v5 = vmulq_f64(v4, (float64x2_t)xmmword_228C1FC40);
  float64x2_t v6 = vnegq_f64(v3);
  v4.f64[0]  = 1.0 / vaddvq_f64(vaddq_f64(vmulq_f64(v3, v3), vmulq_f64(v4, v4)));
  float64x2_t v7 = vmulq_n_f64(v5, v4.f64[0]);
  float64x2_t v8 = vmulq_n_f64(v6, v4.f64[0]);
  long long v9 = *(_OWORD *)&a2->position.vector.f64[2];
  float64x2_t v10 = vmulq_f64(v7, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v11 = (int8x16_t)vnegq_f64(v8);
  float64x2_t v12 = vnegq_f64(*(float64x2_t *)&a2->position.x);
  float64x2_t v13 = (float64x2_t)vextq_s8((int8x16_t)v10, (int8x16_t)vnegq_f64(v10), 8uLL);
  float64x2_t v14 = (float64x2_t)vextq_s8(v11, (int8x16_t)v8, 8uLL);
  float64x2_t v15 = vmulq_laneq_f64(v10, v12, 1);
  float64x2_t v16 = (float64x2_t)vextq_s8((int8x16_t)v8, v11, 8uLL);
  float64x2_t v17 = vmlsq_lane_f64(vmlsq_lane_f64(vmulq_laneq_f64(v8, v12, 1), v14, a2->position.x, 0), v13, *(double *)&v9, 0);
  float64x2_t v18 = vmlsq_lane_f64(vmlsq_lane_f64(v15, v13, a2->position.x, 0), v16, *(double *)&v9, 0);
  float64x2_t v19 = vnegq_f64(v18);
  float64x2_t v20 = (float64x2_t)vextq_s8((int8x16_t)v17, (int8x16_t)vnegq_f64(v17), 8uLL);
  float64x2_t v21 = vmlaq_n_f64(vmulq_laneq_f64(v19, v8, 1), (float64x2_t)vextq_s8((int8x16_t)v18, (int8x16_t)v19, 8uLL), v8.f64[0]);
  float64x2_t v22 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v18, v7, 1), (float64x2_t)vextq_s8((int8x16_t)v19, (int8x16_t)v18, 8uLL), v7.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v17, v8, 1), v20, v8.f64[0]));
  float64x2_t v23 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v17, v7, 1), v20, v7.f64[0]), v21);
  long long v24 = *(_OWORD *)&a1->vector.f64[2];
  float64x2_t v25 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v8, *(float64x2_t *)&a1->x, 1), v14, a1->x), v13, *(double *)&v24);
  float64x2_t v26 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v10, *(float64x2_t *)&a1->x, 1), v13, a1->x), v16, *(double *)&v24);
  float64x2_t v27 = vnegq_f64(v26);
  float64x2_t v28 = (float64x2_t)vextq_s8((int8x16_t)v25, (int8x16_t)vnegq_f64(v25), 8uLL);
  float64x2_t v29 = vmlaq_n_f64(vmulq_laneq_f64(v27, v8, 1), (float64x2_t)vextq_s8((int8x16_t)v26, (int8x16_t)v27, 8uLL), v8.f64[0]);
  float64x2_t v30 = vmlaq_n_f64(vmulq_laneq_f64(v25, v8, 1), v28, v8.f64[0]);
  float64x2_t v31 = (float64x2_t)vextq_s8((int8x16_t)v27, (int8x16_t)v26, 8uLL);
  float64x2_t v32 = vmlaq_n_f64(vmulq_laneq_f64(v25, v7, 1), v28, v7.f64[0]);
  float64x2_t result = vaddq_f64(vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v26, v7, 1), v31, v7.f64[0]), v30), v22);
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = *(_OWORD *)&vaddq_f64(vaddq_f64(v32, v29), v23);
  return result;
}

{
  float64x2_t v3;
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  long long v9;
  float64x2_t v10;
  int8x16_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  float64x2_t v17;
  float64x2_t v18;
  float64x2_t v19;
  float64x2_t v20;
  float64x2_t v21;
  float64x2_t v22;
  float64x2_t v23;
  long long v24;
  float64x2_t v25;
  float64x2_t v26;
  float64x2_t v27;
  float64x2_t v28;
  float64x2_t v29;
  float64x2_t v30;
  float64x2_t v31;
  float64x2_t v32;
  float64x2_t result;

  float64x2_t v3 = *(float64x2_t *)a2->rotation.vector.f64;
  float64x2_t v4 = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  float64x2_t v5 = vmulq_f64(v4, (float64x2_t)xmmword_228C1FC40);
  float64x2_t v6 = vnegq_f64(v3);
  v4.f64[0]  = 1.0 / vaddvq_f64(vaddq_f64(vmulq_f64(v3, v3), vmulq_f64(v4, v4)));
  float64x2_t v7 = vmulq_n_f64(v5, v4.f64[0]);
  float64x2_t v8 = vmulq_n_f64(v6, v4.f64[0]);
  long long v9 = *(_OWORD *)&a2->position.vector.f64[2];
  float64x2_t v10 = vmulq_f64(v7, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v11 = (int8x16_t)vnegq_f64(v8);
  float64x2_t v12 = vnegq_f64(*(float64x2_t *)&a2->position.x);
  float64x2_t v13 = (float64x2_t)vextq_s8((int8x16_t)v10, (int8x16_t)vnegq_f64(v10), 8uLL);
  float64x2_t v14 = (float64x2_t)vextq_s8(v11, (int8x16_t)v8, 8uLL);
  float64x2_t v15 = vmulq_laneq_f64(v10, v12, 1);
  float64x2_t v16 = (float64x2_t)vextq_s8((int8x16_t)v8, v11, 8uLL);
  float64x2_t v17 = vmlsq_lane_f64(vmlsq_lane_f64(vmulq_laneq_f64(v8, v12, 1), v14, a2->position.x, 0), v13, *(double *)&v9, 0);
  float64x2_t v18 = vmlsq_lane_f64(vmlsq_lane_f64(v15, v13, a2->position.x, 0), v16, *(double *)&v9, 0);
  float64x2_t v19 = vnegq_f64(v18);
  float64x2_t v20 = (float64x2_t)vextq_s8((int8x16_t)v17, (int8x16_t)vnegq_f64(v17), 8uLL);
  float64x2_t v21 = vmlaq_n_f64(vmulq_laneq_f64(v19, v8, 1), (float64x2_t)vextq_s8((int8x16_t)v18, (int8x16_t)v19, 8uLL), v8.f64[0]);
  float64x2_t v22 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v18, v7, 1), (float64x2_t)vextq_s8((int8x16_t)v19, (int8x16_t)v18, 8uLL), v7.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v17, v8, 1), v20, v8.f64[0]));
  float64x2_t v23 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v17, v7, 1), v20, v7.f64[0]), v21);
  long long v24 = *(_OWORD *)&a1->vector.f64[2];
  float64x2_t v25 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v8, *(float64x2_t *)&a1->x, 1), v14, a1->x), v13, *(double *)&v24);
  float64x2_t v26 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v10, *(float64x2_t *)&a1->x, 1), v13, a1->x), v16, *(double *)&v24);
  float64x2_t v27 = vnegq_f64(v26);
  float64x2_t v28 = (float64x2_t)vextq_s8((int8x16_t)v25, (int8x16_t)vnegq_f64(v25), 8uLL);
  float64x2_t v29 = vmlaq_n_f64(vmulq_laneq_f64(v27, v8, 1), (float64x2_t)vextq_s8((int8x16_t)v26, (int8x16_t)v27, 8uLL), v8.f64[0]);
  float64x2_t v30 = vmlaq_n_f64(vmulq_laneq_f64(v25, v8, 1), v28, v8.f64[0]);
  float64x2_t v31 = (float64x2_t)vextq_s8((int8x16_t)v27, (int8x16_t)v26, 8uLL);
  float64x2_t v32 = vmlaq_n_f64(vmulq_laneq_f64(v25, v7, 1), v28, v7.f64[0]);
  float64x2_t result = vaddq_f64(vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v26, v7, 1), v31, v7.f64[0]), v30), v22);
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = *(_OWORD *)&vaddq_f64(vaddq_f64(v32, v29), v23);
  return result;
}

double protocol witness for static Primitive3D.zero.getter in conformance SPPoint3D@<D0>(_OWORD *a1@<X8>)
{
  double result = 0.0;
  *a1  = 0u;
  a1[1]  = 0u;
  return result;
}

void protocol witness for static Primitive3D.infinity.getter in conformance SPPoint3D(void *a1@<X8>)
{
  a1[2]  = 0x7FF0000000000000;
  a1[3]  = 0;
  *a1  = 0x7FF0000000000000;
  a1[1]  = 0x7FF0000000000000;
}

unint64_t protocol witness for Primitive3D.isZero.getter in conformance SPPoint3D(double a1, float64x2_t a2)
{
  long long v3 = *(_OWORD *)v2;
  double v4 = v2[3];
  v6.double z = v2[2];
  v6.vector.f64[3]  = v4;
  *(_OWORD *)&v6.x  = v3;
  return SPPoint3DIsZero(&v6, *(double *)&v3, a2);
}

unint64_t protocol witness for Primitive3D.isFinite.getter in conformance SPPoint3D(double a1, float64x2_t a2, double a3, double a4, double a5, float64x2_t a6)
{
  long long v7 = *(_OWORD *)v6;
  double v8 = v6[3];
  v10.double z = v6[2];
  v10.vector.f64[3]  = v8;
  *(_OWORD *)&v10.x  = v7;
  return SPPoint3DIsFinite(&v10, *(double *)&v7, a2, a3, a4, a5, a6);
}

unint64_t protocol witness for Primitive3D.isNaN.getter in conformance SPPoint3D(double a1, float64x2_t a2)
{
  long long v3 = *(_OWORD *)v2;
  double v4 = v2[3];
  v6.double z = v2[2];
  v6.vector.f64[3]  = v4;
  *(_OWORD *)&v6.x  = v3;
  return SPPoint3DIsNaN(&v6, *(double *)&v3, a2);
}

uint64_t protocol witness for Primitive3D.applying(_:) in conformance SPPoint3D(long long *a1, uint64_t a2, uint64_t a3)
{
  return protocol witness for Primitive3D.applying(_:) in conformance SPPoint3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPPoint3DApplyAffineTransform);
}

{
  return protocol witness for Primitive3D.applying(_:) in conformance SPPoint3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPPoint3DApplyProjectiveTransform);
}

{
  return protocol witness for Primitive3D.applying(_:) in conformance SPPoint3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPPoint3DApplyPose);
}

uint64_t protocol witness for Primitive3D.unapplying(_:) in conformance SPPoint3D(long long *a1, uint64_t a2, uint64_t a3)
{
  return protocol witness for Primitive3D.applying(_:) in conformance SPPoint3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPPoint3DUnapplyAffineTransform);
}

{
  return protocol witness for Primitive3D.applying(_:) in conformance SPPoint3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPPoint3DUnapplyProjectiveTransform);
}

{
  return protocol witness for Primitive3D.applying(_:) in conformance SPPoint3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPPoint3DUnapplyPose);
}

uint64_t protocol witness for Primitive3D.applying(_:) in conformance SPPoint3D(long long *a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(long long *, _OWORD *))
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  long long v13 = *v4;
  uint64_t v14 = *((void *)v4 + 3);
  uint64_t v18 = *((void *)v4 + 2);
  uint64_t v19 = v14;
  long long v17 = v13;
  v16[0]  = v5;
  v16[1]  = v6;
  v16[2]  = v7;
  _OWORD v16[3] = v8;
  v16[4]  = v9;
  v16[5]  = v10;
  v16[6]  = v11;
  v16[7]  = v12;
  return a4(&v17, v16);
}

{
  long long *v4;
  long long v5;
  long long v6;
  long long v7;
  uint64_t v8;
  uint64_t v9;
  long long v10;
  uint64_t v11;
  _OWORD v13[3];
  uint64_t v14;
  uint64_t v15;
  long long v16;
  uint64_t v17;
  uint64_t v18;

  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = *((void *)a1 + 6);
  long long v9 = *((void *)a1 + 7);
  long long v10 = *v4;
  long long v11 = *((void *)v4 + 3);
  long long v17 = *((void *)v4 + 2);
  uint64_t v18 = v11;
  float64x2_t v16 = v10;
  v13[0]  = v5;
  v13[1]  = v6;
  uint64_t v14 = v8;
  float64x2_t v15 = v9;
  v13[2]  = v7;
  return a4(&v16, v13);
}

double SPPoint3D.applying(_:)(long long *a1, double a2, double a3, double a4)
{
  return SPPoint3D.applying(_:)(a1, (void (*)(double *__return_ptr, void *, _OWORD *))SPPoint3DApplyScaledPose, a2, a3, a4);
}

float64x2_t SPPoint3DApplyScaledPose@<Q0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>, float64x2_t a4@<Q2>)
{
  a4.f64[0]  = a2[4].f64[0];
  *(void *)&double v4 = *(_OWORD *)&vmulq_f64(a1[1], a4);
  float64x2_t v5 = vmulq_n_f64(*a1, a4.f64[0]);
  float64x2_t v6 = a2[2];
  float64x2_t v7 = a2[3];
  float64x2_t v8 = vmulq_f64(v7, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v9 = (int8x16_t)vnegq_f64(v6);
  float64x2_t v10 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)vnegq_f64(v8), 8uLL);
  float64x2_t v11 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v6, v5, 1), (float64x2_t)vextq_s8(v9, (int8x16_t)v6, 8uLL), v5.f64[0]), v10, v4);
  float64x2_t v12 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v8, v5, 1), v10, v5.f64[0]), (float64x2_t)vextq_s8((int8x16_t)v6, v9, 8uLL), v4);
  float64x2_t v13 = vnegq_f64(v12);
  float64x2_t v14 = (float64x2_t)vextq_s8((int8x16_t)v11, (int8x16_t)vnegq_f64(v11), 8uLL);
  float64x2_t v15 = vmlaq_n_f64(vmulq_laneq_f64(v13, v6, 1), (float64x2_t)vextq_s8((int8x16_t)v12, (int8x16_t)v13, 8uLL), v6.f64[0]);
  float64x2_t v16 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v12, v7, 1), (float64x2_t)vextq_s8((int8x16_t)v13, (int8x16_t)v12, 8uLL), v7.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v11, v6, 1), v14, v6.f64[0]));
  *(void *)&v14.f64[0]  = *(_OWORD *)&vaddq_f64(a2[1], vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v11, v7, 1), v14, v7.f64[0]), v15));
  float64x2_t result = vaddq_f64(*a2, v16);
  *a3  = result;
  a3[1].f64[0]  = v14.f64[0];
  return result;
}

double SPPoint3D.unapplying(_:)(long long *a1, double a2, double a3, double a4)
{
  return SPPoint3D.applying(_:)(a1, (void (*)(double *__return_ptr, void *, _OWORD *))SPPoint3DUnapplyScaledPose, a2, a3, a4);
}

float64_t SPPoint3DUnapplyScaledPose@<D0>(uint64_t a1@<X0>, uint64_t a2@<X1>, float64x2_t *a3@<X8>)
{
  float64x2_t v3 = *(float64x2_t *)(a2 + 32);
  float64x2_t v4 = *(float64x2_t *)(a2 + 48);
  float64x2_t v5 = vmulq_f64(v4, (float64x2_t)xmmword_228C1FC40);
  float64x2_t v6 = vnegq_f64(v3);
  v4.f64[0]  = 1.0 / vaddvq_f64(vaddq_f64(vmulq_f64(v3, v3), vmulq_f64(v4, v4)));
  float64x2_t v7 = vmulq_n_f64(v5, v4.f64[0]);
  float64x2_t v8 = vmulq_n_f64(v6, v4.f64[0]);
  long long v9 = *(_OWORD *)(a2 + 16);
  float64x2_t v10 = vmulq_f64(v7, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v11 = (int8x16_t)vnegq_f64(v8);
  float64x2_t v12 = vnegq_f64(*(float64x2_t *)a2);
  float64x2_t v13 = (float64x2_t)vextq_s8((int8x16_t)v10, (int8x16_t)vnegq_f64(v10), 8uLL);
  float64x2_t v14 = (float64x2_t)vextq_s8(v11, (int8x16_t)v8, 8uLL);
  float64x2_t v15 = (float64x2_t)vextq_s8((int8x16_t)v8, v11, 8uLL);
  float64x2_t v16 = vmlsq_lane_f64(vmlsq_lane_f64(vmulq_laneq_f64(v10, v12, 1), v13, *(double *)a2, 0), v15, *(double *)&v9, 0);
  float64x2_t v17 = vmlsq_lane_f64(vmlsq_lane_f64(vmulq_laneq_f64(v8, v12, 1), v14, *(double *)a2, 0), v13, *(double *)&v9, 0);
  float64x2_t v18 = vnegq_f64(v16);
  float64x2_t v19 = (float64x2_t)vextq_s8((int8x16_t)v17, (int8x16_t)vnegq_f64(v17), 8uLL);
  float64x2_t v20 = vmlaq_n_f64(vmulq_laneq_f64(v17, v8, 1), v19, v8.f64[0]);
  float64x2_t v21 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v17, v7, 1), v19, v7.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v18, v8, 1), (float64x2_t)vextq_s8((int8x16_t)v16, (int8x16_t)v18, 8uLL), v8.f64[0]));
  long long v22 = *(_OWORD *)(a1 + 16);
  float64x2_t v23 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v10, *(float64x2_t *)a1, 1), v13, *(double *)a1), v15, *(double *)&v22);
  float64x2_t v24 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v8, *(float64x2_t *)a1, 1), v14, *(double *)a1), v13, *(double *)&v22);
  float64x2_t v25 = vnegq_f64(v23);
  float64x2_t v26 = (float64x2_t)vextq_s8((int8x16_t)v24, (int8x16_t)vnegq_f64(v24), 8uLL);
  float64x2_t v27 = vmlaq_n_f64(vmulq_laneq_f64(v25, v8, 1), (float64x2_t)vextq_s8((int8x16_t)v23, (int8x16_t)v25, 8uLL), v8.f64[0]);
  float64x2_t v28 = vmlaq_n_f64(vmulq_laneq_f64(v23, v7, 1), (float64x2_t)vextq_s8((int8x16_t)v25, (int8x16_t)v23, 8uLL), v7.f64[0]);
  float64x2_t v29 = vaddq_f64(vaddq_f64(v28, vmlaq_n_f64(vmulq_laneq_f64(v24, v8, 1), v26, v8.f64[0])), vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v16, v7, 1), (float64x2_t)vextq_s8((int8x16_t)v18, (int8x16_t)v16, 8uLL), v7.f64[0]), v20));
  v28.f64[0]  = *(float64_t *)(a2 + 64);
  *(void *)&float64_t result = *(_OWORD *)&vdivq_f64(vaddq_f64(vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v24, v7, 1), v26, v7.f64[0]), v27), v21), v28);
  *a3  = vdivq_f64(v29, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v28.f64[0], 0));
  a3[1].f64[0]  = result;
  return result;
}

float64_t protocol witness for Rotatable3D.rotated(by:) in conformance SPPoint3D@<D0>(float64x2_t *a1@<X8>, SPRotation3D a2@<Q1:Q0>, float64x2_t a3@<Q5>)
{
  long long v4 = *(_OWORD *)v3;
  double v5 = v3[3];
  v8.double z = v3[2];
  v8.vector.f64[3]  = v5;
  v7[1]  = *(float64x2_t *)&a2.quaternion.vector.f64[2];
  *(_OWORD *)&v8.x  = v4;
  v7[0]  = *(float64x2_t *)a2.vector.f64;
  return SPPoint3DRotate(&v8, a2, v7, a1, a3);
}

float64_t protocol witness for Rotatable3D.rotated(by:) in conformance SPPoint3D@<D0>(float64x2_t *a1@<X8>, simd_quatd a2@<Q1:Q0>, float64x2_t a3@<Q7>)
{
  long long v4 = *(_OWORD *)v3;
  double v5 = v3[3];
  v8.double z = v3[2];
  v8.vector.f64[3]  = v5;
  v7[1]  = *(float64x2_t *)&a2.vector.f64[2];
  *(_OWORD *)&v8.x  = v4;
  v7[0]  = *(float64x2_t *)a2.vector.f64;
  return SPPoint3DRotateByQuaternion(&v8, a2, v7, a1, a3);
}

BOOL static SPPoint3D.== infix(_:_:)(double a1, double a2, double a3, double a4, double a5, double a6)
{
  BOOL v6 = a1 == a4;
  if (a2 != a5) {
    BOOL v6 = 0;
  }
  return a3 == a6 && v6;
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance SPPoint3D(double *a1, double *a2)
{
  return *a1 == *a2 && a1[1] == a2[1] && a1[2] == a2[2];
}

Swift::Bool __swiftcall SPPoint3D.isApproximatelyEqual(to:tolerance:)(SPPoint3D *to, Swift::Double tolerance)
{
  v10.x  = v5;
  v10.y  = v6;
  v10.double z = v7;
  v9.x  = tolerance;
  v9.y  = v2;
  v9.double z = v3;
  return SPPoint3DAlmostEqualToPoint(&v10, &v9, v4);
}

BOOL SPPoint3DAlmostEqualToPoint(SPPoint3D *a1, SPPoint3D *a2, double a3)
{
  return vabdd_f64(a1->x, a2->x) < a3 && vabdd_f64(a1->y, a2->y) < a3 && vabdd_f64(a1->z, a2->z) < a3;
}

void SPPoint3D.hash(into:)(__n128 a1, double a2, double a3)
{
  a1.n128_f64[1]  = a2;
  specialized SIMD.hash(into:)(a1, a3);
}

Swift::Int SPPoint3D.hashValue.getter(double a1, double a2, double a3)
{
  Hasher.init(_seed:)();
  v3.n128_f64[0]  = a1;
  v3.n128_f64[1]  = a2;
  specialized SIMD.hash(into:)(v3, a3);
  return Hasher._finalize()();
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPPoint3D()
{
  __n128 v2 = *v0;
  __n128 v3 = v0[1];
  Hasher.init(_seed:)();
  specialized SIMD.hash(into:)(v2, v3.n128_f64[0]);
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance SPPoint3D()
{
  specialized SIMD.hash(into:)(*v0, v0[1].n128_f64[0]);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPPoint3D()
{
  __n128 v2 = *v0;
  __n128 v3 = v0[1];
  Hasher.init(_seed:)();
  specialized SIMD.hash(into:)(v2, v3.n128_f64[0]);
  return Hasher._finalize()();
}

uint64_t protocol witness for static Equatable.== infix(_:_:) in conformance SPPoint3D.CodingKeys(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return specialized == infix<A>(_:_:)(*a1, *a2);
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPPoint3D.CodingKeys()
{
  return Hasher._finalize()();
}

uint64_t protocol witness for Hashable.hash(into:) in conformance SPPoint3D.CodingKeys()
{
  String.hash(into:)();

  return swift_bridgeObjectRelease();
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPPoint3D.CodingKeys()
{
  return Hasher._finalize()();
}

unint64_t protocol witness for RawRepresentable.init(rawValue:) in conformance SPPoint3D.CodingKeys@<X0>(Swift::String *a1@<X0>, unsigned char *a2@<X8>)
{
  unint64_t result = specialized SPPoint3D.CodingKeys.init(rawValue:)(*a1);
  *a2  = result;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance SPPoint3D.CodingKeys(void *a1@<X8>)
{
  *a1  = *v1 + 120;
  a1[1]  = 0xE100000000000000;
}

uint64_t protocol witness for CodingKey.stringValue.getter in conformance SPPoint3D.CodingKeys()
{
  return *v0 + 120;
}

unint64_t protocol witness for CodingKey.init(stringValue:) in conformance SPPoint3D.CodingKeys@<X0>(Swift::String a1@<X1:X0>, unsigned char *a2@<X8>)
{
  unint64_t result = specialized SPPoint3D.CodingKeys.init(rawValue:)(a1);
  *a2  = result;
  return result;
}

uint64_t protocol witness for CodingKey.intValue.getter in conformance SPPoint3D.CodingKeys()
{
  return 0;
}

void protocol witness for CodingKey.init(intValue:) in conformance SPPoint3D.CodingKeys(unsigned char *a1@<X8>)
{
  *a1  = 3;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPPoint3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys();

  return MEMORY[0x270FA00B0](a1, v2);
}

uint64_t protocol witness for CustomDebugStringConvertible.debugDescription.getter in conformance SPPoint3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys();

  return MEMORY[0x270FA00B8](a1, v2);
}

uint64_t SPPoint3D.encode(to:)(void *a1)
{
  uint64_t v3 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedEncodingContainer<SPPoint3D.CodingKeys>);
  uint64_t v4 = *(void *)(v3 - 8);
  MEMORY[0x270FA5388](v3);
  double v6 = &v8[-((v5 + 15) & 0xFFFFFFFFFFFFFFF0)];
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys();
  dispatch thunk of Encoder.container<A>(keyedBy:)();
  v8[15]  = 0;
  KeyedEncodingContainer.encode(_:forKey:)();
  if (!v1)
  {
    v8[14]  = 1;
    KeyedEncodingContainer.encode(_:forKey:)();
    v8[13]  = 2;
    KeyedEncodingContainer.encode(_:forKey:)();
  }
  return (*(uint64_t (**)(unsigned char *, uint64_t))(v4 + 8))(v6, v3);
}

double SPPoint3D.init(from:)(void *a1)
{
  return specialized SPPoint3D.init(from:)(a1);
}

void protocol witness for Decodable.init(from:) in conformance SPPoint3D(void *a1@<X0>, uint64_t a2@<X8>)
{
  double v4 = specialized SPPoint3D.init(from:)(a1);
  if (!v2)
  {
    *(void *)(a2 + 24)  = 0;
    *(double *)a2  = v4;
    *(void *)(a2 + 8)  = v5;
    *(void *)(a2 + 16)  = v6;
  }
}

uint64_t protocol witness for Encodable.encode(to:) in conformance SPPoint3D(void *a1)
{
  return SPPoint3D.encode(to:)(a1);
}

uint64_t SPPoint3D.description.getter()
{
  _StringGuts.grow(_:)(21);
  v0._countAndFlagsBits  = 540702760;
  v0._object  = (void *)0xE400000000000000;
  String.append(_:)(v0);
  Double.write<A>(to:)();
  v1._countAndFlagsBits  = 0x203A79202CLL;
  v1._object  = (void *)0xE500000000000000;
  String.append(_:)(v1);
  Double.write<A>(to:)();
  v2._countAndFlagsBits  = 0x203A7A202CLL;
  v2._object  = (void *)0xE500000000000000;
  String.append(_:)(v2);
  Double.write<A>(to:)();
  v3._countAndFlagsBits  = 41;
  v3._object  = (void *)0xE100000000000000;
  String.append(_:)(v3);
  return 0;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPPoint3D()
{
  return SPPoint3D.description.getter();
}

uint64_t SPPoint3D.customMirror.getter(double a1, double a2, double a3)
{
  uint64_t v6 = type metadata accessor for Mirror.AncestorRepresentation();
  uint64_t v7 = *(void *)(v6 - 8);
  MEMORY[0x270FA5388](v6);
  SPPoint3D v9 = (char *)v19 - ((v8 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v10 = __swift_instantiateConcreteTypeFromMangledName((uint64_t *)&demangling cache variable for type metadata for Mirror.DisplayStyle?);
  MEMORY[0x270FA5388](v10 - 8);
  float64x2_t v12 = (char *)v19 - ((v11 + 15) & 0xFFFFFFFFFFFFFFF0);
  *(double *)float64x2_t v19 = a1;
  *(double *)&v19[1]  = a2;
  *(double *)&v19[2]  = a3;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<(String, Any)>);
  uint64_t v13 = swift_allocObject();
  *(_OWORD *)(v13 + 16)  = xmmword_228C1FC50;
  *(void *)(v13 + 32)  = 120;
  *(void *)(v13 + 40)  = 0xE100000000000000;
  uint64_t v14 = MEMORY[0x263F8D538];
  *(double *)(v13 + 48)  = a1;
  *(void *)(v13 + 72)  = v14;
  *(void *)(v13 + 80)  = 121;
  *(void *)(v13 + 88)  = 0xE100000000000000;
  *(double *)(v13 + 96)  = a2;
  *(void *)(v13 + 120)  = v14;
  *(void *)(v13 + 128)  = 122;
  *(void *)(v13 + 136)  = 0xE100000000000000;
  *(void *)(v13 + 168)  = v14;
  *(double *)(v13 + 144)  = a3;
  uint64_t v15 = *MEMORY[0x263F8E808];
  uint64_t v16 = type metadata accessor for Mirror.DisplayStyle();
  uint64_t v17 = *(void *)(v16 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v17 + 104))(v12, v15, v16);
  (*(void (**)(char *, void, uint64_t, uint64_t))(v17 + 56))(v12, 0, 1, v16);
  (*(void (**)(char *, void, uint64_t))(v7 + 104))(v9, *MEMORY[0x263F8E830], v6);
  type metadata accessor for SPPoint3D(0);
  return Mirror.init<A>(_:children:displayStyle:ancestorRepresentation:)();
}

uint64_t protocol witness for CustomReflectable.customMirror.getter in conformance SPPoint3D()
{
  return SPPoint3D.customMirror.getter(*v0, v0[1], v0[2]);
}

void specialized SIMD.hash(into:)(__n128 a1, double a2)
{
  unint64_t v5 = a1.n128_u64[1];
  if ((a1.n128_u64[0] & 0x7FFFFFFFFFFFFFFFLL) != 0) {
    Swift::UInt64 v2 = a1.n128_u64[0];
  }
  else {
    Swift::UInt64 v2 = 0;
  }
  Hasher._combine(_:)(v2);
  if ((v5 & 0x7FFFFFFFFFFFFFFFLL) != 0) {
    Swift::UInt64 v3 = v5;
  }
  else {
    Swift::UInt64 v3 = 0;
  }
  Hasher._combine(_:)(v3);
  if ((*(void *)&a2 & 0x7FFFFFFFFFFFFFFFLL) != 0) {
    Swift::UInt64 v4 = *(void *)&a2;
  }
  else {
    Swift::UInt64 v4 = 0;
  }
  Hasher._combine(_:)(v4);
}

void *__swift_project_boxed_opaque_existential_1(void *result, uint64_t a2)
{
  if ((*(_DWORD *)(*(void *)(a2 - 8) + 80) & 0x20000) != 0) {
    return (void *)(*result
  }
                    + ((*(_DWORD *)(*(void *)(a2 - 8) + 80) + 16) & ~(unint64_t)*(_DWORD *)(*(void *)(a2 - 8) + 80)));
  return result;
}

unint64_t lazy protocol witness table accessor for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys()
{
  unint64_t result = lazy protocol witness table cache variable for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys);
  }
  return result;
}

void specialized SIMD.hash(into:)(__n128 a1, __n128 a2)
{
  unint64_t v6 = a1.n128_u64[1];
  if ((a1.n128_u64[0] & 0x7FFFFFFFFFFFFFFFLL) != 0) {
    Swift::UInt64 v2 = a1.n128_u64[0];
  }
  else {
    Swift::UInt64 v2 = 0;
  }
  Hasher._combine(_:)(v2);
  if ((v6 & 0x7FFFFFFFFFFFFFFFLL) != 0) {
    Swift::UInt64 v3 = v6;
  }
  else {
    Swift::UInt64 v3 = 0;
  }
  Hasher._combine(_:)(v3);
  if ((a2.n128_u64[0] & 0x7FFFFFFFFFFFFFFFLL) != 0) {
    Swift::UInt64 v4 = a2.n128_u64[0];
  }
  else {
    Swift::UInt64 v4 = 0;
  }
  Hasher._combine(_:)(v4);
  if ((a2.n128_u64[1] & 0x7FFFFFFFFFFFFFFFLL) != 0) {
    Swift::UInt64 v5 = a2.n128_u64[1];
  }
  else {
    Swift::UInt64 v5 = 0;
  }
  Hasher._combine(_:)(v5);
}

unint64_t specialized SPPoint3D.CodingKeys.init(rawValue:)(Swift::String string)
{
  object  = string._object;
  v2._countAndFlagsBits  = string._countAndFlagsBits;
  v2._object  = object;
  unint64_t v3 = _findStringSwitchCase(cases:string:)((Swift::OpaquePointer)&outlined read-only object #0 of SPPoint3D.CodingKeys.init(rawValue:), v2);
  swift_bridgeObjectRelease();
  if (v3 >= 3) {
    return 3;
  }
  else {
    return v3;
  }
}

double specialized SPPoint3D.init(from:)(void *a1)
{
  uint64_t v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedDecodingContainer<SPPoint3D.CodingKeys>);
  uint64_t v5 = *(void *)(v4 - 8);
  MEMORY[0x270FA5388](v4);
  uint64_t v7 = (char *)&v14 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPPoint3D.CodingKeys and conformance SPPoint3D.CodingKeys();
  dispatch thunk of Decoder.container<A>(keyedBy:)();
  if (!v1)
  {
    LOBYTE(v14)  = 0;
    KeyedDecodingContainer.decode(_:forKey:)();
    double v9 = v8;
    LOBYTE(v14)  = 1;
    KeyedDecodingContainer.decode(_:forKey:)();
    double v11 = v10;
    LOBYTE(v14)  = 2;
    KeyedDecodingContainer.decode(_:forKey:)();
    SPPoint3DMake(v9, v11, v13, &v14);
    double v2 = v14;
    (*(void (**)(char *, uint64_t))(v5 + 8))(v7, v4);
  }
  __swift_destroy_boxed_opaque_existential_1(a1);
  return v2;
}

uint64_t instantiation function for generic protocol witness table for SPPoint3D(void *a1)
{
  a1[1]  = lazy protocol witness table accessor for type SPPoint3D and conformance SPPoint3D(&lazy protocol witness table cache variable for type SPPoint3D and conformance SPPoint3D);
  a1[2]  = lazy protocol witness table accessor for type SPPoint3D and conformance SPPoint3D(&lazy protocol witness table cache variable for type SPPoint3D and conformance SPPoint3D);
  uint64_t result = lazy protocol witness table accessor for type SPPoint3D and conformance SPPoint3D(&lazy protocol witness table cache variable for type SPPoint3D and conformance SPPoint3D);
  a1[3]  = result;
  return result;
}

uint64_t lazy protocol witness table accessor for type SPPoint3D and conformance SPPoint3D(unint64_t *a1)
{
  uint64_t result = *a1;
  if (!result)
  {
    type metadata accessor for SPPoint3D(255);
    uint64_t result = swift_getWitnessTable();
    atomic_store(result, a1);
  }
  return result;
}

uint64_t base witness table accessor for Equatable in SPPoint3D()
{
  return lazy protocol witness table accessor for type SPPoint3D and conformance SPPoint3D(&lazy protocol witness table cache variable for type SPPoint3D and conformance SPPoint3D);
}

unsigned char *__swift_memcpy1_1(unsigned char *result, unsigned char *a2)
{
  *uint64_t result = *a2;
  return result;
}

uint64_t getEnumTagSinglePayload for SPPoint3D.CodingKeys(unsigned __int8 *a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0xFE) {
    goto LABEL_17;
  }
  if (a2 + 2 >= 0xFFFF00) {
    int v2 = 4;
  }
  else {
    int v2 = 2;
  }
  if ((a2 + 2) >> 8 < 0xFF) {
    int v3 = 1;
  }
  else {
    int v3 = v2;
  }
  if (v3 == 4)
  {
    int v4 = *(_DWORD *)(a1 + 1);
    if (v4) {
      return (*a1 | (v4 << 8)) - 2;
    }
  }
  else
  {
    if (v3 == 2)
    {
      int v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1)) {
        goto LABEL_17;
      }
      return (*a1 | (v4 << 8)) - 2;
    }
    int v4 = a1[1];
    if (a1[1]) {
      return (*a1 | (v4 << 8)) - 2;
    }
  }
LABEL_17:
  unsigned int v6 = *a1;
  BOOL v7 = v6 >= 3;
  int v8 = v6 - 3;
  if (!v7) {
    int v8 = -1;
  }
  return (v8 + 1);
}

unsigned char *storeEnumTagSinglePayload for SPPoint3D.CodingKeys(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 2 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 2) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFE) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFD)
  {
    unsigned int v6 = ((a2 - 254) >> 8) + 1;
    *uint64_t result = a2 + 2;
    switch(v5)
    {
      case 1:
        result[1]  = v6;
        break;
      case 2:
        *(_WORD *)(result + 1)  = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x228BEFD90);
      case 4:
        *(_DWORD *)(result + 1)  = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1]  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1)  = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1)  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 2;
        break;
    }
  }
  return result;
}

uint64_t getEnumTag for SPPoint3D.CodingKeys(unsigned __int8 *a1)
{
  return *a1;
}

unsigned char *destructiveInjectEnumTag for SPPoint3D.CodingKeys(unsigned char *result, char a2)
{
  *uint64_t result = a2;
  return result;
}

ValueMetadata *type metadata accessor for SPPoint3D.CodingKeys()
{
  return &type metadata for SPPoint3D.CodingKeys;
}

float64_t SPPoint3DRotateByQuaternion@<D0>(SPPoint3D *a1@<X0>, simd_quatd a2@<0:Q0, 16:Q1>, float64x2_t *a3@<X1>, float64x2_t *a4@<X8>, float64x2_t _Q7@<Q7>)
{
  _Q3  = *a3;
  _Q2  = a3[1];
  float64x2_t v9 = *(float64x2_t *)&a1->vector.f64[2];
  _D5  = a3->f64[1];
  __asm { FMLS            D4, D2, V2.D[0] }
  _Q7.f64[0]  = a3[1].f64[1];
  __asm { FMLA            D4, D7, V2.D[1] }
  double v17 = vmlad_n_f64(vmuld_lane_f64(_Q2.f64[0], _Q2, 1), _D5, a3->f64[0]);
  v18.f64[0]  = vmuld_lane_f64(_D5, _Q2, 1);
  v5.f64[0]  = vmlad_n_f64(-(_D5 * _Q7.f64[0]), _Q2.f64[0], a3->f64[0]);
  v5.f64[0]  = v5.f64[0] + v5.f64[0];
  _Q4.f64[1]  = v17 + v17;
  double v19 = vmlad_n_f64(-(_Q2.f64[0] * _Q7.f64[0]), _D5, a3->f64[0]);
  v20.f64[0]  = v19 + v19;
  __asm
  {
    FMLA            D6, D5, V3.D[1]
    FMLA            D6, D7, V2.D[1]
    FMLS            D6, D3, V3.D[0]
    FMLA            D19, D2, V3.D[1]
  }
  _Q19.f64[0]  = _Q19.f64[0] + _Q19.f64[0];
  v20.f64[1]  = _D6;
  float64_t v24 = -(a3->f64[0] * _Q7.f64[0]);
  __asm
  {
    FMLS            D7, D3, V3.D[0]
    FMLS            D7, D5, V3.D[1]
  }
  _Q3.f64[0]  = a3[1].f64[0];
  v18.f64[1]  = v24;
  float64x2_t v25 = vmlaq_f64(v18, (float64x2_t)vzip1q_s64(*(int64x2_t *)a3, (int64x2_t)_Q2), _Q3);
  float64x2_t v26 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q4, a1->x), v20, *(float64x2_t *)&a1->x, 1), vaddq_f64(v25, v25), v9.f64[0]), (float64x2_t)0);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)&a1->x, v5), _Q19, *(float64x2_t *)&a1->x, 1), v9, _Q7), (float64x2_t)0);
  *a4  = v26;
  a4[1].f64[0]  = result;
  return result;
}

float64_t SPPoint3DRotate@<D0>(SPPoint3D *a1@<X0>, SPRotation3D a2@<0:Q0, 16:Q1>, float64x2_t *a3@<X1>, float64x2_t *a4@<X8>, float64x2_t _Q5@<Q5>)
{
  _Q2  = *a3;
  _Q0  = a3[1];
  _D3  = a3->f64[1];
  __asm { FMLS            D1, D0, V0.D[0] }
  _Q5.f64[0]  = a3[1].f64[1];
  __asm { FMLA            D1, D5, V0.D[1] }
  double v15 = vmlad_n_f64(vmuld_lane_f64(_Q0.f64[0], _Q0, 1), _D3, a3->f64[0]);
  v16.f64[0]  = vmuld_lane_f64(_D3, _Q0, 1);
  v5.f64[0]  = vmlad_n_f64(-(_D3 * _Q5.f64[0]), _Q0.f64[0], a3->f64[0]);
  v5.f64[0]  = v5.f64[0] + v5.f64[0];
  _Q1.f64[1]  = v15 + v15;
  double v17 = vmlad_n_f64(-(_Q0.f64[0] * _Q5.f64[0]), _D3, a3->f64[0]);
  v18.f64[0]  = v17 + v17;
  __asm
  {
    FMLA            D4, D3, V2.D[1]
    FMLA            D4, D5, V0.D[1]
    FMLS            D4, D2, V2.D[0]
    FMLA            D17, D0, V2.D[1]
  }
  v18.f64[1]  = _D4;
  float64_t v23 = -(a3->f64[0] * _Q5.f64[0]);
  float64x2_t v24 = (float64x2_t)vzip1q_s64(*(int64x2_t *)a3, (int64x2_t)_Q0);
  __asm
  {
    FMLS            D5, D2, V2.D[0]
    FMLS            D5, D3, V2.D[1]
  }
  _Q2.f64[0]  = a3[1].f64[0];
  _Q0.f64[0]  = _D17 + _D17;
  v16.f64[1]  = v23;
  float64x2_t v25 = vmlaq_f64(v16, v24, _Q2);
  float64x2_t v26 = *(float64x2_t *)&a1->vector.f64[2];
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)&a1->x, v5), _Q0, *(float64x2_t *)&a1->x, 1), v26, _Q5), (float64x2_t)0);
  *a4  = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q1, a1->x), v18, *(float64x2_t *)&a1->x, 1), vaddq_f64(v25, v25), v26.f64[0]), (float64x2_t)0);
  a4[1].f64[0]  = result;
  return result;
}

float64x2_t SPPoint3DUnapplyProjectiveTransform@<Q0>(uint64_t a1@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v5 = a2[2];
  float64x2_t v6 = a2[3];
  float64x2_t v7 = a2[4];
  float64x2_t v8 = a2[5];
  float64x2_t v9 = a2[6];
  float64x2_t v10 = a2[7];
  float64x2_t v11 = (float64x2_t)vextq_s8((int8x16_t)v5, (int8x16_t)v6, 8uLL);
  float64x2_t v12 = (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v5, 8uLL);
  float64x2_t v13 = (float64x2_t)vextq_s8((int8x16_t)v9, (int8x16_t)v10, 8uLL);
  float64x2_t v14 = (float64x2_t)vextq_s8((int8x16_t)v10, (int8x16_t)v9, 8uLL);
  float64x2_t v15 = (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v8, 8uLL);
  float64x2_t v16 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)v7, 8uLL);
  float64x2_t v17 = vnegq_f64(v16);
  float64x2_t v18 = vnegq_f64(v15);
  float64x2_t v19 = vmlaq_f64(vmulq_f64(v9, v18), v13, v7);
  float64x2_t v20 = vmlaq_f64(vmulq_f64(v6, vmlaq_f64(vmulq_f64(v14, v18), v13, v16)), vmlaq_f64(vmulq_f64(v10, v17), v14, v8), v11);
  int64x2_t v21 = (int64x2_t)vmulq_f64(a2[1], vmlaq_f64(vmlaq_f64(vmulq_f64(v5, vmlaq_f64(vmulq_f64(v13, v17), v14, v15)), v19, v12), vmlaq_f64(vmulq_f64(v14, vnegq_f64(v7)), v9, v16), v11));
  int64x2_t v22 = (int64x2_t)vmulq_f64(*a2, vmlaq_f64(v20, vmlaq_f64(vmulq_f64(v13, vnegq_f64(v8)), v10, v15), v12));
  if (vaddvq_f64(vsubq_f64((float64x2_t)vzip1q_s64(v22, v21), (float64x2_t)vzip2q_s64(v22, v21))) == 0.0)
  {
    float64x2_t v23 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    float64x2_t v24 = v23;
    float64x2_t v25 = v23;
    float64x2_t v26 = v23;
    float64x2_t v28 = v23;
    float64x2_t v27 = v23;
    float64x2_t v29 = v23;
    float64x2_t v30 = v23;
  }
  else
  {
    __invert_d4();
    float64x2_t v23 = 0u;
    float64x2_t v24 = 0u;
    float64x2_t v25 = 0u;
    float64x2_t v26 = 0u;
    float64x2_t v28 = 0u;
    float64x2_t v27 = 0u;
    float64x2_t v29 = 0u;
    float64x2_t v30 = 0u;
  }
  long long v31 = *(_OWORD *)(a1 + 16);
  uint64_t v32 = *(_OWORD *)&vaddq_f64(v30, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v24, *(double *)a1), v26, *(float64x2_t *)a1, 1), v27, *(double *)&v31));
  float64x2_t result = vaddq_f64(v29, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v23, *(double *)a1), v25, *(float64x2_t *)a1, 1), v28, *(double *)&v31));
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = v32;
  return result;
}

{
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  float64x2_t v17;
  float64x2_t v18;
  float64x2_t v19;
  float64x2_t v20;
  int64x2_t v21;
  int64x2_t v22;
  float64x2_t v23;
  float64x2_t v24;
  float64x2_t v25;
  float64x2_t v26;
  float64x2_t v27;
  float64x2_t v28;
  float64x2_t v29;
  float64x2_t v30;
  long long v31;
  uint64_t v32;
  float64x2_t result;

  float64x2_t v5 = a2[2];
  float64x2_t v6 = a2[3];
  float64x2_t v7 = a2[4];
  float64x2_t v8 = a2[5];
  float64x2_t v9 = a2[6];
  float64x2_t v10 = a2[7];
  float64x2_t v11 = (float64x2_t)vextq_s8((int8x16_t)v5, (int8x16_t)v6, 8uLL);
  float64x2_t v12 = (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v5, 8uLL);
  float64x2_t v13 = (float64x2_t)vextq_s8((int8x16_t)v9, (int8x16_t)v10, 8uLL);
  float64x2_t v14 = (float64x2_t)vextq_s8((int8x16_t)v10, (int8x16_t)v9, 8uLL);
  float64x2_t v15 = (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v8, 8uLL);
  float64x2_t v16 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)v7, 8uLL);
  float64x2_t v17 = vnegq_f64(v16);
  float64x2_t v18 = vnegq_f64(v15);
  float64x2_t v19 = vmlaq_f64(vmulq_f64(v9, v18), v13, v7);
  float64x2_t v20 = vmlaq_f64(vmulq_f64(v6, vmlaq_f64(vmulq_f64(v14, v18), v13, v16)), vmlaq_f64(vmulq_f64(v10, v17), v14, v8), v11);
  int64x2_t v21 = (int64x2_t)vmulq_f64(a2[1], vmlaq_f64(vmlaq_f64(vmulq_f64(v5, vmlaq_f64(vmulq_f64(v13, v17), v14, v15)), v19, v12), vmlaq_f64(vmulq_f64(v14, vnegq_f64(v7)), v9, v16), v11));
  int64x2_t v22 = (int64x2_t)vmulq_f64(*a2, vmlaq_f64(v20, vmlaq_f64(vmulq_f64(v13, vnegq_f64(v8)), v10, v15), v12));
  if (vaddvq_f64(vsubq_f64((float64x2_t)vzip1q_s64(v22, v21), (float64x2_t)vzip2q_s64(v22, v21))) == 0.0)
  {
    float64x2_t v23 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    float64x2_t v24 = v23;
    float64x2_t v25 = v23;
    float64x2_t v26 = v23;
    float64x2_t v28 = v23;
    float64x2_t v27 = v23;
    float64x2_t v29 = v23;
    float64x2_t v30 = v23;
  }
  else
  {
    __invert_d4();
    float64x2_t v23 = 0u;
    float64x2_t v24 = 0u;
    float64x2_t v25 = 0u;
    float64x2_t v26 = 0u;
    float64x2_t v28 = 0u;
    float64x2_t v27 = 0u;
    float64x2_t v29 = 0u;
    float64x2_t v30 = 0u;
  }
  long long v31 = *(_OWORD *)(a1 + 16);
  uint64_t v32 = *(_OWORD *)&vaddq_f64(v30, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v24, *(double *)a1), v26, *(float64x2_t *)a1, 1), v27, *(double *)&v31));
  float64x2_t result = vaddq_f64(v29, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v23, *(double *)a1), v25, *(float64x2_t *)a1, 1), v28, *(double *)&v31));
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = v32;
  return result;
}

float64_t SPPoint3DUnapplyAffineTransform@<D0>(float64x2_t *a1@<X0>, uint64_t a2@<X1>, float64x2_t *a3@<X8>)
{
  v5.f64[0]  = *(float64_t *)(a2 + 80);
  v5.f64[1]  = *(float64_t *)(a2 + 64);
  v6.f64[0]  = *(float64_t *)(a2 + 48);
  v6.f64[1]  = *(float64_t *)(a2 + 32);
  if (vmulq_f64(*(float64x2_t *)(a2 + 16), vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)(a2 + 64), vnegq_f64((float64x2_t)vdupq_laneq_s64(*(int64x2_t *)(a2 + 32), 1))), *(float64x2_t *)(a2 + 32), *(float64x2_t *)(a2 + 64), 1)).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)a2, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 64), *(int8x16_t *)(a2 + 80), 8uLL), vnegq_f64(v6)), v5, (float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 32), *(int8x16_t *)(a2 + 48), 8uLL)))) == 0.0)
  {
    float64x2_t v7 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    float64x2_t v9 = v7;
    float64x2_t v8 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v10 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v12 = v7;
    float64x2_t v11 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v14 = v7;
    float64x2_t v13 = (float64x2_t)xmmword_228C1FC60;
  }
  else
  {
    float64x2_t v18 = *(float64x2_t *)(a2 + 96);
    float64x2_t v19 = *(float64x2_t *)(a2 + 112);
    __invert_d3();
    float64x2_t v7 = 0u;
    float64x2_t v8 = 0u;
    float64x2_t v9 = 0u;
    float64x2_t v10 = 0u;
    float64x2_t v12 = 0u;
    float64x2_t v11 = 0u;
    float64x2_t v13 = vnegq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v18, (float64x2_t)0), (float64x2_t)0, v18, 1), v19, (float64x2_t)0));
    float64x2_t v14 = vnegq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v18.f64[0]), (float64x2_t)0, v18, 1), (float64x2_t)0, v19.f64[0]));
  }
  float64x2_t v15 = a1[1];
  float64x2_t v16 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v7, a1->f64[0]), v9, *a1, 1), v12, v15.f64[0]);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(v13, vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v8, *a1), v10, *a1, 1), v15, v11));
  *a3  = vaddq_f64(v14, v16);
  a3[1].f64[0]  = result;
  return result;
}

{
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  float64_t result;
  float64x2_t v18;
  float64x2_t v19;

  v5.f64[0]  = *(float64_t *)(a2 + 80);
  v5.f64[1]  = *(float64_t *)(a2 + 64);
  v6.f64[0]  = *(float64_t *)(a2 + 48);
  v6.f64[1]  = *(float64_t *)(a2 + 32);
  if (vmulq_f64(*(float64x2_t *)(a2 + 16), vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)(a2 + 64), vnegq_f64((float64x2_t)vdupq_laneq_s64(*(int64x2_t *)(a2 + 32), 1))), *(float64x2_t *)(a2 + 32), *(float64x2_t *)(a2 + 64), 1)).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)a2, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 64), *(int8x16_t *)(a2 + 80), 8uLL), vnegq_f64(v6)), v5, (float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 32), *(int8x16_t *)(a2 + 48), 8uLL)))) == 0.0)
  {
    float64x2_t v7 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    float64x2_t v9 = v7;
    float64x2_t v8 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v10 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v12 = v7;
    float64x2_t v11 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v14 = v7;
    float64x2_t v13 = (float64x2_t)xmmword_228C1FC60;
  }
  else
  {
    float64x2_t v18 = *(float64x2_t *)(a2 + 96);
    float64x2_t v19 = *(float64x2_t *)(a2 + 112);
    __invert_d3();
    float64x2_t v7 = 0u;
    float64x2_t v8 = 0u;
    float64x2_t v9 = 0u;
    float64x2_t v10 = 0u;
    float64x2_t v12 = 0u;
    float64x2_t v11 = 0u;
    float64x2_t v13 = vnegq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v18, (float64x2_t)0), (float64x2_t)0, v18, 1), v19, (float64x2_t)0));
    float64x2_t v14 = vnegq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v18.f64[0]), (float64x2_t)0, v18, 1), (float64x2_t)0, v19.f64[0]));
  }
  float64x2_t v15 = a1[1];
  float64x2_t v16 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v7, a1->f64[0]), v9, *a1, 1), v12, v15.f64[0]);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(v13, vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v8, *a1), v10, *a1, 1), v15, v11));
  *a3  = vaddq_f64(v14, v16);
  a3[1].f64[0]  = result;
  return result;
}

float64_t SPPoint3DApplyProjectiveTransform@<D0>(uint64_t a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>)
{
  long long v3 = *(_OWORD *)(a1 + 16);
  float64x2_t v4 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, *(double *)a1), a2[2], *(float64x2_t *)a1, 1), a2[4], *(double *)&v3);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(a2[7], vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(a2[1], *(double *)a1), a2[3], *(float64x2_t *)a1, 1), a2[5], *(double *)&v3));
  *a3  = vaddq_f64(a2[6], v4);
  a3[1].f64[0]  = result;
  return result;
}

{
  long long v3;
  float64x2_t v4;
  float64_t result;

  long long v3 = *(_OWORD *)(a1 + 16);
  float64x2_t v4 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, *(double *)a1), a2[2], *(float64x2_t *)a1, 1), a2[4], *(double *)&v3);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(a2[7], vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(a2[1], *(double *)a1), a2[3], *(float64x2_t *)a1, 1), a2[5], *(double *)&v3));
  *a3  = vaddq_f64(a2[6], v4);
  a3[1].f64[0]  = result;
  return result;
}

{
  long long v3;
  float64x2_t v4;
  float64_t result;

  long long v3 = *(_OWORD *)(a1 + 16);
  float64x2_t v4 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, *(double *)a1), a2[2], *(float64x2_t *)a1, 1), a2[4], *(double *)&v3);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(a2[7], vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(a2[1], *(double *)a1), a2[3], *(float64x2_t *)a1, 1), a2[5], *(double *)&v3));
  *a3  = vaddq_f64(a2[6], v4);
  a3[1].f64[0]  = result;
  return result;
}

float64_t SPPoint3DApplyAffineTransform@<D0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>)
{
  float64x2_t v3 = a1[1];
  float64x2_t v4 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, a1->f64[0]), a2[2], *a1, 1), a2[4], v3.f64[0]);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(a2[7], vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*a1, a2[1]), a2[3], *a1, 1), v3, a2[5]));
  *a3  = vaddq_f64(a2[6], v4);
  a3[1].f64[0]  = result;
  return result;
}

{
  float64x2_t v3;
  float64x2_t v4;
  float64_t result;

  float64x2_t v3 = a1[1];
  float64x2_t v4 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, a1->f64[0]), a2[2], *a1, 1), a2[4], v3.f64[0]);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(a2[7], vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*a1, a2[1]), a2[3], *a1, 1), v3, a2[5]));
  *a3  = vaddq_f64(a2[6], v4);
  a3[1].f64[0]  = result;
  return result;
}

{
  float64x2_t v3;
  float64x2_t v4;
  float64_t result;

  float64x2_t v3 = a1[1];
  float64x2_t v4 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, a1->f64[0]), a2[2], *a1, 1), a2[4], v3.f64[0]);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(a2[7], vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*a1, a2[1]), a2[3], *a1, 1), v3, a2[5]));
  *a3  = vaddq_f64(a2[6], v4);
  a3[1].f64[0]  = result;
  return result;
}

unint64_t SPPoint3DIsNaN(SPPoint3D *a1, double a2, float64x2_t a3)
{
  a3.f64[0]  = a1->z;
  int8x16_t v3 = vorrq_s8((int8x16_t)vcltzq_f64(*(float64x2_t *)&a1->x), (int8x16_t)vcgezq_f64(*(float64x2_t *)&a1->x));
  return vornq_s8(vornq_s8((int8x16_t)vdupq_laneq_s64((int64x2_t)vmvnq_s8(v3), 1), vorrq_s8((int8x16_t)vcltzq_f64(a3), (int8x16_t)vcgezq_f64(a3))), v3).u64[0] >> 63;
}

unint64_t SPPoint3DIsFinite(SPPoint3D *a1, double a2, float64x2_t a3, double a4, double a5, double a6, float64x2_t a7)
{
  a3.f64[0]  = a1->z;
  a7.f64[0]  = INFINITY;
  int64x2_t v7 = (int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(*(float64x2_t *)&a1->x), (int8x16_t)vcgezq_f64(*(float64x2_t *)&a1->x)), (int8x16_t)vceqq_f64(vabsq_f64(*(float64x2_t *)&a1->x), (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL)));
  return vandq_s8((int8x16_t)vdupq_laneq_s64(v7, 1), vandq_s8((int8x16_t)vshlq_n_s64((int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(a3), (int8x16_t)vcgezq_f64(a3)), (int8x16_t)vceqq_f64(vabsq_f64(a3), a7)), 0x3FuLL), (int8x16_t)v7)).u64[0] >> 63;
}

unint64_t SPPoint3DIsZero(SPPoint3D *a1, double a2, float64x2_t a3)
{
  a3.f64[0]  = a1->z;
  int64x2_t v3 = vceqzq_f64(*(float64x2_t *)&a1->x);
  return vandq_s8((int8x16_t)vdupq_laneq_s64(v3, 1), vandq_s8((int8x16_t)vceqzq_f64(a3), (int8x16_t)v3)).u64[0] >> 63;
}

uint64_t __swift_destroy_boxed_opaque_existential_1(void *a1)
{
  uint64_t v1 = *(void *)(a1[3] - 8);
  if ((*(unsigned char *)(v1 + 82) & 2) != 0) {
    return MEMORY[0x270FA0520](*a1);
  }
  else {
    return (*(uint64_t (**)(void))(v1 + 8))();
  }
}

double static SPPoint3D.+ infix(_:_:)(double a1, double a2, double a3, double a4, double a5, double a6)
{
  return static SPPoint3D.+ infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

{
  return static SPPoint3D.+ infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

uint64_t static SPPoint3D.+= infix(_:_:)(float64x2_t *a1, float64x2_t a2, float64_t a3, double a4)
{
  return static SPPoint3D.+= infix(_:_:)(a1, (uint64_t (*)(_OWORD *))SPSize3DMakeWithVector, a2, a3, a4);
}

{
  return static SPPoint3D.+= infix(_:_:)(a1, (uint64_t (*)(_OWORD *))SPSize3DMakeWithVector, a2, a3, a4);
}

double static SPPoint3D.- infix(_:_:)(double a1, double a2, double a3, double a4, double a5, double a6)
{
  return static SPPoint3D.- infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

{
  return static SPPoint3D.- infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

{
  return static SPPoint3D.- infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

uint64_t static SPPoint3D.-= infix(_:_:)(float64x2_t *a1, float64x2_t a2, float64_t a3, double a4)
{
  return static SPPoint3D.-= infix(_:_:)(a1, (uint64_t (*)(_OWORD *))SPSize3DMakeWithVector, a2, a3, a4);
}

{
  return static SPPoint3D.-= infix(_:_:)(a1, (uint64_t (*)(_OWORD *))SPSize3DMakeWithVector, a2, a3, a4);
}

double static SPPoint3D.- prefix(_:)(float64x2_t a1, float64_t a2, double a3)
{
  return static SPPoint3D.- prefix(_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3);
}

double static SPPoint3D.* infix(_:_:)(double a1, float64x2_t a2, float64_t a3, double a4)
{
  return static SPPoint3D.* infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4);
}

uint64_t static SPPoint3D.*= infix(_:_:)(float64x2_t *a1, double a2)
{
  return static SPPoint3D.*= infix(_:_:)(a1, (uint64_t (*)(_OWORD *))SPSize3DMakeWithVector, a2);
}

double static SPPoint3D.* infix(_:_:)(float64x2_t a1, float64_t a2, double a3, double a4)
{
  return static SPPoint3D.* infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4);
}

double static SPPoint3D./ infix(_:_:)(float64x2_t a1, float64_t a2, double a3, double a4)
{
  return static SPPoint3D./ infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4);
}

uint64_t static SPPoint3D./= infix(_:_:)(float64x2_t *a1, double a2)
{
  return static SPPoint3D./= infix(_:_:)(a1, (uint64_t (*)(_OWORD *))SPSize3DMakeWithVector, a2);
}

double static SPPoint3D.* infix(_:_:)(long long *a1, double a2, double a3, double a4)
{
  return static SPPoint3D.* infix(_:_:)(a1, (void (*)(double *__return_ptr, void *, _OWORD *))SPPoint3DApplyAffineTransform, a2, a3, a4);
}

{
  return static SPPoint3D.* infix(_:_:)(a1, (void (*)(double *__return_ptr, void *, _OWORD *))SPPoint3DApplyProjectiveTransform, a2, a3, a4);
}

double static SPPoint3D.* infix(_:_:)(long long *a1, void (*a2)(double *__return_ptr, void *, _OWORD *), double a3, double a4, double a5)
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  *(double *)float64x2_t v16 = a3;
  *(double *)&v16[1]  = a4;
  *(double *)&v16[2]  = a5;
  v15[0]  = v5;
  v15[1]  = v6;
  v15[2]  = v7;
  v15[3]  = v8;
  v15[4]  = v9;
  v15[5]  = v10;
  v15[6]  = v11;
  v15[7]  = v12;
  a2(&v14, v16, v15);
  return v14;
}

float64_t static SPPoint3D.* infix(_:_:)(double *a1, double a2, double a3, double a4)
{
  long long v4 = *(_OWORD *)a1;
  long long v5 = *((_OWORD *)a1 + 1);
  long long v6 = *((_OWORD *)a1 + 2);
  double v7 = a1[6];
  double v8 = a1[7];
  v12.x  = a2;
  v12.y  = a3;
  v12.double z = a4;
  *(_OWORD *)&v11.position.x  = v4;
  *(_OWORD *)&v11.position.vector.f64[2]  = v5;
  v11.rotation.vector.f64[2]  = v7;
  v11.rotation.vector.f64[3]  = v8;
  *(_OWORD *)v11.rotation.vector.f64  = v6;
  SPPoint3DApplyPose(&v12, &v11, &v10);
  return v10.f64[0];
}

double protocol witness for static AdditiveArithmetic.+ infix(_:_:) in conformance SPSize3D@<D0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, __n128 *a3@<X8>)
{
  *(double *)&unint64_t v3 = a1[1].f64[0] + a2[1].f64[0];
  v5[0]  = (__n128)vaddq_f64(*a1, *a2);
  v5[1]  = (__n128)v3;
  *(void *)&double result = SPSize3DMakeWithVector(v5, a3).n128_u64[0];
  return result;
}

double static SPSize3D.+ infix(_:_:)(double a1, double a2, double a3, double a4, double a5, double a6)
{
  return static SPPoint3D.+ infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

double protocol witness for static AdditiveArithmetic.+= infix(_:_:) in conformance SPSize3D(float64x2_t *a1, float64x2_t *a2)
{
  *(double *)&unint64_t v2 = a2[1].f64[0] + a1[1].f64[0];
  v4[0]  = (__n128)vaddq_f64(*a2, *a1);
  v4[1]  = (__n128)v2;
  *(void *)&double result = SPSize3DMakeWithVector(v4, (__n128 *)a1).n128_u64[0];
  return result;
}

uint64_t static SPSize3D.+= infix(_:_:)(float64x2_t *a1, float64x2_t a2, float64_t a3, double a4)
{
  return static SPPoint3D.+= infix(_:_:)(a1, (uint64_t (*)(_OWORD *))SPSize3DMakeWithVector, a2, a3, a4);
}

{
  return static SPPoint3D.+= infix(_:_:)(a1, (uint64_t (*)(_OWORD *))SPSize3DMakeWithVector, a2, a3, a4);
}

double protocol witness for static AdditiveArithmetic.- infix(_:_:) in conformance SPSize3D@<D0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, __n128 *a3@<X8>)
{
  *(double *)&unint64_t v3 = a1[1].f64[0] - a2[1].f64[0];
  v5[0]  = (__n128)vsubq_f64(*a1, *a2);
  v5[1]  = (__n128)v3;
  *(void *)&double result = SPSize3DMakeWithVector(v5, a3).n128_u64[0];
  return result;
}

double static SPSize3D.- infix(_:_:)(double a1, double a2, double a3, double a4, double a5, double a6)
{
  return static SPPoint3D.- infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

double protocol witness for static AdditiveArithmetic.-= infix(_:_:) in conformance SPSize3D(float64x2_t *a1, float64x2_t *a2)
{
  *(double *)&unint64_t v2 = a1[1].f64[0] - a2[1].f64[0];
  v4[0]  = (__n128)vsubq_f64(*a1, *a2);
  v4[1]  = (__n128)v2;
  *(void *)&double result = SPSize3DMakeWithVector(v4, (__n128 *)a1).n128_u64[0];
  return result;
}

uint64_t static SPSize3D.-= infix(_:_:)(float64x2_t *a1, float64x2_t a2, float64_t a3, double a4)
{
  return static SPPoint3D.-= infix(_:_:)(a1, (uint64_t (*)(_OWORD *))SPSize3DMakeWithVector, a2, a3, a4);
}

{
  return static SPPoint3D.-= infix(_:_:)(a1, (uint64_t (*)(_OWORD *))SPSize3DMakeWithVector, a2, a3, a4);
}

double static SPSize3D.- prefix(_:)(float64x2_t a1, float64_t a2, double a3)
{
  return static SPPoint3D.- prefix(_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3);
}

__n128 SPSize3DMakeWithVector@<Q0>(__n128 *a1@<X0>, __n128 *a2@<X8>)
{
  __n128 result = *a1;
  unint64_t v3 = a1[1].n128_u64[0];
  *a2  = *a1;
  a2[1].n128_u64[0]  = v3;
  return result;
}

{
  __n128 result;
  unint64_t v3;

  __n128 result = *a1;
  unint64_t v3 = a1[1].n128_u64[0];
  *a2  = *a1;
  a2[1].n128_u64[0]  = v3;
  return result;
}

double static SPSize3D.* infix(_:_:)(double a1, float64x2_t a2, float64_t a3, double a4)
{
  return static SPPoint3D.* infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4);
}

double static SPSize3D.* infix(_:_:)(float64x2_t a1, float64_t a2, double a3, double a4)
{
  return static SPPoint3D.* infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4);
}

uint64_t static SPSize3D.*= infix(_:_:)(float64x2_t *a1, double a2)
{
  return static SPPoint3D.*= infix(_:_:)(a1, (uint64_t (*)(_OWORD *))SPSize3DMakeWithVector, a2);
}

double static SPSize3D./ infix(_:_:)(float64x2_t a1, float64_t a2, double a3, double a4)
{
  return static SPPoint3D./ infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4);
}

uint64_t static SPSize3D./= infix(_:_:)(float64x2_t *a1, double a2)
{
  return static SPPoint3D./= infix(_:_:)(a1, (uint64_t (*)(_OWORD *))SPSize3DMakeWithVector, a2);
}

double static SPSize3D.* infix(_:_:)(long long *a1, double a2, double a3, double a4)
{
  return static SPSize3D.* infix(_:_:)(a1, (void (*)(double *__return_ptr, void *, _OWORD *))SPSize3DApplyAffineTransform, a2, a3, a4);
}

{
  return static SPSize3D.* infix(_:_:)(a1, (void (*)(double *__return_ptr, void *, _OWORD *))SPSize3DApplyProjectiveTransform, a2, a3, a4);
}

double static SPSize3D.* infix(_:_:)(long long *a1, void (*a2)(double *__return_ptr, void *, _OWORD *), double a3, double a4, double a5)
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  *(double *)float64x2_t v16 = a3;
  *(double *)&v16[1]  = a4;
  *(double *)&v16[2]  = a5;
  v15[0]  = v5;
  v15[1]  = v6;
  v15[2]  = v7;
  v15[3]  = v8;
  v15[4]  = v9;
  v15[5]  = v10;
  v15[6]  = v11;
  v15[7]  = v12;
  a2(&v14, v16, v15);
  return v14;
}

float64_t static SPVector3D.* infix(_:_:)(uint64_t a1, float64_t a2, float64_t a3, double a4, double a5, double a6, double a7, double a8, float64x2_t a9)
{
  float64x2_t v9 = *(float64x2_t *)a1;
  float64x2_t v10 = *(float64x2_t *)(a1 + 16);
  float64x2_t v11 = *(float64x2_t *)(a1 + 32);
  uint64_t v12 = *(void *)(a1 + 48);
  uint64_t v13 = *(void *)(a1 + 56);
  v19.f64[0]  = a2;
  v19.f64[1]  = a3;
  double v20 = a4;
  v16[0]  = v9;
  v16[1]  = v10;
  uint64_t v17 = v12;
  uint64_t v18 = v13;
  v16[2]  = v11;
  SPSize3DApplyPose(&v19, v16, &v15, a9);
  return v15.f64[0];
}

double static SPRect3D.* infix(_:_:)@<D0>(long long *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  return static SPRect3D.* infix(_:_:)(a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRect3DApplyAffineTransform, a3);
}

{
  return static SPRect3D.* infix(_:_:)(a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRect3DApplyProjectiveTransform, a3);
}

float64x2_t SPRect3DApplyAffineTransform@<Q0>(uint64_t a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>, float64x2_t a4@<Q4>, float64x2_t a5@<Q5>, float64x2_t a6@<Q6>, float64x2_t a7@<Q7>)
{
  uint64_t v7 = 0;
  uint64_t v43 = *MEMORY[0x263EF8340];
  long long v41 = 0u;
  *(_OWORD *)float64x2_t v42 = 0u;
  float64x2_t v39 = 0u;
  long long v40 = 0u;
  long long v37 = 0u;
  long long v38 = 0u;
  long long v35 = 0u;
  long long v36 = 0u;
  long long v33 = 0u;
  long long v34 = 0u;
  float64x2_t v31 = 0u;
  long long v32 = 0u;
  long long v29 = 0u;
  long long v30 = 0u;
  memset(v28, 0, sizeof(v28));
  float64x2_t v8 = *(float64x2_t *)(a1 + 32);
  double v9 = *(double *)(a1 + 48);
  do
  {
    float64x2_t v10 = &v28[v7];
    long long v11 = *(_OWORD *)(a1 + 16);
    *float64x2_t v10 = *(_OWORD *)a1;
    v10[1]  = v11;
    v7 += 2;
  }
  while (v7 != 16);
  uint64_t v12 = 0;
  *((double *)&v29 + 1)  = v8.f64[1] + *((double *)&v29 + 1);
  float64x2_t v31 = vaddq_f64(v8, v31);
  *(double *)&long long v33 = v8.f64[0] + *(double *)&v33;
  *(double *)&long long v36 = v9 + *(double *)&v36;
  *((double *)&v37 + 1)  = v8.f64[1] + *((double *)&v37 + 1);
  *(double *)&long long v38 = v9 + *(double *)&v38;
  float64x2_t v39 = vaddq_f64(v8, v39);
  *(double *)&long long v40 = v9 + *(double *)&v40;
  *(double *)&long long v41 = v8.f64[0] + *(double *)&v41;
  v42[0]  = v9 + v42[0];
  float64x2_t v13 = *a2;
  float64x2_t v14 = a2[2];
  float64x2_t v15 = a2[4];
  float64x2_t v16 = a2[6];
  a4.f64[0]  = a2[1].f64[0];
  a5.f64[0]  = a2[3].f64[0];
  a6.f64[0]  = a2[5].f64[0];
  a7.f64[0]  = a2[7].f64[0];
  *(void *)&v13.f64[1]  = vextq_s8((int8x16_t)v13, (int8x16_t)v13, 8uLL).u64[0];
  *(void *)&v14.f64[1]  = vextq_s8((int8x16_t)v14, (int8x16_t)v14, 8uLL).u64[0];
  *(void *)&v15.f64[1]  = vextq_s8((int8x16_t)v15, (int8x16_t)v15, 8uLL).u64[0];
  *(void *)&v16.f64[1]  = vextq_s8((int8x16_t)v16, (int8x16_t)v16, 8uLL).u64[0];
  do
  {
    uint64_t v17 = &v28[v12];
    float64x2_t v19 = (float64x2_t)v28[v12];
    float64x2_t v18 = (float64x2_t)v28[v12 + 1];
    *uint64_t v17 = vaddq_f64(v16, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v13, v19.f64[0]), v14, v19, 1), v15, v18.f64[0]));
    *((void *)v17 + 2)  = *(_OWORD *)&vaddq_f64(a7, vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v19, a4), a5, v19, 1), v18, a6));
    v12 += 2;
  }
  while (v12 != 16);
  uint64_t v20 = 0;
  a3[2]  = 0u;
  a3[3]  = 0u;
  a3[1]  = 0u;
  float64x2_t v21 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v22.f64[0]  = INFINITY;
  float64x2_t v23 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
  v24.f64[0]  = -INFINITY;
  do
  {
    float64x2_t v25 = (float64x2_t)v28[v20];
    long long v26 = v28[v20 + 1];
    float64x2_t v21 = vminnmq_f64(v21, v25);
    float64x2_t v22 = vminnmq_f64((float64x2_t)*(unint64_t *)&v22.f64[0], (float64x2_t)(unint64_t)v26);
    float64x2_t v23 = vmaxnmq_f64(v23, v25);
    float64x2_t v24 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v24.f64[0], (float64x2_t)(unint64_t)v26);
    v20 += 2;
  }
  while (v20 != 16);
  *a3  = v21;
  a3[1].f64[0]  = v22.f64[0];
  float64x2_t result = vsubq_f64(v23, v21);
  a3[2]  = result;
  *(void *)&a3[3].f64[0]  = *(_OWORD *)&vsubq_f64(v24, v22);
  return result;
}

{
  uint64_t v7;
  float64x2_t v8;
  double v9;
  _OWORD *v10;
  long long v11;
  uint64_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  _OWORD *v17;
  float64x2_t v18;
  float64x2_t v19;
  uint64_t v20;
  float64x2_t v21;
  float64x2_t v22;
  float64x2_t v23;
  float64x2_t v24;
  float64x2_t v25;
  long long v26;
  float64x2_t result;
  _OWORD v28[2];
  long long v29;
  long long v30;
  float64x2_t v31;
  long long v32;
  long long v33;
  long long v34;
  long long v35;
  long long v36;
  long long v37;
  long long v38;
  float64x2_t v39;
  long long v40;
  long long v41;
  double v42[3];
  uint64_t v43;

  uint64_t v7 = 0;
  uint64_t v43 = *MEMORY[0x263EF8340];
  long long v41 = 0u;
  *(_OWORD *)float64x2_t v42 = 0u;
  float64x2_t v39 = 0u;
  long long v40 = 0u;
  long long v37 = 0u;
  long long v38 = 0u;
  long long v35 = 0u;
  long long v36 = 0u;
  long long v33 = 0u;
  long long v34 = 0u;
  float64x2_t v31 = 0u;
  long long v32 = 0u;
  long long v29 = 0u;
  long long v30 = 0u;
  memset(v28, 0, sizeof(v28));
  float64x2_t v8 = *(float64x2_t *)(a1 + 32);
  double v9 = *(double *)(a1 + 48);
  do
  {
    float64x2_t v10 = &v28[v7];
    long long v11 = *(_OWORD *)(a1 + 16);
    *float64x2_t v10 = *(_OWORD *)a1;
    v10[1]  = v11;
    v7 += 2;
  }
  while (v7 != 16);
  uint64_t v12 = 0;
  *((double *)&v29 + 1)  = v8.f64[1] + *((double *)&v29 + 1);
  float64x2_t v31 = vaddq_f64(v8, v31);
  *(double *)&long long v33 = v8.f64[0] + *(double *)&v33;
  *(double *)&long long v36 = v9 + *(double *)&v36;
  *((double *)&v37 + 1)  = v8.f64[1] + *((double *)&v37 + 1);
  *(double *)&long long v38 = v9 + *(double *)&v38;
  float64x2_t v39 = vaddq_f64(v8, v39);
  *(double *)&long long v40 = v9 + *(double *)&v40;
  *(double *)&long long v41 = v8.f64[0] + *(double *)&v41;
  v42[0]  = v9 + v42[0];
  float64x2_t v13 = *a2;
  float64x2_t v14 = a2[2];
  float64x2_t v15 = a2[4];
  float64x2_t v16 = a2[6];
  a4.f64[0]  = a2[1].f64[0];
  a5.f64[0]  = a2[3].f64[0];
  a6.f64[0]  = a2[5].f64[0];
  a7.f64[0]  = a2[7].f64[0];
  *(void *)&v13.f64[1]  = vextq_s8((int8x16_t)v13, (int8x16_t)v13, 8uLL).u64[0];
  *(void *)&v14.f64[1]  = vextq_s8((int8x16_t)v14, (int8x16_t)v14, 8uLL).u64[0];
  *(void *)&v15.f64[1]  = vextq_s8((int8x16_t)v15, (int8x16_t)v15, 8uLL).u64[0];
  *(void *)&v16.f64[1]  = vextq_s8((int8x16_t)v16, (int8x16_t)v16, 8uLL).u64[0];
  do
  {
    uint64_t v17 = &v28[v12];
    float64x2_t v19 = (float64x2_t)v28[v12];
    float64x2_t v18 = (float64x2_t)v28[v12 + 1];
    *uint64_t v17 = vaddq_f64(v16, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v13, v19.f64[0]), v14, v19, 1), v15, v18.f64[0]));
    *((void *)v17 + 2)  = *(_OWORD *)&vaddq_f64(a7, vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v19, a4), a5, v19, 1), v18, a6));
    v12 += 2;
  }
  while (v12 != 16);
  uint64_t v20 = 0;
  a3[2]  = 0u;
  a3[3]  = 0u;
  a3[1]  = 0u;
  float64x2_t v21 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v22.f64[0]  = INFINITY;
  float64x2_t v23 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
  v24.f64[0]  = -INFINITY;
  do
  {
    float64x2_t v25 = (float64x2_t)v28[v20];
    long long v26 = v28[v20 + 1];
    float64x2_t v21 = vminnmq_f64(v21, v25);
    float64x2_t v22 = vminnmq_f64((float64x2_t)*(unint64_t *)&v22.f64[0], (float64x2_t)(unint64_t)v26);
    float64x2_t v23 = vmaxnmq_f64(v23, v25);
    float64x2_t v24 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v24.f64[0], (float64x2_t)(unint64_t)v26);
    v20 += 2;
  }
  while (v20 != 16);
  *a3  = v21;
  a3[1].f64[0]  = v22.f64[0];
  float64x2_t result = vsubq_f64(v23, v21);
  a3[2]  = result;
  *(void *)&a3[3].f64[0]  = *(_OWORD *)&vsubq_f64(v24, v22);
  return result;
}

{
  uint64_t v7;
  float64x2_t v8;
  double v9;
  _OWORD *v10;
  long long v11;
  uint64_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  _OWORD *v17;
  float64x2_t v18;
  float64x2_t v19;
  uint64_t v20;
  float64x2_t v21;
  float64x2_t v22;
  float64x2_t v23;
  float64x2_t v24;
  float64x2_t v25;
  long long v26;
  float64x2_t result;
  _OWORD v28[2];
  long long v29;
  long long v30;
  float64x2_t v31;
  long long v32;
  long long v33;
  long long v34;
  long long v35;
  long long v36;
  long long v37;
  long long v38;
  float64x2_t v39;
  long long v40;
  long long v41;
  double v42[3];
  uint64_t v43;

  uint64_t v7 = 0;
  uint64_t v43 = *MEMORY[0x263EF8340];
  long long v41 = 0u;
  *(_OWORD *)float64x2_t v42 = 0u;
  float64x2_t v39 = 0u;
  long long v40 = 0u;
  long long v37 = 0u;
  long long v38 = 0u;
  long long v35 = 0u;
  long long v36 = 0u;
  long long v33 = 0u;
  long long v34 = 0u;
  float64x2_t v31 = 0u;
  long long v32 = 0u;
  long long v29 = 0u;
  long long v30 = 0u;
  memset(v28, 0, sizeof(v28));
  float64x2_t v8 = *(float64x2_t *)(a1 + 32);
  double v9 = *(double *)(a1 + 48);
  do
  {
    float64x2_t v10 = &v28[v7];
    long long v11 = *(_OWORD *)(a1 + 16);
    *float64x2_t v10 = *(_OWORD *)a1;
    v10[1]  = v11;
    v7 += 2;
  }
  while (v7 != 16);
  uint64_t v12 = 0;
  *((double *)&v29 + 1)  = v8.f64[1] + *((double *)&v29 + 1);
  float64x2_t v31 = vaddq_f64(v8, v31);
  *(double *)&long long v33 = v8.f64[0] + *(double *)&v33;
  *(double *)&long long v36 = v9 + *(double *)&v36;
  *((double *)&v37 + 1)  = v8.f64[1] + *((double *)&v37 + 1);
  *(double *)&long long v38 = v9 + *(double *)&v38;
  float64x2_t v39 = vaddq_f64(v8, v39);
  *(double *)&long long v40 = v9 + *(double *)&v40;
  *(double *)&long long v41 = v8.f64[0] + *(double *)&v41;
  v42[0]  = v9 + v42[0];
  float64x2_t v13 = *a2;
  float64x2_t v14 = a2[2];
  float64x2_t v15 = a2[4];
  float64x2_t v16 = a2[6];
  a4.f64[0]  = a2[1].f64[0];
  a5.f64[0]  = a2[3].f64[0];
  a6.f64[0]  = a2[5].f64[0];
  a7.f64[0]  = a2[7].f64[0];
  *(void *)&v13.f64[1]  = vextq_s8((int8x16_t)v13, (int8x16_t)v13, 8uLL).u64[0];
  *(void *)&v14.f64[1]  = vextq_s8((int8x16_t)v14, (int8x16_t)v14, 8uLL).u64[0];
  *(void *)&v15.f64[1]  = vextq_s8((int8x16_t)v15, (int8x16_t)v15, 8uLL).u64[0];
  *(void *)&v16.f64[1]  = vextq_s8((int8x16_t)v16, (int8x16_t)v16, 8uLL).u64[0];
  do
  {
    uint64_t v17 = &v28[v12];
    float64x2_t v19 = (float64x2_t)v28[v12];
    float64x2_t v18 = (float64x2_t)v28[v12 + 1];
    *uint64_t v17 = vaddq_f64(v16, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v13, v19.f64[0]), v14, v19, 1), v15, v18.f64[0]));
    *((void *)v17 + 2)  = *(_OWORD *)&vaddq_f64(a7, vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v19, a4), a5, v19, 1), v18, a6));
    v12 += 2;
  }
  while (v12 != 16);
  uint64_t v20 = 0;
  a3[2]  = 0u;
  a3[3]  = 0u;
  a3[1]  = 0u;
  float64x2_t v21 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v22.f64[0]  = INFINITY;
  float64x2_t v23 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
  v24.f64[0]  = -INFINITY;
  do
  {
    float64x2_t v25 = (float64x2_t)v28[v20];
    long long v26 = v28[v20 + 1];
    float64x2_t v21 = vminnmq_f64(v21, v25);
    float64x2_t v22 = vminnmq_f64((float64x2_t)*(unint64_t *)&v22.f64[0], (float64x2_t)(unint64_t)v26);
    float64x2_t v23 = vmaxnmq_f64(v23, v25);
    float64x2_t v24 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v24.f64[0], (float64x2_t)(unint64_t)v26);
    v20 += 2;
  }
  while (v20 != 16);
  *a3  = v21;
  a3[1].f64[0]  = v22.f64[0];
  float64x2_t result = vsubq_f64(v23, v21);
  a3[2]  = result;
  *(void *)&a3[3].f64[0]  = *(_OWORD *)&vsubq_f64(v24, v22);
  return result;
}

double static SPRect3D.* infix(_:_:)@<D0>(long long *a1@<X0>, uint64_t a2@<X1>, void (*a3)(_OWORD *__return_ptr, _OWORD *, _OWORD *)@<X2>, uint64_t a4@<X8>)
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  long long v13 = *(_OWORD *)(a2 + 16);
  long long v14 = *(_OWORD *)(a2 + 32);
  uint64_t v15 = *(void *)(a2 + 48);
  uint64_t v16 = *(void *)(a2 + 56);
  v26[0]  = *(_OWORD *)a2;
  v26[1]  = v13;
  uint64_t v27 = v15;
  uint64_t v28 = v16;
  v26[2]  = v14;
  v25[0]  = v5;
  v25[1]  = v6;
  v25[2]  = v7;
  v25[3]  = v8;
  double v25[4] = v9;
  v25[5]  = v10;
  v25[6]  = v11;
  v25[7]  = v12;
  a3(v22, v26, v25);
  double result = *(double *)v22;
  long long v18 = v22[1];
  long long v19 = v22[2];
  uint64_t v20 = v23;
  uint64_t v21 = v24;
  *(_OWORD *)a4  = v22[0];
  *(_OWORD *)(a4 + 16)  = v18;
  *(void *)(a4 + 48)  = v20;
  *(void *)(a4 + 56)  = v21;
  *(_OWORD *)(a4 + 32)  = v19;
  return result;
}

float64x2_t SPRect3DApplyProjectiveTransform@<Q0>(uint64_t a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>)
{
  uint64_t v3 = 0;
  uint64_t v43 = *MEMORY[0x263EF8340];
  long long v41 = 0u;
  *(_OWORD *)float64x2_t v42 = 0u;
  float64x2_t v39 = 0u;
  long long v40 = 0u;
  long long v37 = 0u;
  long long v38 = 0u;
  long long v35 = 0u;
  long long v36 = 0u;
  long long v33 = 0u;
  long long v34 = 0u;
  float64x2_t v31 = 0u;
  long long v32 = 0u;
  long long v29 = 0u;
  long long v30 = 0u;
  memset(v28, 0, sizeof(v28));
  float64x2_t v4 = *(float64x2_t *)(a1 + 32);
  double v5 = *(double *)(a1 + 48);
  do
  {
    long long v6 = &v28[v3];
    long long v7 = *(_OWORD *)(a1 + 16);
    *long long v6 = *(_OWORD *)a1;
    v6[1]  = v7;
    v3 += 2;
  }
  while (v3 != 16);
  uint64_t v8 = 0;
  *((double *)&v29 + 1)  = v4.f64[1] + *((double *)&v29 + 1);
  float64x2_t v31 = vaddq_f64(v4, v31);
  *(double *)&long long v33 = v4.f64[0] + *(double *)&v33;
  *(double *)&long long v36 = v5 + *(double *)&v36;
  *((double *)&v37 + 1)  = v4.f64[1] + *((double *)&v37 + 1);
  *(double *)&long long v38 = v5 + *(double *)&v38;
  float64x2_t v39 = vaddq_f64(v4, v39);
  *(double *)&long long v40 = v5 + *(double *)&v40;
  *(double *)&long long v41 = v4.f64[0] + *(double *)&v41;
  v42[0]  = v5 + v42[0];
  float64x2_t v10 = *a2;
  float64x2_t v9 = a2[1];
  float64x2_t v12 = a2[2];
  float64x2_t v11 = a2[3];
  float64x2_t v14 = a2[4];
  float64x2_t v13 = a2[5];
  float64x2_t v16 = a2[6];
  float64x2_t v15 = a2[7];
  do
  {
    uint64_t v17 = &v28[v8];
    float64x2_t v19 = (float64x2_t)v28[v8];
    long long v18 = v28[v8 + 1];
    *uint64_t v17 = vaddq_f64(v16, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v10, v19.f64[0]), v12, v19, 1), v14, *(double *)&v18));
    *((void *)v17 + 2)  = *(_OWORD *)&vaddq_f64(v15, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v9, v19.f64[0]), v11, v19, 1), v13, *(double *)&v18));
    v8 += 2;
  }
  while (v8 != 16);
  uint64_t v20 = 0;
  a3[2]  = 0u;
  a3[3]  = 0u;
  a3[1]  = 0u;
  float64x2_t v21 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v22.f64[0]  = INFINITY;
  float64x2_t v23 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
  v24.f64[0]  = -INFINITY;
  do
  {
    float64x2_t v25 = (float64x2_t)v28[v20];
    long long v26 = v28[v20 + 1];
    float64x2_t v21 = vminnmq_f64(v21, v25);
    float64x2_t v22 = vminnmq_f64((float64x2_t)*(unint64_t *)&v22.f64[0], (float64x2_t)(unint64_t)v26);
    float64x2_t v23 = vmaxnmq_f64(v23, v25);
    float64x2_t v24 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v24.f64[0], (float64x2_t)(unint64_t)v26);
    v20 += 2;
  }
  while (v20 != 16);
  *a3  = v21;
  a3[1].f64[0]  = v22.f64[0];
  float64x2_t result = vsubq_f64(v23, v21);
  a3[2]  = result;
  *(void *)&a3[3].f64[0]  = *(_OWORD *)&vsubq_f64(v24, v22);
  return result;
}

{
  uint64_t v3;
  float64x2_t v4;
  double v5;
  _OWORD *v6;
  long long v7;
  uint64_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  _OWORD *v17;
  long long v18;
  float64x2_t v19;
  uint64_t v20;
  float64x2_t v21;
  float64x2_t v22;
  float64x2_t v23;
  float64x2_t v24;
  float64x2_t v25;
  long long v26;
  float64x2_t result;
  _OWORD v28[2];
  long long v29;
  long long v30;
  float64x2_t v31;
  long long v32;
  long long v33;
  long long v34;
  long long v35;
  long long v36;
  long long v37;
  long long v38;
  float64x2_t v39;
  long long v40;
  long long v41;
  double v42[3];
  uint64_t v43;

  uint64_t v3 = 0;
  uint64_t v43 = *MEMORY[0x263EF8340];
  long long v41 = 0u;
  *(_OWORD *)float64x2_t v42 = 0u;
  float64x2_t v39 = 0u;
  long long v40 = 0u;
  long long v37 = 0u;
  long long v38 = 0u;
  long long v35 = 0u;
  long long v36 = 0u;
  long long v33 = 0u;
  long long v34 = 0u;
  float64x2_t v31 = 0u;
  long long v32 = 0u;
  long long v29 = 0u;
  long long v30 = 0u;
  memset(v28, 0, sizeof(v28));
  float64x2_t v4 = *(float64x2_t *)(a1 + 32);
  double v5 = *(double *)(a1 + 48);
  do
  {
    long long v6 = &v28[v3];
    long long v7 = *(_OWORD *)(a1 + 16);
    *long long v6 = *(_OWORD *)a1;
    v6[1]  = v7;
    v3 += 2;
  }
  while (v3 != 16);
  uint64_t v8 = 0;
  *((double *)&v29 + 1)  = v4.f64[1] + *((double *)&v29 + 1);
  float64x2_t v31 = vaddq_f64(v4, v31);
  *(double *)&long long v33 = v4.f64[0] + *(double *)&v33;
  *(double *)&long long v36 = v5 + *(double *)&v36;
  *((double *)&v37 + 1)  = v4.f64[1] + *((double *)&v37 + 1);
  *(double *)&long long v38 = v5 + *(double *)&v38;
  float64x2_t v39 = vaddq_f64(v4, v39);
  *(double *)&long long v40 = v5 + *(double *)&v40;
  *(double *)&long long v41 = v4.f64[0] + *(double *)&v41;
  v42[0]  = v5 + v42[0];
  float64x2_t v10 = *a2;
  float64x2_t v9 = a2[1];
  float64x2_t v12 = a2[2];
  float64x2_t v11 = a2[3];
  float64x2_t v14 = a2[4];
  float64x2_t v13 = a2[5];
  float64x2_t v16 = a2[6];
  float64x2_t v15 = a2[7];
  do
  {
    uint64_t v17 = &v28[v8];
    float64x2_t v19 = (float64x2_t)v28[v8];
    long long v18 = v28[v8 + 1];
    *uint64_t v17 = vaddq_f64(v16, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v10, v19.f64[0]), v12, v19, 1), v14, *(double *)&v18));
    *((void *)v17 + 2)  = *(_OWORD *)&vaddq_f64(v15, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v9, v19.f64[0]), v11, v19, 1), v13, *(double *)&v18));
    v8 += 2;
  }
  while (v8 != 16);
  uint64_t v20 = 0;
  a3[2]  = 0u;
  a3[3]  = 0u;
  a3[1]  = 0u;
  float64x2_t v21 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v22.f64[0]  = INFINITY;
  float64x2_t v23 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
  v24.f64[0]  = -INFINITY;
  do
  {
    float64x2_t v25 = (float64x2_t)v28[v20];
    long long v26 = v28[v20 + 1];
    float64x2_t v21 = vminnmq_f64(v21, v25);
    float64x2_t v22 = vminnmq_f64((float64x2_t)*(unint64_t *)&v22.f64[0], (float64x2_t)(unint64_t)v26);
    float64x2_t v23 = vmaxnmq_f64(v23, v25);
    float64x2_t v24 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v24.f64[0], (float64x2_t)(unint64_t)v26);
    v20 += 2;
  }
  while (v20 != 16);
  *a3  = v21;
  a3[1].f64[0]  = v22.f64[0];
  float64x2_t result = vsubq_f64(v23, v21);
  a3[2]  = result;
  *(void *)&a3[3].f64[0]  = *(_OWORD *)&vsubq_f64(v24, v22);
  return result;
}

{
  uint64_t v3;
  float64x2_t v4;
  double v5;
  _OWORD *v6;
  long long v7;
  uint64_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  _OWORD *v17;
  long long v18;
  float64x2_t v19;
  uint64_t v20;
  float64x2_t v21;
  float64x2_t v22;
  float64x2_t v23;
  float64x2_t v24;
  float64x2_t v25;
  long long v26;
  float64x2_t result;
  _OWORD v28[2];
  long long v29;
  long long v30;
  float64x2_t v31;
  long long v32;
  long long v33;
  long long v34;
  long long v35;
  long long v36;
  long long v37;
  long long v38;
  float64x2_t v39;
  long long v40;
  long long v41;
  double v42[3];
  uint64_t v43;

  uint64_t v3 = 0;
  uint64_t v43 = *MEMORY[0x263EF8340];
  long long v41 = 0u;
  *(_OWORD *)float64x2_t v42 = 0u;
  float64x2_t v39 = 0u;
  long long v40 = 0u;
  long long v37 = 0u;
  long long v38 = 0u;
  long long v35 = 0u;
  long long v36 = 0u;
  long long v33 = 0u;
  long long v34 = 0u;
  float64x2_t v31 = 0u;
  long long v32 = 0u;
  long long v29 = 0u;
  long long v30 = 0u;
  memset(v28, 0, sizeof(v28));
  float64x2_t v4 = *(float64x2_t *)(a1 + 32);
  double v5 = *(double *)(a1 + 48);
  do
  {
    long long v6 = &v28[v3];
    long long v7 = *(_OWORD *)(a1 + 16);
    *long long v6 = *(_OWORD *)a1;
    v6[1]  = v7;
    v3 += 2;
  }
  while (v3 != 16);
  uint64_t v8 = 0;
  *((double *)&v29 + 1)  = v4.f64[1] + *((double *)&v29 + 1);
  float64x2_t v31 = vaddq_f64(v4, v31);
  *(double *)&long long v33 = v4.f64[0] + *(double *)&v33;
  *(double *)&long long v36 = v5 + *(double *)&v36;
  *((double *)&v37 + 1)  = v4.f64[1] + *((double *)&v37 + 1);
  *(double *)&long long v38 = v5 + *(double *)&v38;
  float64x2_t v39 = vaddq_f64(v4, v39);
  *(double *)&long long v40 = v5 + *(double *)&v40;
  *(double *)&long long v41 = v4.f64[0] + *(double *)&v41;
  v42[0]  = v5 + v42[0];
  float64x2_t v10 = *a2;
  float64x2_t v9 = a2[1];
  float64x2_t v12 = a2[2];
  float64x2_t v11 = a2[3];
  float64x2_t v14 = a2[4];
  float64x2_t v13 = a2[5];
  float64x2_t v16 = a2[6];
  float64x2_t v15 = a2[7];
  do
  {
    uint64_t v17 = &v28[v8];
    float64x2_t v19 = (float64x2_t)v28[v8];
    long long v18 = v28[v8 + 1];
    *uint64_t v17 = vaddq_f64(v16, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v10, v19.f64[0]), v12, v19, 1), v14, *(double *)&v18));
    *((void *)v17 + 2)  = *(_OWORD *)&vaddq_f64(v15, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v9, v19.f64[0]), v11, v19, 1), v13, *(double *)&v18));
    v8 += 2;
  }
  while (v8 != 16);
  uint64_t v20 = 0;
  a3[2]  = 0u;
  a3[3]  = 0u;
  a3[1]  = 0u;
  float64x2_t v21 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v22.f64[0]  = INFINITY;
  float64x2_t v23 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
  v24.f64[0]  = -INFINITY;
  do
  {
    float64x2_t v25 = (float64x2_t)v28[v20];
    long long v26 = v28[v20 + 1];
    float64x2_t v21 = vminnmq_f64(v21, v25);
    float64x2_t v22 = vminnmq_f64((float64x2_t)*(unint64_t *)&v22.f64[0], (float64x2_t)(unint64_t)v26);
    float64x2_t v23 = vmaxnmq_f64(v23, v25);
    float64x2_t v24 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v24.f64[0], (float64x2_t)(unint64_t)v26);
    v20 += 2;
  }
  while (v20 != 16);
  *a3  = v21;
  a3[1].f64[0]  = v22.f64[0];
  float64x2_t result = vsubq_f64(v23, v21);
  a3[2]  = result;
  *(void *)&a3[3].f64[0]  = *(_OWORD *)&vsubq_f64(v24, v22);
  return result;
}

float64_t static SPRect3D.* infix(_:_:)@<D0>(double *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  long long v4 = *(_OWORD *)a1;
  long long v5 = *((_OWORD *)a1 + 1);
  long long v6 = *((_OWORD *)a1 + 2);
  double v7 = a1[6];
  double v8 = a1[7];
  long long v9 = *(_OWORD *)(a2 + 16);
  long long v10 = *(_OWORD *)(a2 + 32);
  double v11 = *(double *)(a2 + 48);
  double v12 = *(double *)(a2 + 56);
  *(_OWORD *)&v22.origin.x  = *(_OWORD *)a2;
  *(_OWORD *)&v22.origin.vector.f64[2]  = v9;
  v22.size.depth  = v11;
  v22.size.vector.f64[3]  = v12;
  *(_OWORD *)&v22.size.width  = v10;
  *(_OWORD *)&v21.position.x  = v4;
  *(_OWORD *)&v21.position.vector.f64[2]  = v5;
  v21.rotation.vector.f64[2]  = v7;
  v21.rotation.vector.f64[3]  = v8;
  *(_OWORD *)v21.rotation.vector.f64  = v6;
  SPRect3DApplyPose(&v22, &v21, v18);
  float64_t result = v18[0].f64[0];
  float64x2_t v14 = v18[1];
  float64x2_t v15 = v18[2];
  uint64_t v16 = v19;
  uint64_t v17 = v20;
  *(float64x2_t *)a3  = v18[0];
  *(float64x2_t *)(a3 + 16)  = v14;
  *(void *)(a3 + 48)  = v16;
  *(void *)(a3 + 56)  = v17;
  *(float64x2_t *)(a3 + 32)  = v15;
  return result;
}

float64x2_t SPRect3DApplyPose@<Q0>(SPRect3D *a1@<X0>, SPPose3D *a2@<X1>, float64x2_t *a3@<X8>)
{
  uint64_t v3 = 0;
  uint64_t v50 = *MEMORY[0x263EF8340];
  long long v48 = 0u;
  *(_OWORD *)float64x2_t v49 = 0u;
  float64x2_t v46 = 0u;
  long long v47 = 0u;
  long long v44 = 0u;
  long long v45 = 0u;
  long long v42 = 0u;
  long long v43 = 0u;
  long long v40 = 0u;
  long long v41 = 0u;
  float64x2_t v38 = 0u;
  long long v39 = 0u;
  long long v36 = 0u;
  long long v37 = 0u;
  memset(v35, 0, sizeof(v35));
  float64x2_t v4 = *(float64x2_t *)&a1->size.width;
  depth  = a1->size.depth;
  do
  {
    long long v6 = &v35[v3];
    long long v7 = *(_OWORD *)&a1->origin.vector.f64[2];
    *long long v6 = *(_OWORD *)&a1->origin.x;
    v6[1]  = v7;
    v3 += 2;
  }
  while (v3 != 16);
  uint64_t v8 = 0;
  *((double *)&v36 + 1)  = v4.f64[1] + *((double *)&v36 + 1);
  float64x2_t v38 = vaddq_f64(v4, v38);
  *(double *)&long long v40 = v4.f64[0] + *(double *)&v40;
  *(double *)&long long v43 = depth + *(double *)&v43;
  *((double *)&v44 + 1)  = v4.f64[1] + *((double *)&v44 + 1);
  *(double *)&long long v45 = depth + *(double *)&v45;
  float64x2_t v46 = vaddq_f64(v4, v46);
  *(double *)&long long v47 = depth + *(double *)&v47;
  *(double *)&long long v48 = v4.f64[0] + *(double *)&v48;
  v49[0]  = depth + v49[0];
  float64x2_t v10 = *(float64x2_t *)&a2->position.x;
  float64x2_t v9 = *(float64x2_t *)&a2->position.vector.f64[2];
  float64x2_t v11 = *(float64x2_t *)a2->rotation.vector.f64;
  float64x2_t v12 = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  float64x2_t v13 = vmulq_f64(v12, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v14 = (int8x16_t)vnegq_f64(v11);
  float64x2_t v15 = (float64x2_t)vextq_s8((int8x16_t)v13, (int8x16_t)vnegq_f64(v13), 8uLL);
  float64x2_t v16 = (float64x2_t)vextq_s8(v14, (int8x16_t)v11, 8uLL);
  float64x2_t v17 = (float64x2_t)vextq_s8((int8x16_t)v11, v14, 8uLL);
  float64x2_t v18 = (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v11.f64[0], 0);
  float64x2_t v19 = (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v12.f64[0], 0);
  do
  {
    uint64_t v20 = &v35[v8];
    float64x2_t v22 = (float64x2_t)v35[v8];
    long long v21 = v35[v8 + 1];
    float64x2_t v23 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v11, v22, 1), v16, v22.f64[0]), v15, *(double *)&v21);
    float64x2_t v24 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v13, v22, 1), v15, v22.f64[0]), v17, *(double *)&v21);
    float64x2_t v25 = vnegq_f64(v24);
    float64x2_t v26 = (float64x2_t)vextq_s8((int8x16_t)v23, (int8x16_t)vnegq_f64(v23), 8uLL);
    *uint64_t v20 = vaddq_f64(v10, vaddq_f64(vmlaq_f64(vmulq_laneq_f64(v24, v12, 1), (float64x2_t)vextq_s8((int8x16_t)v25, (int8x16_t)v24, 8uLL), v19), vmlaq_f64(vmulq_laneq_f64(v23, v11, 1), v26, v18)));
    *((void *)v20 + 2)  = *(_OWORD *)&vaddq_f64(v9, vaddq_f64(vmlaq_f64(vmulq_laneq_f64(v23, v12, 1), v26, v19), vmlaq_f64(vmulq_laneq_f64(v25, v11, 1), (float64x2_t)vextq_s8((int8x16_t)v24, (int8x16_t)v25, 8uLL), v18)));
    v8 += 2;
  }
  while (v8 != 16);
  uint64_t v27 = 0;
  a3[2]  = 0u;
  a3[3]  = 0u;
  a3[1]  = 0u;
  float64x2_t v28 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v29.f64[0]  = INFINITY;
  float64x2_t v30 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
  v31.f64[0]  = -INFINITY;
  do
  {
    float64x2_t v32 = (float64x2_t)v35[v27];
    long long v33 = v35[v27 + 1];
    float64x2_t v28 = vminnmq_f64(v28, v32);
    float64x2_t v29 = vminnmq_f64((float64x2_t)*(unint64_t *)&v29.f64[0], (float64x2_t)(unint64_t)v33);
    float64x2_t v30 = vmaxnmq_f64(v30, v32);
    float64x2_t v31 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v31.f64[0], (float64x2_t)(unint64_t)v33);
    v27 += 2;
  }
  while (v27 != 16);
  *a3  = v28;
  a3[1].f64[0]  = v29.f64[0];
  float64x2_t result = vsubq_f64(v30, v28);
  a3[2]  = result;
  *(void *)&a3[3].f64[0]  = *(_OWORD *)&vsubq_f64(v31, v29);
  return result;
}

{
  uint64_t v3;
  float64x2_t v4;
  double depth;
  _OWORD *v6;
  long long v7;
  uint64_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  int8x16_t v14;
  float64x2_t v15;
  float64x2_t v16;
  float64x2_t v17;
  float64x2_t v18;
  float64x2_t v19;
  _OWORD *v20;
  long long v21;
  float64x2_t v22;
  float64x2_t v23;
  float64x2_t v24;
  float64x2_t v25;
  float64x2_t v26;
  uint64_t v27;
  float64x2_t v28;
  float64x2_t v29;
  float64x2_t v30;
  float64x2_t v31;
  float64x2_t v32;
  long long v33;
  float64x2_t result;
  _OWORD v35[2];
  long long v36;
  long long v37;
  float64x2_t v38;
  long long v39;
  long long v40;
  long long v41;
  long long v42;
  long long v43;
  long long v44;
  long long v45;
  float64x2_t v46;
  long long v47;
  long long v48;
  double v49[3];
  uint64_t v50;

  uint64_t v3 = 0;
  uint64_t v50 = *MEMORY[0x263EF8340];
  long long v48 = 0u;
  *(_OWORD *)float64x2_t v49 = 0u;
  float64x2_t v46 = 0u;
  long long v47 = 0u;
  long long v44 = 0u;
  long long v45 = 0u;
  long long v42 = 0u;
  long long v43 = 0u;
  long long v40 = 0u;
  long long v41 = 0u;
  float64x2_t v38 = 0u;
  long long v39 = 0u;
  long long v36 = 0u;
  long long v37 = 0u;
  memset(v35, 0, sizeof(v35));
  float64x2_t v4 = *(float64x2_t *)&a1->size.width;
  depth  = a1->size.depth;
  do
  {
    long long v6 = &v35[v3];
    long long v7 = *(_OWORD *)&a1->origin.vector.f64[2];
    *long long v6 = *(_OWORD *)&a1->origin.x;
    v6[1]  = v7;
    v3 += 2;
  }
  while (v3 != 16);
  uint64_t v8 = 0;
  *((double *)&v36 + 1)  = v4.f64[1] + *((double *)&v36 + 1);
  float64x2_t v38 = vaddq_f64(v4, v38);
  *(double *)&long long v40 = v4.f64[0] + *(double *)&v40;
  *(double *)&long long v43 = depth + *(double *)&v43;
  *((double *)&v44 + 1)  = v4.f64[1] + *((double *)&v44 + 1);
  *(double *)&long long v45 = depth + *(double *)&v45;
  float64x2_t v46 = vaddq_f64(v4, v46);
  *(double *)&long long v47 = depth + *(double *)&v47;
  *(double *)&long long v48 = v4.f64[0] + *(double *)&v48;
  v49[0]  = depth + v49[0];
  float64x2_t v10 = *(float64x2_t *)&a2->position.x;
  float64x2_t v9 = *(float64x2_t *)&a2->position.vector.f64[2];
  float64x2_t v11 = *(float64x2_t *)a2->rotation.vector.f64;
  float64x2_t v12 = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  float64x2_t v13 = vmulq_f64(v12, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v14 = (int8x16_t)vnegq_f64(v11);
  float64x2_t v15 = (float64x2_t)vextq_s8((int8x16_t)v13, (int8x16_t)vnegq_f64(v13), 8uLL);
  float64x2_t v16 = (float64x2_t)vextq_s8(v14, (int8x16_t)v11, 8uLL);
  float64x2_t v17 = (float64x2_t)vextq_s8((int8x16_t)v11, v14, 8uLL);
  float64x2_t v18 = (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v11.f64[0], 0);
  float64x2_t v19 = (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v12.f64[0], 0);
  do
  {
    uint64_t v20 = &v35[v8];
    float64x2_t v22 = (float64x2_t)v35[v8];
    long long v21 = v35[v8 + 1];
    float64x2_t v23 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v11, v22, 1), v16, v22.f64[0]), v15, *(double *)&v21);
    float64x2_t v24 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v13, v22, 1), v15, v22.f64[0]), v17, *(double *)&v21);
    float64x2_t v25 = vnegq_f64(v24);
    float64x2_t v26 = (float64x2_t)vextq_s8((int8x16_t)v23, (int8x16_t)vnegq_f64(v23), 8uLL);
    *uint64_t v20 = vaddq_f64(v10, vaddq_f64(vmlaq_f64(vmulq_laneq_f64(v24, v12, 1), (float64x2_t)vextq_s8((int8x16_t)v25, (int8x16_t)v24, 8uLL), v19), vmlaq_f64(vmulq_laneq_f64(v23, v11, 1), v26, v18)));
    *((void *)v20 + 2)  = *(_OWORD *)&vaddq_f64(v9, vaddq_f64(vmlaq_f64(vmulq_laneq_f64(v23, v12, 1), v26, v19), vmlaq_f64(vmulq_laneq_f64(v25, v11, 1), (float64x2_t)vextq_s8((int8x16_t)v24, (int8x16_t)v25, 8uLL), v18)));
    v8 += 2;
  }
  while (v8 != 16);
  uint64_t v27 = 0;
  a3[2]  = 0u;
  a3[3]  = 0u;
  a3[1]  = 0u;
  float64x2_t v28 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v29.f64[0]  = INFINITY;
  float64x2_t v30 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
  v31.f64[0]  = -INFINITY;
  do
  {
    float64x2_t v32 = (float64x2_t)v35[v27];
    long long v33 = v35[v27 + 1];
    float64x2_t v28 = vminnmq_f64(v28, v32);
    float64x2_t v29 = vminnmq_f64((float64x2_t)*(unint64_t *)&v29.f64[0], (float64x2_t)(unint64_t)v33);
    float64x2_t v30 = vmaxnmq_f64(v30, v32);
    float64x2_t v31 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v31.f64[0], (float64x2_t)(unint64_t)v33);
    v27 += 2;
  }
  while (v27 != 16);
  *a3  = v28;
  a3[1].f64[0]  = v29.f64[0];
  float64x2_t result = vsubq_f64(v30, v28);
  a3[2]  = result;
  *(void *)&a3[3].f64[0]  = *(_OWORD *)&vsubq_f64(v31, v29);
  return result;
}

double static SPAffineTransform3D.* infix(_:_:)@<D0>(_OWORD *a1@<X0>, long long *a2@<X1>, _OWORD *a3@<X8>)
{
  return static SPAffineTransform3D.* infix(_:_:)(a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPAffineTransform3DConcatenation, a3);
}

__n128 SPAffineTransform3DConcatenation@<Q0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v4 = *a1;
  float64x2_t v3 = a1[1];
  float64x2_t v6 = a1[2];
  float64x2_t v5 = a1[3];
  float64x2_t v8 = a1[4];
  float64x2_t v7 = a1[5];
  float64x2_t v9 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v11 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v10 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v13 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v12 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  float64x2_t v14 = (float64x2_t)vceqq_f64(v10, v5);
  float64x2_t v15 = (float64x2_t)vceqq_f64(v13, v8);
  float64x2_t v16 = (float64x2_t)vceqq_f64(v12, v7);
  int64x2_t v17 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v11, v6), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], *a1)), (int8x16_t)v15);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v17, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)v14, (int8x16_t)vceqq_f64(v9, v3)), (int8x16_t)v16), (int8x16_t)v17)).u64[0] & 0x8000000000000000) != 0)
  {
    float64x2_t v14 = a2[2];
    float64x2_t v16 = a2[4];
    float64x2_t v15 = a2[5];
    int8x16_t v18 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v10, a2[3]), (int8x16_t)vceqq_f64(v9, a2[1])), (int8x16_t)vceqq_f64(v12, v15));
    int64x2_t v19 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v11, v14), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], *a2)), (int8x16_t)vceqq_f64(v13, v16));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v19, 1), vandq_s8(v18, (int8x16_t)v19)).u64[0] & 0x8000000000000000) != 0)
    {
      float64x2_t v38 = vaddq_f64(a1[7], a2[7]);
      a1[6]  = vaddq_f64(a1[6], a2[6]);
      a1[7]  = v38;
      float64x2_t v39 = a1[5];
      *(float64x2_t *)(a3 + 64)  = a1[4];
      *(float64x2_t *)(a3 + 80)  = v39;
      float64x2_t v40 = a1[1];
      *(float64x2_t *)a3  = *a1;
      *(float64x2_t *)(a3 + 16)  = v40;
      float64x2_t v41 = a1[3];
      *(float64x2_t *)(a3 + 32)  = a1[2];
      *(float64x2_t *)(a3 + 48)  = v41;
      __n128 result = (__n128)a1[6];
      float64x2_t v42 = a1[7];
      *(__n128 *)(a3 + 96)  = result;
      *(float64x2_t *)(a3 + 112)  = v42;
      return result;
    }
  }
  float64x2_t v20 = a1[6];
  float64x2_t v21 = a1[7];
  int64x2_t v22 = vceqzq_f64(v20);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v22, 1), vandq_s8((int8x16_t)vceqzq_f64(v21), (int8x16_t)v22)).u64[0] & 0x8000000000000000) != 0)
  {
    float64x2_t v24 = a2[6];
    float64x2_t v23 = a2[7];
    int64x2_t v25 = vceqzq_f64(v24);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v25, 1), vandq_s8((int8x16_t)vceqzq_f64(v23), (int8x16_t)v25)).u64[0] & 0x8000000000000000) != 0)
    {
      uint64_t v43 = 0;
      float64x2_t v44 = a2[1];
      float64x2_t v45 = a2[2];
      float64x2_t v46 = a2[3];
      float64x2_t v47 = a2[4];
      float64x2_t v48 = a2[5];
      float64x2_t v52 = *a2;
      float64x2_t v53 = v44;
      float64x2_t v54 = v45;
      float64x2_t v55 = v46;
      float64x2_t v56 = v47;
      float64x2_t v57 = v48;
      __n128 v60 = 0u;
      long long v61 = 0u;
      long long v62 = 0u;
      long long v63 = 0u;
      v64  = 0u;
      v65  = 0u;
      *(void *)&v4.f64[1]  = vextq_s8((int8x16_t)v4, (int8x16_t)v4, 8uLL).u64[0];
      *(void *)&v6.f64[1]  = vextq_s8((int8x16_t)v6, (int8x16_t)v6, 8uLL).u64[0];
      *(void *)&v8.f64[1]  = vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL).u64[0];
      do
      {
        float64x2_t v50 = *(float64x2_t *)((char *)&v52 + v43);
        float64x2_t v49 = *(float64x2_t *)((char *)&v52 + v43 + 16);
        float64x2_t v51 = (float64x2_t *)((char *)&v60 + v43);
        *float64x2_t v51 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v4, v50.f64[0]), v6, v50, 1), v8, v49.f64[0]);
        v51[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v3, v50), v5, v50, 1), v49, v7);
        v43 += 32;
      }
      while (v43 != 96);
      __n128 result = v60;
      long long v33 = v61;
      long long v34 = v62;
      long long v35 = v63;
      long long v36 = v64;
      long long v37 = v65;
      goto LABEL_9;
    }
  }
  else
  {
    v23.f64[0]  = a2[7].f64[0];
    float64x2_t v24 = a2[6];
  }
  uint64_t v26 = 0;
  v3.f64[1]  = 0.0;
  v5.f64[1]  = 0.0;
  v7.f64[1]  = 0.0;
  v21.f64[1]  = 1.0;
  float64x2_t v27 = a2[2];
  float64x2_t v28 = a2[4];
  v14.f64[0]  = a2[1].f64[0];
  v15.f64[0]  = a2[3].f64[0];
  v16.f64[0]  = a2[5].f64[0];
  *(void *)&v24.f64[1]  = vextq_s8((int8x16_t)v24, (int8x16_t)v24, 8uLL).u64[0];
  v23.f64[1]  = 1.0;
  float64x2_t v52 = *a2;
  float64x2_t v53 = v14;
  float64x2_t v54 = v27;
  float64x2_t v55 = v15;
  float64x2_t v56 = v28;
  float64x2_t v57 = v16;
  float64x2_t v58 = v24;
  float64x2_t v59 = v23;
  __n128 v60 = 0u;
  long long v61 = 0u;
  long long v62 = 0u;
  long long v63 = 0u;
  v64  = 0u;
  v65  = 0u;
  float64x2_t v66 = 0u;
  float64x2_t v67 = 0u;
  do
  {
    float64x2_t v30 = *(float64x2_t *)((char *)&v52 + v26);
    float64x2_t v29 = *(float64x2_t *)((char *)&v52 + v26 + 16);
    float64x2_t v31 = (float64x2_t *)((char *)&v60 + v26);
    *float64x2_t v31 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v4, v30.f64[0]), v6, v30, 1), v8, v29.f64[0]), v20, v29, 1);
    v31[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v3, v30.f64[0]), v5, v30, 1), v7, v29.f64[0]), v21, v29, 1);
    v26 += 32;
  }
  while (v26 != 128);
  __n128 result = v60;
  long long v33 = v61;
  long long v34 = v62;
  long long v35 = v63;
  long long v36 = v64;
  long long v37 = v65;
  float64x2_t v20 = v66;
  float64x2_t v21 = v67;
LABEL_9:
  *(__n128 *)a3  = result;
  *(_OWORD *)(a3 + 16)  = v33;
  *(_OWORD *)(a3 + 32)  = v34;
  *(_OWORD *)(a3 + 48)  = v35;
  *(_OWORD *)(a3 + 64)  = v36;
  *(_OWORD *)(a3 + 80)  = v37;
  *(float64x2_t *)(a3 + 96)  = v20;
  *(float64x2_t *)(a3 + 112)  = v21;
  return result;
}

double static SPAffineTransform3D.*= infix(_:_:)(_OWORD *a1, long long *a2)
{
  return static SPAffineTransform3D.*= infix(_:_:)(a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPAffineTransform3DConcatenation);
}

double static SPProjectiveTransform3D.* infix(_:_:)@<D0>(_OWORD *a1@<X0>, long long *a2@<X1>, _OWORD *a3@<X8>)
{
  return static SPAffineTransform3D.* infix(_:_:)(a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPProjectiveTransform3DConcatenation, a3);
}

double static SPAffineTransform3D.* infix(_:_:)@<D0>(_OWORD *a1@<X0>, long long *a2@<X1>, void (*a3)(_OWORD *__return_ptr, _OWORD *, _OWORD *)@<X2>, _OWORD *a4@<X8>)
{
  long long v5 = *a2;
  long long v6 = a2[1];
  long long v7 = a2[2];
  long long v8 = a2[3];
  long long v9 = a2[4];
  long long v10 = a2[5];
  long long v11 = a2[6];
  long long v12 = a2[7];
  long long v13 = a1[1];
  long long v14 = a1[2];
  long long v15 = a1[3];
  long long v16 = a1[4];
  long long v17 = a1[5];
  long long v18 = a1[6];
  long long v19 = a1[7];
  v30[0]  = *a1;
  v30[1]  = v13;
  v30[2]  = v14;
  v30[3]  = v15;
  v30[4]  = v16;
  v30[5]  = v17;
  v30[6]  = v18;
  v30[7]  = v19;
  v29[0]  = v5;
  v29[1]  = v6;
  v29[2]  = v7;
  v29[3]  = v8;
  v29[4]  = v9;
  v29[5]  = v10;
  _OWORD v29[6] = v11;
  v29[7]  = v12;
  a3(v28, v30, v29);
  double result = *(double *)v28;
  long long v21 = v28[1];
  long long v22 = v28[2];
  long long v23 = v28[3];
  long long v24 = v28[4];
  long long v25 = v28[5];
  long long v26 = v28[6];
  long long v27 = v28[7];
  *a4  = v28[0];
  a4[1]  = v21;
  a4[2]  = v22;
  a4[3]  = v23;
  a4[4]  = v24;
  a4[5]  = v25;
  a4[6]  = v26;
  a4[7]  = v27;
  return result;
}

float64x2_t *SPProjectiveTransform3DConcatenation@<X0>(float64x2_t *result@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>)
{
  float64x2_t v4 = *result;
  float64x2_t v3 = result[1];
  float64x2_t v5 = result[2];
  float64x2_t v6 = result[3];
  float64x2_t v8 = result[4];
  float64x2_t v7 = result[5];
  float64x2_t v10 = result[6];
  float64x2_t v9 = result[7];
  *(void *)&v11.f64[0]  = vdupq_laneq_s64((int64x2_t)v7, 1).u64[0];
  v11.f64[1]  = result[7].f64[1];
  long long v12 = xmmword_228C1F7A0;
  if (vmaxv_u16((uint16x4_t)vmovn_s32((int32x4_t)vmvnq_s8((int8x16_t)vuzp1q_s32((int32x4_t)vceqzq_f64((float64x2_t)vzip2q_s64((int64x2_t)v6, (int64x2_t)v3)), (int32x4_t)vceqq_f64(v11, (float64x2_t)xmmword_228C1F7A0))))))goto LABEL_6; {
  int64x2_t v13 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(v4, (float64x2_t)xmmword_228C1F7D0), (int8x16_t)vceqzq_f64(v3));
  }
  if ((vandq_s8((int8x16_t)v13, (int8x16_t)vdupq_laneq_s64(v13, 1)).u64[0] & 0x8000000000000000) != 0)
  {
    int64x2_t v14 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(v5, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqzq_f64(v6));
    if ((vandq_s8((int8x16_t)v14, (int8x16_t)vdupq_laneq_s64(v14, 1)).u64[0] & 0x8000000000000000) != 0)
    {
      int64x2_t v15 = (int64x2_t)vandq_s8((int8x16_t)vceqzq_f64(v8), (int8x16_t)vceqq_f64(v7, (float64x2_t)xmmword_228C1F7D0));
      if ((vandq_s8((int8x16_t)v15, (int8x16_t)vdupq_laneq_s64(v15, 1)).u64[0] & 0x8000000000000000) != 0
        && a2[1].f64[1] == 0.0
        && a2[3].f64[1] == 0.0
        && a2[5].f64[1] == 0.0
        && a2[7].f64[1] == 1.0)
      {
        int64x2_t v36 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(*a2, (float64x2_t)xmmword_228C1F7D0), (int8x16_t)vceqzq_f64(a2[1]));
        if ((vandq_s8((int8x16_t)v36, (int8x16_t)vdupq_laneq_s64(v36, 1)).u64[0] & 0x8000000000000000) != 0)
        {
          int64x2_t v37 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(a2[2], (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqzq_f64(a2[3]));
          if ((vandq_s8((int8x16_t)v37, (int8x16_t)vdupq_laneq_s64(v37, 1)).u64[0] & 0x8000000000000000) != 0)
          {
            int64x2_t v38 = (int64x2_t)vandq_s8((int8x16_t)vceqzq_f64(a2[4]), (int8x16_t)vceqq_f64(a2[5], (float64x2_t)xmmword_228C1F7D0));
            if ((vandq_s8((int8x16_t)v38, (int8x16_t)vdupq_laneq_s64(v38, 1)).u64[0] & 0x8000000000000000) != 0)
            {
              float64x2_t v25 = vaddq_f64(v10, a2[6]);
              *(void *)&long long v12 = *(_OWORD *)&vaddq_f64(v9, a2[7]);
              *((void *)&v12 + 1)  = *(void *)&result[7].f64[1];
              goto LABEL_9;
            }
          }
        }
      }
    }
  }
  int64x2_t v16 = vceqzq_f64(v10);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v16, 1), vandq_s8((int8x16_t)vceqzq_f64(v9), (int8x16_t)v16)).u64[0] & 0x8000000000000000) != 0
    && a2[1].f64[1] == 0.0
    && a2[3].f64[1] == 0.0
    && a2[5].f64[1] == 0.0
    && a2[7].f64[1] == 1.0
    && (int64x2_t v26 = vceqzq_f64(a2[6]),
        (vandq_s8((int8x16_t)vdupq_laneq_s64(v26, 1), vandq_s8((int8x16_t)vceqzq_f64(a2[7]), (int8x16_t)v26)).u64[0] & 0x8000000000000000) != 0))
  {
    uint64_t v27 = 0;
    float64x2_t v28 = a2[1];
    float64x2_t v29 = a2[2];
    float64x2_t v30 = a2[3];
    float64x2_t v31 = a2[4];
    float64x2_t v32 = a2[5];
    float64x2_t v39 = *a2;
    float64x2_t v40 = v28;
    float64x2_t v41 = v29;
    float64x2_t v42 = v30;
    float64x2_t v43 = v31;
    float64x2_t v44 = v32;
    float64x2_t v47 = 0u;
    float64x2_t v48 = 0u;
    float64x2_t v49 = 0u;
    float64x2_t v50 = 0u;
    float64x2_t v51 = 0u;
    float64x2_t v52 = 0u;
    *(void *)&v4.f64[1]  = vextq_s8((int8x16_t)v4, (int8x16_t)v4, 8uLL).u64[0];
    *(void *)&v5.f64[1]  = vextq_s8((int8x16_t)v5, (int8x16_t)v5, 8uLL).u64[0];
    *(void *)&v8.f64[1]  = vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL).u64[0];
    do
    {
      float64x2_t v34 = *(float64x2_t *)((char *)&v39 + v27);
      float64x2_t v33 = *(float64x2_t *)((char *)&v39 + v27 + 16);
      long long v35 = (float64x2_t *)((char *)&v47 + v27);
      *long long v35 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v4, v34.f64[0]), v5, v34, 1), v8, v33.f64[0]);
      v35[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v3, v34), v6, v34, 1), v33, v7);
      v27 += 32;
    }
    while (v27 != 96);
    float64x2_t v4 = v47;
    float64x2_t v5 = v49;
    float64x2_t v3 = (float64x2_t)*(unint64_t *)&v48.f64[0];
    float64x2_t v8 = v51;
    float64x2_t v6 = (float64x2_t)*(unint64_t *)&v50.f64[0];
    float64x2_t v7 = (float64x2_t)*(unint64_t *)&v52.f64[0];
    float64x2_t v25 = 0uLL;
  }
  else
  {
LABEL_6:
    uint64_t v17 = 0;
    float64x2_t v18 = a2[5];
    float64x2_t v43 = a2[4];
    float64x2_t v44 = v18;
    float64x2_t v19 = a2[7];
    float64x2_t v45 = a2[6];
    float64x2_t v46 = v19;
    float64x2_t v20 = a2[1];
    float64x2_t v39 = *a2;
    float64x2_t v40 = v20;
    float64x2_t v21 = a2[3];
    float64x2_t v41 = a2[2];
    float64x2_t v42 = v21;
    float64x2_t v47 = 0u;
    float64x2_t v48 = 0u;
    float64x2_t v49 = 0u;
    float64x2_t v50 = 0u;
    float64x2_t v51 = 0u;
    float64x2_t v52 = 0u;
    float64x2_t v53 = 0u;
    long long v54 = 0u;
    do
    {
      float64x2_t v23 = *(float64x2_t *)((char *)&v39 + v17);
      float64x2_t v22 = *(float64x2_t *)((char *)&v39 + v17 + 16);
      long long v24 = (float64x2_t *)((char *)&v47 + v17);
      *long long v24 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v4, v23.f64[0]), v5, v23, 1), v8, v22.f64[0]), v10, v22, 1);
      v24[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v3, v23.f64[0]), v6, v23, 1), v7, v22.f64[0]), v9, v22, 1);
      v17 += 32;
    }
    while (v17 != 128);
    float64x2_t v4 = v47;
    float64x2_t v3 = v48;
    float64x2_t v5 = v49;
    float64x2_t v6 = v50;
    float64x2_t v8 = v51;
    float64x2_t v7 = v52;
    float64x2_t v25 = v53;
    long long v12 = v54;
  }
LABEL_9:
  *a3  = v4;
  a3[1]  = v3;
  a3[2]  = v5;
  a3[3]  = v6;
  a3[4]  = v8;
  a3[5]  = v7;
  a3[6]  = v25;
  a3[7]  = (float64x2_t)v12;
  return result;
}

{
  float64x2_t v3;
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  long long v12;
  int64x2_t v13;
  int64x2_t v14;
  int64x2_t v15;
  int64x2_t v16;
  uint64_t v17;
  float64x2_t v18;
  float64x2_t v19;
  float64x2_t v20;
  float64x2_t v21;
  float64x2_t v22;
  float64x2_t v23;
  float64x2_t *v24;
  float64x2_t v25;
  int64x2_t v26;
  uint64_t v27;
  float64x2_t v28;
  float64x2_t v29;
  float64x2_t v30;
  float64x2_t v31;
  float64x2_t v32;
  float64x2_t v33;
  float64x2_t v34;
  float64x2_t *v35;
  int64x2_t v36;
  int64x2_t v37;
  int64x2_t v38;
  float64x2_t v39;
  float64x2_t v40;
  float64x2_t v41;
  float64x2_t v42;
  float64x2_t v43;
  float64x2_t v44;
  float64x2_t v45;
  float64x2_t v46;
  float64x2_t v47;
  float64x2_t v48;
  float64x2_t v49;
  float64x2_t v50;
  float64x2_t v51;
  float64x2_t v52;
  float64x2_t v53;
  long long v54;

  float64x2_t v4 = *result;
  float64x2_t v3 = result[1];
  float64x2_t v5 = result[2];
  float64x2_t v6 = result[3];
  float64x2_t v8 = result[4];
  float64x2_t v7 = result[5];
  float64x2_t v10 = result[6];
  float64x2_t v9 = result[7];
  *(void *)&v11.f64[0]  = vdupq_laneq_s64((int64x2_t)v7, 1).u64[0];
  v11.f64[1]  = result[7].f64[1];
  long long v12 = xmmword_228C1F7A0;
  if (vmaxv_u16((uint16x4_t)vmovn_s32((int32x4_t)vmvnq_s8((int8x16_t)vuzp1q_s32((int32x4_t)vceqzq_f64((float64x2_t)vzip2q_s64((int64x2_t)v6, (int64x2_t)v3)), (int32x4_t)vceqq_f64(v11, (float64x2_t)xmmword_228C1F7A0))))))goto LABEL_6; {
  int64x2_t v13 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(v4, (float64x2_t)xmmword_228C1F7D0), (int8x16_t)vceqzq_f64(v3));
  }
  if ((vandq_s8((int8x16_t)v13, (int8x16_t)vdupq_laneq_s64(v13, 1)).u64[0] & 0x8000000000000000) != 0)
  {
    int64x2_t v14 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(v5, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqzq_f64(v6));
    if ((vandq_s8((int8x16_t)v14, (int8x16_t)vdupq_laneq_s64(v14, 1)).u64[0] & 0x8000000000000000) != 0)
    {
      int64x2_t v15 = (int64x2_t)vandq_s8((int8x16_t)vceqzq_f64(v8), (int8x16_t)vceqq_f64(v7, (float64x2_t)xmmword_228C1F7D0));
      if ((vandq_s8((int8x16_t)v15, (int8x16_t)vdupq_laneq_s64(v15, 1)).u64[0] & 0x8000000000000000) != 0
        && a2[1].f64[1] == 0.0
        && a2[3].f64[1] == 0.0
        && a2[5].f64[1] == 0.0
        && a2[7].f64[1] == 1.0)
      {
        int64x2_t v36 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(*a2, (float64x2_t)xmmword_228C1F7D0), (int8x16_t)vceqzq_f64(a2[1]));
        if ((vandq_s8((int8x16_t)v36, (int8x16_t)vdupq_laneq_s64(v36, 1)).u64[0] & 0x8000000000000000) != 0)
        {
          int64x2_t v37 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(a2[2], (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqzq_f64(a2[3]));
          if ((vandq_s8((int8x16_t)v37, (int8x16_t)vdupq_laneq_s64(v37, 1)).u64[0] & 0x8000000000000000) != 0)
          {
            int64x2_t v38 = (int64x2_t)vandq_s8((int8x16_t)vceqzq_f64(a2[4]), (int8x16_t)vceqq_f64(a2[5], (float64x2_t)xmmword_228C1F7D0));
            if ((vandq_s8((int8x16_t)v38, (int8x16_t)vdupq_laneq_s64(v38, 1)).u64[0] & 0x8000000000000000) != 0)
            {
              float64x2_t v25 = vaddq_f64(v10, a2[6]);
              *(void *)&long long v12 = *(_OWORD *)&vaddq_f64(v9, a2[7]);
              *((void *)&v12 + 1)  = *(void *)&result[7].f64[1];
              goto LABEL_9;
            }
          }
        }
      }
    }
  }
  int64x2_t v16 = vceqzq_f64(v10);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v16, 1), vandq_s8((int8x16_t)vceqzq_f64(v9), (int8x16_t)v16)).u64[0] & 0x8000000000000000) != 0
    && a2[1].f64[1] == 0.0
    && a2[3].f64[1] == 0.0
    && a2[5].f64[1] == 0.0
    && a2[7].f64[1] == 1.0
    && (int64x2_t v26 = vceqzq_f64(a2[6]),
        (vandq_s8((int8x16_t)vdupq_laneq_s64(v26, 1), vandq_s8((int8x16_t)vceqzq_f64(a2[7]), (int8x16_t)v26)).u64[0] & 0x8000000000000000) != 0))
  {
    uint64_t v27 = 0;
    float64x2_t v28 = a2[1];
    float64x2_t v29 = a2[2];
    float64x2_t v30 = a2[3];
    float64x2_t v31 = a2[4];
    float64x2_t v32 = a2[5];
    float64x2_t v39 = *a2;
    float64x2_t v40 = v28;
    float64x2_t v41 = v29;
    float64x2_t v42 = v30;
    float64x2_t v43 = v31;
    float64x2_t v44 = v32;
    float64x2_t v47 = 0u;
    float64x2_t v48 = 0u;
    float64x2_t v49 = 0u;
    float64x2_t v50 = 0u;
    float64x2_t v51 = 0u;
    float64x2_t v52 = 0u;
    *(void *)&v4.f64[1]  = vextq_s8((int8x16_t)v4, (int8x16_t)v4, 8uLL).u64[0];
    *(void *)&v5.f64[1]  = vextq_s8((int8x16_t)v5, (int8x16_t)v5, 8uLL).u64[0];
    *(void *)&v8.f64[1]  = vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL).u64[0];
    do
    {
      float64x2_t v34 = *(float64x2_t *)((char *)&v39 + v27);
      float64x2_t v33 = *(float64x2_t *)((char *)&v39 + v27 + 16);
      long long v35 = (float64x2_t *)((char *)&v47 + v27);
      *long long v35 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v4, v34.f64[0]), v5, v34, 1), v8, v33.f64[0]);
      v35[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v3, v34), v6, v34, 1), v33, v7);
      v27 += 32;
    }
    while (v27 != 96);
    float64x2_t v4 = v47;
    float64x2_t v5 = v49;
    float64x2_t v3 = (float64x2_t)*(unint64_t *)&v48.f64[0];
    float64x2_t v8 = v51;
    float64x2_t v6 = (float64x2_t)*(unint64_t *)&v50.f64[0];
    float64x2_t v7 = (float64x2_t)*(unint64_t *)&v52.f64[0];
    float64x2_t v25 = 0uLL;
  }
  else
  {
LABEL_6:
    uint64_t v17 = 0;
    float64x2_t v18 = a2[5];
    float64x2_t v43 = a2[4];
    float64x2_t v44 = v18;
    float64x2_t v19 = a2[7];
    float64x2_t v45 = a2[6];
    float64x2_t v46 = v19;
    float64x2_t v20 = a2[1];
    float64x2_t v39 = *a2;
    float64x2_t v40 = v20;
    float64x2_t v21 = a2[3];
    float64x2_t v41 = a2[2];
    float64x2_t v42 = v21;
    float64x2_t v47 = 0u;
    float64x2_t v48 = 0u;
    float64x2_t v49 = 0u;
    float64x2_t v50 = 0u;
    float64x2_t v51 = 0u;
    float64x2_t v52 = 0u;
    float64x2_t v53 = 0u;
    long long v54 = 0u;
    do
    {
      float64x2_t v23 = *(float64x2_t *)((char *)&v39 + v17);
      float64x2_t v22 = *(float64x2_t *)((char *)&v39 + v17 + 16);
      long long v24 = (float64x2_t *)((char *)&v47 + v17);
      *long long v24 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v4, v23.f64[0]), v5, v23, 1), v8, v22.f64[0]), v10, v22, 1);
      v24[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v3, v23.f64[0]), v6, v23, 1), v7, v22.f64[0]), v9, v22, 1);
      v17 += 32;
    }
    while (v17 != 128);
    float64x2_t v4 = v47;
    float64x2_t v3 = v48;
    float64x2_t v5 = v49;
    float64x2_t v6 = v50;
    float64x2_t v8 = v51;
    float64x2_t v7 = v52;
    float64x2_t v25 = v53;
    long long v12 = v54;
  }
LABEL_9:
  *a3  = v4;
  a3[1]  = v3;
  a3[2]  = v5;
  a3[3]  = v6;
  a3[4]  = v8;
  a3[5]  = v7;
  a3[6]  = v25;
  a3[7]  = (float64x2_t)v12;
  return result;
}

double static SPProjectiveTransform3D.*= infix(_:_:)(_OWORD *a1, long long *a2)
{
  return static SPAffineTransform3D.*= infix(_:_:)(a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPProjectiveTransform3DConcatenation);
}

double static SPAffineTransform3D.*= infix(_:_:)(_OWORD *a1, long long *a2, void (*a3)(_OWORD *__return_ptr, _OWORD *, _OWORD *))
{
  long long v4 = *a2;
  long long v5 = a2[1];
  long long v6 = a2[2];
  long long v7 = a2[3];
  long long v8 = a2[4];
  long long v9 = a2[5];
  long long v10 = a2[6];
  long long v11 = a2[7];
  long long v12 = a1[1];
  long long v13 = a1[2];
  long long v14 = a1[3];
  long long v15 = a1[4];
  long long v16 = a1[5];
  long long v17 = a1[6];
  long long v18 = a1[7];
  v29[0]  = *a1;
  v29[1]  = v12;
  v29[2]  = v13;
  v29[3]  = v14;
  v29[4]  = v15;
  v29[5]  = v16;
  _OWORD v29[6] = v17;
  v29[7]  = v18;
  v28[0]  = v4;
  v28[1]  = v5;
  v28[2]  = v6;
  v28[3]  = v7;
  v28[4]  = v8;
  v28[5]  = v9;
  v28[6]  = v10;
  v28[7]  = v11;
  a3(v27, v29, v28);
  double result = *(double *)v27;
  long long v20 = v27[1];
  long long v21 = v27[2];
  long long v22 = v27[3];
  long long v23 = v27[4];
  long long v24 = v27[5];
  long long v25 = v27[6];
  long long v26 = v27[7];
  *a1  = v27[0];
  a1[1]  = v20;
  a1[2]  = v21;
  a1[3]  = v22;
  a1[4]  = v23;
  a1[5]  = v24;
  a1[6]  = v25;
  a1[7]  = v26;
  return result;
}

uint64_t * infix<A>(_:_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(uint64_t, uint64_t))(a3 + 8))(a2, a3);
}

double static SPVector3D.+ infix(_:_:)(double a1, double a2, double a3, double a4, double a5, double a6)
{
  return static SPPoint3D.+ infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

{
  return static SPPoint3D.+ infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

{
  return static SPPoint3D.+ infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

{
  return static SPPoint3D.+ infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

{
  return static SPPoint3D.+ infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

double static SPVector3D.- infix(_:_:)(double a1, double a2, double a3, double a4, double a5, double a6)
{
  return static SPPoint3D.- infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

{
  return static SPPoint3D.- infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

{
  return static SPPoint3D.- infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

{
  return static SPPoint3D.- infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

{
  return static SPPoint3D.- infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4, a5, a6);
}

double static SPVector3D.- prefix(_:)(float64x2_t a1, float64_t a2, double a3)
{
  return static SPPoint3D.- prefix(_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3);
}

double static SPPoint3D.- prefix(_:)(void (*a1)(double *__return_ptr, _OWORD *), float64x2_t a2, float64_t a3, double a4)
{
  a2.f64[1]  = a3;
  v6[0]  = vsubq_f64((float64x2_t)0, a2);
  v6[1]  = COERCE_UNSIGNED_INT64(0.0 - a4);
  a1(&v5, v6);
  return v5;
}

uint64_t static SPVector3D.+= infix(_:_:)(float64x2_t *a1, float64x2_t a2, float64_t a3, double a4)
{
  return static SPPoint3D.+= infix(_:_:)(a1, (uint64_t (*)(_OWORD *))SPSize3DMakeWithVector, a2, a3, a4);
}

uint64_t static SPVector3D.-= infix(_:_:)(float64x2_t *a1, float64x2_t a2, float64_t a3, double a4)
{
  return static SPPoint3D.-= infix(_:_:)(a1, (uint64_t (*)(_OWORD *))SPSize3DMakeWithVector, a2, a3, a4);
}

double static SPVector3D.* infix(_:_:)(double a1, float64x2_t a2, float64_t a3, double a4)
{
  return static SPPoint3D.* infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4);
}

double static SPPoint3D.* infix(_:_:)(void (*a1)(double *__return_ptr, _OWORD *), double a2, float64x2_t a3, float64_t a4, double a5)
{
  a3.f64[1]  = a4;
  v7[0]  = vmulq_n_f64(a3, a2);
  v7[1]  = COERCE_UNSIGNED_INT64(a2 * a5);
  a1(&v6, v7);
  return v6;
}

double static SPVector3D.* infix(_:_:)(float64x2_t a1, float64_t a2, double a3, double a4)
{
  return static SPPoint3D.* infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4);
}

double static SPPoint3D.* infix(_:_:)(void (*a1)(double *__return_ptr, _OWORD *), float64x2_t a2, float64_t a3, double a4, double a5)
{
  a2.f64[1]  = a3;
  v7[0]  = vmulq_n_f64(a2, a5);
  v7[1]  = COERCE_UNSIGNED_INT64(a4 * a5);
  a1(&v6, v7);
  return v6;
}

uint64_t static SPVector3D.*= infix(_:_:)(float64x2_t *a1, double a2)
{
  return static SPPoint3D.*= infix(_:_:)(a1, (uint64_t (*)(_OWORD *))SPSize3DMakeWithVector, a2);
}

uint64_t static SPPoint3D.*= infix(_:_:)(float64x2_t *a1, uint64_t (*a2)(_OWORD *), double a3)
{
  *(double *)&unint64_t v3 = a1[1].f64[0] * a3;
  v5[0]  = vmulq_n_f64(*a1, a3);
  v5[1]  = v3;
  return a2(v5);
}

double static SPVector3D./ infix(_:_:)(float64x2_t a1, float64_t a2, double a3, double a4)
{
  return static SPPoint3D./ infix(_:_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1, a2, a3, a4);
}

double static SPPoint3D./ infix(_:_:)(void (*a1)(double *__return_ptr, _OWORD *), float64x2_t a2, float64_t a3, double a4, double a5)
{
  a2.f64[1]  = a3;
  v7[0]  = vdivq_f64(a2, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&a5, 0));
  v7[1]  = COERCE_UNSIGNED_INT64(a4 / a5);
  a1(&v6, v7);
  return v6;
}

uint64_t static SPVector3D./= infix(_:_:)(float64x2_t *a1, double a2)
{
  return static SPPoint3D./= infix(_:_:)(a1, (uint64_t (*)(_OWORD *))SPSize3DMakeWithVector, a2);
}

uint64_t static SPPoint3D./= infix(_:_:)(float64x2_t *a1, uint64_t (*a2)(_OWORD *), double a3)
{
  *(double *)&unint64_t v3 = a1[1].f64[0] / a3;
  v5[0]  = vdivq_f64(*a1, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&a3, 0));
  v5[1]  = v3;
  return a2(v5);
}

double static SPVector3D.* infix(_:_:)(long long *a1, double a2, double a3, double a4)
{
  return static SPVector3D.* infix(_:_:)(a1, (void (*)(double *__return_ptr, void *, _OWORD *))SPSize3DApplyAffineTransform, a2, a3, a4);
}

{
  return static SPVector3D.* infix(_:_:)(a1, (void (*)(double *__return_ptr, void *, _OWORD *))SPSize3DApplyProjectiveTransform, a2, a3, a4);
}

double SPSize3DApplyAffineTransform@<D0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>)
{
  float64x2_t v3 = a1[1];
  float64x2_t v4 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, a1->f64[0]), a2[2], *a1, 1), a2[4], v3.f64[0]);
  float64x2_t v5 = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*a1, a2[1]), a2[3], *a1, 1), v3, a2[5]);
  double result = 0.0;
  *(void *)&v5.f64[0]  = *(_OWORD *)&vmlaq_f64(v5, (float64x2_t)0, a2[7]);
  *a3  = vmlaq_f64(v4, (float64x2_t)0, a2[6]);
  a3[1].f64[0]  = v5.f64[0];
  return result;
}

{
  float64x2_t v3;
  float64x2_t v4;
  float64x2_t v5;
  double result;

  float64x2_t v3 = a1[1];
  float64x2_t v4 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, a1->f64[0]), a2[2], *a1, 1), a2[4], v3.f64[0]);
  float64x2_t v5 = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*a1, a2[1]), a2[3], *a1, 1), v3, a2[5]);
  double result = 0.0;
  *(void *)&v5.f64[0]  = *(_OWORD *)&vmlaq_f64(v5, (float64x2_t)0, a2[7]);
  *a3  = vmlaq_f64(v4, (float64x2_t)0, a2[6]);
  a3[1].f64[0]  = v5.f64[0];
  return result;
}

{
  float64x2_t v3;
  float64x2_t v4;
  float64x2_t v5;
  double result;

  float64x2_t v3 = a1[1];
  float64x2_t v4 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, a1->f64[0]), a2[2], *a1, 1), a2[4], v3.f64[0]);
  float64x2_t v5 = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*a1, a2[1]), a2[3], *a1, 1), v3, a2[5]);
  double result = 0.0;
  *(void *)&v5.f64[0]  = *(_OWORD *)&vmlaq_f64(v5, (float64x2_t)0, a2[7]);
  *a3  = vmlaq_f64(v4, (float64x2_t)0, a2[6]);
  a3[1].f64[0]  = v5.f64[0];
  return result;
}

double static SPVector3D.* infix(_:_:)(long long *a1, void (*a2)(double *__return_ptr, void *, _OWORD *), double a3, double a4, double a5)
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  *(double *)long long v16 = a3;
  *(double *)&v16[1]  = a4;
  *(double *)&v16[2]  = a5;
  v15[0]  = v5;
  v15[1]  = v6;
  v15[2]  = v7;
  v15[3]  = v8;
  v15[4]  = v9;
  v15[5]  = v10;
  v15[6]  = v11;
  v15[7]  = v12;
  a2(&v14, v16, v15);
  return v14;
}

double SPSize3DApplyProjectiveTransform@<D0>(uint64_t a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>)
{
  long long v3 = *(_OWORD *)(a1 + 16);
  float64x2_t v4 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, *(double *)a1), a2[2], *(float64x2_t *)a1, 1), a2[4], *(double *)&v3);
  float64x2_t v5 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(a2[1], *(double *)a1), a2[3], *(float64x2_t *)a1, 1), a2[5], *(double *)&v3);
  double result = 0.0;
  *(void *)&v5.f64[0]  = *(_OWORD *)&vmlaq_f64(v5, (float64x2_t)0, a2[7]);
  *a3  = vmlaq_f64(v4, (float64x2_t)0, a2[6]);
  a3[1].f64[0]  = v5.f64[0];
  return result;
}

{
  long long v3;
  float64x2_t v4;
  float64x2_t v5;
  double result;

  long long v3 = *(_OWORD *)(a1 + 16);
  float64x2_t v4 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, *(double *)a1), a2[2], *(float64x2_t *)a1, 1), a2[4], *(double *)&v3);
  float64x2_t v5 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(a2[1], *(double *)a1), a2[3], *(float64x2_t *)a1, 1), a2[5], *(double *)&v3);
  double result = 0.0;
  *(void *)&v5.f64[0]  = *(_OWORD *)&vmlaq_f64(v5, (float64x2_t)0, a2[7]);
  *a3  = vmlaq_f64(v4, (float64x2_t)0, a2[6]);
  a3[1].f64[0]  = v5.f64[0];
  return result;
}

{
  long long v3;
  float64x2_t v4;
  float64x2_t v5;
  double result;

  long long v3 = *(_OWORD *)(a1 + 16);
  float64x2_t v4 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, *(double *)a1), a2[2], *(float64x2_t *)a1, 1), a2[4], *(double *)&v3);
  float64x2_t v5 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(a2[1], *(double *)a1), a2[3], *(float64x2_t *)a1, 1), a2[5], *(double *)&v3);
  double result = 0.0;
  *(void *)&v5.f64[0]  = *(_OWORD *)&vmlaq_f64(v5, (float64x2_t)0, a2[7]);
  *a3  = vmlaq_f64(v4, (float64x2_t)0, a2[6]);
  a3[1].f64[0]  = v5.f64[0];
  return result;
}

float64_t SPSize3DApplyPose@<D0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>, float64x2_t _Q7@<Q7>)
{
  float64x2_t v6 = a1[1];
  _Q3  = a2[2];
  _Q2  = a2[3];
  _D5  = a2[2].f64[1];
  __asm { FMLS            D4, D2, V2.D[0] }
  _Q7.f64[0]  = a2[3].f64[1];
  __asm { FMLA            D4, D7, V2.D[1] }
  double v16 = vmlad_n_f64(vmuld_lane_f64(_Q2.f64[0], _Q2, 1), _D5, _Q3.f64[0]);
  v17.f64[0]  = vmuld_lane_f64(_D5, _Q2, 1);
  v4.f64[0]  = vmlad_n_f64(-(_D5 * _Q7.f64[0]), _Q2.f64[0], _Q3.f64[0]);
  v4.f64[0]  = v4.f64[0] + v4.f64[0];
  _Q4.f64[1]  = v16 + v16;
  double v18 = vmlad_n_f64(-(_Q2.f64[0] * _Q7.f64[0]), _D5, _Q3.f64[0]);
  v19.f64[0]  = v18 + v18;
  __asm
  {
    FMLA            D6, D5, V3.D[1]
    FMLA            D6, D7, V2.D[1]
    FMLS            D6, D3, V3.D[0]
    FMLA            D19, D2, V3.D[1]
  }
  _Q19.f64[0]  = _Q19.f64[0] + _Q19.f64[0];
  v19.f64[1]  = _D6;
  float64_t v23 = -(_Q3.f64[0] * _Q7.f64[0]);
  float64x2_t v24 = (float64x2_t)vzip1q_s64((int64x2_t)_Q3, (int64x2_t)_Q2);
  __asm
  {
    FMLS            D7, D3, V3.D[0]
    FMLS            D7, D5, V3.D[1]
  }
  _Q3.f64[0]  = a2[3].f64[0];
  v17.f64[1]  = v23;
  float64x2_t v25 = vmlaq_f64(v17, v24, _Q3);
  float64x2_t v26 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q4, a1->f64[0]), v19, *a1, 1), vaddq_f64(v25, v25), v6.f64[0]), (float64x2_t)0);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*a1, v4), _Q19, *a1, 1), v6, _Q7), (float64x2_t)0);
  *a3  = v26;
  a3[1].f64[0]  = result;
  return result;
}

double static SPPoint3D.+ infix(_:_:)(void (*a1)(double *__return_ptr, _OWORD *), double a2, double a3, double a4, double a5, double a6, double a7)
{
  *(double *)&long long v7 = a2 + a5;
  *((double *)&v7 + 1)  = a3 + a6;
  v10[0]  = v7;
  v10[1]  = COERCE_UNSIGNED_INT64(a4 + a7);
  a1(&v9, v10);
  return v9;
}

double static SPPoint3D.- infix(_:_:)(void (*a1)(double *__return_ptr, _OWORD *), double a2, double a3, double a4, double a5, double a6, double a7)
{
  *(double *)&long long v7 = a2 - a5;
  *((double *)&v7 + 1)  = a3 - a6;
  v10[0]  = v7;
  v10[1]  = COERCE_UNSIGNED_INT64(a4 - a7);
  a1(&v9, v10);
  return v9;
}

double static SPRay3D.* infix(_:_:)@<D0>(double *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  long long v4 = *(_OWORD *)a1;
  long long v5 = *((_OWORD *)a1 + 1);
  long long v6 = *((_OWORD *)a1 + 2);
  double v7 = a1[6];
  double v8 = a1[7];
  long long v9 = *(_OWORD *)(a2 + 16);
  long long v10 = *(_OWORD *)(a2 + 32);
  double v11 = *(double *)(a2 + 48);
  double v12 = *(double *)(a2 + 56);
  *(_OWORD *)&v22.origin.x  = *(_OWORD *)a2;
  *(_OWORD *)&v22.origin.vector.f64[2]  = v9;
  v22.direction.double z = v11;
  v22.direction.vector.f64[3]  = v12;
  *(_OWORD *)&v22.direction.x  = v10;
  *(_OWORD *)&v21.position.x  = v4;
  *(_OWORD *)&v21.position.vector.f64[2]  = v5;
  v21.rotation.vector.f64[2]  = v7;
  v21.rotation.vector.f64[3]  = v8;
  *(_OWORD *)v21.rotation.vector.f64  = v6;
  SPRay3DApplyPose(&v22, &v21, v18);
  double result = *(double *)v18;
  long long v14 = v18[1];
  long long v15 = v18[2];
  uint64_t v16 = v19;
  uint64_t v17 = v20;
  *(_OWORD *)a3  = v18[0];
  *(_OWORD *)(a3 + 16)  = v14;
  *(void *)(a3 + 48)  = v16;
  *(void *)(a3 + 56)  = v17;
  *(_OWORD *)(a3 + 32)  = v15;
  return result;
}

__n128 SPRay3DApplyPose@<Q0>(SPRay3D *a1@<X0>, SPPose3D *a2@<X1>, _OWORD *a3@<X8>)
{
  float64x2_t v6 = *(float64x2_t *)&a1->origin.vector.f64[2];
  float64x2_t v8 = *(float64x2_t *)&a1->direction.x;
  float64x2_t v7 = *(float64x2_t *)&a1->direction.vector.f64[2];
  _Q6  = *(float64x2_t *)a2->rotation.vector.f64;
  _Q5  = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  _D7  = a2->rotation.vector.f64[1];
  __asm { FMLS            D3, D5, V5.D[0] }
  _Q17.f64[0]  = a2->rotation.vector.f64[3];
  __asm { FMLA            D3, D17, V5.D[1] }
  v5.f64[0]  = vmlad_n_f64(vmuld_lane_f64(_Q5.f64[0], _Q5, 1), _D7, _Q6.f64[0]);
  v18.f64[0]  = vmuld_lane_f64(_D7, _Q5, 1);
  double v19 = vmlad_n_f64(-(_D7 * _Q17.f64[0]), _Q5.f64[0], _Q6.f64[0]);
  _Q3.f64[1]  = v5.f64[0] + v5.f64[0];
  v5.f64[0]  = v19 + v19;
  double v20 = vmlad_n_f64(-(_Q5.f64[0] * _Q17.f64[0]), _D7, _Q6.f64[0]);
  v21.f64[0]  = v20 + v20;
  __asm
  {
    FMLA            D16, D7, V6.D[1]
    FMLA            D16, D17, V5.D[1]
    FMLS            D16, D6, V6.D[0]
    FMLA            D21, D5, V6.D[1]
  }
  v21.f64[1]  = _Q16.f64[0];
  _Q16.f64[0]  = _D21 + _D21;
  float64_t v23 = -(_Q6.f64[0] * _Q17.f64[0]);
  float64x2_t v24 = (float64x2_t)vzip1q_s64((int64x2_t)_Q6, (int64x2_t)_Q5);
  __asm
  {
    FMLS            D17, D6, V6.D[0]
    FMLS            D17, D7, V6.D[1]
  }
  _Q6.f64[0]  = a2->rotation.vector.f64[2];
  v18.f64[1]  = v23;
  float64x2_t v25 = vmlaq_f64(v18, v24, _Q6);
  float64x2_t v26 = vaddq_f64(v25, v25);
  float64x2_t v27 = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)&a1->origin.x, v5), _Q16, *(float64x2_t *)&a1->origin.x, 1), v6, _Q17);
  float64x2_t v28 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q3, a1->origin.x), v21, *(float64x2_t *)&a1->origin.x, 1), v26, v6.f64[0]), (float64x2_t)0);
  unint64_t v29 = vextq_s8((int8x16_t)v28, (int8x16_t)v28, 8uLL).u64[0];
  unint64_t v30 = *(_OWORD *)&vaddq_f64(v27, (float64x2_t)0);
  float64x2_t v31 = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v8, v5), _Q16, v8, 1), v7, _Q17);
  float64x2_t v32 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q3, v8.f64[0]), v21, v8, 1), v26, v7.f64[0]), (float64x2_t)0);
  float64x2_t v33 = vaddq_f64(v31, (float64x2_t)0);
  float64x2_t v34 = (float64x2_t)*(unint64_t *)&v33.f64[0];
  int64x2_t v35 = vceqzq_f64(v32);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v35, 1), vandq_s8((int8x16_t)vceqzq_f64(v33), (int8x16_t)v35)).u64[0] & 0x8000000000000000) == 0)
  {
    float64x2_t v36 = vmulq_f64((float64x2_t)*(unint64_t *)&v33.f64[0], (float64x2_t)*(unint64_t *)&v33.f64[0]);
    v36.f64[0]  = 1.0 / sqrt(v36.f64[0] + vaddvq_f64(vmulq_f64(v32, v32)));
    *(void *)&v32.f64[1]  = vextq_s8((int8x16_t)v32, (int8x16_t)v32, 8uLL).u64[0];
    float64x2_t v32 = vmulq_n_f64(v32, v36.f64[0]);
    float64x2_t v34 = (float64x2_t)(unint64_t)*(_OWORD *)&vmulq_f64(v33, v36);
  }
  *(void *)&v28.f64[1]  = v29;
  *(float64x2_t *)&a1->origin.x  = v28;
  *(_OWORD *)&a1->origin.vector.f64[2]  = v30;
  *(float64x2_t *)&a1->direction.x  = v32;
  *(float64x2_t *)&a1->direction.vector.f64[2]  = v34;
  float64x2_t v37 = *(float64x2_t *)&a2->position.vector.f64[2];
  float64x2_t v38 = *(float64x2_t *)&a1->origin.vector.f64[2];
  int64x2_t v39 = vceqzq_f64(v32);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v39, 1), vandq_s8((int8x16_t)vceqzq_f64(v34), (int8x16_t)v39)).u64[0] & 0x8000000000000000) == 0)
  {
    float64x2_t v40 = vmulq_f64(v34, v34);
    v40.f64[0]  = 1.0 / sqrt(v40.f64[0] + vaddvq_f64(vmulq_f64(v32, v32)));
    float64x2_t v32 = vmulq_n_f64(v32, v40.f64[0]);
    float64x2_t v34 = (float64x2_t)(unint64_t)*(_OWORD *)&vmulq_f64(v34, v40);
  }
  *(float64x2_t *)&a1->origin.x  = vaddq_f64(*(float64x2_t *)&a2->position.x, v28);
  *(_OWORD *)&a1->origin.vector.f64[2]  = (unint64_t)*(_OWORD *)&vaddq_f64(v37, v38);
  *(float64x2_t *)&a1->direction.x  = v32;
  *(float64x2_t *)&a1->direction.vector.f64[2]  = v34;
  long long v41 = *(_OWORD *)&a1->direction.vector.f64[2];
  a3[2]  = *(_OWORD *)&a1->direction.x;
  a3[3]  = v41;
  __n128 result = *(__n128 *)&a1->origin.x;
  long long v43 = *(_OWORD *)&a1->origin.vector.f64[2];
  *a3  = *(_OWORD *)&a1->origin.x;
  a3[1]  = v43;
  return result;
}

{
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v18;
  double v19;
  double v20;
  float64x2_t v21;
  float64_t v23;
  float64x2_t v24;
  float64x2_t v25;
  float64x2_t v26;
  float64x2_t v27;
  float64x2_t v28;
  unint64_t v29;
  unint64_t v30;
  float64x2_t v31;
  float64x2_t v32;
  float64x2_t v33;
  float64x2_t v34;
  int64x2_t v35;
  float64x2_t v36;
  float64x2_t v37;
  float64x2_t v38;
  int64x2_t v39;
  float64x2_t v40;
  long long v41;
  __n128 result;
  long long v43;

  float64x2_t v6 = *(float64x2_t *)&a1->origin.vector.f64[2];
  float64x2_t v8 = *(float64x2_t *)&a1->direction.x;
  float64x2_t v7 = *(float64x2_t *)&a1->direction.vector.f64[2];
  _Q6  = *(float64x2_t *)a2->rotation.vector.f64;
  _Q5  = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  _D7  = a2->rotation.vector.f64[1];
  __asm { FMLS            D3, D5, V5.D[0] }
  _Q17.f64[0]  = a2->rotation.vector.f64[3];
  __asm { FMLA            D3, D17, V5.D[1] }
  v5.f64[0]  = vmlad_n_f64(vmuld_lane_f64(_Q5.f64[0], _Q5, 1), _D7, _Q6.f64[0]);
  v18.f64[0]  = vmuld_lane_f64(_D7, _Q5, 1);
  double v19 = vmlad_n_f64(-(_D7 * _Q17.f64[0]), _Q5.f64[0], _Q6.f64[0]);
  _Q3.f64[1]  = v5.f64[0] + v5.f64[0];
  v5.f64[0]  = v19 + v19;
  double v20 = vmlad_n_f64(-(_Q5.f64[0] * _Q17.f64[0]), _D7, _Q6.f64[0]);
  v21.f64[0]  = v20 + v20;
  __asm
  {
    FMLA            D16, D7, V6.D[1]
    FMLA            D16, D17, V5.D[1]
    FMLS            D16, D6, V6.D[0]
    FMLA            D21, D5, V6.D[1]
  }
  v21.f64[1]  = _Q16.f64[0];
  _Q16.f64[0]  = _D21 + _D21;
  float64_t v23 = -(_Q6.f64[0] * _Q17.f64[0]);
  float64x2_t v24 = (float64x2_t)vzip1q_s64((int64x2_t)_Q6, (int64x2_t)_Q5);
  __asm
  {
    FMLS            D17, D6, V6.D[0]
    FMLS            D17, D7, V6.D[1]
  }
  _Q6.f64[0]  = a2->rotation.vector.f64[2];
  v18.f64[1]  = v23;
  float64x2_t v25 = vmlaq_f64(v18, v24, _Q6);
  float64x2_t v26 = vaddq_f64(v25, v25);
  float64x2_t v27 = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)&a1->origin.x, v5), _Q16, *(float64x2_t *)&a1->origin.x, 1), v6, _Q17);
  float64x2_t v28 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q3, a1->origin.x), v21, *(float64x2_t *)&a1->origin.x, 1), v26, v6.f64[0]), (float64x2_t)0);
  unint64_t v29 = vextq_s8((int8x16_t)v28, (int8x16_t)v28, 8uLL).u64[0];
  unint64_t v30 = *(_OWORD *)&vaddq_f64(v27, (float64x2_t)0);
  float64x2_t v31 = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v8, v5), _Q16, v8, 1), v7, _Q17);
  float64x2_t v32 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q3, v8.f64[0]), v21, v8, 1), v26, v7.f64[0]), (float64x2_t)0);
  float64x2_t v33 = vaddq_f64(v31, (float64x2_t)0);
  float64x2_t v34 = (float64x2_t)*(unint64_t *)&v33.f64[0];
  int64x2_t v35 = vceqzq_f64(v32);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v35, 1), vandq_s8((int8x16_t)vceqzq_f64(v33), (int8x16_t)v35)).u64[0] & 0x8000000000000000) == 0)
  {
    float64x2_t v36 = vmulq_f64((float64x2_t)*(unint64_t *)&v33.f64[0], (float64x2_t)*(unint64_t *)&v33.f64[0]);
    v36.f64[0]  = 1.0 / sqrt(v36.f64[0] + vaddvq_f64(vmulq_f64(v32, v32)));
    *(void *)&v32.f64[1]  = vextq_s8((int8x16_t)v32, (int8x16_t)v32, 8uLL).u64[0];
    float64x2_t v32 = vmulq_n_f64(v32, v36.f64[0]);
    float64x2_t v34 = (float64x2_t)(unint64_t)*(_OWORD *)&vmulq_f64(v33, v36);
  }
  *(void *)&v28.f64[1]  = v29;
  *(float64x2_t *)&a1->origin.x  = v28;
  *(_OWORD *)&a1->origin.vector.f64[2]  = v30;
  *(float64x2_t *)&a1->direction.x  = v32;
  *(float64x2_t *)&a1->direction.vector.f64[2]  = v34;
  float64x2_t v37 = *(float64x2_t *)&a2->position.vector.f64[2];
  float64x2_t v38 = *(float64x2_t *)&a1->origin.vector.f64[2];
  int64x2_t v39 = vceqzq_f64(v32);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v39, 1), vandq_s8((int8x16_t)vceqzq_f64(v34), (int8x16_t)v39)).u64[0] & 0x8000000000000000) == 0)
  {
    float64x2_t v40 = vmulq_f64(v34, v34);
    v40.f64[0]  = 1.0 / sqrt(v40.f64[0] + vaddvq_f64(vmulq_f64(v32, v32)));
    float64x2_t v32 = vmulq_n_f64(v32, v40.f64[0]);
    float64x2_t v34 = (float64x2_t)(unint64_t)*(_OWORD *)&vmulq_f64(v34, v40);
  }
  *(float64x2_t *)&a1->origin.x  = vaddq_f64(*(float64x2_t *)&a2->position.x, v28);
  *(_OWORD *)&a1->origin.vector.f64[2]  = (unint64_t)*(_OWORD *)&vaddq_f64(v37, v38);
  *(float64x2_t *)&a1->direction.x  = v32;
  *(float64x2_t *)&a1->direction.vector.f64[2]  = v34;
  long long v41 = *(_OWORD *)&a1->direction.vector.f64[2];
  a3[2]  = *(_OWORD *)&a1->direction.x;
  a3[3]  = v41;
  __n128 result = *(__n128 *)&a1->origin.x;
  long long v43 = *(_OWORD *)&a1->origin.vector.f64[2];
  *a3  = *(_OWORD *)&a1->origin.x;
  a3[1]  = v43;
  return result;
}

uint64_t static SPPoint3D.+= infix(_:_:)(float64x2_t *a1, uint64_t (*a2)(_OWORD *), float64x2_t a3, float64_t a4, double a5)
{
  a3.f64[1]  = a4;
  *(double *)&unint64_t v5 = a1[1].f64[0] + a5;
  v7[0]  = vaddq_f64(*a1, a3);
  v7[1]  = v5;
  return a2(v7);
}

uint64_t static SPPoint3D.-= infix(_:_:)(float64x2_t *a1, uint64_t (*a2)(_OWORD *), float64x2_t a3, float64_t a4, double a5)
{
  a3.f64[1]  = a4;
  *(double *)&unint64_t v5 = a1[1].f64[0] - a5;
  v7[0]  = vsubq_f64(*a1, a3);
  v7[1]  = v5;
  return a2(v7);
}

float64_t static SPPose3D.* infix(_:_:)@<D0>(uint64_t a1@<X0>, double *a2@<X1>, uint64_t a3@<X8>)
{
  long long v4 = *(_OWORD *)a2;
  long long v5 = *((_OWORD *)a2 + 1);
  long long v6 = *((_OWORD *)a2 + 2);
  double v7 = a2[6];
  double v8 = a2[7];
  long long v9 = *(_OWORD *)(a1 + 16);
  long long v10 = *(_OWORD *)(a1 + 32);
  double v11 = *(double *)(a1 + 48);
  double v12 = *(double *)(a1 + 56);
  *(_OWORD *)&v22.position.x  = *(_OWORD *)a1;
  *(_OWORD *)&v22.position.vector.f64[2]  = v9;
  v22.rotation.vector.f64[2]  = v11;
  v22.rotation.vector.f64[3]  = v12;
  *(_OWORD *)v22.rotation.vector.f64  = v10;
  *(_OWORD *)&v21.position.x  = v4;
  *(_OWORD *)&v21.position.vector.f64[2]  = v5;
  v21.rotation.vector.f64[2]  = v7;
  v21.rotation.vector.f64[3]  = v8;
  *(_OWORD *)v21.rotation.vector.f64  = v6;
  SPPose3DConcatenation(&v22, &v21, v18);
  float64_t result = v18[0].f64[0];
  float64x2_t v14 = v18[1];
  float64x2_t v15 = v18[2];
  uint64_t v16 = v19;
  uint64_t v17 = v20;
  *(float64x2_t *)a3  = v18[0];
  *(float64x2_t *)(a3 + 16)  = v14;
  *(void *)(a3 + 48)  = v16;
  *(void *)(a3 + 56)  = v17;
  *(float64x2_t *)(a3 + 32)  = v15;
  return result;
}

float64_t SPPose3DConcatenation@<D0>(SPPose3D *a1@<X0>, SPPose3D *a2@<X1>, float64x2_t *a3@<X8>)
{
  float64x2_t v4 = *(float64x2_t *)a1->rotation.vector.f64;
  float64x2_t v3 = *(float64x2_t *)&a1->rotation.quaternion.vector.f64[2];
  float64x2_t v6 = *(float64x2_t *)a2->rotation.vector.f64;
  float64x2_t v5 = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  float64x2_t v7 = vnegq_f64(v6);
  float64x2_t v8 = (float64x2_t)vextq_s8((int8x16_t)v5, (int8x16_t)vnegq_f64(v5), 8uLL);
  float64x2_t v9 = (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v7, 8uLL);
  float64x2_t v10 = vmlaq_n_f64(vmulq_laneq_f64(v7, v4, 1), v9, v4.f64[0]);
  float64x2_t v11 = (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v6, 8uLL);
  float64x2_t v12 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v6, v3, 1), v11, v3.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v5, v4, 1), v8, v4.f64[0]));
  float64x2_t v13 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v5, v3, 1), v8, v3.f64[0]), v10);
  double v14 = vaddvq_f64(vaddq_f64(vmulq_f64(v12, v12), vmulq_f64(v13, v13)));
  if (v14 == 0.0)
  {
    float64x2_t v15 = (float64x2_t)xmmword_228C1F7A0;
    float64x2_t v16 = 0uLL;
  }
  else
  {
    double v17 = 1.0 / sqrt(v14);
    float64x2_t v15 = vmulq_n_f64(v13, v17);
    float64x2_t v16 = vmulq_n_f64(v12, v17);
  }
  long long v18 = *(_OWORD *)&a1->position.vector.f64[2];
  float64x2_t v19 = vmulq_f64(v5, (float64x2_t)xmmword_228C1FC40);
  float64x2_t v20 = (float64x2_t)vextq_s8((int8x16_t)v19, (int8x16_t)vnegq_f64(v19), 8uLL);
  float64x2_t v21 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v6, *(float64x2_t *)&a1->position.x, 1), v11, a1->position.x), v20, *(double *)&v18);
  float64x2_t v22 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v19, *(float64x2_t *)&a1->position.x, 1), v20, a1->position.x), v9, *(double *)&v18);
  float64x2_t v23 = vnegq_f64(v22);
  float64x2_t v24 = (float64x2_t)vextq_s8((int8x16_t)v21, (int8x16_t)vnegq_f64(v21), 8uLL);
  float64x2_t v25 = vmlaq_n_f64(vmulq_laneq_f64(v21, v6, 1), v24, v6.f64[0]);
  float64x2_t v26 = vmlaq_n_f64(vmulq_laneq_f64(v23, v6, 1), (float64x2_t)vextq_s8((int8x16_t)v22, (int8x16_t)v23, 8uLL), v6.f64[0]);
  float64x2_t v27 = (float64x2_t)vextq_s8((int8x16_t)v23, (int8x16_t)v22, 8uLL);
  float64x2_t v28 = vmlaq_n_f64(vmulq_laneq_f64(v21, v5, 1), v24, v5.f64[0]);
  float64x2_t v29 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v22, v5, 1), v27, v5.f64[0]), v25);
  float64x2_t v30 = vaddq_f64(v28, v26);
  float64x2_t v31 = *(float64x2_t *)&a2->position.vector.f64[2];
  *a3  = vaddq_f64(*(float64x2_t *)&a2->position.x, v29);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(v31, v30);
  a3[1].f64[0]  = result;
  a3[2]  = v16;
  a3[3]  = v15;
  return result;
}

float64_t static SPPose3D.*= infix(_:_:)(uint64_t a1, double *a2)
{
  long long v3 = *(_OWORD *)a2;
  long long v4 = *((_OWORD *)a2 + 1);
  long long v5 = *((_OWORD *)a2 + 2);
  double v6 = a2[6];
  double v7 = a2[7];
  long long v8 = *(_OWORD *)(a1 + 16);
  long long v9 = *(_OWORD *)(a1 + 32);
  double v10 = *(double *)(a1 + 48);
  double v11 = *(double *)(a1 + 56);
  *(_OWORD *)&v21.position.x  = *(_OWORD *)a1;
  *(_OWORD *)&v21.position.vector.f64[2]  = v8;
  v21.rotation.vector.f64[2]  = v10;
  v21.rotation.vector.f64[3]  = v11;
  *(_OWORD *)v21.rotation.vector.f64  = v9;
  *(_OWORD *)&v20.position.x  = v3;
  *(_OWORD *)&v20.position.vector.f64[2]  = v4;
  v20.rotation.vector.f64[2]  = v6;
  v20.rotation.vector.f64[3]  = v7;
  *(_OWORD *)v20.rotation.vector.f64  = v5;
  SPPose3DConcatenation(&v21, &v20, v17);
  float64_t result = v17[0].f64[0];
  float64x2_t v13 = v17[1];
  float64x2_t v14 = v17[2];
  uint64_t v15 = v18;
  uint64_t v16 = v19;
  *(float64x2_t *)a1  = v17[0];
  *(float64x2_t *)(a1 + 16)  = v13;
  *(void *)(a1 + 48)  = v15;
  *(void *)(a1 + 56)  = v16;
  *(float64x2_t *)(a1 + 32)  = v14;
  return result;
}

double static SPRotation3D.* infix(_:_:)(float64x2_t a1, float64x2_t a2, double a3)
{
  uint64_t v7 = 0;
  uint64_t v8 = 0x3FF0000000000000;
  float64x2_t v6 = 0uLL;
  v5[0]  = a1;
  v5[1]  = a2;
  if (a2.f64[1] >= 0.0) {
    SPRotation3DSlerp(&v6, v5, (uint64_t)&v4, a3);
  }
  else {
    SPRotation3DSlerpLongest(&v6, v5, (uint64_t)&v4, a3);
  }
  return v4;
}

float64_t SPRotation3DSlerpLongest@<D0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>, double a4@<D0>)
{
  float64x2_t v13 = 0u;
  long long v14 = 0u;
  float64x2_t v5 = *a1;
  float64x2_t v6 = a1[1];
  float64x2_t v7 = *a2;
  float64x2_t v8 = a2[1];
  if (vaddvq_f64(vaddq_f64(vmulq_f64(*a1, *a2), vmulq_f64(v6, v8))) >= 0.0)
  {
    float64x2_t v17 = vnegq_f64(v7);
    float64x2_t v18 = vnegq_f64(v8);
    float64x2_t v15 = v5;
    float64x2_t v16 = v6;
    long long v9 = &v15;
    double v10 = &v17;
  }
  else
  {
    float64x2_t v17 = *a1;
    float64x2_t v18 = v6;
    float64x2_t v15 = v7;
    float64x2_t v16 = v8;
    long long v9 = &v17;
    double v10 = &v15;
  }
  _simd_slerp_internal(v9, v10, &v13, a4);
  float64_t result = v13.f64[0];
  long long v12 = v14;
  *(float64x2_t *)a3  = v13;
  *(_OWORD *)(a3 + 16)  = v12;
  return result;
}

float64_t SPRotation3DSlerp@<D0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>, double a4@<D0>)
{
  float64x2_t v13 = 0u;
  long long v14 = 0u;
  float64x2_t v5 = *a1;
  float64x2_t v6 = a1[1];
  float64x2_t v7 = *a2;
  float64x2_t v8 = a2[1];
  if (vaddvq_f64(vaddq_f64(vmulq_f64(*a1, *a2), vmulq_f64(v6, v8))) >= 0.0)
  {
    float64x2_t v17 = *a1;
    float64x2_t v18 = v6;
    float64x2_t v15 = v7;
    float64x2_t v16 = v8;
    long long v9 = &v17;
    double v10 = &v15;
  }
  else
  {
    float64x2_t v17 = vnegq_f64(v7);
    float64x2_t v18 = vnegq_f64(v8);
    float64x2_t v15 = v5;
    float64x2_t v16 = v6;
    long long v9 = &v15;
    double v10 = &v17;
  }
  _simd_slerp_internal(v9, v10, &v13, a4);
  float64_t result = v13.f64[0];
  long long v12 = v14;
  *(float64x2_t *)a3  = v13;
  *(_OWORD *)(a3 + 16)  = v12;
  return result;
}

double static SPRotation3D.* infix(_:_:)(double a1, float64x2_t a2, float64x2_t a3)
{
  uint64_t v7 = 0;
  uint64_t v8 = 0x3FF0000000000000;
  float64x2_t v6 = 0uLL;
  v5[0]  = a2;
  v5[1]  = a3;
  if (a3.f64[1] >= 0.0) {
    SPRotation3DSlerp(&v6, v5, (uint64_t)&v4, a1);
  }
  else {
    SPRotation3DSlerpLongest(&v6, v5, (uint64_t)&v4, a1);
  }
  return v4;
}

double static SPRotation3D.* infix(_:_:)(float64x2_t a1, float64x2_t a2, float64x2_t a3, int8x16_t a4)
{
  float64x2_t v4 = vnegq_f64(a3);
  float64x2_t v5 = (float64x2_t)vextq_s8(a4, (int8x16_t)vnegq_f64((float64x2_t)a4), 8uLL);
  float64x2_t v6 = vmlaq_n_f64(vmulq_laneq_f64(v4, a1, 1), (float64x2_t)vextq_s8((int8x16_t)a3, (int8x16_t)v4, 8uLL), a1.f64[0]);
  *(float64x2_t *)v10.vector.f64  = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(a3, a2, 1), (float64x2_t)vextq_s8((int8x16_t)v4, (int8x16_t)a3, 8uLL), a2.f64[0]), vmlaq_n_f64(vmulq_laneq_f64((float64x2_t)a4, a1, 1), v5, a1.f64[0]));
  *(float64x2_t *)&v10.vector.f64[2]  = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64((float64x2_t)a4, a2, 1), v5, a2.f64[0]), v6);
  simd_quatd v9 = v10;
  SPRotation3DMakeWithQuaternion(v10, (uint64_t)&v9, &v8);
  return *(double *)&v8;
}

double static SPRotation3D.*= infix(_:_:)(float64x2_t *a1, float64x2_t a2, int8x16_t a3)
{
  float64x2_t v3 = a1[1];
  float64x2_t v4 = vnegq_f64(a2);
  float64x2_t v5 = (float64x2_t)vextq_s8(a3, (int8x16_t)vnegq_f64((float64x2_t)a3), 8uLL);
  float64x2_t v6 = vmlaq_n_f64(vmulq_laneq_f64(v4, *a1, 1), (float64x2_t)vextq_s8((int8x16_t)a2, (int8x16_t)v4, 8uLL), a1->f64[0]);
  *(float64x2_t *)v9.vector.f64  = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64((float64x2_t)a3, *a1, 1), v5, a1->f64[0]), vmlaq_n_f64(vmulq_laneq_f64(a2, v3, 1), (float64x2_t)vextq_s8((int8x16_t)v4, (int8x16_t)a2, 8uLL), v3.f64[0]));
  *(float64x2_t *)&v9.vector.f64[2]  = vaddq_f64(v6, vmlaq_n_f64(vmulq_laneq_f64((float64x2_t)a3, v3, 1), v5, v3.f64[0]));
  simd_quatd v8 = v9;
  *(void *)&double result = SPRotation3DMakeWithQuaternion(v9, (uint64_t)&v8, a1).n128_u64[0];
  return result;
}

double SPAngleNegate(SPAngle a1)
{
  return -a1.radians;
}

double static SPAngle.+ infix(_:_:)(double a1, double a2)
{
  return a1 + a2;
}

double static SPAngle.+= infix(_:_:)(double *a1, double a2)
{
  double result = *a1 + a2;
  *a1  = result;
  return result;
}

double static SPAngle.- infix(_:_:)(double a1, double a2)
{
  return a1 - a2;
}

double static SPAngle.-= infix(_:_:)(double *a1, double a2)
{
  double result = *a1 - a2;
  *a1  = result;
  return result;
}

void protocol witness for static AdditiveArithmetic.zero.getter in conformance SPAngle(void *a1@<X8>)
{
  *a1  = 0;
}

double protocol witness for static AdditiveArithmetic.+ infix(_:_:) in conformance SPAngle@<D0>(double *a1@<X0>, double *a2@<X1>, double *a3@<X8>)
{
  double result = *a1 + *a2;
  *a3  = result;
  return result;
}

double protocol witness for static AdditiveArithmetic.+= infix(_:_:) in conformance SPAngle(double *a1, double *a2)
{
  double result = *a2 + *a1;
  *a1  = result;
  return result;
}

double protocol witness for static AdditiveArithmetic.- infix(_:_:) in conformance SPAngle@<D0>(double *a1@<X0>, double *a2@<X1>, double *a3@<X8>)
{
  double result = *a1 - *a2;
  *a3  = result;
  return result;
}

double protocol witness for static AdditiveArithmetic.-= infix(_:_:) in conformance SPAngle(double *a1, double *a2)
{
  double result = *a1 - *a2;
  *a1  = result;
  return result;
}

float64_t static SPScaledPose3D.* infix(_:_:)@<D0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>, float64x2_t a4@<Q6>)
{
  float64x2_t v5 = *(float64x2_t *)a2;
  long long v6 = *(_OWORD *)(a2 + 16);
  long long v7 = *(_OWORD *)(a2 + 32);
  uint64_t v8 = *(void *)(a2 + 48);
  uint64_t v9 = *(void *)(a2 + 56);
  float64x2_t v11 = *(float64x2_t *)(a1 + 16);
  long long v12 = *(_OWORD *)(a1 + 32);
  uint64_t v13 = *(void *)(a1 + 56);
  a4.f64[0]  = *(float64_t *)(a1 + 64);
  float64x2_t v10 = *(float64x2_t *)a1;
  uint64_t v27 = *(void *)(a1 + 48);
  uint64_t v28 = v13;
  unint64_t v29 = *(void *)&a4.f64[0];
  *(void *)&long long v25 = v8;
  *((void *)&v25 + 1)  = v9;
  SPScaledPose3DConcatenation((uint64_t)&v26, (uint64_t)&v24, (uint64_t)&v19, v10, v11, a4, v19, v20, v21, v22, v23, v5, v6, v7, v25, *(uint64_t *)&v10.f64[0], *(uint64_t *)&v10.f64[1], *(uint64_t *)&v11.f64[0], *(uint64_t *)&v11.f64[1],
    v12,
    *((uint64_t *)&v12 + 1));
  float64_t result = v19.f64[0];
  long long v15 = v20;
  long long v16 = v21;
  long long v17 = v22;
  uint64_t v18 = v23;
  *(float64x2_t *)a3  = v19;
  *(_OWORD *)(a3 + 16)  = v15;
  *(_OWORD *)(a3 + 48)  = v17;
  *(_OWORD *)(a3 + 32)  = v16;
  *(void *)(a3 + 64)  = v18;
  return result;
}

float64x2_t *SPScaledPose3DConcatenation@<X0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>, float64x2_t a4@<Q3>, float64x2_t a5@<Q4>, float64x2_t a6@<Q6>, float64x2_t a7, long long a8, long long a9, long long a10, long long a11, float64x2_t a12, long long a13, long long a14, long long a15, uint64_t a16, uint64_t a17, uint64_t a18, uint64_t a19, uint64_t a20,uint64_t a21)
{
  a21  = *MEMORY[0x263EF8340];
  long long v21 = *(_OWORD *)(a2 + 16);
  a12  = *(float64x2_t *)a2;
  a13  = v21;
  long long v22 = *(_OWORD *)(a2 + 48);
  a14  = *(_OWORD *)(a2 + 32);
  a15  = v22;
  a16  = 0x3FF0000000000000;
  long long v23 = *(_OWORD *)(a1 + 48);
  a9  = *(_OWORD *)(a1 + 32);
  a10  = v23;
  a11  = *(_OWORD *)(a1 + 64);
  long long v24 = *(_OWORD *)(a1 + 16);
  a7  = *(float64x2_t *)a1;
  a8  = v24;
  return SPScaledPose3DConcatenation(&a7, &a12, a3, a4, a5, a6);
}

{
  long long v21;
  long long v22;
  long long v23;
  long long v24;

  a21  = *MEMORY[0x263EF8340];
  long long v21 = *(_OWORD *)(a1 + 16);
  a12  = *(float64x2_t *)a1;
  a13  = v21;
  long long v22 = *(_OWORD *)(a1 + 48);
  a14  = *(_OWORD *)(a1 + 32);
  a15  = v22;
  a16  = 0x3FF0000000000000;
  long long v23 = *(_OWORD *)(a2 + 48);
  a9  = *(_OWORD *)(a2 + 32);
  a10  = v23;
  a11  = *(_OWORD *)(a2 + 64);
  long long v24 = *(_OWORD *)(a2 + 16);
  a7  = *(float64x2_t *)a2;
  a8  = v24;
  return SPScaledPose3DConcatenation(&a12, &a7, a3, a4, a5, a6);
}

double static SPScaledPose3D.* infix(_:_:)@<D0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>, float64x2_t a4@<Q3>)
{
  float64x2_t v5 = *(float64x2_t *)a2;
  float64x2_t v6 = *(float64x2_t *)(a2 + 16);
  float64x2_t v7 = *(float64x2_t *)(a2 + 32);
  uint64_t v8 = *(void *)(a2 + 48);
  uint64_t v9 = *(void *)(a2 + 56);
  a4.f64[0]  = *(float64_t *)(a2 + 64);
  float64x2_t v10 = *(float64x2_t *)(a1 + 16);
  float64x2_t v11 = *(float64x2_t *)(a1 + 32);
  uint64_t v12 = *(void *)(a1 + 48);
  uint64_t v13 = *(void *)(a1 + 56);
  uint64_t v14 = *(void *)(a1 + 64);
  v29[0]  = *(float64x2_t *)a1;
  v29[1]  = v10;
  uint64_t v30 = v12;
  uint64_t v31 = v13;
  v29[2]  = v11;
  uint64_t v32 = v14;
  v25[0]  = v5;
  v25[1]  = v6;
  uint64_t v26 = v8;
  uint64_t v27 = v9;
  v25[2]  = v7;
  unint64_t v28 = *(void *)&a4.f64[0];
  SPScaledPose3DConcatenation(v29, v25, (uint64_t)v21, a4, v29[0], v11);
  double result = *(double *)v21;
  long long v16 = v21[1];
  long long v17 = v21[2];
  uint64_t v18 = v22;
  uint64_t v19 = v23;
  uint64_t v20 = v24;
  *(_OWORD *)a3  = v21[0];
  *(_OWORD *)(a3 + 16)  = v16;
  *(void *)(a3 + 48)  = v18;
  *(void *)(a3 + 56)  = v19;
  *(_OWORD *)(a3 + 32)  = v17;
  *(void *)(a3 + 64)  = v20;
  return result;
}

float64x2_t *SPScaledPose3DConcatenation@<X0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>, float64x2_t a4@<Q3>, float64x2_t a5@<Q4>, float64x2_t a6@<Q6>)
{
  v151  = *MEMORY[0x263EF8340];
  float64x2_t v8 = a1[3];
  v145  = a1[2];
  v146  = v8;
  v147  = a1[4];
  float64x2_t v9 = a1[1];
  v143  = *a1;
  v144  = v9;
  SPAffineTransform3DMakeWithScaledPose(&v143, (long long *)&v127, v143, a4, a5, a6);
  float64x2_t v10 = a2[3];
  v145  = a2[2];
  v146  = v10;
  v147  = a2[4];
  float64x2_t v11 = a2[1];
  v143  = *a2;
  v144  = v11;
  double result = SPAffineTransform3DMakeWithScaledPose(&v143, (long long *)&v119, v143, v12, v13, v14);
  float64x2_t v17 = v127;
  float64x2_t v16 = v128;
  float64x2_t v19 = v129;
  float64x2_t v18 = v130;
  float64x2_t v21 = v131;
  float64x2_t v20 = v132;
  float64x2_t v22 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v24 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v23 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v26 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v25 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v27 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v24, v129), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v127)), (int8x16_t)vceqq_f64(v26, v131));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v27, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v23, v130), (int8x16_t)vceqq_f64(v22, v128)), (int8x16_t)vceqq_f64(v25, v132)), (int8x16_t)v27)).u64[0] & 0x8000000000000000) != 0&& (int8x16_t v28 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v23, v122), (int8x16_t)vceqq_f64(v22, v120)), (int8x16_t)vceqq_f64(v25, v124)), v29 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v24, v121), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v119)), (int8x16_t)vceqq_f64(v26, v123)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v29, 1), vandq_s8(v28, (int8x16_t)v29)).u64[0] & 0x8000000000000000) != 0))
  {
    *(void *)&v30.f64[0]  = *(_OWORD *)&vaddq_f64(v134, v126);
    float64x2_t v31 = vaddq_f64(v133, (float64x2_t)v125);
  }
  else
  {
    float64x2_t v31 = v133;
    v30.f64[0]  = v134.f64[0];
    int64x2_t v32 = vceqzq_f64(v133);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v32, 1), vandq_s8((int8x16_t)vceqzq_f64(v134), (int8x16_t)v32)).u64[0] & 0x8000000000000000) != 0)
    {
      v34.i64[0]  = v125.i64[0];
      *(float64_t *)&long long v33 = v126.f64[0];
      int64x2_t v38 = vceqzq_f64((float64x2_t)v125);
      unint64_t v39 = vandq_s8((int8x16_t)vdupq_laneq_s64(v38, 1), vandq_s8((int8x16_t)vceqzq_f64(v126), (int8x16_t)v38)).u64[0];
      float64x2_t v36 = v119;
      float64_t v37 = v120.f64[0];
      if ((v39 & 0x8000000000000000) != 0)
      {
        v115  = 0;
        v135  = v119;
        v136  = v120;
        v137  = v121;
        v138  = v122;
        v139  = v123;
        v140  = v124;
        v143  = 0u;
        v144  = 0u;
        v145  = 0u;
        v146  = 0u;
        v147  = 0u;
        v148  = 0u;
        *(void *)&v17.f64[1]  = vextq_s8((int8x16_t)v127, (int8x16_t)v127, 8uLL).u64[0];
        *(void *)&v19.f64[1]  = vextq_s8((int8x16_t)v129, (int8x16_t)v129, 8uLL).u64[0];
        *(void *)&v21.f64[1]  = vextq_s8((int8x16_t)v131, (int8x16_t)v131, 8uLL).u64[0];
        do
        {
          v117  = *(float64x2_t *)((char *)&v135 + v115);
          v116  = *(float64x2_t *)((char *)&v135 + v115 + 16);
          v118  = (float64x2_t *)((char *)&v143 + v115);
          *v118  = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v17, v117.f64[0]), v19, v117, 1), v21, v116.f64[0]);
          v118[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v16, v117), v18, v117, 1), v116, v20);
          v115 += 32;
        }
        while (v115 != 96);
        float64x2_t v17 = v143;
        float64x2_t v16 = v144;
        float64x2_t v19 = v145;
        float64x2_t v18 = v146;
        float64x2_t v21 = v147;
        float64x2_t v20 = v148;
        goto LABEL_10;
      }
      unint64_t v35 = vextq_s8(v125, v125, 8uLL).u64[0];
    }
    else
    {
      *(float64_t *)&long long v33 = v126.f64[0];
      int8x16_t v34 = v125;
      unint64_t v35 = vextq_s8(v34, v34, 8uLL).u64[0];
      float64x2_t v36 = v119;
      float64_t v37 = v120.f64[0];
    }
    uint64_t v40 = 0;
    v16.f64[1]  = 0.0;
    v18.f64[1]  = 0.0;
    v20.f64[1]  = 0.0;
    v30.f64[1]  = 1.0;
    v34.i64[1]  = v35;
    *((void *)&v33 + 1)  = 1.0;
    v135  = v36;
    v136  = (float64x2_t)*(unint64_t *)&v37;
    v137  = v121;
    v138  = (float64x2_t)*(unint64_t *)&v122.f64[0];
    v139  = v123;
    v140  = (float64x2_t)*(unint64_t *)&v124.f64[0];
    v141  = v34;
    v142  = v33;
    v143  = 0u;
    v144  = 0u;
    v145  = 0u;
    v146  = 0u;
    v147  = 0u;
    v148  = 0u;
    v149  = 0u;
    v150  = 0u;
    do
    {
      float64x2_t v42 = *(float64x2_t *)((char *)&v135 + v40);
      float64x2_t v41 = *(float64x2_t *)((char *)&v135 + v40 + 16);
      long long v43 = (float64x2_t *)((char *)&v143 + v40);
      *long long v43 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v17, v42.f64[0]), v19, v42, 1), v21, v41.f64[0]), v31, v41, 1);
      v43[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v16, v42.f64[0]), v18, v42, 1), v20, v41.f64[0]), v30, v41, 1);
      v40 += 32;
    }
    while (v40 != 128);
    float64x2_t v17 = v143;
    float64x2_t v16 = v144;
    float64x2_t v19 = v145;
    float64x2_t v18 = v146;
    float64x2_t v21 = v147;
    float64x2_t v20 = v148;
    float64x2_t v31 = v149;
    *(void *)&v30.f64[0]  = v150;
  }
LABEL_10:
  v44.f64[0]  = v20.f64[0];
  v44.f64[1]  = v21.f64[0];
  v45.f64[0]  = v18.f64[0];
  v45.f64[1]  = v19.f64[0];
  float64x2_t v46 = vmulq_f64(v17, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v21, (int8x16_t)v20, 8uLL), vnegq_f64(v45)), v44, (float64x2_t)vextq_s8((int8x16_t)v19, (int8x16_t)v18, 8uLL)));
  float64x2_t v47 = vmulq_f64(v16, vmlaq_laneq_f64(vmulq_f64(v21, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v19, 1))), v19, v21, 1));
  if (v47.f64[0] + vaddvq_f64(v46) < 0.0) {
    v47.f64[0]  = -1.0;
  }
  else {
    v47.f64[0]  = 1.0;
  }
  v48.f64[0]  = sqrt(vmulq_f64(v16, v16).f64[0] + vaddvq_f64(vmulq_f64(v17, v17)));
  float64x2_t v49 = vmulq_f64(v18, v18);
  v48.f64[1]  = sqrt(v49.f64[0] + vaddvq_f64(vmulq_f64(v19, v19)));
  v49.f64[0]  = sqrt(vmulq_f64(v20, v20).f64[0] + vaddvq_f64(vmulq_f64(v21, v21)));
  float64x2_t v50 = vmulq_n_f64(v48, v47.f64[0]);
  float64x2_t v51 = vmulq_f64(v49, v47);
  float64x2_t v52 = vdivq_f64(v17, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v50.f64[0], 0));
  float64x2_t v53 = vdivq_f64(v16, v50);
  float64x2_t v54 = (float64x2_t)vdupq_laneq_s64((int64x2_t)v50, 1);
  float64x2_t v55 = vdivq_f64(v19, v54);
  float64x2_t v56 = vdivq_f64(v18, v54);
  float64x2_t v57 = vdivq_f64(v21, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v51.f64[0], 0));
  float64x2_t v58 = vdivq_f64(v20, v51);
  float64x2_t v59 = vmulq_f64(v56, v53);
  double v60 = vmulq_f64(v53, v53).f64[0] + vaddvq_f64(vmulq_f64(v52, v52));
  v59.f64[0]  = (v59.f64[0] + vaddvq_f64(vmulq_f64(v55, v52))) / v60;
  float64x2_t v61 = vsubq_f64(v55, vmulq_n_f64(v52, v59.f64[0]));
  float64x2_t v62 = vsubq_f64(v56, vmulq_f64(v53, v59));
  float64x2_t v63 = vmulq_f64(v58, v53);
  v63.f64[0]  = (v63.f64[0] + vaddvq_f64(vmulq_f64(v57, v52))) / v60;
  v64  = vmulq_n_f64(v52, v63.f64[0]);
  v65  = vsubq_f64(v58, vmulq_f64(v53, v63));
  float64x2_t v66 = vsubq_f64(v57, v64);
  float64x2_t v67 = vmulq_f64(v58, v62);
  v67.f64[0]  = v67.f64[0] + vaddvq_f64(vmulq_f64(v57, v61));
  v57.f64[0]  = vmulq_f64(v62, v62).f64[0] + vaddvq_f64(vmulq_f64(v61, v61));
  v67.f64[0]  = v67.f64[0] / v57.f64[0];
  float64x2_t v68 = vmulq_f64(v62, v67);
  float64x2_t v69 = vmulq_n_f64(v61, v67.f64[0]);
  float64x2_t v70 = vsubq_f64(v66, v69);
  float64x2_t v71 = vsubq_f64(v65, v68);
  v69.f64[0]  = 1.0 / sqrt(v60);
  *(void *)&double v72 = *(_OWORD *)&vmulq_f64(v53, v69);
  v73  = (int8x16_t)vmulq_n_f64(v52, v69.f64[0]);
  v74  = (float64x2_t)vextq_s8(v73, v73, 8uLL);
  v69.f64[0]  = 1.0 / sqrt(v57.f64[0]);
  v75  = vmulq_f64(v62, v69);
  v76  = vmulq_n_f64(v61, v69.f64[0]);
  v77  = vmulq_f64(v71, v71);
  v77.f64[0]  = 1.0 / sqrt(v77.f64[0] + vaddvq_f64(vmulq_f64(v70, v70)));
  v78  = vmulq_f64(v71, v77).f64[0];
  v79  = (int8x16_t)vmulq_n_f64(v70, v77.f64[0]);
  v80  = (float64x2_t)vextq_s8(v79, v79, 8uLL);
  v81  = *(double *)v73.i64 + v76.f64[1] + v78;
  if (v81 >= 0.0)
  {
    v89  = sqrt(v81 + 1.0);
    v90  = v89 + v89;
    v91  = 1.0 / (v89 + v89);
    v85.f64[0]  = v91 * vsubq_f64(v75, v80).f64[0];
    v86  = (*(double *)v79.i64 - v72) * v91;
    v87.f64[0]  = v91 * vsubq_f64(v74, v76).f64[0];
    v88  = v90 * 0.25;
  }
  else if (*(double *)v73.i64 < v76.f64[1] || *(double *)v73.i64 < v78)
  {
    v92  = 1.0 - *(double *)v73.i64;
    v93  = v76.f64[1] < v78;
    v94  = sqrt(v92 - v76.f64[1] + v78);
    v95  = v94 + v94;
    v96  = 1.0 / v95;
    v97  = (v72 + *(double *)v79.i64) * (1.0 / v95);
    v98  = 1.0 / v95 * vaddq_f64(v75, v80).f64[0];
    v99  = v95 * 0.25;
    v100  = v96 * vsubq_f64(v74, v76).f64[0];
    v101  = sqrt(v76.f64[1] + v92 - v78);
    v102  = v101 + v101;
    v103  = 1.0 / v102 * vaddq_f64(v74, v76).f64[0];
    v104  = v102 * 0.25;
    v105  = 1.0 / v102 * vaddq_f64(v75, v80).f64[0];
    v106  = (*(double *)v79.i64 - v72) * (1.0 / v102);
    if (v93) {
      v85.f64[0]  = v97;
    }
    else {
      v85.f64[0]  = v103;
    }
    if (v93) {
      v86  = v98;
    }
    else {
      v86  = v104;
    }
    if (v93) {
      v87.f64[0]  = v99;
    }
    else {
      v87.f64[0]  = v105;
    }
    if (v93) {
      v88  = v100;
    }
    else {
      v88  = v106;
    }
  }
  else
  {
    v82  = sqrt(*(double *)v73.i64 + 1.0 - v76.f64[1] - v78);
    v83  = v82 + v82;
    v84  = 1.0 / v83;
    v85.f64[0]  = v83 * 0.25;
    v86  = v84 * vaddq_f64(v74, v76).f64[0];
    v87.f64[0]  = (v72 + *(double *)v79.i64) * v84;
    v88  = v84 * vsubq_f64(v75, v80).f64[0];
  }
  v85.f64[1]  = v86;
  v87.f64[1]  = v88;
  v107  = (int64x2_t)vornq_s8(vmvnq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v85), (int8x16_t)vcgezq_f64(v85))), vorrq_s8((int8x16_t)vcltzq_f64(v87), (int8x16_t)vcgezq_f64(v87)));
  v108  = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if ((vorrq_s8((int8x16_t)v107, (int8x16_t)vdupq_laneq_s64(v107, 1)).u64[0] & 0x8000000000000000) != 0)
  {
    v111  = v108;
    v110  = v108;
  }
  else
  {
    v109  = vaddvq_f64(vaddq_f64(vmulq_f64(v85, v85), vmulq_f64(v87, v87)));
    if (v109 == 0.0)
    {
      v110  = (float64x2_t)xmmword_228C1F7A0;
      v111  = 0uLL;
    }
    else
    {
      v112  = 1.0 / sqrt(v109);
      v110  = vmulq_n_f64(v87, v112);
      v111  = vmulq_n_f64(v85, v112);
    }
  }
  v113  = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v111), (int8x16_t)vcgezq_f64(v111)), (int8x16_t)vceqq_f64(vabsq_f64(v111), v108)), vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v110), (int8x16_t)vcgezq_f64(v110)), (int8x16_t)vceqq_f64(vabsq_f64(v110), v108)));
  if ((vandq_s8((int8x16_t)v113, (int8x16_t)vdupq_laneq_s64(v113, 1)).u64[0] & 0x8000000000000000) != 0
    && (*(void *)&v114  = vextq_s8((int8x16_t)v50, (int8x16_t)v50, 8uLL).u64[0],
        vabdd_f64(v50.f64[0], v114) < 0.0000000149011612)
    && vabdd_f64(v51.f64[0], v114) < 0.0000000149011612)
  {
    *(float64x2_t *)a3  = v31;
    *(float64_t *)(a3 + 16)  = v30.f64[0];
    *(void *)(a3 + 24)  = 0;
    *(float64x2_t *)(a3 + 32)  = v111;
    *(float64x2_t *)(a3 + 48)  = v110;
    *(float64_t *)(a3 + 64)  = v50.f64[0];
  }
  else
  {
    *(_OWORD *)(a3 + 32)  = xmmword_228C1FFE0;
    *(_OWORD *)(a3 + 48)  = unk_228C1FFF0;
    *(_OWORD *)(a3 + 64)  = xmmword_228C20000;
    *(_OWORD *)a3  = SPScaledPose3DInvalid;
    *(_OWORD *)(a3 + 16)  = unk_228C1FFD0;
  }
  return result;
}

float64_t static SPScaledPose3D.* infix(_:_:)@<D0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>, float64x2_t a4@<Q3>)
{
  float64x2_t v5 = *(float64x2_t *)a2;
  long long v6 = *(_OWORD *)(a2 + 16);
  long long v7 = *(_OWORD *)(a2 + 32);
  uint64_t v8 = *(void *)(a2 + 48);
  uint64_t v9 = *(void *)(a2 + 56);
  a4.f64[0]  = *(float64_t *)(a2 + 64);
  long long v11 = *(_OWORD *)(a1 + 16);
  float64x2_t v12 = *(float64x2_t *)(a1 + 32);
  uint64_t v13 = *(void *)(a1 + 56);
  float64x2_t v10 = *(float64x2_t *)a1;
  uint64_t v28 = *(void *)(a1 + 48);
  uint64_t v29 = v13;
  v27[2]  = v12;
  *(void *)&long long v25 = v8;
  *((void *)&v25 + 1)  = v9;
  SPScaledPose3DConcatenation((uint64_t)v27, (uint64_t)&v24, (uint64_t)&v19, a4, v10, v12, v19, v20, v21, v22, v23, v5, v6, v7, v25, *(uint64_t *)&a4.f64[0], v26, *(uint64_t *)&v10.f64[0], *(uint64_t *)&v10.f64[1],
    v11,
    *((uint64_t *)&v11 + 1));
  float64_t result = v19.f64[0];
  long long v15 = v20;
  long long v16 = v21;
  long long v17 = v22;
  uint64_t v18 = v23;
  *(float64x2_t *)a3  = v19;
  *(_OWORD *)(a3 + 16)  = v15;
  *(_OWORD *)(a3 + 48)  = v17;
  *(_OWORD *)(a3 + 32)  = v16;
  *(void *)(a3 + 64)  = v18;
  return result;
}

double static SPScaledPose3D.*= infix(_:_:)(uint64_t a1, uint64_t a2, double a3, double a4, double a5, float64x2_t a6)
{
  float64x2_t v7 = *(float64x2_t *)a2;
  float64x2_t v8 = *(float64x2_t *)(a2 + 16);
  float64x2_t v9 = *(float64x2_t *)(a2 + 32);
  uint64_t v10 = *(void *)(a2 + 48);
  uint64_t v11 = *(void *)(a2 + 56);
  a6.f64[0]  = *(float64_t *)(a2 + 64);
  float64x2_t v12 = *(float64x2_t *)(a1 + 16);
  float64x2_t v13 = *(float64x2_t *)(a1 + 32);
  uint64_t v14 = *(void *)(a1 + 48);
  uint64_t v15 = *(void *)(a1 + 56);
  uint64_t v16 = *(void *)(a1 + 64);
  v31[0]  = *(float64x2_t *)a1;
  v31[1]  = v12;
  uint64_t v32 = v14;
  uint64_t v33 = v15;
  v31[2]  = v13;
  uint64_t v34 = v16;
  v27[0]  = v7;
  v27[1]  = v8;
  uint64_t v28 = v10;
  uint64_t v29 = v11;
  v27[2]  = v9;
  unint64_t v30 = *(void *)&a6.f64[0];
  SPScaledPose3DConcatenation(v31, v27, (uint64_t)v23, a6, v31[0], v13);
  double result = *(double *)v23;
  long long v18 = v23[1];
  long long v19 = v23[2];
  uint64_t v20 = v24;
  uint64_t v21 = v25;
  uint64_t v22 = v26;
  *(_OWORD *)a1  = v23[0];
  *(_OWORD *)(a1 + 16)  = v18;
  *(void *)(a1 + 48)  = v20;
  *(void *)(a1 + 56)  = v21;
  *(_OWORD *)(a1 + 32)  = v19;
  *(void *)(a1 + 64)  = v22;
  return result;
}

uint64_t base witness table accessor for Equatable in SPSize3D()
{
  return lazy protocol witness table accessor for type SPSize3D and conformance SPSize3D(&lazy protocol witness table cache variable for type SPSize3D and conformance SPSize3D, type metadata accessor for SPSize3D);
}

{
  return lazy protocol witness table accessor for type SPSize3D and conformance SPSize3D(&lazy protocol witness table cache variable for type SPSize3D and conformance SPSize3D);
}

uint64_t base witness table accessor for Equatable in SPAngle()
{
  return lazy protocol witness table accessor for type SPSize3D and conformance SPSize3D((unint64_t *)&lazy protocol witness table cache variable for type SPAngle and conformance SPAngle, type metadata accessor for SPAngle);
}

uint64_t lazy protocol witness table accessor for type SPSize3D and conformance SPSize3D(unint64_t *a1, void (*a2)(uint64_t))
{
  uint64_t result = *a1;
  if (!result)
  {
    a2(255);
    uint64_t result = swift_getWitnessTable();
    atomic_store(result, a1);
  }
  return result;
}

void _simd_slerp_internal(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>, double a4@<D0>)
{
  double v6 = 1.0;
  double v7 = 1.0 - a4;
  float64x2_t v8 = a1[1];
  float64x2_t v9 = a2[1];
  float64x2_t v10 = vsubq_f64(*a1, *a2);
  float64x2_t v11 = vsubq_f64(v8, v9);
  v10.f64[0]  = sqrt(vaddvq_f64(vaddq_f64(vmulq_f64(v10, v10), vmulq_f64(v11, v11))));
  float64x2_t v33 = *a2;
  float64x2_t v34 = *a1;
  float64x2_t v12 = vaddq_f64(*a1, *a2);
  float64x2_t v35 = v8;
  float64x2_t v32 = v9;
  float64x2_t v13 = vaddq_f64(v8, v9);
  long double v14 = atan2(v10.f64[0], sqrt(vaddvq_f64(vaddq_f64(vmulq_f64(v12, v12), vmulq_f64(v13, v13)))));
  double v15 = v14 + v14;
  BOOL v16 = v14 + v14 == 0.0;
  double v17 = 1.0;
  if (!v16) {
    double v17 = sin(v15) / v15;
  }
  double v18 = 1.0 / v17;
  if (v7 * v15 != 0.0) {
    double v6 = sin(v7 * v15) / (v7 * v15);
  }
  float64x2_t v19 = (float64x2_t)vdupq_lane_s64(COERCE__INT64(v7 * (v18 * v6)), 0);
  double v20 = v15 * a4;
  double v21 = 1.0;
  if (v20 != 0.0)
  {
    float64x2_t v31 = v19;
    long double v22 = sin(v20);
    float64x2_t v19 = v31;
    double v21 = v22 / v20;
  }
  double v23 = v18 * v21 * a4;
  float64x2_t v24 = vmulq_n_f64(v32, v23);
  float64x2_t v25 = vmlaq_f64(vmulq_n_f64(v33, v23), v34, v19);
  float64x2_t v26 = vmlaq_f64(v24, v35, v19);
  double v27 = vaddvq_f64(vaddq_f64(vmulq_f64(v25, v25), vmulq_f64(v26, v26)));
  if (v27 == 0.0)
  {
    float64x2_t v28 = (float64x2_t)xmmword_228C1F7A0;
    float64x2_t v29 = 0uLL;
  }
  else
  {
    double v30 = 1.0 / sqrt(v27);
    float64x2_t v28 = vmulq_n_f64(v26, v30);
    float64x2_t v29 = vmulq_n_f64(v25, v30);
  }
  *a3  = v29;
  a3[1]  = v28;
}

float64x2_t *SPAffineTransform3DMakeWithScaledPose@<X0>(float64x2_t *result@<X0>, long long *a2@<X8>, float64x2_t a3@<Q1>, float64x2_t _Q3@<Q3>, float64x2_t _Q4@<Q4>, float64x2_t a6@<Q6>)
{
  a6.f64[0]  = result[4].f64[0];
  _Q18  = result[2];
  _Q16  = result[3];
  v8.f64[0]  = 0.0;
  v8.f64[1]  = a6.f64[0];
  _D19  = result[2].f64[1];
  __asm { FMLS            D0, D16, V16.D[0] }
  _Q4.f64[0]  = result[3].f64[1];
  __asm { FMLA            D0, D4, V16.D[1] }
  a3.f64[0]  = vmlad_n_f64(vmuld_lane_f64(_Q16.f64[0], _Q16, 1), _D19, _Q18.f64[0]);
  float64_t v15 = a3.f64[0] + a3.f64[0];
  v16.f64[0]  = vmuld_lane_f64(_D19, _Q16, 1);
  a3.f64[0]  = vmlad_n_f64(-(_D19 * _Q4.f64[0]), _Q16.f64[0], _Q18.f64[0]);
  a3.f64[0]  = a3.f64[0] + a3.f64[0];
  _Q0.f64[1]  = v15;
  double v18 = vmlad_n_f64(-(_Q16.f64[0] * _Q4.f64[0]), _D19, _Q18.f64[0]);
  v22.f64[0]  = v18 + v18;
  __asm
  {
    FMLA            D21, D19, V18.D[1]
    FMLA            D21, D4, V16.D[1]
    FMLS            D21, D18, V18.D[0]
    FMLA            D3, D16, V18.D[1]
  }
  _Q3.f64[0]  = _Q3.f64[0] + _Q3.f64[0];
  v22.f64[1]  = _D21;
  float64_t v23 = -(_Q18.f64[0] * _Q4.f64[0]);
  float64x2_t v24 = (float64x2_t)vzip1q_s64((int64x2_t)_Q18, (int64x2_t)_Q16);
  __asm
  {
    FMLS            D4, D18, V18.D[0]
    FMLS            D4, D19, V18.D[1]
  }
  _Q18.f64[0]  = result[3].f64[0];
  v16.f64[1]  = v23;
  float64x2_t v25 = vmlaq_f64(v16, v24, _Q18);
  float64x2_t v26 = vaddq_f64(v25, v25);
  float64x2_t v28 = *(float64x2_t *)MEMORY[0x263EF8988];
  float64x2_t v27 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v30 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v29 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v32 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v31 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v33 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v32, v26), (int8x16_t)vceqq_f64(v30, v22)), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], _Q0));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v33, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v31, _Q4), (int8x16_t)vceqq_f64(v29, _Q3)), (int8x16_t)vceqq_f64(v27, a3)), (int8x16_t)v33)).u64[0] & 0x8000000000000000) == 0|| (int64x2_t v34 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v30, v8), (int8x16_t)vceqq_f64(v28, (float64x2_t)*(unint64_t *)&a6.f64[0])), (int8x16_t)vceqq_f64(v32, (float64x2_t)0)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v34, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v29, (float64x2_t)0), (int8x16_t)vceqq_f64(v27, (float64x2_t)0)), (int8x16_t)vceqq_f64(v31, a6)), (int8x16_t)v34)).u64[0] & 0x8000000000000000) == 0))
  {
    uint64_t v35 = 0;
    float64x2_t v59 = (float64x2_t)*(unint64_t *)&a6.f64[0];
    float64x2_t v60 = 0uLL;
    float64x2_t v61 = v8;
    float64x2_t v62 = 0uLL;
    float64x2_t v63 = 0uLL;
    v64  = a6;
    long long v68 = 0uLL;
    float64x2_t v69 = 0uLL;
    long long v70 = 0uLL;
    float64x2_t v71 = 0uLL;
    float64x2_t v72 = 0uLL;
    v73  = 0uLL;
    *(void *)&_Q0.f64[1]  = vextq_s8((int8x16_t)_Q0, (int8x16_t)_Q0, 8uLL).u64[0];
    *(void *)&v22.f64[1]  = vextq_s8((int8x16_t)v22, (int8x16_t)v22, 8uLL).u64[0];
    *(void *)&v26.f64[1]  = vextq_s8((int8x16_t)v26, (int8x16_t)v26, 8uLL).u64[0];
    do
    {
      float64x2_t v37 = *(float64x2_t *)((char *)&v59 + v35);
      float64x2_t v36 = *(float64x2_t *)((char *)&v59 + v35 + 16);
      int64x2_t v38 = (float64x2_t *)((char *)&v68 + v35);
      *int64x2_t v38 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q0, v37.f64[0]), v22, v37, 1), v26, v36.f64[0]);
      v38[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(a3, v37), _Q3, v37, 1), v36, _Q4);
      v35 += 32;
    }
    while (v35 != 96);
    _Q0  = (float64x2_t)v68;
    a3  = v69;
    float64x2_t v22 = (float64x2_t)v70;
    _Q3  = v71;
    float64x2_t v26 = v72;
    _Q4  = v73;
  }
  float64x2_t v40 = *result;
  float64x2_t v39 = result[1];
  long long v41 = xmmword_228C1F7D0;
  long long v42 = xmmword_228C1F7A0;
  __asm { FMOV            V24.2D, #1.0 }
  int64x2_t v44 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v30, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqq_f64(v28, (float64x2_t)xmmword_228C1F7D0)), (int8x16_t)vceqzq_f64(v32));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v44, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqzq_f64(v29), (int8x16_t)vceqzq_f64(v27)), (int8x16_t)vceqq_f64(v31, _Q24)), (int8x16_t)v44)).u64[0] & 0x8000000000000000) != 0&& (int8x16_t v45 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v29, _Q3), (int8x16_t)vceqq_f64(v31, _Q4)), (int8x16_t)vceqq_f64(v27, a3)), v46 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v30, v22), (int8x16_t)vceqq_f64(v32, v26)), (int8x16_t)vceqq_f64(v28, _Q0)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v46, 1), vandq_s8(v45, (int8x16_t)v46)).u64[0] & 0x8000000000000000) != 0))
  {
    float64x2_t v54 = 0uLL;
    float64x2_t v40 = vaddq_f64(v40, (float64x2_t)0);
    float64x2_t v39 = vaddq_f64(v39, (float64x2_t)0);
    float64x2_t v53 = 0uLL;
    float64x2_t v52 = 0uLL;
  }
  else
  {
    int64x2_t v47 = vceqzq_f64(v40);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v47, 1), vandq_s8((int8x16_t)vceqzq_f64(v39), (int8x16_t)v47)).u64[0] & 0x8000000000000000) != 0)
    {
      uint64_t v55 = 0;
      float64x2_t v59 = _Q0;
      float64x2_t v60 = a3;
      float64x2_t v61 = v22;
      float64x2_t v62 = _Q3;
      float64x2_t v63 = v26;
      v64  = _Q4;
      long long v68 = 0u;
      float64x2_t v69 = 0u;
      long long v70 = 0u;
      float64x2_t v71 = 0u;
      float64x2_t v72 = 0u;
      v73  = 0u;
      do
      {
        float64x2_t v57 = *(float64x2_t *)((char *)&v59 + v55);
        float64x2_t v56 = *(float64x2_t *)((char *)&v59 + v55 + 16);
        float64x2_t v58 = (float64x2_t *)((char *)&v68 + v55);
        *float64x2_t v58 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)xmmword_228C1F7D0, v57.f64[0]), (float64x2_t)xmmword_228C1F7A0, v57, 1), (float64x2_t)0, v56.f64[0]);
        v58[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v57, (float64x2_t)0), (float64x2_t)0, v57, 1), _Q24, v56);
        v55 += 32;
      }
      while (v55 != 96);
      long long v41 = v68;
      float64x2_t v52 = v69;
      long long v42 = v70;
      float64x2_t v53 = v71;
      float64x2_t v54 = v72;
      _Q24  = v73;
    }
    else
    {
      uint64_t v48 = 0;
      v39.f64[1]  = 1.0;
      float64x2_t v59 = _Q0;
      float64x2_t v60 = (float64x2_t)*(unint64_t *)&a3.f64[0];
      float64x2_t v61 = v22;
      float64x2_t v62 = (float64x2_t)*(unint64_t *)&_Q3.f64[0];
      float64x2_t v63 = v26;
      v64  = (float64x2_t)*(unint64_t *)&_Q4.f64[0];
      v65  = 0;
      uint64_t v66 = 0;
      long long v67 = xmmword_228C1F7A0;
      long long v68 = 0u;
      float64x2_t v69 = 0u;
      long long v70 = 0u;
      float64x2_t v71 = 0u;
      float64x2_t v72 = 0u;
      v73  = 0u;
      v74  = 0u;
      v75  = 0u;
      do
      {
        float64x2_t v50 = *(float64x2_t *)((char *)&v59 + v48);
        float64x2_t v49 = *(float64x2_t *)((char *)&v59 + v48 + 16);
        float64x2_t v51 = (float64x2_t *)((char *)&v68 + v48);
        *float64x2_t v51 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)xmmword_228C1F7D0, v50.f64[0]), (float64x2_t)xmmword_228C1F7A0, v50, 1), (float64x2_t)0, v49.f64[0]), v40, v49, 1);
        v51[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v50.f64[0]), (float64x2_t)0, v50, 1), (float64x2_t)xmmword_228C1F7D0, v49.f64[0]), v39, v49, 1);
        v48 += 32;
      }
      while (v48 != 128);
      long long v41 = v68;
      float64x2_t v52 = v69;
      long long v42 = v70;
      float64x2_t v53 = v71;
      float64x2_t v54 = v72;
      _Q24  = v73;
      float64x2_t v40 = v74;
      float64x2_t v39 = v75;
    }
  }
  *a2  = v41;
  a2[1]  = (__int128)v52;
  a2[2]  = v42;
  a2[3]  = (__int128)v53;
  a2[4]  = (__int128)v54;
  a2[5]  = (__int128)_Q24;
  a2[6]  = (__int128)v40;
  a2[7]  = (__int128)v39;
  return result;
}

double SPScaledPose3D.concatenating(_:)@<D0>(uint64_t a1@<X0>, uint64_t a2@<X8>, float64x2_t a3@<Q3>)
{
  float64x2_t v5 = *(float64x2_t *)a1;
  float64x2_t v6 = *(float64x2_t *)(a1 + 16);
  float64x2_t v7 = *(float64x2_t *)(a1 + 32);
  uint64_t v8 = *(void *)(a1 + 48);
  uint64_t v9 = *(void *)(a1 + 56);
  a3.f64[0]  = *(float64_t *)(a1 + 64);
  float64x2_t v10 = *(float64x2_t *)(v3 + 16);
  float64x2_t v11 = *(float64x2_t *)(v3 + 32);
  uint64_t v12 = *(void *)(v3 + 48);
  uint64_t v13 = *(void *)(v3 + 56);
  uint64_t v14 = *(void *)(v3 + 64);
  v29[0]  = *(float64x2_t *)v3;
  v29[1]  = v10;
  uint64_t v30 = v12;
  uint64_t v31 = v13;
  v29[2]  = v11;
  uint64_t v32 = v14;
  v25[0]  = v5;
  v25[1]  = v6;
  uint64_t v26 = v8;
  uint64_t v27 = v9;
  v25[2]  = v7;
  unint64_t v28 = *(void *)&a3.f64[0];
  SPScaledPose3DConcatenation(v29, v25, (uint64_t)v21, a3, v29[0], v11);
  double result = *(double *)v21;
  long long v16 = v21[1];
  long long v17 = v21[2];
  uint64_t v18 = v22;
  uint64_t v19 = v23;
  uint64_t v20 = v24;
  *(_OWORD *)a2  = v21[0];
  *(_OWORD *)(a2 + 16)  = v16;
  *(void *)(a2 + 48)  = v18;
  *(void *)(a2 + 56)  = v19;
  *(_OWORD *)(a2 + 32)  = v17;
  *(void *)(a2 + 64)  = v20;
  return result;
}

void SPScaledPose3DConcatenation(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>, float64x2_t a4@<Q3>, float64x2_t a5@<Q4>, float64x2_t a6@<Q6>)
{
  v126  = *MEMORY[0x263EF8340];
  float64x2_t v8 = a1[3];
  v120  = a1[2];
  v121  = v8;
  v122  = a1[4];
  float64x2_t v9 = a1[1];
  v118  = *a1;
  v119  = v9;
  SPAffineTransform3DMakeWithScaledPose(&v118, (long long *)&v102, v118, a4, a5, a6);
  float64x2_t v10 = a2[3];
  v120  = a2[2];
  v121  = v10;
  v122  = a2[4];
  float64x2_t v11 = a2[1];
  v118  = *a2;
  v119  = v11;
  SPAffineTransform3DMakeWithScaledPose(&v118, (long long *)&v94, v118, v12, v13, v14);
  float64x2_t v16 = v102;
  float64x2_t v15 = v103;
  float64x2_t v18 = v104;
  float64x2_t v17 = v105;
  float64x2_t v20 = v106;
  float64x2_t v19 = v107;
  float64x2_t v21 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v23 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v22 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v25 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v24 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v26 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v23, v104), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v102)), (int8x16_t)vceqq_f64(v25, v106));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v26, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v22, v105), (int8x16_t)vceqq_f64(v21, v103)), (int8x16_t)vceqq_f64(v24, v107)), (int8x16_t)v26)).u64[0] & 0x8000000000000000) != 0)
  {
    int8x16_t v27 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v22, v97), (int8x16_t)vceqq_f64(v21, v95)), (int8x16_t)vceqq_f64(v24, v99));
    int64x2_t v28 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v23, v96), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v94)), (int8x16_t)vceqq_f64(v25, v98));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v28, 1), vandq_s8(v27, (int8x16_t)v28)).u64[0] & 0x8000000000000000) != 0)
    {
      *(void *)&v93  = *(_OWORD *)&vaddq_f64(v109, v101);
      float64x2_t v43 = vaddq_f64(v108, (float64x2_t)v100);
LABEL_10:
      v92  = v43;
      goto LABEL_11;
    }
  }
  float64x2_t v30 = v108;
  v29.f64[0]  = v109.f64[0];
  int64x2_t v31 = vceqzq_f64(v108);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v31, 1), vandq_s8((int8x16_t)vceqzq_f64(v109), (int8x16_t)v31)).u64[0] & 0x8000000000000000) == 0)
  {
    *(float64_t *)&long long v32 = v101.f64[0];
    int8x16_t v33 = v100;
    unint64_t v34 = vextq_s8(v33, v33, 8uLL).u64[0];
    float64x2_t v35 = v94;
    float64_t v36 = v95.f64[0];
    goto LABEL_7;
  }
  v33.i64[0]  = v100.i64[0];
  *(float64_t *)&long long v32 = v101.f64[0];
  int64x2_t v37 = vceqzq_f64((float64x2_t)v100);
  unint64_t v38 = vandq_s8((int8x16_t)vdupq_laneq_s64(v37, 1), vandq_s8((int8x16_t)vceqzq_f64(v101), (int8x16_t)v37)).u64[0];
  float64x2_t v35 = v94;
  float64_t v36 = v95.f64[0];
  if ((v38 & 0x8000000000000000) == 0)
  {
    unint64_t v34 = vextq_s8(v100, v100, 8uLL).u64[0];
LABEL_7:
    uint64_t v39 = 0;
    v15.f64[1]  = 0.0;
    v17.f64[1]  = 0.0;
    v19.f64[1]  = 0.0;
    v29.f64[1]  = 1.0;
    v33.i64[1]  = v34;
    *((void *)&v32 + 1)  = 1.0;
    v110  = v35;
    v111  = (float64x2_t)*(unint64_t *)&v36;
    v112  = v96;
    v113  = (float64x2_t)*(unint64_t *)&v97.f64[0];
    v114  = v98;
    v115  = (float64x2_t)*(unint64_t *)&v99.f64[0];
    v116  = v33;
    v117  = v32;
    v118  = 0u;
    v119  = 0u;
    v120  = 0u;
    v121  = 0u;
    v122  = 0u;
    v123  = 0u;
    v124  = 0u;
    v125  = 0u;
    do
    {
      float64x2_t v41 = *(float64x2_t *)((char *)&v110 + v39);
      float64x2_t v40 = *(float64x2_t *)((char *)&v110 + v39 + 16);
      long long v42 = (float64x2_t *)((char *)&v118 + v39);
      *long long v42 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v16, v41.f64[0]), v18, v41, 1), v20, v40.f64[0]), v30, v40, 1);
      v42[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v15, v41.f64[0]), v17, v41, 1), v19, v40.f64[0]), v29, v40, 1);
      v39 += 32;
    }
    while (v39 != 128);
    float64x2_t v16 = v118;
    float64x2_t v15 = v119;
    float64x2_t v18 = v120;
    float64x2_t v17 = v121;
    float64x2_t v20 = v122;
    float64x2_t v19 = v123;
    v93  = *(double *)&v125;
    float64x2_t v43 = v124;
    goto LABEL_10;
  }
  v86  = 0;
  v110  = v94;
  v111  = v95;
  v112  = v96;
  v113  = v97;
  v114  = v98;
  v115  = v99;
  v118  = 0u;
  v119  = 0u;
  v120  = 0u;
  v121  = 0u;
  v122  = 0u;
  v123  = 0u;
  *(void *)&v16.f64[1]  = vextq_s8((int8x16_t)v102, (int8x16_t)v102, 8uLL).u64[0];
  *(void *)&v18.f64[1]  = vextq_s8((int8x16_t)v104, (int8x16_t)v104, 8uLL).u64[0];
  *(void *)&v20.f64[1]  = vextq_s8((int8x16_t)v106, (int8x16_t)v106, 8uLL).u64[0];
  do
  {
    v88  = *(float64x2_t *)((char *)&v110 + v86);
    v87  = *(float64x2_t *)((char *)&v110 + v86 + 16);
    v89  = (float64x2_t *)((char *)&v118 + v86);
    *v89  = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v16, v88.f64[0]), v18, v88, 1), v20, v87.f64[0]);
    v89[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v15, v88), v17, v88, 1), v87, v19);
    v86 += 32;
  }
  while (v86 != 96);
  v92  = v30;
  v93  = v29.f64[0];
  float64x2_t v16 = v118;
  float64x2_t v15 = v119;
  float64x2_t v18 = v120;
  float64x2_t v17 = v121;
  float64x2_t v20 = v122;
  float64x2_t v19 = v123;
LABEL_11:
  v44.f64[0]  = v19.f64[0];
  v44.f64[1]  = v20.f64[0];
  v45.f64[0]  = v17.f64[0];
  v45.f64[1]  = v18.f64[0];
  float64x2_t v46 = vmulq_f64(v16, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v20, (int8x16_t)v19, 8uLL), vnegq_f64(v45)), v44, (float64x2_t)vextq_s8((int8x16_t)v18, (int8x16_t)v17, 8uLL)));
  float64x2_t v48 = vmulq_f64(v15, vmlaq_laneq_f64(vmulq_f64(v20, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v18, 1))), v18, v20, 1));
  BOOL v47 = v48.f64[0] + vaddvq_f64(v46) < 0.0;
  v48.f64[0]  = -1.0;
  if (!v47) {
    v48.f64[0]  = 1.0;
  }
  v49.f64[0]  = sqrt(vmulq_f64(v15, v15).f64[0] + vaddvq_f64(vmulq_f64(v16, v16)));
  float64x2_t v50 = vmulq_f64(v19, v19);
  v50.f64[0]  = sqrt(v50.f64[0] + vaddvq_f64(vmulq_f64(v20, v20)));
  v49.f64[1]  = sqrt(vmulq_f64(v17, v17).f64[0] + vaddvq_f64(vmulq_f64(v18, v18)));
  float64x2_t v51 = vmulq_n_f64(v49, v48.f64[0]);
  float64x2_t v52 = vdivq_f64(v16, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v51.f64[0], 0));
  float64x2_t v53 = vdivq_f64(v15, v51);
  v90  = vmulq_f64(v50, v48);
  v91  = (int8x16_t)v51;
  float64x2_t v54 = (float64x2_t)vdupq_laneq_s64((int64x2_t)v51, 1);
  float64x2_t v55 = vdivq_f64(v18, v54);
  float64x2_t v56 = vdivq_f64(v17, v54);
  float64x2_t v57 = vdivq_f64(v20, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v90.f64[0], 0));
  float64x2_t v58 = vdivq_f64(v19, v90);
  float64x2_t v59 = vmulq_f64(v56, v53);
  float64x2_t v60 = vmulq_f64(v53, v53);
  v60.f64[0]  = v60.f64[0] + vaddvq_f64(vmulq_f64(v52, v52));
  v59.f64[0]  = (v59.f64[0] + vaddvq_f64(vmulq_f64(v55, v52))) / v60.f64[0];
  float64x2_t v61 = vsubq_f64(v55, vmulq_n_f64(v52, v59.f64[0]));
  float64x2_t v62 = vsubq_f64(v56, vmulq_f64(v53, v59));
  float64x2_t v63 = vmulq_f64(v58, v53);
  v63.f64[0]  = (v63.f64[0] + vaddvq_f64(vmulq_f64(v57, v52))) / v60.f64[0];
  v64  = vmulq_n_f64(v52, v63.f64[0]);
  v65  = vsubq_f64(v58, vmulq_f64(v53, v63));
  float64x2_t v66 = vsubq_f64(v57, v64);
  float64x2_t v67 = vmulq_f64(v58, v62);
  v67.f64[0]  = v67.f64[0] + vaddvq_f64(vmulq_f64(v57, v61));
  float64x2_t v68 = vmulq_f64(v62, v62);
  v68.f64[0]  = v68.f64[0] + vaddvq_f64(vmulq_f64(v61, v61));
  v67.f64[0]  = v67.f64[0] / v68.f64[0];
  float64x2_t v69 = vmulq_f64(v62, v67);
  float64x2_t v70 = vsubq_f64(v66, vmulq_n_f64(v61, v67.f64[0]));
  float64x2_t v71 = vsubq_f64(v65, v69);
  v60.f64[0]  = 1.0 / sqrt(v60.f64[0]);
  v68.f64[0]  = 1.0 / sqrt(v68.f64[0]);
  float64x2_t v72 = vmulq_n_f64(v61, v68.f64[0]);
  v73  = vmulq_f64(v62, v68);
  v74  = vmulq_f64(v71, v71);
  v74.f64[0]  = 1.0 / sqrt(v74.f64[0] + vaddvq_f64(vmulq_f64(v70, v70)));
  v110  = 0u;
  v111  = 0u;
  v118  = vmulq_n_f64(v52, v60.f64[0]);
  v119  = vmulq_f64(v53, v60);
  v120  = v72;
  v121  = v73;
  v122  = vmulq_n_f64(v70, v74.f64[0]);
  v123  = vmulq_f64(v71, v74);
  simd_quaternion((uint64_t)&v118, (uint64_t)&v110);
  v75  = (int64x2_t)vornq_s8(vmvnq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v110), (int8x16_t)vcgezq_f64(v110))), vorrq_s8((int8x16_t)vcltzq_f64(v111), (int8x16_t)vcgezq_f64(v111)));
  v76  = vorrq_s8((int8x16_t)v75, (int8x16_t)vdupq_laneq_s64(v75, 1)).u64[0];
  v77  = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if ((v76 & 0x8000000000000000) != 0)
  {
    v82  = v77;
    v79  = v77;
LABEL_18:
    v81  = v92;
    v80  = v93;
    goto LABEL_19;
  }
  v78  = vaddvq_f64(vaddq_f64(vmulq_f64(v110, v110), vmulq_f64(v111, v111)));
  if (v78 != 0.0)
  {
    v83  = 1.0 / sqrt(v78);
    v79  = vmulq_n_f64(v111, v83);
    v82  = vmulq_n_f64(v110, v83);
    goto LABEL_18;
  }
  v79  = (float64x2_t)xmmword_228C1F7A0;
  v81  = v92;
  v80  = v93;
  v82  = 0uLL;
LABEL_19:
  v84  = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v82), (int8x16_t)vcgezq_f64(v82)), (int8x16_t)vceqq_f64(vabsq_f64(v82), v77)), vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v79), (int8x16_t)vcgezq_f64(v79)), (int8x16_t)vceqq_f64(vabsq_f64(v79), v77)));
  if ((vandq_s8((int8x16_t)v84, (int8x16_t)vdupq_laneq_s64(v84, 1)).u64[0] & 0x8000000000000000) != 0
    && (*(void *)&v85  = vextq_s8(v91, v91, 8uLL).u64[0], vabdd_f64(*(double *)v91.i64, v85) < 0.0000000149011612)
    && vabdd_f64(v90.f64[0], v85) < 0.0000000149011612)
  {
    *(float64x2_t *)a3  = v81;
    *(float64_t *)(a3 + 16)  = v80;
    *(void *)(a3 + 24)  = 0;
    *(float64x2_t *)(a3 + 32)  = v82;
    *(float64x2_t *)(a3 + 48)  = v79;
    *(void *)(a3 + 64)  = v91.i64[0];
  }
  else
  {
    *(_OWORD *)(a3 + 32)  = xmmword_228C202B0;
    *(_OWORD *)(a3 + 48)  = unk_228C202C0;
    *(_OWORD *)(a3 + 64)  = xmmword_228C202D0;
    *(_OWORD *)a3  = SPScaledPose3DInvalid_0;
    *(_OWORD *)(a3 + 16)  = unk_228C202A0;
  }
}

double SPScaledPose3D.init(position:rotation:scale:)@<D0>(uint64_t a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>, __n128 a5@<Q3>, __n128 a6@<Q4>, double a7@<D5>)
{
  v19.x  = a2;
  v19.y  = a3;
  v19.double z = a4;
  v18[0]  = a5;
  v18[1]  = a6;
  SPScaledPose3DMake(&v19, v18, (uint64_t)v14, a7);
  double result = *(double *)v14;
  long long v9 = v14[1];
  long long v10 = v14[2];
  uint64_t v11 = v15;
  uint64_t v12 = v16;
  uint64_t v13 = v17;
  *(_OWORD *)a1  = v14[0];
  *(_OWORD *)(a1 + 16)  = v9;
  *(void *)(a1 + 48)  = v11;
  *(void *)(a1 + 56)  = v12;
  *(_OWORD *)(a1 + 32)  = v10;
  *(void *)(a1 + 64)  = v13;
  return result;
}

_OWORD *SPScaledPose3DMake@<X0>(_OWORD *result@<X0>, _OWORD *a2@<X1>, uint64_t a3@<X8>, double a4@<D0>)
{
  long long v4 = result[1];
  *(_OWORD *)a3  = *result;
  *(_OWORD *)(a3 + 16)  = v4;
  long long v5 = a2[1];
  *(_OWORD *)(a3 + 32)  = *a2;
  *(_OWORD *)(a3 + 48)  = v5;
  *(double *)(a3 + 64)  = a4;
  return result;
}

double SPScaledPose3D.init(position:rotation:scale:)@<D0>(uint64_t a1@<X8>, __n128 a2@<Q0>, __n128 a3@<Q1>, __n128 a4@<Q2>, __n128 a5@<Q3>, double a6@<D4>)
{
  v18[0]  = a2;
  v18[1]  = a3;
  v17[0]  = a4;
  v17[1]  = a5;
  SPScaledPose3DMakeWithVector((uint64_t)v18, v17, (uint64_t)v13, a6);
  double result = *(double *)v13;
  long long v8 = v13[1];
  long long v9 = v13[2];
  uint64_t v10 = v14;
  uint64_t v11 = v15;
  uint64_t v12 = v16;
  *(_OWORD *)a1  = v13[0];
  *(_OWORD *)(a1 + 16)  = v8;
  *(void *)(a1 + 48)  = v10;
  *(void *)(a1 + 56)  = v11;
  *(_OWORD *)(a1 + 32)  = v9;
  *(void *)(a1 + 64)  = v12;
  return result;
}

uint64_t SPScaledPose3DMakeWithVector@<X0>(uint64_t result@<X0>, _OWORD *a2@<X1>, uint64_t a3@<X8>, double a4@<D0>)
{
  uint64_t v4 = *(void *)(result + 16);
  *(_OWORD *)a3  = *(_OWORD *)result;
  *(void *)(a3 + 16)  = v4;
  long long v5 = a2[1];
  *(_OWORD *)(a3 + 32)  = *a2;
  *(_OWORD *)(a3 + 48)  = v5;
  *(double *)(a3 + 64)  = a4;
  return result;
}

double SPScaledPose3D.init(position:rotation:scale:)@<D0>(uint64_t a1@<X8>, float32x4_t a2@<Q0>, float32x4_t a3@<Q1>, float a4@<S2>)
{
  v16[0]  = vcvtq_f64_f32(*(float32x2_t *)a2.f32);
  v16[1]  = vcvt_hight_f64_f32(a2);
  v15[0]  = vcvtq_f64_f32(*(float32x2_t *)a3.f32);
  v15[1]  = vcvt_hight_f64_f32(a3);
  SPScaledPose3DMakeWithVector((uint64_t)v16, v15, (uint64_t)v11, a4);
  double result = *(double *)v11;
  long long v6 = v11[1];
  long long v7 = v11[2];
  uint64_t v8 = v12;
  uint64_t v9 = v13;
  uint64_t v10 = v14;
  *(_OWORD *)a1  = v11[0];
  *(_OWORD *)(a1 + 16)  = v6;
  *(void *)(a1 + 48)  = v8;
  *(void *)(a1 + 56)  = v9;
  *(_OWORD *)(a1 + 32)  = v7;
  *(void *)(a1 + 64)  = v10;
  return result;
}

double SPScaledPose3D.init(position:target:scale:up:)@<D0>(uint64_t a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>, double a5@<D3>, double a6@<D4>, double a7@<D5>, double a8@<D6>, double a9@<D7>, long long a10)
{
  v23.x  = a2;
  v23.y  = a3;
  v23.double z = a4;
  v22.x  = a5;
  v22.y  = a6;
  v22.double z = a7;
  v21.x  = a9;
  *(_OWORD *)&v21.vector.f64[1]  = a10;
  SPScaledPose3DMakeLookAt(&v23, &v22, a8, &v21, (uint64_t)v17);
  double result = *(double *)v17;
  long long v12 = v17[1];
  long long v13 = v17[2];
  uint64_t v14 = v18;
  uint64_t v15 = v19;
  uint64_t v16 = v20;
  *(_OWORD *)a1  = v17[0];
  *(_OWORD *)(a1 + 16)  = v12;
  *(void *)(a1 + 48)  = v14;
  *(void *)(a1 + 56)  = v15;
  *(_OWORD *)(a1 + 32)  = v13;
  *(void *)(a1 + 64)  = v16;
  return result;
}

void SPScaledPose3DMakeLookAt(SPPoint3D *a1@<X0>, SPPoint3D *a2@<X1>, double a3@<D0>, SPVector3D *a4@<X2>, uint64_t a5@<X8>)
{
  float64x2_t v8 = *(float64x2_t *)&a4->vector.f64[2];
  float64x2_t v9 = vsubq_f64(*(float64x2_t *)&a2->x, *(float64x2_t *)&a1->x);
  float64x2_t v10 = vsubq_f64(*(float64x2_t *)&a2->vector.f64[2], *(float64x2_t *)&a1->vector.f64[2]);
  float64x2_t v11 = vmulq_f64(v9, v9);
  v11.f64[0]  = 1.0 / sqrt(vmulq_f64(v10, v10).f64[0] + vaddvq_f64(v11));
  float64x2_t v12 = vmulq_n_f64(v9, v11.f64[0]);
  float64x2_t v13 = vmulq_f64(v10, v11);
  float64x2_t v14 = vmulq_f64(v8, v8);
  v14.f64[0]  = 1.0 / sqrt(v14.f64[0] + vaddvq_f64(vmulq_f64(*(float64x2_t *)&a4->x, *(float64x2_t *)&a4->x)));
  float64x2_t v15 = vmulq_n_f64(*(float64x2_t *)&a4->x, v14.f64[0]);
  float64x2_t v16 = vmulq_f64(v8, v14);
  v17.f64[0]  = v13.f64[0];
  v17.f64[1]  = v12.f64[0];
  float64x2_t v18 = vmlaq_laneq_f64(vmulq_laneq_f64(vnegq_f64(v13), v15, 1), v16, v12, 1);
  v16.f64[1]  = v15.f64[0];
  float64x2_t v19 = vmlaq_f64(vmulq_f64(v16, vnegq_f64(v12)), v15, v17);
  float64x2_t v20 = vmulq_f64(v19, v19);
  double v21 = vmulq_f64(v18, v18).f64[0];
  v18.f64[1]  = v19.f64[0];
  v20.f64[0]  = 1.0 / sqrt(v20.f64[1] + v21 + v20.f64[0]);
  float64x2_t v22 = vmulq_n_f64(v18, v20.f64[0]);
  float64x2_t v23 = vmulq_laneq_f64(v20, v19, 1);
  float64x2_t v24 = vnegq_f64(v22);
  float64x2_t v25 = vnegq_f64(v23);
  v23.f64[1]  = v22.f64[0];
  float64x2_t v26 = vmlaq_f64(vmulq_f64(v17, v24), v12, v23);
  float64x2_t v27 = vmlaq_laneq_f64(vmulq_laneq_f64(v25, v12, 1), v13, v22, 1);
  float64x2_t v28 = vmulq_f64(v26, v26);
  v22.f64[0]  = vmulq_f64(v27, v27).f64[0];
  v27.f64[1]  = v26.f64[0];
  v28.f64[0]  = 1.0 / sqrt(v28.f64[1] + v22.f64[0] + v28.f64[0]);
  v34[0]  = v24;
  v34[1]  = v25;
  v34[2]  = vmulq_n_f64(v27, v28.f64[0]);
  v34[3]  = vmulq_laneq_f64(v28, v26, 1);
  v34[4]  = v12;
  v34[5]  = v13;
  simd_quaternion((uint64_t)v34, (uint64_t)&v35);
  double v29 = vaddvq_f64(vaddq_f64(vmulq_f64(v35, v35), vmulq_f64(v36, v36)));
  if (v29 == 0.0)
  {
    float64x2_t v30 = (float64x2_t)xmmword_228C1F7A0;
    float64x2_t v31 = 0uLL;
  }
  else
  {
    double v32 = 1.0 / sqrt(v29);
    float64x2_t v30 = vmulq_n_f64(v36, v32);
    float64x2_t v31 = vmulq_n_f64(v35, v32);
  }
  long long v33 = *(_OWORD *)&a1->vector.f64[2];
  *(_OWORD *)a5  = *(_OWORD *)&a1->x;
  *(_OWORD *)(a5 + 16)  = v33;
  *(float64x2_t *)(a5 + 32)  = v31;
  *(float64x2_t *)(a5 + 48)  = v30;
  *(double *)(a5 + 64)  = a3;
}

double SPScaledPose3D.init(forward:scale:up:)@<D0>(uint64_t a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>, double a5@<D3>, double a6@<D4>, double a7@<D5>, double a8@<D6>)
{
  v20.x  = a2;
  v20.y  = a3;
  v20.double z = a4;
  v19.x  = a6;
  v19.y  = a7;
  v19.double z = a8;
  SPScaledPose3DMakeLookAt(&v20, a5, &v19, (uint64_t)v15);
  double result = *(double *)v15;
  long long v10 = v15[1];
  long long v11 = v15[2];
  uint64_t v12 = v16;
  uint64_t v13 = v17;
  uint64_t v14 = v18;
  *(_OWORD *)a1  = v15[0];
  *(_OWORD *)(a1 + 16)  = v10;
  *(void *)(a1 + 48)  = v12;
  *(void *)(a1 + 56)  = v13;
  *(_OWORD *)(a1 + 32)  = v11;
  *(void *)(a1 + 64)  = v14;
  return result;
}

void SPScaledPose3DMakeLookAt(SPVector3D *a1@<X0>, double a2@<D0>, SPVector3D *a3@<X1>, uint64_t a4@<X8>)
{
  float64x2_t v6 = *(float64x2_t *)&a1->vector.f64[2];
  float64x2_t v7 = *(float64x2_t *)&a3->vector.f64[2];
  float64x2_t v8 = vmulq_f64(v6, v6);
  v8.f64[0]  = 1.0 / sqrt(v8.f64[0] + vaddvq_f64(vmulq_f64(*(float64x2_t *)&a1->x, *(float64x2_t *)&a1->x)));
  float64x2_t v9 = vmulq_n_f64(*(float64x2_t *)&a1->x, v8.f64[0]);
  float64x2_t v10 = vmulq_f64(v6, v8);
  float64x2_t v11 = vmulq_f64(v7, v7);
  v11.f64[0]  = 1.0 / sqrt(v11.f64[0] + vaddvq_f64(vmulq_f64(*(float64x2_t *)&a3->x, *(float64x2_t *)&a3->x)));
  float64x2_t v12 = vmulq_n_f64(*(float64x2_t *)&a3->x, v11.f64[0]);
  float64x2_t v13 = vmulq_f64(v7, v11);
  v11.f64[0]  = v10.f64[0];
  v11.f64[1]  = v9.f64[0];
  float64x2_t v14 = vmlaq_laneq_f64(vmulq_laneq_f64(vnegq_f64(v10), v12, 1), v13, v9, 1);
  v13.f64[1]  = v12.f64[0];
  float64x2_t v15 = vmlaq_f64(vmulq_f64(v13, vnegq_f64(v9)), v12, v11);
  float64x2_t v16 = vmulq_f64(v15, v15);
  double v17 = vmulq_f64(v14, v14).f64[0];
  v14.f64[1]  = v15.f64[0];
  v16.f64[0]  = 1.0 / sqrt(v16.f64[1] + v17 + v16.f64[0]);
  float64x2_t v18 = vmulq_n_f64(v14, v16.f64[0]);
  float64x2_t v19 = vmulq_laneq_f64(v16, v15, 1);
  float64x2_t v20 = vnegq_f64(v18);
  float64x2_t v21 = vnegq_f64(v19);
  v19.f64[1]  = v18.f64[0];
  float64x2_t v22 = vmlaq_f64(vmulq_f64(v11, v20), v9, v19);
  float64x2_t v23 = vmlaq_laneq_f64(vmulq_laneq_f64(v21, v9, 1), v10, v18, 1);
  float64x2_t v24 = vmulq_f64(v22, v22);
  v18.f64[0]  = vmulq_f64(v23, v23).f64[0];
  v23.f64[1]  = v22.f64[0];
  v24.f64[0]  = 1.0 / sqrt(v24.f64[1] + v18.f64[0] + v24.f64[0]);
  v29[0]  = v20;
  v29[1]  = v21;
  v29[2]  = vmulq_n_f64(v23, v24.f64[0]);
  v29[3]  = vmulq_laneq_f64(v24, v22, 1);
  v29[4]  = v9;
  v29[5]  = v10;
  simd_quaternion((uint64_t)v29, (uint64_t)&v30);
  double v25 = vaddvq_f64(vaddq_f64(vmulq_f64(v30, v30), vmulq_f64(v31, v31)));
  if (v25 == 0.0)
  {
    float64x2_t v26 = (float64x2_t)xmmword_228C1F7A0;
    float64x2_t v27 = 0uLL;
  }
  else
  {
    double v28 = 1.0 / sqrt(v25);
    float64x2_t v26 = vmulq_n_f64(v31, v28);
    float64x2_t v27 = vmulq_n_f64(v30, v28);
  }
  *(_OWORD *)a4  = 0uLL;
  *(_OWORD *)(a4 + 16)  = 0uLL;
  *(float64x2_t *)(a4 + 32)  = v27;
  *(float64x2_t *)(a4 + 48)  = v26;
  *(double *)(a4 + 64)  = a2;
}

BOOL SPScaledPose3D.init(transform:)@<W0>(long long *a1@<X0>, uint64_t a2@<X8>)
{
  return SPScaledPose3D.init(transform:)(a1, (void (*)(long long *__return_ptr, long long *))SPScaledPose3DMakeWithAffineTransform, a2);
}

{
  return SPScaledPose3D.init(transform:)(a1, (void (*)(long long *__return_ptr, long long *))SPScaledPose3DMakeWithProjectiveTransform, a2);
}

void SPScaledPose3DMakeWithAffineTransform(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  float64x2_t v5 = *(float64x2_t *)a1;
  float64x2_t v4 = *(float64x2_t *)(a1 + 16);
  float64x2_t v7 = *(float64x2_t *)(a1 + 32);
  float64x2_t v6 = *(float64x2_t *)(a1 + 48);
  float64x2_t v8 = *(float64x2_t *)(a1 + 64);
  float64x2_t v9 = *(float64x2_t *)(a1 + 80);
  v10.f64[0]  = *(float64_t *)(a1 + 80);
  v10.f64[1]  = *(float64_t *)(a1 + 64);
  v11.f64[0]  = *(float64_t *)(a1 + 48);
  v11.f64[1]  = *(float64_t *)(a1 + 32);
  float64x2_t v12 = vmulq_f64(*(float64x2_t *)a1, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)v9, 8uLL), vnegq_f64(v11)), v10, (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v6, 8uLL)));
  float64x2_t v14 = vmulq_f64(v4, vmlaq_laneq_f64(vmulq_f64(v8, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v7, 1))), v7, v8, 1));
  BOOL v13 = v14.f64[0] + vaddvq_f64(v12) < 0.0;
  v14.f64[0]  = -1.0;
  if (!v13) {
    v14.f64[0]  = 1.0;
  }
  v15.f64[0]  = sqrt(vmulq_f64(v4, v4).f64[0] + vaddvq_f64(vmulq_f64(v5, v5)));
  float64x2_t v16 = vmulq_f64(v9, v9);
  v16.f64[0]  = sqrt(v16.f64[0] + vaddvq_f64(vmulq_f64(v8, v8)));
  v15.f64[1]  = sqrt(vmulq_f64(v6, v6).f64[0] + vaddvq_f64(vmulq_f64(v7, v7)));
  float64x2_t v17 = vmulq_n_f64(v15, v14.f64[0]);
  float64x2_t v18 = vdivq_f64(v5, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v17.f64[0], 0));
  float64x2_t v19 = vdivq_f64(v4, v17);
  float64x2_t v48 = vmulq_f64(v16, v14);
  int8x16_t v49 = (int8x16_t)v17;
  float64x2_t v20 = (float64x2_t)vdupq_laneq_s64((int64x2_t)v17, 1);
  float64x2_t v21 = vdivq_f64(v7, v20);
  float64x2_t v22 = vdivq_f64(v6, v20);
  float64x2_t v23 = vdivq_f64(v8, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v48.f64[0], 0));
  float64x2_t v24 = vdivq_f64(v9, v48);
  float64x2_t v25 = vmulq_f64(v22, v19);
  float64x2_t v26 = vmulq_f64(v19, v19);
  v26.f64[0]  = v26.f64[0] + vaddvq_f64(vmulq_f64(v18, v18));
  v25.f64[0]  = (v25.f64[0] + vaddvq_f64(vmulq_f64(v21, v18))) / v26.f64[0];
  float64x2_t v27 = vsubq_f64(v21, vmulq_n_f64(v18, v25.f64[0]));
  float64x2_t v28 = vsubq_f64(v22, vmulq_f64(v19, v25));
  float64x2_t v29 = vmulq_f64(v24, v19);
  v29.f64[0]  = (v29.f64[0] + vaddvq_f64(vmulq_f64(v23, v18))) / v26.f64[0];
  float64x2_t v30 = vmulq_n_f64(v18, v29.f64[0]);
  float64x2_t v31 = vsubq_f64(v24, vmulq_f64(v19, v29));
  float64x2_t v32 = vsubq_f64(v23, v30);
  float64x2_t v33 = vmulq_f64(v23, v27);
  v33.f64[0]  = vmulq_f64(v24, v28).f64[0] + vaddvq_f64(v33);
  float64x2_t v34 = vmulq_f64(v28, v28);
  v34.f64[0]  = v34.f64[0] + vaddvq_f64(vmulq_f64(v27, v27));
  v33.f64[0]  = v33.f64[0] / v34.f64[0];
  float64x2_t v35 = vmulq_f64(v28, v33);
  float64x2_t v36 = vsubq_f64(v32, vmulq_n_f64(v27, v33.f64[0]));
  float64x2_t v37 = vsubq_f64(v31, v35);
  v26.f64[0]  = 1.0 / sqrt(v26.f64[0]);
  v34.f64[0]  = 1.0 / sqrt(v34.f64[0]);
  v12.f64[0]  = 1.0 / sqrt(vmulq_f64(v37, v37).f64[0] + vaddvq_f64(vmulq_f64(v36, v36)));
  float64x2_t v51 = 0u;
  float64x2_t v52 = 0u;
  v50[0]  = vmulq_n_f64(v18, v26.f64[0]);
  v50[1]  = vmulq_f64(v19, v26);
  v50[2]  = vmulq_n_f64(v27, v34.f64[0]);
  v50[3]  = vmulq_f64(v28, v34);
  v50[4]  = vmulq_n_f64(v36, v12.f64[0]);
  v50[5]  = vmulq_f64(v37, v12);
  simd_quaternion((uint64_t)v50, (uint64_t)&v51);
  int64x2_t v38 = (int64x2_t)vornq_s8(vmvnq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v51), (int8x16_t)vcgezq_f64(v51))), vorrq_s8((int8x16_t)vcltzq_f64(v52), (int8x16_t)vcgezq_f64(v52)));
  unint64_t v39 = vorrq_s8((int8x16_t)v38, (int8x16_t)vdupq_laneq_s64(v38, 1)).u64[0];
  float64x2_t v40 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if ((v39 & 0x8000000000000000) != 0)
  {
    float64x2_t v43 = v40;
    float64x2_t v42 = v40;
  }
  else
  {
    double v41 = vaddvq_f64(vaddq_f64(vmulq_f64(v51, v51), vmulq_f64(v52, v52)));
    if (v41 == 0.0)
    {
      float64x2_t v42 = (float64x2_t)xmmword_228C1F7A0;
      float64x2_t v43 = 0uLL;
    }
    else
    {
      double v44 = 1.0 / sqrt(v41);
      float64x2_t v42 = vmulq_n_f64(v52, v44);
      float64x2_t v43 = vmulq_n_f64(v51, v44);
    }
  }
  int64x2_t v45 = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v43), (int8x16_t)vcgezq_f64(v43)), (int8x16_t)vceqq_f64(vabsq_f64(v43), v40)), vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v42), (int8x16_t)vcgezq_f64(v42)), (int8x16_t)vceqq_f64(vabsq_f64(v42), v40)));
  if ((vandq_s8((int8x16_t)v45, (int8x16_t)vdupq_laneq_s64(v45, 1)).u64[0] & 0x8000000000000000) != 0
    && (*(void *)&double v46 = vextq_s8(v49, v49, 8uLL).u64[0], vabdd_f64(*(double *)v49.i64, v46) < 0.0000000149011612)
    && vabdd_f64(v48.f64[0], v46) < 0.0000000149011612)
  {
    uint64_t v47 = *(void *)(a1 + 112);
    *(_OWORD *)a2  = *(_OWORD *)(a1 + 96);
    *(void *)(a2 + 16)  = v47;
    *(void *)(a2 + 24)  = 0;
    *(float64x2_t *)(a2 + 32)  = v43;
    *(float64x2_t *)(a2 + 48)  = v42;
    *(void *)(a2 + 64)  = v49.i64[0];
  }
  else
  {
    *(_OWORD *)(a2 + 32)  = xmmword_228C202B0;
    *(_OWORD *)(a2 + 48)  = unk_228C202C0;
    *(_OWORD *)(a2 + 64)  = xmmword_228C202D0;
    *(_OWORD *)a2  = SPScaledPose3DInvalid_0;
    *(_OWORD *)(a2 + 16)  = unk_228C202A0;
  }
}

BOOL SPScaledPose3DIsValid(uint64_t a1)
{
  float64x2_t v1 = *(float64x2_t *)(a1 + 48);
  int8x16_t v2 = vorrq_s8((int8x16_t)vcltzq_f64(v1), (int8x16_t)vcgezq_f64(v1));
  float64x2_t v3 = vabsq_f64(v1);
  float64x2_t v4 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  int64x2_t v5 = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(*(float64x2_t *)(a1 + 32)), (int8x16_t)vcgezq_f64(*(float64x2_t *)(a1 + 32))), (int8x16_t)vceqq_f64(vabsq_f64(*(float64x2_t *)(a1 + 32)), v4)), vbicq_s8(v2, (int8x16_t)vceqq_f64(v3, v4)));
  float64x2_t v6 = (float64x2_t)vdupq_laneq_s64(v5, 1);
  if ((vandq_s8((int8x16_t)v5, (int8x16_t)v6).u64[0] & 0x8000000000000000) != 0
    && (v6.f64[0]  = *(float64_t *)(a1 + 16),
        float64x2_t v7 = (float64x2_t)vcltzq_f64(*(float64x2_t *)a1),
        int8x16_t v8 = vorrq_s8((int8x16_t)v7, (int8x16_t)vcgezq_f64(*(float64x2_t *)a1)),
        v7.f64[0]  = INFINITY,
        int64x2_t v9 = (int64x2_t)vbicq_s8(v8, (int8x16_t)vceqq_f64(vabsq_f64(*(float64x2_t *)a1), v4)),
        (vandq_s8((int8x16_t)vdupq_laneq_s64(v9, 1), vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v6), (int8x16_t)vcgezq_f64(v6)), (int8x16_t)vceqq_f64(vabsq_f64(v6), v7)), (int8x16_t)v9)).u64[0] & 0x8000000000000000) != 0))
  {
    return ((*(void *)(a1 + 64) & 0x7FFFFFFFFFFFFFFFuLL) - 0x10000000000000) >> 53 < 0x3FF;
  }
  else
  {
    return 0;
  }
}

BOOL SPScaledPose3D.init(transform:)@<W0>(long long *a1@<X0>, void (*a2)(long long *__return_ptr, long long *)@<X1>, uint64_t a3@<X8>)
{
  long long v4 = a1[1];
  long long v5 = a1[2];
  long long v6 = a1[3];
  long long v7 = a1[4];
  long long v8 = a1[5];
  long long v9 = a1[6];
  long long v10 = a1[7];
  long long v27 = *a1;
  long long v28 = v4;
  long long v29 = v5;
  long long v30 = v6;
  long long v31 = v7;
  long long v32 = v8;
  long long v33 = v9;
  long long v34 = v10;
  a2(&v22, &v27);
  uint64_t v11 = *((void *)&v23 + 1);
  uint64_t v12 = v23;
  uint64_t v13 = *((void *)&v22 + 1);
  uint64_t v14 = v22;
  uint64_t v15 = *((void *)&v25 + 1);
  uint64_t v16 = v25;
  uint64_t v17 = *((void *)&v24 + 1);
  uint64_t v18 = v24;
  uint64_t v19 = v26;
  long long v28 = v23;
  long long v27 = v22;
  long long v30 = v25;
  long long v29 = v24;
  *(void *)&long long v31 = v26;
  BOOL result = SPScaledPose3DIsValid((uint64_t)&v27);
  if (result)
  {
    uint64_t v21 = v19;
  }
  else
  {
    uint64_t v14 = 0;
    uint64_t v13 = 0;
    uint64_t v12 = 0;
    uint64_t v11 = 0;
    uint64_t v18 = 0;
    uint64_t v17 = 0;
    uint64_t v16 = 0;
    uint64_t v15 = 0;
    uint64_t v21 = 0;
  }
  *(void *)a3  = v14;
  *(void *)(a3 + 8)  = v13;
  *(void *)(a3 + 16)  = v12;
  *(void *)(a3 + 24)  = v11;
  *(void *)(a3 + 32)  = v18;
  *(void *)(a3 + 40)  = v17;
  *(void *)(a3 + 48)  = v16;
  *(void *)(a3 + 56)  = v15;
  *(void *)(a3 + 64)  = v21;
  *(void *)(a3 + 72)  = 0;
  *(unsigned char *)(a3 + 80)  = !result;
  return result;
}

void SPScaledPose3DMakeWithProjectiveTransform(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  float64x2_t v5 = *(float64x2_t *)a1;
  float64x2_t v4 = *(float64x2_t *)(a1 + 16);
  float64x2_t v7 = *(float64x2_t *)(a1 + 32);
  float64x2_t v6 = *(float64x2_t *)(a1 + 48);
  float64x2_t v8 = *(float64x2_t *)(a1 + 64);
  float64x2_t v9 = *(float64x2_t *)(a1 + 80);
  v10.f64[0]  = *(float64_t *)(a1 + 80);
  v10.f64[1]  = *(float64_t *)(a1 + 64);
  v11.f64[0]  = *(float64_t *)(a1 + 48);
  v11.f64[1]  = *(float64_t *)(a1 + 32);
  float64x2_t v12 = vmulq_f64(*(float64x2_t *)a1, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)v9, 8uLL), vnegq_f64(v11)), v10, (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v6, 8uLL)));
  float64x2_t v14 = vmulq_f64(v4, vmlaq_laneq_f64(vmulq_f64(v8, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v7, 1))), v7, v8, 1));
  BOOL v13 = v14.f64[0] + vaddvq_f64(v12) < 0.0;
  v14.f64[0]  = -1.0;
  if (!v13) {
    v14.f64[0]  = 1.0;
  }
  v15.f64[0]  = sqrt(vmulq_f64(v4, v4).f64[0] + vaddvq_f64(vmulq_f64(v5, v5)));
  float64x2_t v16 = vmulq_f64(v9, v9);
  v16.f64[0]  = sqrt(v16.f64[0] + vaddvq_f64(vmulq_f64(v8, v8)));
  v15.f64[1]  = sqrt(vmulq_f64(v6, v6).f64[0] + vaddvq_f64(vmulq_f64(v7, v7)));
  float64x2_t v17 = vmulq_n_f64(v15, v14.f64[0]);
  float64x2_t v18 = vdivq_f64(v5, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v17.f64[0], 0));
  float64x2_t v19 = vdivq_f64(v4, v17);
  float64x2_t v48 = vmulq_f64(v16, v14);
  int8x16_t v49 = (int8x16_t)v17;
  float64x2_t v20 = (float64x2_t)vdupq_laneq_s64((int64x2_t)v17, 1);
  float64x2_t v21 = vdivq_f64(v7, v20);
  float64x2_t v22 = vdivq_f64(v6, v20);
  float64x2_t v23 = vdivq_f64(v8, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v48.f64[0], 0));
  float64x2_t v24 = vdivq_f64(v9, v48);
  float64x2_t v25 = vmulq_f64(v22, v19);
  float64x2_t v26 = vmulq_f64(v19, v19);
  v26.f64[0]  = v26.f64[0] + vaddvq_f64(vmulq_f64(v18, v18));
  v25.f64[0]  = (v25.f64[0] + vaddvq_f64(vmulq_f64(v21, v18))) / v26.f64[0];
  float64x2_t v27 = vsubq_f64(v21, vmulq_n_f64(v18, v25.f64[0]));
  float64x2_t v28 = vsubq_f64(v22, vmulq_f64(v19, v25));
  float64x2_t v29 = vmulq_f64(v24, v19);
  v29.f64[0]  = (v29.f64[0] + vaddvq_f64(vmulq_f64(v23, v18))) / v26.f64[0];
  float64x2_t v30 = vmulq_n_f64(v18, v29.f64[0]);
  float64x2_t v31 = vsubq_f64(v24, vmulq_f64(v19, v29));
  float64x2_t v32 = vsubq_f64(v23, v30);
  float64x2_t v33 = vmulq_f64(v23, v27);
  v33.f64[0]  = vmulq_f64(v24, v28).f64[0] + vaddvq_f64(v33);
  float64x2_t v34 = vmulq_f64(v28, v28);
  v34.f64[0]  = v34.f64[0] + vaddvq_f64(vmulq_f64(v27, v27));
  v33.f64[0]  = v33.f64[0] / v34.f64[0];
  float64x2_t v35 = vmulq_f64(v28, v33);
  float64x2_t v36 = vsubq_f64(v32, vmulq_n_f64(v27, v33.f64[0]));
  float64x2_t v37 = vsubq_f64(v31, v35);
  v26.f64[0]  = 1.0 / sqrt(v26.f64[0]);
  v34.f64[0]  = 1.0 / sqrt(v34.f64[0]);
  v12.f64[0]  = 1.0 / sqrt(vmulq_f64(v37, v37).f64[0] + vaddvq_f64(vmulq_f64(v36, v36)));
  float64x2_t v51 = 0u;
  float64x2_t v52 = 0u;
  v50[0]  = vmulq_n_f64(v18, v26.f64[0]);
  v50[1]  = vmulq_f64(v19, v26);
  v50[2]  = vmulq_n_f64(v27, v34.f64[0]);
  v50[3]  = vmulq_f64(v28, v34);
  v50[4]  = vmulq_n_f64(v36, v12.f64[0]);
  v50[5]  = vmulq_f64(v37, v12);
  simd_quaternion((uint64_t)v50, (uint64_t)&v51);
  int64x2_t v38 = (int64x2_t)vornq_s8(vmvnq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v51), (int8x16_t)vcgezq_f64(v51))), vorrq_s8((int8x16_t)vcltzq_f64(v52), (int8x16_t)vcgezq_f64(v52)));
  unint64_t v39 = vorrq_s8((int8x16_t)v38, (int8x16_t)vdupq_laneq_s64(v38, 1)).u64[0];
  float64x2_t v40 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if ((v39 & 0x8000000000000000) != 0)
  {
    float64x2_t v43 = v40;
    float64x2_t v42 = v40;
  }
  else
  {
    double v41 = vaddvq_f64(vaddq_f64(vmulq_f64(v51, v51), vmulq_f64(v52, v52)));
    if (v41 == 0.0)
    {
      float64x2_t v42 = (float64x2_t)xmmword_228C1F7A0;
      float64x2_t v43 = 0uLL;
    }
    else
    {
      double v44 = 1.0 / sqrt(v41);
      float64x2_t v42 = vmulq_n_f64(v52, v44);
      float64x2_t v43 = vmulq_n_f64(v51, v44);
    }
  }
  int64x2_t v45 = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v43), (int8x16_t)vcgezq_f64(v43)), (int8x16_t)vceqq_f64(vabsq_f64(v43), v40)), vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v42), (int8x16_t)vcgezq_f64(v42)), (int8x16_t)vceqq_f64(vabsq_f64(v42), v40)));
  if ((vandq_s8((int8x16_t)v45, (int8x16_t)vdupq_laneq_s64(v45, 1)).u64[0] & 0x8000000000000000) != 0
    && (*(void *)&double v46 = vextq_s8(v49, v49, 8uLL).u64[0], vabdd_f64(*(double *)v49.i64, v46) < 0.0000000149011612)
    && vabdd_f64(v48.f64[0], v46) < 0.0000000149011612)
  {
    uint64_t v47 = *(void *)(a1 + 112);
    *(_OWORD *)a2  = *(_OWORD *)(a1 + 96);
    *(void *)(a2 + 16)  = v47;
    *(void *)(a2 + 24)  = 0;
    *(float64x2_t *)(a2 + 32)  = v43;
    *(float64x2_t *)(a2 + 48)  = v42;
    *(void *)(a2 + 64)  = v49.i64[0];
  }
  else
  {
    *(_OWORD *)(a2 + 32)  = xmmword_228C202B0;
    *(_OWORD *)(a2 + 48)  = unk_228C202C0;
    *(_OWORD *)(a2 + 64)  = xmmword_228C202D0;
    *(_OWORD *)a2  = SPScaledPose3DInvalid_0;
    *(_OWORD *)(a2 + 16)  = unk_228C202A0;
  }
}

BOOL SPScaledPose3D.init(_:)@<W0>(long long *a1@<X0>, uint64_t a2@<X8>)
{
  long long v3 = a1[1];
  long long v4 = a1[2];
  long long v5 = a1[3];
  long long v6 = a1[4];
  long long v7 = a1[5];
  long long v8 = a1[6];
  long long v9 = a1[7];
  long long v26 = *a1;
  long long v27 = v3;
  long long v28 = v4;
  long long v29 = v5;
  long long v30 = v6;
  long long v31 = v7;
  long long v32 = v8;
  long long v33 = v9;
  SPScaledPose3DMakeWith4x4Matrix((uint64_t)&v26, (uint64_t)&v21);
  uint64_t v10 = *((void *)&v22 + 1);
  uint64_t v11 = v22;
  uint64_t v12 = *((void *)&v21 + 1);
  uint64_t v13 = v21;
  uint64_t v14 = *((void *)&v24 + 1);
  uint64_t v15 = v24;
  uint64_t v16 = *((void *)&v23 + 1);
  uint64_t v17 = v23;
  uint64_t v18 = v25;
  long long v27 = v22;
  long long v26 = v21;
  long long v29 = v24;
  long long v28 = v23;
  *(void *)&long long v30 = v25;
  BOOL result = SPScaledPose3DIsValid((uint64_t)&v26);
  if (result)
  {
    uint64_t v20 = v18;
  }
  else
  {
    uint64_t v13 = 0;
    uint64_t v12 = 0;
    uint64_t v11 = 0;
    uint64_t v10 = 0;
    uint64_t v17 = 0;
    uint64_t v16 = 0;
    uint64_t v15 = 0;
    uint64_t v14 = 0;
    uint64_t v20 = 0;
  }
  *(void *)a2  = v13;
  *(void *)(a2 + 8)  = v12;
  *(void *)(a2 + 16)  = v11;
  *(void *)(a2 + 24)  = v10;
  *(void *)(a2 + 32)  = v17;
  *(void *)(a2 + 40)  = v16;
  *(void *)(a2 + 48)  = v15;
  *(void *)(a2 + 56)  = v14;
  *(void *)(a2 + 64)  = v20;
  *(void *)(a2 + 72)  = 0;
  *(unsigned char *)(a2 + 80)  = !result;
  return result;
}

void SPScaledPose3DMakeWith4x4Matrix(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  float64x2_t v4 = *(float64x2_t *)a1;
  float64x2_t v3 = *(float64x2_t *)(a1 + 16);
  float64x2_t v6 = *(float64x2_t *)(a1 + 32);
  float64x2_t v5 = *(float64x2_t *)(a1 + 48);
  float64x2_t v7 = *(float64x2_t *)(a1 + 64);
  float64x2_t v8 = *(float64x2_t *)(a1 + 80);
  long long v46 = *(_OWORD *)(a1 + 96);
  long long v47 = *(_OWORD *)(a1 + 112);
  v9.f64[0]  = *(float64_t *)(a1 + 80);
  v9.f64[1]  = *(float64_t *)(a1 + 64);
  v10.f64[0]  = *(float64_t *)(a1 + 48);
  v10.f64[1]  = *(float64_t *)(a1 + 32);
  float64x2_t v11 = vmulq_f64(*(float64x2_t *)a1, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v8, 8uLL), vnegq_f64(v10)), v9, (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v5, 8uLL)));
  float64x2_t v13 = vmulq_f64(v3, vmlaq_laneq_f64(vmulq_f64(v7, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v6, 1))), v6, v7, 1));
  BOOL v12 = v13.f64[0] + vaddvq_f64(v11) < 0.0;
  v13.f64[0]  = -1.0;
  if (!v12) {
    v13.f64[0]  = 1.0;
  }
  v14.f64[0]  = sqrt(vmulq_f64(v3, v3).f64[0] + vaddvq_f64(vmulq_f64(v4, v4)));
  float64x2_t v15 = vmulq_f64(v8, v8);
  v15.f64[0]  = sqrt(v15.f64[0] + vaddvq_f64(vmulq_f64(v7, v7)));
  v14.f64[1]  = sqrt(vmulq_f64(v5, v5).f64[0] + vaddvq_f64(vmulq_f64(v6, v6)));
  float64x2_t v16 = vmulq_n_f64(v14, v13.f64[0]);
  float64x2_t v17 = vdivq_f64(v4, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v16.f64[0], 0));
  float64x2_t v18 = vdivq_f64(v3, v16);
  float64x2_t v48 = vmulq_f64(v15, v13);
  int8x16_t v49 = (int8x16_t)v16;
  float64x2_t v19 = (float64x2_t)vdupq_laneq_s64((int64x2_t)v16, 1);
  float64x2_t v20 = vdivq_f64(v6, v19);
  float64x2_t v21 = vdivq_f64(v5, v19);
  float64x2_t v22 = vdivq_f64(v7, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v48.f64[0], 0));
  float64x2_t v23 = vdivq_f64(v8, v48);
  float64x2_t v24 = vmulq_f64(v21, v18);
  float64x2_t v25 = vmulq_f64(v18, v18);
  v25.f64[0]  = v25.f64[0] + vaddvq_f64(vmulq_f64(v17, v17));
  v24.f64[0]  = (v24.f64[0] + vaddvq_f64(vmulq_f64(v20, v17))) / v25.f64[0];
  float64x2_t v26 = vsubq_f64(v20, vmulq_n_f64(v17, v24.f64[0]));
  float64x2_t v27 = vsubq_f64(v21, vmulq_f64(v18, v24));
  float64x2_t v28 = vmulq_f64(v23, v18);
  v28.f64[0]  = (v28.f64[0] + vaddvq_f64(vmulq_f64(v22, v17))) / v25.f64[0];
  float64x2_t v29 = vmulq_n_f64(v17, v28.f64[0]);
  float64x2_t v30 = vsubq_f64(v23, vmulq_f64(v18, v28));
  float64x2_t v31 = vsubq_f64(v22, v29);
  float64x2_t v32 = vmulq_f64(v22, v26);
  v32.f64[0]  = vmulq_f64(v23, v27).f64[0] + vaddvq_f64(v32);
  float64x2_t v33 = vmulq_f64(v27, v27);
  v33.f64[0]  = v33.f64[0] + vaddvq_f64(vmulq_f64(v26, v26));
  v32.f64[0]  = v32.f64[0] / v33.f64[0];
  float64x2_t v34 = vmulq_f64(v27, v32);
  float64x2_t v35 = vsubq_f64(v31, vmulq_n_f64(v26, v32.f64[0]));
  float64x2_t v36 = vsubq_f64(v30, v34);
  v25.f64[0]  = 1.0 / sqrt(v25.f64[0]);
  v33.f64[0]  = 1.0 / sqrt(v33.f64[0]);
  v11.f64[0]  = 1.0 / sqrt(vmulq_f64(v36, v36).f64[0] + vaddvq_f64(vmulq_f64(v35, v35)));
  float64x2_t v51 = 0u;
  float64x2_t v52 = 0u;
  v50[0]  = vmulq_n_f64(v17, v25.f64[0]);
  v50[1]  = vmulq_f64(v18, v25);
  v50[2]  = vmulq_n_f64(v26, v33.f64[0]);
  v50[3]  = vmulq_f64(v27, v33);
  v50[4]  = vmulq_n_f64(v35, v11.f64[0]);
  v50[5]  = vmulq_f64(v36, v11);
  simd_quaternion((uint64_t)v50, (uint64_t)&v51);
  int64x2_t v37 = (int64x2_t)vornq_s8(vmvnq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v51), (int8x16_t)vcgezq_f64(v51))), vorrq_s8((int8x16_t)vcltzq_f64(v52), (int8x16_t)vcgezq_f64(v52)));
  unint64_t v38 = vorrq_s8((int8x16_t)v37, (int8x16_t)vdupq_laneq_s64(v37, 1)).u64[0];
  float64x2_t v39 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if ((v38 & 0x8000000000000000) != 0)
  {
    float64x2_t v42 = v39;
    float64x2_t v41 = v39;
  }
  else
  {
    double v40 = vaddvq_f64(vaddq_f64(vmulq_f64(v51, v51), vmulq_f64(v52, v52)));
    if (v40 == 0.0)
    {
      float64x2_t v41 = (float64x2_t)xmmword_228C1F7A0;
      float64x2_t v42 = 0uLL;
    }
    else
    {
      double v43 = 1.0 / sqrt(v40);
      float64x2_t v41 = vmulq_n_f64(v52, v43);
      float64x2_t v42 = vmulq_n_f64(v51, v43);
    }
  }
  int64x2_t v44 = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v42), (int8x16_t)vcgezq_f64(v42)), (int8x16_t)vceqq_f64(vabsq_f64(v42), v39)), vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v41), (int8x16_t)vcgezq_f64(v41)), (int8x16_t)vceqq_f64(vabsq_f64(v41), v39)));
  if ((vandq_s8((int8x16_t)v44, (int8x16_t)vdupq_laneq_s64(v44, 1)).u64[0] & 0x8000000000000000) != 0
    && (*(void *)&double v45 = vextq_s8(v49, v49, 8uLL).u64[0], vabdd_f64(*(double *)v49.i64, v45) < 0.0000000149011612)
    && vabdd_f64(v48.f64[0], v45) < 0.0000000149011612)
  {
    *(_OWORD *)a2  = v46;
    *(_OWORD *)(a2 + 16)  = (unint64_t)v47;
    *(float64x2_t *)(a2 + 32)  = v42;
    *(float64x2_t *)(a2 + 48)  = v41;
    *(void *)(a2 + 64)  = v49.i64[0];
  }
  else
  {
    *(_OWORD *)(a2 + 32)  = xmmword_228C202B0;
    *(_OWORD *)(a2 + 48)  = unk_228C202C0;
    *(_OWORD *)(a2 + 64)  = xmmword_228C202D0;
    *(_OWORD *)a2  = SPScaledPose3DInvalid_0;
    *(_OWORD *)(a2 + 16)  = unk_228C202A0;
  }
}

BOOL SPScaledPose3D.init(_:)@<W0>(uint64_t a1@<X8>, float32x4_t a2@<Q0>, float32x4_t a3@<Q1>, float32x4_t a4@<Q2>, float32x4_t a5@<Q3>)
{
  float64x2_t v22 = vcvtq_f64_f32(*(float32x2_t *)a2.f32);
  float64x2_t v23 = vcvt_hight_f64_f32(a2);
  float64x2_t v24 = vcvtq_f64_f32(*(float32x2_t *)a3.f32);
  float64x2_t v25 = vcvt_hight_f64_f32(a3);
  float64x2_t v26 = vcvtq_f64_f32(*(float32x2_t *)a4.f32);
  float64x2_t v27 = vcvt_hight_f64_f32(a4);
  float64x2_t v28 = vcvtq_f64_f32(*(float32x2_t *)a5.f32);
  float64x2_t v29 = vcvt_hight_f64_f32(a5);
  SPScaledPose3DMakeWith4x4Matrix((uint64_t)&v22, (uint64_t)&v17);
  float64_t v6 = v18.f64[1];
  float64_t v7 = v18.f64[0];
  float64_t v8 = v17.f64[1];
  float64_t v9 = v17.f64[0];
  float64_t v10 = v20.f64[1];
  float64_t v11 = v20.f64[0];
  float64_t v12 = v19.f64[1];
  float64_t v13 = v19.f64[0];
  float64_t v14 = v21;
  float64x2_t v23 = v18;
  float64x2_t v22 = v17;
  float64x2_t v25 = v20;
  float64x2_t v24 = v19;
  v26.f64[0]  = v21;
  BOOL result = SPScaledPose3DIsValid((uint64_t)&v22);
  if (result)
  {
    float64_t v16 = v14;
  }
  else
  {
    float64_t v9 = 0.0;
    float64_t v8 = 0.0;
    float64_t v7 = 0.0;
    float64_t v6 = 0.0;
    float64_t v13 = 0.0;
    float64_t v12 = 0.0;
    float64_t v11 = 0.0;
    float64_t v10 = 0.0;
    float64_t v16 = 0.0;
  }
  *(float64_t *)a1  = v9;
  *(float64_t *)(a1 + 8)  = v8;
  *(float64_t *)(a1 + 16)  = v7;
  *(float64_t *)(a1 + 24)  = v6;
  *(float64_t *)(a1 + 32)  = v13;
  *(float64_t *)(a1 + 40)  = v12;
  *(float64_t *)(a1 + 48)  = v11;
  *(float64_t *)(a1 + 56)  = v10;
  *(float64_t *)(a1 + 64)  = v16;
  *(void *)(a1 + 72)  = 0;
  *(unsigned char *)(a1 + 80)  = !result;
  return result;
}

double SPScaledPose3D.matrix.getter@<D0>(_OWORD *a1@<X8>, float64x2_t a2@<Q3>, float64x2_t a3@<Q4>, float64x2_t a4@<Q6>)
{
  float64x2_t v6 = *(float64x2_t *)(v4 + 16);
  float64x2_t v7 = *(float64x2_t *)(v4 + 32);
  uint64_t v8 = *(void *)(v4 + 48);
  uint64_t v9 = *(void *)(v4 + 56);
  a2.f64[0]  = *(float64_t *)(v4 + 64);
  v19[0]  = *(float64x2_t *)v4;
  v19[1]  = v6;
  uint64_t v20 = v8;
  uint64_t v21 = v9;
  v19[2]  = v7;
  unint64_t v22 = *(void *)&a2.f64[0];
  SPScaledPose3DGet4x4Matrix(v19, v18, a2, a3, a4);
  double result = *(double *)v18;
  long long v11 = v18[1];
  long long v12 = v18[2];
  long long v13 = v18[3];
  long long v14 = v18[4];
  long long v15 = v18[5];
  long long v16 = v18[6];
  long long v17 = v18[7];
  *a1  = v18[0];
  a1[1]  = v11;
  a1[2]  = v12;
  a1[3]  = v13;
  a1[4]  = v14;
  a1[5]  = v15;
  a1[6]  = v16;
  a1[7]  = v17;
  return result;
}

double SPScaledPose3DGet4x4Matrix@<D0>(float64x2_t *a1@<X0>, _OWORD *a2@<X8>, float64x2_t a3@<Q3>, float64x2_t a4@<Q4>, float64x2_t a5@<Q6>)
{
  uint64_t v25 = *MEMORY[0x263EF8340];
  long long v22 = 0u;
  long long v23 = 0u;
  long long v20 = 0u;
  long long v21 = 0u;
  long long v18 = 0u;
  long long v19 = 0u;
  long long v16 = 0u;
  long long v17 = 0u;
  float64x2_t v6 = a1[3];
  v24[2]  = a1[2];
  v24[3]  = v6;
  v24[4]  = a1[4];
  float64x2_t v7 = a1[1];
  v24[0]  = *a1;
  v24[1]  = v7;
  SPAffineTransform3DMakeWithScaledPose(v24, &v16, v24[0], a3, a4, a5);
  double result = *(double *)&v16;
  unint64_t v9 = v17;
  long long v10 = v18;
  unint64_t v11 = v19;
  long long v12 = v20;
  unint64_t v13 = v21;
  *(void *)&long long v14 = v23;
  *((void *)&v14 + 1)  = 1.0;
  long long v15 = v22;
  *a2  = v16;
  a2[1]  = v9;
  a2[2]  = v10;
  a2[3]  = v11;
  a2[4]  = v12;
  a2[5]  = v13;
  a2[6]  = v15;
  a2[7]  = v14;
  return result;
}

double SPScaledPose3D.inverse.getter@<D0>(uint64_t a1@<X8>, float64x2_t a2@<Q3>, float64x2_t a3@<Q4>, float64x2_t a4@<Q6>)
{
  float64x2_t v6 = *(float64x2_t *)(v4 + 16);
  float64x2_t v7 = *(float64x2_t *)(v4 + 32);
  uint64_t v8 = *(void *)(v4 + 48);
  uint64_t v9 = *(void *)(v4 + 56);
  a2.f64[0]  = *(float64_t *)(v4 + 64);
  v20[0]  = *(float64x2_t *)v4;
  v20[1]  = v6;
  uint64_t v21 = v8;
  uint64_t v22 = v9;
  v20[2]  = v7;
  unint64_t v23 = *(void *)&a2.f64[0];
  SPScaledPose3DGetInverse(v20, (uint64_t)v16, a2, a3, a4);
  double result = *(double *)v16;
  long long v11 = v16[1];
  long long v12 = v16[2];
  uint64_t v13 = v17;
  uint64_t v14 = v18;
  uint64_t v15 = v19;
  *(_OWORD *)a1  = v16[0];
  *(_OWORD *)(a1 + 16)  = v11;
  *(void *)(a1 + 48)  = v13;
  *(void *)(a1 + 56)  = v14;
  *(_OWORD *)(a1 + 32)  = v12;
  *(void *)(a1 + 64)  = v15;
  return result;
}

void SPScaledPose3DGetInverse(float64x2_t *a1@<X0>, uint64_t a2@<X8>, float64x2_t a3@<Q3>, float64x2_t a4@<Q4>, float64x2_t a5@<Q6>)
{
  v81  = *MEMORY[0x263EF8340];
  float64x2_t v67 = 0u;
  float64x2_t v68 = 0u;
  v65  = 0u;
  int8x16_t v66 = 0u;
  int64x2_t v63 = 0u;
  v64  = 0u;
  float64x2_t v61 = 0u;
  float64x2_t v62 = 0u;
  float64x2_t v6 = a1[3];
  v77  = a1[2];
  v78  = v6;
  v79  = a1[4];
  float64x2_t v7 = a1[1];
  v75  = *a1;
  v76  = v7;
  SPAffineTransform3DMakeWithScaledPose(&v75, (long long *)&v61, v75, a3, a4, a5);
  *(void *)&v8.f64[0]  = v66.i64[0];
  v8.f64[1]  = v65.f64[0];
  *(void *)&v9.f64[0]  = v64.i64[0];
  *(void *)&v9.f64[1]  = v63.i64[0];
  v8.f64[0]  = vmulq_f64(v62, vmlaq_laneq_f64(vmulq_f64(v65, vnegq_f64((float64x2_t)vdupq_laneq_s64(v63, 1))), (float64x2_t)v63, v65, 1)).f64[0]+ vaddvq_f64(vmulq_f64(v61, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v65, v66, 8uLL), vnegq_f64(v9)), v8, (float64x2_t)vextq_s8((int8x16_t)v63, v64, 8uLL))));
  int64x2_t v10 = vdupq_n_s64(0x7FF0000000000000uLL);
  float64x2_t v60 = (float64x2_t)v10;
  if (v8.f64[0] == 0.0)
  {
    float64x2_t v11 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v12 = (float64x2_t)v10;
    float64x2_t v13 = (float64x2_t)v10;
    float64x2_t v14 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v16 = (float64x2_t)v10;
    float64x2_t v15 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v54 = (float64x2_t)v10;
    uint64_t v55 = 0x7FF0000000000000;
  }
  else
  {
    float64x2_t v56 = v67;
    float64x2_t v58 = v68;
    v73  = 0u;
    v74  = 0u;
    float64x2_t v71 = 0u;
    float64x2_t v72 = 0u;
    float64x2_t v69 = 0u;
    float64x2_t v70 = 0u;
    v75  = v61;
    v76  = v62;
    v77  = (float64x2_t)v63;
    v78  = (float64x2_t)v64;
    v79  = v65;
    v80  = (float64x2_t)v66;
    __invert_d3();
    float64x2_t v12 = v69;
    float64x2_t v11 = v70;
    float64x2_t v13 = v71;
    float64x2_t v14 = v72;
    float64x2_t v16 = v73;
    float64x2_t v15 = v74;
    float64x2_t v54 = vnegq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v69, v56.f64[0]), v71, v56, 1), v73, v58.f64[0]));
    uint64_t v55 = *(_OWORD *)&vnegq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v56, v70), v72, v56, 1), v58, v74));
  }
  v17.f64[0]  = v15.f64[0];
  v17.f64[1]  = v16.f64[0];
  v18.f64[0]  = v14.f64[0];
  v18.f64[1]  = v13.f64[0];
  float64x2_t v19 = vmulq_f64(v12, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v16, (int8x16_t)v15, 8uLL), vnegq_f64(v18)), v17, (float64x2_t)vextq_s8((int8x16_t)v13, (int8x16_t)v14, 8uLL)));
  float64x2_t v21 = vmulq_f64(v11, vmlaq_laneq_f64(vmulq_f64(v16, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v13, 1))), v13, v16, 1));
  BOOL v20 = v21.f64[0] + vaddvq_f64(v19) < 0.0;
  v21.f64[0]  = -1.0;
  if (!v20) {
    v21.f64[0]  = 1.0;
  }
  v22.f64[0]  = sqrt(vmulq_f64(v11, v11).f64[0] + vaddvq_f64(vmulq_f64(v12, v12)));
  float64x2_t v23 = vmulq_f64(v15, v15);
  v23.f64[0]  = sqrt(v23.f64[0] + vaddvq_f64(vmulq_f64(v16, v16)));
  v22.f64[1]  = sqrt(vmulq_f64(v14, v14).f64[0] + vaddvq_f64(vmulq_f64(v13, v13)));
  float64x2_t v24 = vmulq_n_f64(v22, v21.f64[0]);
  float64x2_t v25 = vmulq_f64(v23, v21);
  float64x2_t v26 = vdivq_f64(v12, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v24.f64[0], 0));
  float64x2_t v27 = (float64x2_t)vdupq_laneq_s64((int64x2_t)v24, 1);
  double v57 = v25.f64[0];
  int8x16_t v59 = (int8x16_t)v24;
  float64x2_t v28 = vdivq_f64(v11, v24);
  float64x2_t v29 = vdivq_f64(v13, v27);
  float64x2_t v30 = vdivq_f64(v14, v27);
  float64x2_t v31 = vdivq_f64(v16, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v25.f64[0], 0));
  float64x2_t v32 = vdivq_f64(v15, v25);
  float64x2_t v33 = vmulq_f64(v30, v28);
  float64x2_t v34 = vmulq_f64(v28, v28);
  v34.f64[0]  = v34.f64[0] + vaddvq_f64(vmulq_f64(v26, v26));
  v33.f64[0]  = (v33.f64[0] + vaddvq_f64(vmulq_f64(v29, v26))) / v34.f64[0];
  float64x2_t v35 = vsubq_f64(v29, vmulq_n_f64(v26, v33.f64[0]));
  float64x2_t v36 = vsubq_f64(v30, vmulq_f64(v28, v33));
  float64x2_t v37 = vmulq_f64(v32, v28);
  v37.f64[0]  = (v37.f64[0] + vaddvq_f64(vmulq_f64(v31, v26))) / v34.f64[0];
  float64x2_t v38 = vmulq_n_f64(v26, v37.f64[0]);
  float64x2_t v39 = vsubq_f64(v32, vmulq_f64(v28, v37));
  float64x2_t v40 = vsubq_f64(v31, v38);
  float64x2_t v41 = vmulq_f64(v32, v36);
  v41.f64[0]  = v41.f64[0] + vaddvq_f64(vmulq_f64(v31, v35));
  float64x2_t v42 = vmulq_f64(v36, v36);
  v42.f64[0]  = v42.f64[0] + vaddvq_f64(vmulq_f64(v35, v35));
  v41.f64[0]  = v41.f64[0] / v42.f64[0];
  float64x2_t v43 = vmulq_f64(v36, v41);
  float64x2_t v44 = vsubq_f64(v40, vmulq_n_f64(v35, v41.f64[0]));
  float64x2_t v45 = vsubq_f64(v39, v43);
  v34.f64[0]  = 1.0 / sqrt(v34.f64[0]);
  v42.f64[0]  = 1.0 / sqrt(v42.f64[0]);
  v19.f64[0]  = 1.0 / sqrt(vmulq_f64(v45, v45).f64[0] + vaddvq_f64(vmulq_f64(v44, v44)));
  float64x2_t v69 = 0u;
  float64x2_t v70 = 0u;
  v75  = vmulq_n_f64(v26, v34.f64[0]);
  v76  = vmulq_f64(v28, v34);
  v77  = vmulq_n_f64(v35, v42.f64[0]);
  v78  = vmulq_f64(v36, v42);
  v79  = vmulq_n_f64(v44, v19.f64[0]);
  v80  = vmulq_f64(v45, v19);
  simd_quaternion((uint64_t)&v75, (uint64_t)&v69);
  int64x2_t v46 = (int64x2_t)vornq_s8(vmvnq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v69), (int8x16_t)vcgezq_f64(v69))), vorrq_s8((int8x16_t)vcltzq_f64(v70), (int8x16_t)vcgezq_f64(v70)));
  if ((vorrq_s8((int8x16_t)v46, (int8x16_t)vdupq_laneq_s64(v46, 1)).u64[0] & 0x8000000000000000) != 0)
  {
    float64x2_t v49 = v60;
    float64x2_t v48 = v60;
  }
  else
  {
    double v47 = vaddvq_f64(vaddq_f64(vmulq_f64(v69, v69), vmulq_f64(v70, v70)));
    if (v47 == 0.0)
    {
      float64x2_t v48 = (float64x2_t)xmmword_228C1F7A0;
      float64x2_t v49 = 0uLL;
    }
    else
    {
      double v50 = 1.0 / sqrt(v47);
      float64x2_t v48 = vmulq_n_f64(v70, v50);
      float64x2_t v49 = vmulq_n_f64(v69, v50);
    }
  }
  float64x2_t v51 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  int64x2_t v52 = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v49), (int8x16_t)vcgezq_f64(v49)), (int8x16_t)vceqq_f64(vabsq_f64(v49), v51)), vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v48), (int8x16_t)vcgezq_f64(v48)), (int8x16_t)vceqq_f64(vabsq_f64(v48), v51)));
  if ((vandq_s8((int8x16_t)v52, (int8x16_t)vdupq_laneq_s64(v52, 1)).u64[0] & 0x8000000000000000) != 0
    && (*(void *)&double v53 = vextq_s8(v59, v59, 8uLL).u64[0], vabdd_f64(*(double *)v59.i64, v53) < 0.0000000149011612)
    && vabdd_f64(v57, v53) < 0.0000000149011612)
  {
    *(float64x2_t *)a2  = v54;
    *(_OWORD *)(a2 + 16)  = (unint64_t)v55;
    *(float64x2_t *)(a2 + 32)  = v49;
    *(float64x2_t *)(a2 + 48)  = v48;
    *(void *)(a2 + 64)  = v59.i64[0];
  }
  else
  {
    *(_OWORD *)(a2 + 32)  = xmmword_228C202B0;
    *(_OWORD *)(a2 + 48)  = unk_228C202C0;
    *(_OWORD *)(a2 + 64)  = xmmword_228C202D0;
    *(_OWORD *)a2  = SPScaledPose3DInvalid_0;
    *(_OWORD *)(a2 + 16)  = unk_228C202A0;
  }
}

float64_t SPScaledPose3D.concatenating(_:)@<D0>(uint64_t a1@<X0>, uint64_t a2@<X8>, float64x2_t a3@<Q6>)
{
  float64x2_t v5 = *(float64x2_t *)a1;
  long long v6 = *(_OWORD *)(a1 + 16);
  long long v7 = *(_OWORD *)(a1 + 32);
  uint64_t v8 = *(void *)(a1 + 48);
  uint64_t v9 = *(void *)(a1 + 56);
  float64x2_t v11 = *(float64x2_t *)(v3 + 16);
  long long v12 = *(_OWORD *)(v3 + 32);
  uint64_t v13 = *(void *)(v3 + 56);
  a3.f64[0]  = *(float64_t *)(v3 + 64);
  float64x2_t v10 = *(float64x2_t *)v3;
  uint64_t v27 = *(void *)(v3 + 48);
  uint64_t v28 = v13;
  unint64_t v29 = *(void *)&a3.f64[0];
  *(void *)&long long v25 = v8;
  *((void *)&v25 + 1)  = v9;
  SPScaledPose3DConcatenation((uint64_t)&v26, (uint64_t)&v24, (uint64_t)&v19, v10, v11, a3, v19, v20, v21, v22, v23, v5, v6, v7, v25, *(uint64_t *)&v10.f64[0], *(uint64_t *)&v10.f64[1], *(uint64_t *)&v11.f64[0], *(uint64_t *)&v11.f64[1],
    v12,
    *((uint64_t *)&v12 + 1));
  float64_t result = v19.f64[0];
  long long v15 = v20;
  long long v16 = v21;
  long long v17 = v22;
  uint64_t v18 = v23;
  *(float64x2_t *)a2  = v19;
  *(_OWORD *)(a2 + 16)  = v15;
  *(_OWORD *)(a2 + 48)  = v17;
  *(_OWORD *)(a2 + 32)  = v16;
  *(void *)(a2 + 64)  = v18;
  return result;
}

__n128 SPScaledPose3DConcatenation@<Q0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>, float64x2_t a4@<Q3>, float64x2_t a5@<Q4>, float64x2_t a6@<Q6>, float64x2_t a7, long long a8, long long a9, long long a10, long long a11, float64x2_t a12, long long a13, long long a14, long long a15, uint64_t a16, uint64_t a17, uint64_t a18, uint64_t a19, uint64_t a20,uint64_t a21)
{
  a21  = *MEMORY[0x263EF8340];
  long long v21 = *(_OWORD *)(a2 + 16);
  a12  = *(float64x2_t *)a2;
  a13  = v21;
  long long v22 = *(_OWORD *)(a2 + 48);
  a14  = *(_OWORD *)(a2 + 32);
  a15  = v22;
  a16  = 0x3FF0000000000000;
  long long v23 = *(_OWORD *)(a1 + 48);
  a9  = *(_OWORD *)(a1 + 32);
  a10  = v23;
  a11  = *(_OWORD *)(a1 + 64);
  long long v24 = *(_OWORD *)(a1 + 16);
  a7  = *(float64x2_t *)a1;
  a8  = v24;
  SPScaledPose3DConcatenation(&a7, &a12, a3, a4, a5, a6);
  return result;
}

{
  long long v21;
  long long v22;
  long long v23;
  long long v24;
  __n128 result;

  a21  = *MEMORY[0x263EF8340];
  long long v21 = *(_OWORD *)(a1 + 16);
  a12  = *(float64x2_t *)a1;
  a13  = v21;
  long long v22 = *(_OWORD *)(a1 + 48);
  a14  = *(_OWORD *)(a1 + 32);
  a15  = v22;
  a16  = 0x3FF0000000000000;
  long long v23 = *(_OWORD *)(a2 + 48);
  a9  = *(_OWORD *)(a2 + 32);
  a10  = v23;
  a11  = *(_OWORD *)(a2 + 64);
  long long v24 = *(_OWORD *)(a2 + 16);
  a7  = *(float64x2_t *)a2;
  a8  = v24;
  SPScaledPose3DConcatenation(&a12, &a7, a3, a4, a5, a6);
  return result;
}

double static SPScaledPose3D.identity.getter@<D0>(uint64_t a1@<X8>)
{
  double result = 0.0;
  *(_OWORD *)a1  = 0u;
  *(_OWORD *)(a1 + 16)  = 0u;
  *(void *)(a1 + 48)  = 0;
  *(void *)(a1 + 56)  = 0x3FF0000000000000;
  *(_OWORD *)(a1 + 32)  = 0u;
  *(void *)(a1 + 64)  = 0x3FF0000000000000;
  return result;
}

double SPScaledPose3D.flipped(along:)@<D0>(unsigned int a1@<W0>, uint64_t a2@<X8>, float64x2_t a3@<Q3>, float64x2_t a4@<Q4>, float64x2_t a5@<Q6>)
{
  float64x2_t v7 = *(float64x2_t *)(v5 + 16);
  float64x2_t v8 = *(float64x2_t *)(v5 + 32);
  uint64_t v9 = *(void *)(v5 + 48);
  uint64_t v10 = *(void *)(v5 + 56);
  a3.f64[0]  = *(float64_t *)(v5 + 64);
  v21[0]  = *(float64x2_t *)v5;
  v21[1]  = v7;
  uint64_t v22 = v9;
  uint64_t v23 = v10;
  v21[2]  = v8;
  unint64_t v24 = *(void *)&a3.f64[0];
  SPScaledPose3DFlip(v21, a1, (uint64_t)v17, a3, a4, a5);
  double result = *(double *)v17;
  long long v12 = v17[1];
  long long v13 = v17[2];
  uint64_t v14 = v18;
  uint64_t v15 = v19;
  uint64_t v16 = v20;
  *(_OWORD *)a2  = v17[0];
  *(_OWORD *)(a2 + 16)  = v12;
  *(void *)(a2 + 48)  = v14;
  *(void *)(a2 + 56)  = v15;
  *(_OWORD *)(a2 + 32)  = v13;
  *(void *)(a2 + 64)  = v16;
  return result;
}

void SPScaledPose3DFlip(float64x2_t *a1@<X0>, unsigned int a2@<W1>, uint64_t a3@<X8>, float64x2_t a4@<Q3>, float64x2_t a5@<Q4>, float64x2_t a6@<Q6>)
{
  v145  = *MEMORY[0x263EF8340];
  if (a2 > 4 || ((1 << a2) & 0x16) == 0)
  {
    float64x2_t v20 = a1[3];
    *(float64x2_t *)(a3 + 32)  = a1[2];
    *(float64x2_t *)(a3 + 48)  = v20;
    *(float64x2_t *)(a3 + 64)  = a1[4];
    float64x2_t v22 = *a1;
    float64x2_t v21 = a1[1];
LABEL_34:
    *(float64x2_t *)a3  = v22;
    *(float64x2_t *)(a3 + 16)  = v21;
    return;
  }
  v119  = 0u;
  v120  = 0u;
  v117  = 0u;
  v118  = 0u;
  v115  = 0u;
  v116  = 0u;
  v113  = 0u;
  v114  = 0u;
  float64x2_t v8 = a1[3];
  v139  = a1[2];
  v140  = v8;
  v141  = a1[4];
  float64x2_t v9 = a1[1];
  v137  = *a1;
  v138  = v9;
  SPAffineTransform3DMakeWithScaledPose(&v137, (long long *)&v113, v137, a4, a5, a6);
  float64x2_t v11 = v113;
  float64x2_t v10 = v114;
  float64x2_t v13 = v115;
  float64x2_t v12 = v116;
  float64x2_t v15 = v117;
  float64x2_t v14 = v118;
  float64x2_t v17 = v119;
  float64x2_t v16 = v120;
  BOOL v18 = __OFSUB__(a2, 1);
  if (a2 == 1)
  {
    unsigned int v19 = 0;
  }
  else
  {
    BOOL v18 = __OFSUB__(a2, 4);
    if (a2 == 4)
    {
      unsigned int v19 = 2;
    }
    else
    {
      BOOL v18 = __OFSUB__(a2, 2);
      if (a2 != 2) {
        goto LABEL_24;
      }
      unsigned int v19 = 1;
    }
  }
  v127.f64[1]  = 0.0;
  v128  = 0u;
  v125  = 0u;
  v123.f64[0]  = 0.0;
  v124  = 0u;
  v122  = 0u;
  v121  = (float64x2_t)0x3FF0000000000000uLL;
  v123.f64[1]  = 1.0;
  v126  = (float64x2_t)0x3FF0000000000000uLL;
  v121.f64[4 * v19 + v19]  = -1.0;
  v127.f64[0]  = 0.0;
  float64x2_t v24 = v121;
  float64x2_t v23 = v122;
  float64x2_t v26 = v123;
  float64x2_t v25 = v124;
  float64x2_t v28 = v125;
  float64x2_t v27 = v126;
  float64x2_t v30 = v127;
  float64x2_t v29 = v128;
  float64x2_t v32 = *(float64x2_t *)MEMORY[0x263EF8988];
  float64x2_t v31 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v34 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v33 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v36 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v35 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v37 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v34, v123), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v121)), (int8x16_t)vceqq_f64(v36, v125));
  unint64_t v38 = vandq_s8((int8x16_t)vdupq_laneq_s64(v37, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v33, v124), (int8x16_t)vceqq_f64(v31, v122)), (int8x16_t)vceqq_f64(v35, v126)), (int8x16_t)v37)).u64[0];
  if ((v38 & 0x8000000000000000) != 0
    && (int64x2_t v39 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v34, v13), (int8x16_t)vceqq_f64(v32, v11)), (int8x16_t)vceqq_f64(v36, v15)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v39, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v33, v12), (int8x16_t)vceqq_f64(v31, v10)), (int8x16_t)vceqq_f64(v35, v14)), (int8x16_t)v39)).u64[0] & 0x8000000000000000) != 0))
  {
    float64x2_t v17 = vaddq_f64(v17, v127);
    float64x2_t v15 = v125;
    float64x2_t v14 = v126;
    float64x2_t v16 = vaddq_f64(v16, v128);
    float64x2_t v13 = v123;
    float64x2_t v12 = v124;
    float64x2_t v11 = v121;
    float64x2_t v10 = v122;
  }
  else
  {
    int64x2_t v40 = vceqzq_f64(v127);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v40, 1), vandq_s8((int8x16_t)vceqzq_f64(v128), (int8x16_t)v40)).u64[0] & 0x8000000000000000) != 0
      && (int64x2_t v41 = vceqzq_f64(v17),
          (vandq_s8((int8x16_t)vdupq_laneq_s64(v41, 1), vandq_s8((int8x16_t)vceqzq_f64(v16), (int8x16_t)v41)).u64[0] & 0x8000000000000000) != 0))
    {
      v100  = 0;
      v129  = v11;
      v130  = v10;
      v131  = v13;
      v132  = v12;
      v133  = v15;
      v134  = v14;
      v137  = 0u;
      v138  = 0u;
      v139  = 0u;
      v140  = 0u;
      v141  = 0u;
      v142  = 0u;
      v101.f64[0]  = v121.f64[0];
      *(void *)&v101.f64[1]  = vextq_s8((int8x16_t)v121, (int8x16_t)v121, 8uLL).u64[0];
      v102.f64[0]  = v123.f64[0];
      *(void *)&v102.f64[1]  = vextq_s8((int8x16_t)v123, (int8x16_t)v123, 8uLL).u64[0];
      v103.f64[0]  = v125.f64[0];
      *(void *)&v103.f64[1]  = vextq_s8((int8x16_t)v125, (int8x16_t)v125, 8uLL).u64[0];
      do
      {
        v105  = *(float64x2_t *)((char *)&v129 + v100);
        v104  = *(float64x2_t *)((char *)&v129 + v100 + 16);
        v106  = (float64x2_t *)((char *)&v137 + v100);
        *v106  = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v101, v105.f64[0]), v102, v105, 1), v103, v104.f64[0]);
        v106[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v23, v105), v25, v105, 1), v104, v27);
        v100 += 32;
        BOOL v18 = __OFSUB__(v100, 96);
      }
      while (v100 != 96);
      float64x2_t v11 = v137;
      float64x2_t v10 = v138;
      float64x2_t v13 = v139;
      float64x2_t v12 = v140;
      float64x2_t v17 = v30;
      float64x2_t v16 = v29;
      float64x2_t v15 = v141;
      float64x2_t v14 = v142;
    }
    else
    {
      uint64_t v42 = 0;
      float64x2_t v43 = (float64x2_t)*(unint64_t *)&v122.f64[0];
      float64x2_t v44 = (float64x2_t)*(unint64_t *)&v124.f64[0];
      float64x2_t v45 = (float64x2_t)*(unint64_t *)&v126.f64[0];
      v46.f64[0]  = v128.f64[0];
      v46.f64[1]  = 1.0;
      v16.f64[1]  = 1.0;
      v129  = v11;
      v130  = (float64x2_t)*(unint64_t *)&v10.f64[0];
      v131  = v13;
      v132  = (float64x2_t)*(unint64_t *)&v12.f64[0];
      v133  = v15;
      v134  = (float64x2_t)*(unint64_t *)&v14.f64[0];
      v135  = v17;
      v136  = v16;
      v137  = 0u;
      v138  = 0u;
      v139  = 0u;
      v140  = 0u;
      v141  = 0u;
      v142  = 0u;
      v143  = 0u;
      v144  = 0u;
      do
      {
        float64x2_t v48 = *(float64x2_t *)((char *)&v129 + v42);
        float64x2_t v47 = *(float64x2_t *)((char *)&v129 + v42 + 16);
        float64x2_t v49 = (float64x2_t *)((char *)&v137 + v42);
        *float64x2_t v49 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v24, v48.f64[0]), v26, v48, 1), v28, v47.f64[0]), v30, v47, 1);
        v49[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v43, v48.f64[0]), v44, v48, 1), v45, v47.f64[0]), v46, v47, 1);
        v42 += 32;
        BOOL v18 = __OFSUB__(v42, 128);
      }
      while (v42 != 128);
      float64x2_t v11 = v137;
      float64x2_t v10 = v138;
      float64x2_t v13 = v139;
      float64x2_t v12 = v140;
      float64x2_t v15 = v141;
      float64x2_t v14 = v142;
      float64x2_t v17 = v143;
      float64x2_t v16 = v144;
    }
  }
  int8x16_t v50 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v33, v12), (int8x16_t)vceqq_f64(v35, v14)), (int8x16_t)vceqq_f64(v31, v10));
  int64x2_t v51 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v34, v13), (int8x16_t)vceqq_f64(v36, v15)), (int8x16_t)vceqq_f64(v32, v11));
  if (((vandq_s8((int8x16_t)vdupq_laneq_s64(v51, 1), vandq_s8(v50, (int8x16_t)v51)).u64[0] & v38 & 0x8000000000000000) != 0) != v18)
  {
    float64x2_t v17 = vaddq_f64(v30, v17);
    *(void *)&v16.f64[0]  = *(_OWORD *)&vaddq_f64(v29, v16);
  }
  else
  {
    int64x2_t v52 = vceqzq_f64(v17);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v52, 1), vandq_s8((int8x16_t)vceqzq_f64(v16), (int8x16_t)v52)).u64[0] & 0x8000000000000000) != 0
      && (int64x2_t v53 = vceqzq_f64(v30),
          (vandq_s8((int8x16_t)vdupq_laneq_s64(v53, 1), vandq_s8((int8x16_t)vceqzq_f64(v29), (int8x16_t)v53)).u64[0] & 0x8000000000000000) != 0))
    {
      v107  = 0;
      v129  = v24;
      v130  = v23;
      v131  = v26;
      v132  = v25;
      v133  = v28;
      v134  = v27;
      v137  = 0u;
      v138  = 0u;
      v139  = 0u;
      v140  = 0u;
      v141  = 0u;
      v142  = 0u;
      *(void *)&v11.f64[1]  = vextq_s8((int8x16_t)v11, (int8x16_t)v11, 8uLL).u64[0];
      *(void *)&v13.f64[1]  = vextq_s8((int8x16_t)v13, (int8x16_t)v13, 8uLL).u64[0];
      *(void *)&v15.f64[1]  = vextq_s8((int8x16_t)v15, (int8x16_t)v15, 8uLL).u64[0];
      do
      {
        v109  = *(float64x2_t *)((char *)&v129 + v107);
        v108  = *(float64x2_t *)((char *)&v129 + v107 + 16);
        v110  = (float64x2_t *)((char *)&v137 + v107);
        *v110  = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v11, v109.f64[0]), v13, v109, 1), v15, v108.f64[0]);
        v110[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v10, v109), v12, v109, 1), v108, v14);
        v107 += 32;
      }
      while (v107 != 96);
      float64x2_t v11 = v137;
      float64x2_t v10 = v138;
      float64x2_t v13 = v139;
      float64x2_t v12 = v140;
      float64x2_t v15 = v141;
      float64x2_t v14 = v142;
    }
    else
    {
      uint64_t v54 = 0;
      v10.f64[1]  = 0.0;
      v12.f64[1]  = 0.0;
      v14.f64[1]  = 0.0;
      v16.f64[1]  = 1.0;
      v29.f64[1]  = 1.0;
      v129  = v24;
      v130  = (float64x2_t)*(unint64_t *)&v23.f64[0];
      v131  = v26;
      v132  = (float64x2_t)*(unint64_t *)&v25.f64[0];
      v133  = v28;
      v134  = (float64x2_t)*(unint64_t *)&v27.f64[0];
      v135  = v30;
      v136  = v29;
      v137  = 0u;
      v138  = 0u;
      v139  = 0u;
      v140  = 0u;
      v141  = 0u;
      v142  = 0u;
      v143  = 0u;
      v144  = 0u;
      do
      {
        float64x2_t v56 = *(float64x2_t *)((char *)&v129 + v54);
        float64x2_t v55 = *(float64x2_t *)((char *)&v129 + v54 + 16);
        double v57 = (float64x2_t *)((char *)&v137 + v54);
        *double v57 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v11, v56.f64[0]), v13, v56, 1), v15, v55.f64[0]), v17, v55, 1);
        v57[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v10, v56.f64[0]), v12, v56, 1), v14, v55.f64[0]), v16, v55, 1);
        v54 += 32;
      }
      while (v54 != 128);
      float64x2_t v11 = v137;
      float64x2_t v10 = v138;
      float64x2_t v13 = v139;
      float64x2_t v12 = v140;
      float64x2_t v15 = v141;
      float64x2_t v14 = v142;
      float64x2_t v17 = v143;
      v16.f64[0]  = v144.f64[0];
    }
  }
LABEL_24:
  v58.f64[0]  = v14.f64[0];
  v58.f64[1]  = v15.f64[0];
  v59.f64[0]  = v12.f64[0];
  v59.f64[1]  = v13.f64[0];
  float64x2_t v60 = vmulq_f64(v11, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v15, (int8x16_t)v14, 8uLL), vnegq_f64(v59)), v58, (float64x2_t)vextq_s8((int8x16_t)v13, (int8x16_t)v12, 8uLL)));
  float64x2_t v62 = vmulq_f64(v10, vmlaq_laneq_f64(vmulq_f64(v15, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v13, 1))), v13, v15, 1));
  BOOL v61 = v62.f64[0] + vaddvq_f64(v60) < 0.0;
  v62.f64[0]  = -1.0;
  if (!v61) {
    v62.f64[0]  = 1.0;
  }
  v63.f64[0]  = sqrt(vmulq_f64(v10, v10).f64[0] + vaddvq_f64(vmulq_f64(v11, v11)));
  v64  = vmulq_f64(v14, v14);
  v64.f64[0]  = sqrt(v64.f64[0] + vaddvq_f64(vmulq_f64(v15, v15)));
  v63.f64[1]  = sqrt(vmulq_f64(v12, v12).f64[0] + vaddvq_f64(vmulq_f64(v13, v13)));
  v65  = vmulq_n_f64(v63, v62.f64[0]);
  float64x2_t v66 = vdivq_f64(v11, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v65.f64[0], 0));
  float64x2_t v67 = vdivq_f64(v10, v65);
  v111  = vmulq_f64(v64, v62);
  v112  = (int8x16_t)v65;
  float64x2_t v68 = (float64x2_t)vdupq_laneq_s64((int64x2_t)v65, 1);
  float64x2_t v69 = vdivq_f64(v13, v68);
  float64x2_t v70 = vdivq_f64(v12, v68);
  float64x2_t v71 = vdivq_f64(v15, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v111.f64[0], 0));
  float64x2_t v72 = vdivq_f64(v14, v111);
  v73  = vmulq_f64(v70, v67);
  v74  = vmulq_f64(v67, v67);
  v74.f64[0]  = v74.f64[0] + vaddvq_f64(vmulq_f64(v66, v66));
  v73.f64[0]  = (v73.f64[0] + vaddvq_f64(vmulq_f64(v69, v66))) / v74.f64[0];
  v75  = vsubq_f64(v69, vmulq_n_f64(v66, v73.f64[0]));
  v76  = vsubq_f64(v70, vmulq_f64(v67, v73));
  v77  = vmulq_f64(v72, v67);
  v77.f64[0]  = (v77.f64[0] + vaddvq_f64(vmulq_f64(v71, v66))) / v74.f64[0];
  v78  = vmulq_n_f64(v66, v77.f64[0]);
  v79  = vsubq_f64(v72, vmulq_f64(v67, v77));
  v80  = vsubq_f64(v71, v78);
  v81  = vmulq_f64(v72, v76);
  v81.f64[0]  = v81.f64[0] + vaddvq_f64(vmulq_f64(v71, v75));
  v82  = vmulq_f64(v76, v76);
  v82.f64[0]  = v82.f64[0] + vaddvq_f64(vmulq_f64(v75, v75));
  v81.f64[0]  = v81.f64[0] / v82.f64[0];
  v83  = vmulq_f64(v76, v81);
  v84  = vsubq_f64(v80, vmulq_n_f64(v75, v81.f64[0]));
  v85  = vsubq_f64(v79, v83);
  v74.f64[0]  = 1.0 / sqrt(v74.f64[0]);
  v82.f64[0]  = 1.0 / sqrt(v82.f64[0]);
  v86  = vmulq_n_f64(v75, v82.f64[0]);
  v87  = vmulq_f64(v76, v82);
  v88  = vmulq_f64(v85, v85);
  v88.f64[0]  = 1.0 / sqrt(v88.f64[0] + vaddvq_f64(vmulq_f64(v84, v84)));
  v129  = 0u;
  v130  = 0u;
  v137  = vmulq_n_f64(v66, v74.f64[0]);
  v138  = vmulq_f64(v67, v74);
  v139  = v86;
  v140  = v87;
  v141  = vmulq_n_f64(v84, v88.f64[0]);
  v142  = vmulq_f64(v85, v88);
  simd_quaternion((uint64_t)&v137, (uint64_t)&v129);
  v89  = (int64x2_t)vornq_s8(vmvnq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v129), (int8x16_t)vcgezq_f64(v129))), vorrq_s8((int8x16_t)vcltzq_f64(v130), (int8x16_t)vcgezq_f64(v130)));
  v90  = vorrq_s8((int8x16_t)v89, (int8x16_t)vdupq_laneq_s64(v89, 1)).u64[0];
  v91  = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if ((v90 & 0x8000000000000000) != 0)
  {
    v96  = v91;
    v93  = v91;
LABEL_31:
    v95  = v17;
    v94  = v16.f64[0];
    goto LABEL_32;
  }
  v92  = vaddvq_f64(vaddq_f64(vmulq_f64(v129, v129), vmulq_f64(v130, v130)));
  if (v92 != 0.0)
  {
    v97  = 1.0 / sqrt(v92);
    v93  = vmulq_n_f64(v130, v97);
    v96  = vmulq_n_f64(v129, v97);
    goto LABEL_31;
  }
  v93  = (float64x2_t)xmmword_228C1F7A0;
  v95  = v17;
  v94  = v16.f64[0];
  v96  = 0uLL;
LABEL_32:
  v98  = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v96), (int8x16_t)vcgezq_f64(v96)), (int8x16_t)vceqq_f64(vabsq_f64(v96), v91)), vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v93), (int8x16_t)vcgezq_f64(v93)), (int8x16_t)vceqq_f64(vabsq_f64(v93), v91)));
  if ((vandq_s8((int8x16_t)v98, (int8x16_t)vdupq_laneq_s64(v98, 1)).u64[0] & 0x8000000000000000) == 0
    || (*(void *)&v99  = vextq_s8(v112, v112, 8uLL).u64[0], vabdd_f64(*(double *)v112.i64, v99) >= 0.0000000149011612)
    || vabdd_f64(v111.f64[0], v99) >= 0.0000000149011612)
  {
    *(_OWORD *)(a3 + 32)  = xmmword_228C202B0;
    *(_OWORD *)(a3 + 48)  = unk_228C202C0;
    *(_OWORD *)(a3 + 64)  = xmmword_228C202D0;
    float64x2_t v22 = (float64x2_t)SPScaledPose3DInvalid_0;
    float64x2_t v21 = (float64x2_t)unk_228C202A0;
    goto LABEL_34;
  }
  *(float64x2_t *)a3  = v95;
  *(float64_t *)(a3 + 16)  = v94;
  *(void *)(a3 + 24)  = 0;
  *(float64x2_t *)(a3 + 32)  = v96;
  *(float64x2_t *)(a3 + 48)  = v93;
  *(void *)(a3 + 64)  = v112.i64[0];
}

Swift::Void __swiftcall SPScaledPose3D.flip(along:)(SPAxis *along)
{
  float64x2_t v5 = *(float64x2_t *)(v1 + 16);
  float64x2_t v6 = *(float64x2_t *)(v1 + 32);
  uint64_t v7 = *(void *)(v1 + 48);
  uint64_t v8 = *(void *)(v1 + 56);
  v2.f64[0]  = *(float64_t *)(v1 + 64);
  v18[0]  = *(float64x2_t *)v1;
  v18[1]  = v5;
  uint64_t v19 = v7;
  uint64_t v20 = v8;
  v18[2]  = v6;
  float64_t v21 = v2.f64[0];
  SPScaledPose3DFlip(v18, along, (uint64_t)v14, v2, v3, v4);
  long long v9 = v14[1];
  long long v10 = v14[2];
  uint64_t v11 = v15;
  uint64_t v12 = v16;
  uint64_t v13 = v17;
  *(_OWORD *)uint64_t v1 = v14[0];
  *(_OWORD *)(v1 + 16)  = v9;
  *(void *)(v1 + 48)  = v11;
  *(void *)(v1 + 56)  = v12;
  *(_OWORD *)(v1 + 32)  = v10;
  *(void *)(v1 + 64)  = v13;
}

double SPScaledPose3D.translated(by:)@<D0>(uint64_t a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>)
{
  long long v15 = v4[1];
  long long v16 = *v4;
  long long v17 = v4[2];
  uint64_t v6 = *((void *)v4 + 6);
  uint64_t v7 = *((void *)v4 + 7);
  uint64_t v8 = *((void *)v4 + 8);
  v23.width  = a2;
  v23.height  = a3;
  v23.depth  = a4;
  SPVector3DMakeWithSize(&v23, (uint64_t)v22);
  *(_OWORD *)&v23.width  = v16;
  *(_OWORD *)&v23.vector.f64[2]  = v15;
  uint64_t v25 = v6;
  uint64_t v26 = v7;
  long long v24 = v17;
  uint64_t v27 = v8;
  SPScaledPose3DTranslate((uint64_t)&v23, v22, (uint64_t)v18);
  double result = *(double *)v18;
  long long v10 = v18[1];
  long long v11 = v18[2];
  uint64_t v12 = v19;
  uint64_t v13 = v20;
  uint64_t v14 = v21;
  *(_OWORD *)a1  = v18[0];
  *(_OWORD *)(a1 + 16)  = v10;
  *(void *)(a1 + 48)  = v12;
  *(void *)(a1 + 56)  = v13;
  *(_OWORD *)(a1 + 32)  = v11;
  *(void *)(a1 + 64)  = v14;
  return result;
}

double SPScaledPose3D.translated(by:)@<D0>(uint64_t a1@<X8>, float64_t a2@<D0>, float64_t a3@<D1>, double a4@<D2>)
{
  long long v6 = *(_OWORD *)(v4 + 16);
  long long v7 = *(_OWORD *)(v4 + 32);
  uint64_t v8 = *(void *)(v4 + 48);
  uint64_t v9 = *(void *)(v4 + 56);
  uint64_t v10 = *(void *)(v4 + 64);
  v23[0]  = *(_OWORD *)v4;
  v23[1]  = v6;
  uint64_t v24 = v8;
  uint64_t v25 = v9;
  v23[2]  = v7;
  uint64_t v26 = v10;
  v21.f64[0]  = a2;
  v21.f64[1]  = a3;
  double v22 = a4;
  SPScaledPose3DTranslate((uint64_t)v23, &v21, (uint64_t)v17);
  double result = *(double *)v17;
  long long v12 = v17[1];
  long long v13 = v17[2];
  uint64_t v14 = v18;
  uint64_t v15 = v19;
  uint64_t v16 = v20;
  *(_OWORD *)a1  = v17[0];
  *(_OWORD *)(a1 + 16)  = v12;
  *(void *)(a1 + 48)  = v14;
  *(void *)(a1 + 56)  = v15;
  *(_OWORD *)(a1 + 32)  = v13;
  *(void *)(a1 + 64)  = v16;
  return result;
}

double SPScaledPose3DTranslate@<D0>(uint64_t a1@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>)
{
  double result = 0.0;
  uint64_t v4 = *(_OWORD *)&vaddq_f64(*(float64x2_t *)(a1 + 16), a2[1]);
  uint64_t v5 = *(void *)(a1 + 64);
  long long v7 = *(_OWORD *)(a1 + 32);
  long long v6 = *(_OWORD *)(a1 + 48);
  *(float64x2_t *)a3  = vaddq_f64(*(float64x2_t *)a1, *a2);
  *(_OWORD *)(a3 + 16)  = 0u;
  *(void *)(a3 + 16)  = v4;
  *(_OWORD *)(a3 + 32)  = v7;
  *(_OWORD *)(a3 + 48)  = v6;
  *(_OWORD *)(a3 + 64)  = 0u;
  *(void *)(a3 + 64)  = v5;
  return result;
}

double protocol witness for Translatable3D.translated(by:) in conformance SPScaledPose3D@<D0>(uint64_t a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>)
{
  long long v15 = v4[1];
  long long v16 = *v4;
  long long v17 = v4[2];
  uint64_t v6 = *((void *)v4 + 6);
  uint64_t v7 = *((void *)v4 + 7);
  uint64_t v8 = *((void *)v4 + 8);
  v23.width  = a2;
  v23.height  = a3;
  v23.depth  = a4;
  SPVector3DMakeWithSize(&v23, (uint64_t)v22);
  *(_OWORD *)&v23.width  = v16;
  *(_OWORD *)&v23.vector.f64[2]  = v15;
  uint64_t v25 = v6;
  uint64_t v26 = v7;
  long long v24 = v17;
  uint64_t v27 = v8;
  SPScaledPose3DTranslate((uint64_t)&v23, v22, (uint64_t)v18);
  double result = *(double *)v18;
  long long v10 = v18[1];
  long long v11 = v18[2];
  uint64_t v12 = v19;
  uint64_t v13 = v20;
  uint64_t v14 = v21;
  *(_OWORD *)a1  = v18[0];
  *(_OWORD *)(a1 + 16)  = v10;
  *(void *)(a1 + 48)  = v12;
  *(void *)(a1 + 56)  = v13;
  *(_OWORD *)(a1 + 32)  = v11;
  *(void *)(a1 + 64)  = v14;
  return result;
}

double protocol witness for Translatable3D.translated(by:) in conformance SPScaledPose3D@<D0>(uint64_t a1@<X8>, float64_t a2@<D0>, float64_t a3@<D1>, double a4@<D2>)
{
  long long v6 = *(_OWORD *)(v4 + 16);
  long long v7 = *(_OWORD *)(v4 + 32);
  uint64_t v8 = *(void *)(v4 + 48);
  uint64_t v9 = *(void *)(v4 + 56);
  uint64_t v10 = *(void *)(v4 + 64);
  v23[0]  = *(_OWORD *)v4;
  v23[1]  = v6;
  uint64_t v24 = v8;
  uint64_t v25 = v9;
  v23[2]  = v7;
  uint64_t v26 = v10;
  v21.f64[0]  = a2;
  v21.f64[1]  = a3;
  double v22 = a4;
  SPScaledPose3DTranslate((uint64_t)v23, &v21, (uint64_t)v17);
  double result = *(double *)v17;
  long long v12 = v17[1];
  long long v13 = v17[2];
  uint64_t v14 = v18;
  uint64_t v15 = v19;
  uint64_t v16 = v20;
  *(_OWORD *)a1  = v17[0];
  *(_OWORD *)(a1 + 16)  = v12;
  *(void *)(a1 + 48)  = v14;
  *(void *)(a1 + 56)  = v15;
  *(_OWORD *)(a1 + 32)  = v13;
  *(void *)(a1 + 64)  = v16;
  return result;
}

double SPScaledPose3D.rotated(by:)@<D0>(uint64_t a1@<X8>, float64x2_t a2@<Q0>, float64x2_t a3@<Q1>)
{
  long long v5 = *(_OWORD *)(v3 + 16);
  long long v6 = *(_OWORD *)(v3 + 32);
  uint64_t v7 = *(void *)(v3 + 48);
  uint64_t v8 = *(void *)(v3 + 56);
  uint64_t v9 = *(void *)(v3 + 64);
  v21[0]  = *(_OWORD *)v3;
  v21[1]  = v5;
  uint64_t v22 = v7;
  uint64_t v23 = v8;
  v21[2]  = v6;
  uint64_t v24 = v9;
  v20[0]  = a2;
  v20[1]  = a3;
  SPScaledPose3DRotate((uint64_t)v21, v20, (uint64_t)v16);
  double result = *(double *)v16;
  long long v11 = v16[1];
  long long v12 = v16[2];
  uint64_t v13 = v17;
  uint64_t v14 = v18;
  uint64_t v15 = v19;
  *(_OWORD *)a1  = v16[0];
  *(_OWORD *)(a1 + 16)  = v11;
  *(void *)(a1 + 48)  = v13;
  *(void *)(a1 + 56)  = v14;
  *(_OWORD *)(a1 + 32)  = v12;
  *(void *)(a1 + 64)  = v15;
  return result;
}

{
  uint64_t v3;
  long long v5;
  long long v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  double result;
  long long v11;
  long long v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  _OWORD v16[3];
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  float64x2_t v20[2];
  _OWORD v21[3];
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;

  long long v5 = *(_OWORD *)(v3 + 16);
  long long v6 = *(_OWORD *)(v3 + 32);
  uint64_t v7 = *(void *)(v3 + 48);
  uint64_t v8 = *(void *)(v3 + 56);
  uint64_t v9 = *(void *)(v3 + 64);
  v21[0]  = *(_OWORD *)v3;
  v21[1]  = v5;
  uint64_t v22 = v7;
  uint64_t v23 = v8;
  v21[2]  = v6;
  uint64_t v24 = v9;
  v20[0]  = a2;
  v20[1]  = a3;
  SPScaledPose3DRotateByQuaternion((uint64_t)v21, v20, (uint64_t)v16);
  double result = *(double *)v16;
  long long v11 = v16[1];
  long long v12 = v16[2];
  uint64_t v13 = v17;
  uint64_t v14 = v18;
  uint64_t v15 = v19;
  *(_OWORD *)a1  = v16[0];
  *(_OWORD *)(a1 + 16)  = v11;
  *(void *)(a1 + 48)  = v13;
  *(void *)(a1 + 56)  = v14;
  *(_OWORD *)(a1 + 32)  = v12;
  *(void *)(a1 + 64)  = v15;
  return result;
}

float64x2_t SPScaledPose3DRotate@<Q0>(uint64_t a1@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v4 = *(float64x2_t *)(a1 + 32);
  float64x2_t v3 = *(float64x2_t *)(a1 + 48);
  uint64_t v5 = *(void *)(a1 + 64);
  float64x2_t v6 = a2[1];
  float64x2_t v7 = vnegq_f64(v4);
  float64x2_t v8 = (float64x2_t)vextq_s8((int8x16_t)v3, (int8x16_t)vnegq_f64(v3), 8uLL);
  float64x2_t v9 = vmlaq_n_f64(vmulq_laneq_f64(v7, *a2, 1), (float64x2_t)vextq_s8((int8x16_t)v4, (int8x16_t)v7, 8uLL), a2->f64[0]);
  float64x2_t v10 = vmlaq_n_f64(vmulq_laneq_f64(v3, *a2, 1), v8, a2->f64[0]);
  long long v11 = *(_OWORD *)(a1 + 16);
  *(_OWORD *)a3  = *(_OWORD *)a1;
  *(_OWORD *)(a3 + 16)  = v11;
  float64x2_t result = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v3, v6, 1), v8, v6.f64[0]), v9);
  *(void *)(a3 + 72)  = 0;
  *(float64x2_t *)(a3 + 32)  = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v4, v6, 1), (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v4, 8uLL), v6.f64[0]), v10);
  *(float64x2_t *)(a3 + 48)  = result;
  *(void *)(a3 + 64)  = v5;
  return result;
}

float64x2_t SPScaledPose3DRotateByQuaternion@<Q0>(uint64_t a1@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v3 = a2[1];
  float64x2_t v5 = *(float64x2_t *)(a1 + 32);
  float64x2_t v4 = *(float64x2_t *)(a1 + 48);
  float64x2_t v6 = vnegq_f64(v5);
  float64x2_t v7 = (float64x2_t)vextq_s8((int8x16_t)v4, (int8x16_t)vnegq_f64(v4), 8uLL);
  float64x2_t v8 = vmlaq_n_f64(vmulq_laneq_f64(v6, *a2, 1), (float64x2_t)vextq_s8((int8x16_t)v5, (int8x16_t)v6, 8uLL), a2->f64[0]);
  float64x2_t v9 = vmlaq_n_f64(vmulq_laneq_f64(v4, *a2, 1), v7, a2->f64[0]);
  float64x2_t v10 = vmlaq_n_f64(vmulq_laneq_f64(v4, v3, 1), v7, v3.f64[0]);
  float64x2_t result = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v5, v3, 1), (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v5, 8uLL), v3.f64[0]), v9);
  uint64_t v12 = *(void *)(a1 + 64);
  long long v13 = *(_OWORD *)(a1 + 16);
  *(_OWORD *)a3  = *(_OWORD *)a1;
  *(_OWORD *)(a3 + 16)  = v13;
  *(void *)(a3 + 72)  = 0;
  *(float64x2_t *)(a3 + 32)  = result;
  *(float64x2_t *)(a3 + 48)  = vaddq_f64(v10, v8);
  *(void *)(a3 + 64)  = v12;
  return result;
}

double protocol witness for Rotatable3D.rotated(by:) in conformance SPScaledPose3D@<D0>(uint64_t a1@<X8>, float64x2_t a2@<Q0>, float64x2_t a3@<Q1>)
{
  long long v5 = *(_OWORD *)(v3 + 16);
  long long v6 = *(_OWORD *)(v3 + 32);
  uint64_t v7 = *(void *)(v3 + 48);
  uint64_t v8 = *(void *)(v3 + 56);
  uint64_t v9 = *(void *)(v3 + 64);
  v21[0]  = *(_OWORD *)v3;
  v21[1]  = v5;
  uint64_t v22 = v7;
  uint64_t v23 = v8;
  v21[2]  = v6;
  uint64_t v24 = v9;
  v20[0]  = a2;
  v20[1]  = a3;
  SPScaledPose3DRotate((uint64_t)v21, v20, (uint64_t)v16);
  double result = *(double *)v16;
  long long v11 = v16[1];
  long long v12 = v16[2];
  uint64_t v13 = v17;
  uint64_t v14 = v18;
  uint64_t v15 = v19;
  *(_OWORD *)a1  = v16[0];
  *(_OWORD *)(a1 + 16)  = v11;
  *(void *)(a1 + 48)  = v13;
  *(void *)(a1 + 56)  = v14;
  *(_OWORD *)(a1 + 32)  = v12;
  *(void *)(a1 + 64)  = v15;
  return result;
}

{
  uint64_t v3;
  long long v5;
  long long v6;
  uint64_t v7;
  uint64_t v8;
  uint64_t v9;
  double result;
  long long v11;
  long long v12;
  uint64_t v13;
  uint64_t v14;
  uint64_t v15;
  _OWORD v16[3];
  uint64_t v17;
  uint64_t v18;
  uint64_t v19;
  float64x2_t v20[2];
  _OWORD v21[3];
  uint64_t v22;
  uint64_t v23;
  uint64_t v24;

  long long v5 = *(_OWORD *)(v3 + 16);
  long long v6 = *(_OWORD *)(v3 + 32);
  uint64_t v7 = *(void *)(v3 + 48);
  uint64_t v8 = *(void *)(v3 + 56);
  uint64_t v9 = *(void *)(v3 + 64);
  v21[0]  = *(_OWORD *)v3;
  v21[1]  = v5;
  uint64_t v22 = v7;
  uint64_t v23 = v8;
  v21[2]  = v6;
  uint64_t v24 = v9;
  v20[0]  = a2;
  v20[1]  = a3;
  SPScaledPose3DRotateByQuaternion((uint64_t)v21, v20, (uint64_t)v16);
  double result = *(double *)v16;
  long long v11 = v16[1];
  long long v12 = v16[2];
  uint64_t v13 = v17;
  uint64_t v14 = v18;
  uint64_t v15 = v19;
  *(_OWORD *)a1  = v16[0];
  *(_OWORD *)(a1 + 16)  = v11;
  *(void *)(a1 + 48)  = v13;
  *(void *)(a1 + 56)  = v14;
  *(_OWORD *)(a1 + 32)  = v12;
  *(void *)(a1 + 64)  = v15;
  return result;
}

BOOL static SPScaledPose3D.== infix(_:_:)(uint64_t a1, uint64_t a2)
{
  float64x2_t v2 = *(float64x2_t *)a2;
  float64x2_t v3 = *(float64x2_t *)(a2 + 16);
  float64x2_t v4 = *(float64x2_t *)(a2 + 32);
  uint64_t v5 = *(void *)(a2 + 48);
  uint64_t v6 = *(void *)(a2 + 56);
  uint64_t v7 = *(void *)(a2 + 64);
  float64x2_t v8 = *(float64x2_t *)(a1 + 16);
  float64x2_t v9 = *(float64x2_t *)(a1 + 32);
  uint64_t v10 = *(void *)(a1 + 48);
  uint64_t v11 = *(void *)(a1 + 56);
  uint64_t v12 = *(void *)(a1 + 64);
  v18[0]  = *(float64x2_t *)a1;
  v18[1]  = v8;
  uint64_t v19 = v10;
  uint64_t v20 = v11;
  v18[2]  = v9;
  uint64_t v21 = v12;
  v14[0]  = v2;
  v14[1]  = v3;
  uint64_t v15 = v5;
  uint64_t v16 = v6;
  v14[2]  = v4;
  uint64_t v17 = v7;
  return SPScaledPose3DEqualToPose(v18, v14);
}

BOOL SPScaledPose3DEqualToPose(float64x2_t *a1, float64x2_t *a2)
{
  int64x2_t v2 = vceqq_f64(*a1, *a2);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v2, 1), vandq_s8((int8x16_t)vceqq_f64(a1[1], a2[1]), (int8x16_t)v2)).u64[0] & 0x8000000000000000) != 0
    && ((float64x2_t v4 = a1[2],
         float64x2_t v5 = a1[3],
         float64x2_t v6 = a2[2],
         float64x2_t v7 = a2[3],
         int64x2_t v8 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(v4, v6), (int8x16_t)vceqq_f64(v5, v7)),
         (vandq_s8((int8x16_t)v8, (int8x16_t)vdupq_laneq_s64(v8, 1)).u64[0] & 0x8000000000000000) != 0)
     || (int64x2_t v9 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(v4, vnegq_f64(v6)), (int8x16_t)vceqq_f64(v5, vnegq_f64(v7))),
         (vandq_s8((int8x16_t)v9, (int8x16_t)vdupq_laneq_s64(v9, 1)).u64[0] & 0x8000000000000000) != 0)))
  {
    return a1[4].f64[0] == a2[4].f64[0];
  }
  else
  {
    return 0;
  }
}

BOOL SPScaledPose3D.isApproximatelyEqual(to:tolerance:)(uint64_t a1, double a2)
{
  float64x2_t v3 = *(float64x2_t *)a1;
  float64x2_t v4 = *(float64x2_t *)(a1 + 16);
  float64x2_t v5 = *(float64x2_t *)(a1 + 32);
  uint64_t v6 = *(void *)(a1 + 48);
  uint64_t v7 = *(void *)(a1 + 56);
  uint64_t v8 = *(void *)(a1 + 64);
  float64x2_t v9 = *(float64x2_t *)(v2 + 16);
  float64x2_t v10 = *(float64x2_t *)(v2 + 32);
  uint64_t v11 = *(void *)(v2 + 48);
  uint64_t v12 = *(void *)(v2 + 56);
  uint64_t v13 = *(void *)(v2 + 64);
  v19[0]  = *(float64x2_t *)v2;
  v19[1]  = v9;
  uint64_t v20 = v11;
  uint64_t v21 = v12;
  v19[2]  = v10;
  uint64_t v22 = v13;
  v15[0]  = v3;
  v15[1]  = v4;
  uint64_t v16 = v6;
  uint64_t v17 = v7;
  v15[2]  = v5;
  uint64_t v18 = v8;
  return SPScaledPose3DAlmostEqualToPose(v19, v15, a2);
}

BOOL SPScaledPose3DAlmostEqualToPose(float64x2_t *a1, float64x2_t *a2, double a3)
{
  float64x2_t v3 = a1[2];
  float64x2_t v4 = a2[2];
  BOOL v7 = vabdd_f64(v3.f64[0], v4.f64[0]) < a3
    && (float64x2_t v5 = vsubq_f64(v3, v4),
        float64x2_t v6 = vsubq_f64(a1[3], a2[3]),
        v5.f64[0]  = v6.f64[0],
        *(int32x2_t *)&v5.f64[0]  = vmovn_s64(vcgtq_f64((float64x2_t)vdupq_lane_s64(*(uint64_t *)&a3, 0), vabsq_f64(v5))),
        (HIDWORD(v5.f64[0]) & LODWORD(v5.f64[0]) & 1) != 0)
    && fabs(v6.f64[1]) < a3;
  if (vabdd_f64(a1->f64[0], a2->f64[0]) >= a3 || vabdd_f64(a1->f64[1], a2->f64[1]) >= a3)
  {
    BOOL v8 = 0;
    if (!v7) {
      return 0;
    }
  }
  else
  {
    BOOL v8 = vabdd_f64(a1[1].f64[0], a2[1].f64[0]) < a3;
    if (!v7) {
      return 0;
    }
  }
  return vabdd_f64(a1[4].f64[0], a2[4].f64[0]) < a3 && v8;
}

BOOL SPScaledPose3D.isIdentity.getter()
{
  long long v1 = *(_OWORD *)(v0 + 16);
  long long v2 = *(_OWORD *)(v0 + 32);
  uint64_t v3 = *(void *)(v0 + 48);
  uint64_t v4 = *(void *)(v0 + 56);
  uint64_t v5 = *(void *)(v0 + 64);
  v7[0]  = *(_OWORD *)v0;
  v7[1]  = v1;
  uint64_t v8 = v3;
  uint64_t v9 = v4;
  v7[2]  = v2;
  uint64_t v10 = v5;
  return SPScaledPose3DIsIdentity((uint64_t)v7);
}

BOOL SPScaledPose3DIsIdentity(uint64_t a1)
{
  long long v1 = *(_OWORD *)(a1 + 32);
  if (fabs(*(double *)&v1) >= 0.0000000149011612)
  {
    BOOL v3 = 0;
  }
  else
  {
    double v2 = fabs(*(double *)(a1 + 48));
    BOOL v3 = fabs(*(double *)(a1 + 56) + -1.0) < 0.0000000149011612;
    if (fabs(*((double *)&v1 + 1)) >= 0.0000000149011612 || v2 >= 0.0000000149011612) {
      BOOL v3 = 0;
    }
  }
  double v5 = fabs(*(double *)(a1 + 8));
  if (fabs(*(double *)a1) < 0.0000000149011612 && v5 < 0.0000000149011612)
  {
    BOOL v7 = fabs(*(double *)(a1 + 16)) < 0.0000000149011612;
    if (!v3) {
      return 0;
    }
  }
  else
  {
    BOOL v7 = 0;
    if (!v3) {
      return 0;
    }
  }
  return fabs(*(double *)(a1 + 64) + -1.0) < 0.0000000149011612 && v7;
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance SPScaledPose3D(uint64_t a1, uint64_t a2)
{
  float64x2_t v2 = *(float64x2_t *)(a1 + 16);
  float64x2_t v3 = *(float64x2_t *)(a1 + 32);
  uint64_t v4 = *(void *)(a1 + 48);
  uint64_t v5 = *(void *)(a1 + 56);
  uint64_t v6 = *(void *)(a1 + 64);
  float64x2_t v7 = *(float64x2_t *)a2;
  float64x2_t v8 = *(float64x2_t *)(a2 + 16);
  float64x2_t v9 = *(float64x2_t *)(a2 + 32);
  uint64_t v10 = *(void *)(a2 + 48);
  uint64_t v11 = *(void *)(a2 + 56);
  uint64_t v12 = *(void *)(a2 + 64);
  v18[0]  = *(float64x2_t *)a1;
  v18[1]  = v2;
  uint64_t v19 = v4;
  uint64_t v20 = v5;
  v18[2]  = v3;
  uint64_t v21 = v6;
  v14[0]  = v7;
  v14[1]  = v8;
  uint64_t v15 = v10;
  uint64_t v16 = v11;
  v14[2]  = v9;
  uint64_t v17 = v12;
  return SPScaledPose3DEqualToPose(v18, v14);
}

void SPScaledPose3D.hash(into:)()
{
  specialized SIMD.hash(into:)(*(__n128 *)v0, *(double *)(v0 + 16));
  v1.i64[0]  = 0;
  float64x2_t v2 = *(float64x2_t *)(v0 + 32);
  int8x16_t v3 = (int8x16_t)vdupq_lane_s64(vcgtq_s64(v1, (int64x2_t)v2).i64[0], 0);
  specialized SIMD.hash(into:)((__n128)vbslq_s8(v3, (int8x16_t)vnegq_f64(v2), (int8x16_t)v2), (__n128)vbslq_s8(v3, (int8x16_t)vnegq_f64(*(float64x2_t *)(v0 + 48)), *(int8x16_t *)(v0 + 48)));
  if ((*(void *)(v0 + 64) & 0x7FFFFFFFFFFFFFFFLL) != 0) {
    Swift::UInt64 v4 = *(void *)(v0 + 64);
  }
  else {
    Swift::UInt64 v4 = 0;
  }
  Hasher._combine(_:)(v4);
}

Swift::Int SPScaledPose3D.hashValue.getter()
{
  return Hasher._finalize()();
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPScaledPose3D()
{
  return Hasher._finalize()();
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPScaledPose3D()
{
  return Hasher._finalize()();
}

uint64_t protocol witness for static Equatable.== infix(_:_:) in conformance SPScaledPose3D.CodingKeys(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return specialized == infix<A>(_:_:)(*a1, *a2);
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPScaledPose3D.CodingKeys()
{
  return Hasher._finalize()();
}

uint64_t protocol witness for Hashable.hash(into:) in conformance SPScaledPose3D.CodingKeys()
{
  String.hash(into:)();

  return swift_bridgeObjectRelease();
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPScaledPose3D.CodingKeys()
{
  return Hasher._finalize()();
}

unint64_t protocol witness for RawRepresentable.init(rawValue:) in conformance SPScaledPose3D.CodingKeys@<X0>(Swift::String *a1@<X0>, unsigned char *a2@<X8>)
{
  unint64_t result = specialized SPScaledPose3D.CodingKeys.init(rawValue:)(*a1);
  *a2  = result;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance SPScaledPose3D.CodingKeys(uint64_t *a1@<X8>)
{
  int v2 = *v1;
  unint64_t v3 = 0xE800000000000000;
  unint64_t v4 = 0xE800000000000000;
  uint64_t v5 = 0x6E6F697461746F72;
  if (v2 != 1)
  {
    uint64_t v5 = 0x656C616373;
    unint64_t v4 = 0xE500000000000000;
  }
  BOOL v6 = v2 == 0;
  if (*v1) {
    uint64_t v7 = v5;
  }
  else {
    uint64_t v7 = 0x6E6F697469736F70;
  }
  if (!v6) {
    unint64_t v3 = v4;
  }
  *a1  = v7;
  a1[1]  = v3;
}

uint64_t protocol witness for CodingKey.stringValue.getter in conformance SPScaledPose3D.CodingKeys()
{
  uint64_t v1 = 0x6E6F697461746F72;
  if (*v0 != 1) {
    uint64_t v1 = 0x656C616373;
  }
  if (*v0) {
    return v1;
  }
  else {
    return 0x6E6F697469736F70;
  }
}

unint64_t protocol witness for CodingKey.init(stringValue:) in conformance SPScaledPose3D.CodingKeys@<X0>(Swift::String a1@<X1:X0>, unsigned char *a2@<X8>)
{
  unint64_t result = specialized SPScaledPose3D.CodingKeys.init(rawValue:)(a1);
  *a2  = result;
  return result;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPScaledPose3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys();

  return MEMORY[0x270FA00B0](a1, v2);
}

uint64_t protocol witness for CustomDebugStringConvertible.debugDescription.getter in conformance SPScaledPose3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys();

  return MEMORY[0x270FA00B8](a1, v2);
}

uint64_t SPScaledPose3D.encode(to:)(void *a1)
{
  unint64_t v3 = v1;
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedEncodingContainer<SPScaledPose3D.CodingKeys>);
  uint64_t v6 = *(void *)(v5 - 8);
  MEMORY[0x270FA5388](v5);
  float64x2_t v8 = (char *)&v12 - ((v7 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys();
  dispatch thunk of Encoder.container<A>(keyedBy:)();
  long long v9 = v3[1];
  long long v14 = *v3;
  long long v15 = v9;
  char v13 = 0;
  type metadata accessor for SPPoint3D(0);
  lazy protocol witness table accessor for type SPPoint3D and conformance SPPoint3D(&lazy protocol witness table cache variable for type SPPoint3D and conformance SPPoint3D, type metadata accessor for SPPoint3D);
  KeyedEncodingContainer.encode<A>(_:forKey:)();
  if (!v2)
  {
    long long v10 = v3[3];
    long long v14 = v3[2];
    long long v15 = v10;
    char v13 = 1;
    type metadata accessor for SPRotation3D(0);
    lazy protocol witness table accessor for type SPPoint3D and conformance SPPoint3D(&lazy protocol witness table cache variable for type SPRotation3D and conformance SPRotation3D, type metadata accessor for SPRotation3D);
    KeyedEncodingContainer.encode<A>(_:forKey:)();
    LOBYTE(v14)  = 2;
    KeyedEncodingContainer.encode(_:forKey:)();
  }
  return (*(uint64_t (**)(char *, uint64_t))(v6 + 8))(v8, v5);
}

__n128 SPScaledPose3D.init(from:)@<Q0>(void *a1@<X0>, uint64_t a2@<X8>)
{
  specialized SPScaledPose3D.init(from:)(a1, (uint64_t)v6);
  if (!v2)
  {
    long long v5 = v6[3];
    *(_OWORD *)(a2 + 32)  = v6[2];
    *(_OWORD *)(a2 + 48)  = v5;
    *(_OWORD *)(a2 + 64)  = v6[4];
    __n128 result = (__n128)v6[1];
    *(_OWORD *)a2  = v6[0];
    *(__n128 *)(a2 + 16)  = result;
  }
  return result;
}

__n128 protocol witness for Decodable.init(from:) in conformance SPScaledPose3D@<Q0>(void *a1@<X0>, uint64_t a2@<X8>)
{
  specialized SPScaledPose3D.init(from:)(a1, (uint64_t)v6);
  if (!v2)
  {
    long long v5 = v6[3];
    *(_OWORD *)(a2 + 32)  = v6[2];
    *(_OWORD *)(a2 + 48)  = v5;
    *(_OWORD *)(a2 + 64)  = v6[4];
    __n128 result = (__n128)v6[1];
    *(_OWORD *)a2  = v6[0];
    *(__n128 *)(a2 + 16)  = result;
  }
  return result;
}

uint64_t protocol witness for Encodable.encode(to:) in conformance SPScaledPose3D(void *a1)
{
  return SPScaledPose3D.encode(to:)(a1);
}

uint64_t SPScaledPose3D.description.getter()
{
  _StringGuts.grow(_:)(38);
  swift_bridgeObjectRelease();
  _StringGuts.grow(_:)(21);
  v0._countAndFlagsBits  = 540702760;
  v0._object  = (void *)0xE400000000000000;
  String.append(_:)(v0);
  Double.write<A>(to:)();
  v1._countAndFlagsBits  = 0x203A79202CLL;
  v1._object  = (void *)0xE500000000000000;
  String.append(_:)(v1);
  Double.write<A>(to:)();
  v2._countAndFlagsBits  = 0x203A7A202CLL;
  v2._object  = (void *)0xE500000000000000;
  String.append(_:)(v2);
  Double.write<A>(to:)();
  v3._countAndFlagsBits  = 41;
  v3._object  = (void *)0xE100000000000000;
  String.append(_:)(v3);
  v4._countAndFlagsBits  = 0;
  v4._object  = (void *)0xE000000000000000;
  String.append(_:)(v4);
  swift_bridgeObjectRelease();
  v5._countAndFlagsBits  = 0x697461746F72202CLL;
  v5._object  = (void *)0xEC000000203A6E6FLL;
  String.append(_:)(v5);
  v11._countAndFlagsBits  = 0;
  v11._object  = (void *)0xE000000000000000;
  _StringGuts.grow(_:)(16);
  v6._countAndFlagsBits  = 0x6E72657461757128;
  v6._object  = (void *)0xED0000203A6E6F69;
  String.append(_:)(v6);
  type metadata accessor for simd_quatd(0);
  _print_unlocked<A, B>(_:_:)();
  v7._countAndFlagsBits  = 41;
  v7._object  = (void *)0xE100000000000000;
  String.append(_:)(v7);
  String.append(_:)(v11);
  swift_bridgeObjectRelease();
  v8._countAndFlagsBits  = 0x3A656C616373202CLL;
  v8._object  = (void *)0xE900000000000020;
  String.append(_:)(v8);
  v9._countAndFlagsBits  = Double.description.getter();
  String.append(_:)(v9);
  swift_bridgeObjectRelease();
  return 0x6F697469736F7028;
}

uint64_t SPScaledPose3D.customMirror.getter()
{
  uint64_t v1 = type metadata accessor for Mirror.AncestorRepresentation();
  uint64_t v2 = *(void *)(v1 - 8);
  MEMORY[0x270FA5388](v1);
  Swift::String v4 = (char *)v22 - ((v3 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName((uint64_t *)&demangling cache variable for type metadata for Mirror.DisplayStyle?);
  MEMORY[0x270FA5388](v5 - 8);
  Swift::String v7 = (char *)v22 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  long long v8 = v0[3];
  v22[2]  = v0[2];
  v22[3]  = v8;
  v22[4]  = v0[4];
  long long v9 = v0[1];
  v22[0]  = *v0;
  v22[1]  = v9;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<(String, Any)>);
  uint64_t v10 = swift_allocObject();
  *(_OWORD *)(v10 + 16)  = xmmword_228C1FC50;
  *(void *)(v10 + 32)  = 0x6E6F697469736F70;
  *(void *)(v10 + 40)  = 0xE800000000000000;
  type metadata accessor for SPPoint3D(0);
  *(void *)(v10 + 72)  = v11;
  uint64_t v12 = swift_allocObject();
  *(void *)(v10 + 48)  = v12;
  long long v13 = v0[1];
  *(_OWORD *)(v12 + 16)  = *v0;
  *(_OWORD *)(v12 + 32)  = v13;
  *(void *)(v10 + 80)  = 0x6E6F697461746F72;
  *(void *)(v10 + 88)  = 0xE800000000000000;
  type metadata accessor for SPRotation3D(0);
  *(void *)(v10 + 120)  = v14;
  uint64_t v15 = swift_allocObject();
  *(void *)(v10 + 96)  = v15;
  long long v16 = v0[3];
  *(_OWORD *)(v15 + 16)  = v0[2];
  *(_OWORD *)(v15 + 32)  = v16;
  *(void *)(v10 + 128)  = 0x656C616373;
  *(void *)(v10 + 136)  = 0xE500000000000000;
  uint64_t v17 = *((void *)v0 + 8);
  *(void *)(v10 + 168)  = MEMORY[0x263F8D538];
  *(void *)(v10 + 144)  = v17;
  uint64_t v18 = *MEMORY[0x263F8E808];
  uint64_t v19 = type metadata accessor for Mirror.DisplayStyle();
  uint64_t v20 = *(void *)(v19 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v20 + 104))(v7, v18, v19);
  (*(void (**)(char *, void, uint64_t, uint64_t))(v20 + 56))(v7, 0, 1, v19);
  (*(void (**)(char *, void, uint64_t))(v2 + 104))(v4, *MEMORY[0x263F8E830], v1);
  type metadata accessor for SPScaledPose3D(0);
  return Mirror.init<A>(_:children:displayStyle:ancestorRepresentation:)();
}

float64_t SPPose3D.concatenating(_:)@<D0>(uint64_t a1@<X0>, uint64_t a2@<X8>, float64x2_t a3@<Q3>)
{
  float64x2_t v5 = *(float64x2_t *)a1;
  long long v6 = *(_OWORD *)(a1 + 16);
  long long v7 = *(_OWORD *)(a1 + 32);
  uint64_t v8 = *(void *)(a1 + 48);
  uint64_t v9 = *(void *)(a1 + 56);
  a3.f64[0]  = *(float64_t *)(a1 + 64);
  long long v11 = *(_OWORD *)(v3 + 16);
  float64x2_t v12 = *(float64x2_t *)(v3 + 32);
  uint64_t v13 = *(void *)(v3 + 56);
  float64x2_t v10 = *(float64x2_t *)v3;
  uint64_t v28 = *(void *)(v3 + 48);
  uint64_t v29 = v13;
  v27[2]  = v12;
  *(void *)&long long v25 = v8;
  *((void *)&v25 + 1)  = v9;
  SPScaledPose3DConcatenation((uint64_t)v27, (uint64_t)&v24, (uint64_t)&v19, a3, v10, v12, v19, v20, v21, v22, v23, v5, v6, v7, v25, *(uint64_t *)&a3.f64[0], v26, *(uint64_t *)&v10.f64[0], *(uint64_t *)&v10.f64[1],
    v11,
    *((uint64_t *)&v11 + 1));
  float64_t result = v19.f64[0];
  long long v15 = v20;
  long long v16 = v21;
  long long v17 = v22;
  uint64_t v18 = v23;
  *(float64x2_t *)a2  = v19;
  *(_OWORD *)(a2 + 16)  = v15;
  *(_OWORD *)(a2 + 48)  = v17;
  *(_OWORD *)(a2 + 32)  = v16;
  *(void *)(a2 + 64)  = v18;
  return result;
}

unint64_t lazy protocol witness table accessor for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys()
{
  unint64_t result = lazy protocol witness table cache variable for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys);
  }
  return result;
}

unint64_t specialized SPScaledPose3D.CodingKeys.init(rawValue:)(Swift::String string)
{
  object  = string._object;
  v2._countAndFlagsBits  = string._countAndFlagsBits;
  v2._object  = object;
  unint64_t v3 = _findStringSwitchCase(cases:string:)((Swift::OpaquePointer)&outlined read-only object #0 of SPScaledPose3D.CodingKeys.init(rawValue:), v2);
  swift_bridgeObjectRelease();
  if (v3 >= 3) {
    return 3;
  }
  else {
    return v3;
  }
}

uint64_t specialized SPScaledPose3D.init(from:)@<X0>(void *a1@<X0>, uint64_t a2@<X8>)
{
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedDecodingContainer<SPScaledPose3D.CodingKeys>);
  uint64_t v6 = *(void *)(v5 - 8);
  MEMORY[0x270FA5388](v5);
  uint64_t v8 = (char *)&v17 - ((v7 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPScaledPose3D.CodingKeys and conformance SPScaledPose3D.CodingKeys();
  dispatch thunk of Decoder.container<A>(keyedBy:)();
  if (v2) {
    return __swift_destroy_boxed_opaque_existential_1(a1);
  }
  type metadata accessor for SPPoint3D(0);
  LOBYTE(v23[0])  = 0;
  lazy protocol witness table accessor for type SPPoint3D and conformance SPPoint3D(&lazy protocol witness table cache variable for type SPPoint3D and conformance SPPoint3D, type metadata accessor for SPPoint3D);
  KeyedDecodingContainer.decode<A>(_:forKey:)();
  *(void *)&long long v22 = v37;
  double v9 = v35;
  *(void *)&long long v21 = v36;
  double v10 = v34;
  type metadata accessor for SPRotation3D(0);
  LOBYTE(v23[0])  = 1;
  lazy protocol witness table accessor for type SPPoint3D and conformance SPPoint3D(&lazy protocol witness table cache variable for type SPRotation3D and conformance SPRotation3D, type metadata accessor for SPRotation3D);
  KeyedDecodingContainer.decode<A>(_:forKey:)();
  *(void *)&long long v20 = v33;
  uint64_t v19 = v32;
  long long v18 = v31;
  LOBYTE(v23[0])  = 2;
  KeyedDecodingContainer.decode(_:forKey:)();
  v30.x  = v10;
  v30.y  = v9;
  *(void *)&v30.double z = v21;
  *(void *)&v30.vector.f64[3]  = v22;
  uint64_t v28 = v19;
  uint64_t v29 = v20;
  long long v27 = v18;
  SPScaledPose3DMake(&v30, &v27, (uint64_t)v23, v11);
  long long v21 = v23[0];
  long long v20 = v23[1];
  long long v22 = v23[2];
  uint64_t v12 = v24;
  uint64_t v13 = v25;
  uint64_t v14 = v26;
  (*(void (**)(char *, uint64_t))(v6 + 8))(v8, v5);
  uint64_t result = __swift_destroy_boxed_opaque_existential_1(a1);
  long long v16 = v20;
  *(_OWORD *)a2  = v21;
  *(_OWORD *)(a2 + 16)  = v16;
  *(void *)(a2 + 48)  = v12;
  *(void *)(a2 + 56)  = v13;
  *(_OWORD *)(a2 + 32)  = v22;
  *(void *)(a2 + 64)  = v14;
  return result;
}

uint64_t sub_228BF7CAC()
{
  return MEMORY[0x270FA0238](v0, 48, 15);
}

uint64_t base witness table accessor for Equatable in SPScaledPose3D()
{
  return lazy protocol witness table accessor for type SPPoint3D and conformance SPPoint3D(&lazy protocol witness table cache variable for type SPScaledPose3D and conformance SPScaledPose3D, type metadata accessor for SPScaledPose3D);
}

unsigned char *storeEnumTagSinglePayload for SPScaledPose3D.CodingKeys(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 2 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 2) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFE) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFD)
  {
    unsigned int v6 = ((a2 - 254) >> 8) + 1;
    *uint64_t result = a2 + 2;
    switch(v5)
    {
      case 1:
        result[1]  = v6;
        break;
      case 2:
        *(_WORD *)(result + 1)  = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x228BF7E58);
      case 4:
        *(_DWORD *)(result + 1)  = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1]  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1)  = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1)  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 2;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for SPScaledPose3D.CodingKeys()
{
  return &type metadata for SPScaledPose3D.CodingKeys;
}

uint64_t lazy protocol witness table accessor for type SPPoint3D and conformance SPPoint3D(unint64_t *a1, void (*a2)(uint64_t))
{
  uint64_t result = *a1;
  if (!result)
  {
    a2(255);
    uint64_t result = swift_getWitnessTable();
    atomic_store(result, a1);
  }
  return result;
}

void __swiftcall SPPose3D.translated(by:)(SPPose3D *__return_ptr retstr, SPVector3D *by)
{
  long long v7 = *(_OWORD *)(v2 + 16);
  long long v8 = *(_OWORD *)(v2 + 32);
  double v9 = *(double *)(v2 + 48);
  double v10 = *(double *)(v2 + 56);
  *(_OWORD *)&v19.position.x  = *(_OWORD *)v2;
  *(_OWORD *)&v19.position.vector.f64[2]  = v7;
  v19.rotation.vector.f64[2]  = v9;
  v19.rotation.vector.f64[3]  = v10;
  *(_OWORD *)v19.rotation.vector.f64  = v8;
  v18.x  = v3;
  v18.y  = v4;
  v18.double z = v5;
  SPPose3DTranslate(&v19, &v18, (uint64_t)v15);
  long long v11 = v15[1];
  long long v12 = v15[2];
  double v13 = v16;
  double v14 = v17;
  *(_OWORD *)&retstr->position.x  = v15[0];
  *(_OWORD *)&retstr->position.vector.f64[2]  = v11;
  retstr->rotation.vector.f64[2]  = v13;
  retstr->rotation.vector.f64[3]  = v14;
  *(_OWORD *)retstr->rotation.vector.f64  = v12;
}

void __swiftcall SPPose3D.init(_:)(SPPose3D_optional *__return_ptr retstr, simd_double4x4 *a2)
{
  long long v3 = *(_OWORD *)&a2->columns[0].f64[2];
  long long v4 = *(_OWORD *)a2->columns[1].f64;
  long long v5 = *(_OWORD *)&a2->columns[1].f64[2];
  long long v6 = *(_OWORD *)a2->columns[2].f64;
  long long v7 = *(_OWORD *)&a2->columns[2].f64[2];
  long long v8 = *(_OWORD *)a2->columns[3].f64;
  long long v9 = *(_OWORD *)&a2->columns[3].f64[2];
  float64x2_t v23 = *(float64x2_t *)a2->columns[0].f64;
  long long v24 = v3;
  long long v25 = v4;
  long long v26 = v5;
  long long v27 = v6;
  long long v28 = v7;
  long long v29 = v8;
  long long v30 = v9;
  SPPose3DMakeWith4x4Matrix((uint64_t)&v23, &v19);
  double v10 = *((double *)&v20 + 1);
  double v11 = *(double *)&v20;
  double v12 = v19.f64[1];
  double v13 = v19.f64[0];
  double v14 = *((double *)&v22 + 1);
  double v15 = *(double *)&v22;
  double v16 = *((double *)&v21 + 1);
  double v17 = *(double *)&v21;
  long long v24 = v20;
  float64x2_t v23 = v19;
  long long v26 = v22;
  long long v25 = v21;
  int v18 = SPPose3DIsValid(&v23);
  if (!v18)
  {
    double v13 = 0.0;
    double v12 = 0.0;
    double v11 = 0.0;
    double v10 = 0.0;
    double v17 = 0.0;
    double v16 = 0.0;
    double v15 = 0.0;
    double v14 = 0.0;
  }
  retstr->value.position.x  = v13;
  retstr->value.position.y  = v12;
  retstr->value.position.double z = v11;
  retstr->value.position.vector.f64[3]  = v10;
  retstr->value.rotation.vector.f64[0]  = v17;
  retstr->value.rotation.vector.f64[1]  = v16;
  retstr->value.rotation.vector.f64[2]  = v15;
  retstr->value.rotation.vector.f64[3]  = v14;
  retstr->is_nil  = v18 ^ 1;
}

void __swiftcall SPPose3D.init(_:)(SPPose3D_optional *__return_ptr retstr, simd_float4x4 *a2)
{
  float64x2_t v20 = vcvtq_f64_f32(*(float32x2_t *)v2.f32);
  float64x2_t v21 = vcvt_hight_f64_f32(v2);
  float64x2_t v22 = vcvtq_f64_f32(*(float32x2_t *)v3.f32);
  float64x2_t v23 = vcvt_hight_f64_f32(v3);
  float64x2_t v24 = vcvtq_f64_f32(*(float32x2_t *)v4.f32);
  float64x2_t v25 = vcvt_hight_f64_f32(v4);
  float64x2_t v26 = vcvtq_f64_f32(*(float32x2_t *)v5.f32);
  float64x2_t v27 = vcvt_hight_f64_f32(v5);
  SPPose3DMakeWith4x4Matrix((uint64_t)&v20, &v16);
  double v7 = v17.f64[1];
  double v8 = v17.f64[0];
  double v9 = v16.f64[1];
  double v10 = v16.f64[0];
  double v11 = v19.f64[1];
  double v12 = v19.f64[0];
  double v13 = v18.f64[1];
  double v14 = v18.f64[0];
  float64x2_t v21 = v17;
  float64x2_t v20 = v16;
  float64x2_t v23 = v19;
  float64x2_t v22 = v18;
  int v15 = SPPose3DIsValid(&v20);
  if (!v15)
  {
    double v10 = 0.0;
    double v9 = 0.0;
    double v8 = 0.0;
    double v7 = 0.0;
    double v14 = 0.0;
    double v13 = 0.0;
    double v12 = 0.0;
    double v11 = 0.0;
  }
  retstr->value.position.x  = v10;
  retstr->value.position.y  = v9;
  retstr->value.position.double z = v8;
  retstr->value.position.vector.f64[3]  = v7;
  retstr->value.rotation.vector.f64[0]  = v14;
  retstr->value.rotation.vector.f64[1]  = v13;
  retstr->value.rotation.vector.f64[2]  = v12;
  retstr->value.rotation.vector.f64[3]  = v11;
  retstr->is_nil  = v15 ^ 1;
}

void __swiftcall SPPose3D.concatenating(_:)(SPPose3D *__return_ptr retstr, SPPose3D *a2)
{
  long long v4 = *(_OWORD *)&a2->position.x;
  long long v5 = *(_OWORD *)&a2->position.vector.f64[2];
  long long v6 = *(_OWORD *)a2->rotation.vector.f64;
  double v7 = a2->rotation.vector.f64[2];
  double v8 = a2->rotation.vector.f64[3];
  long long v9 = *(_OWORD *)(v2 + 16);
  long long v10 = *(_OWORD *)(v2 + 32);
  double v11 = *(double *)(v2 + 48);
  double v12 = *(double *)(v2 + 56);
  *(_OWORD *)&v21.position.x  = *(_OWORD *)v2;
  *(_OWORD *)&v21.position.vector.f64[2]  = v9;
  v21.rotation.vector.f64[2]  = v11;
  v21.rotation.vector.f64[3]  = v12;
  *(_OWORD *)v21.rotation.vector.f64  = v10;
  *(_OWORD *)&v20.position.x  = v4;
  *(_OWORD *)&v20.position.vector.f64[2]  = v5;
  v20.rotation.vector.f64[2]  = v7;
  v20.rotation.vector.f64[3]  = v8;
  *(_OWORD *)v20.rotation.vector.f64  = v6;
  SPPose3DConcatenation(&v21, &v20, v17);
  float64x2_t v13 = v17[1];
  float64x2_t v14 = v17[2];
  double v15 = v18;
  double v16 = v19;
  *(float64x2_t *)&retstr->position.x  = v17[0];
  *(float64x2_t *)&retstr->position.vector.f64[2]  = v13;
  retstr->rotation.vector.f64[2]  = v15;
  retstr->rotation.vector.f64[3]  = v16;
  *(float64x2_t *)retstr->rotation.vector.f64  = v14;
}

void __swiftcall SPPose3D.init(position:rotation:)(SPPose3D *__return_ptr retstr, SPPoint3D *position, SPRotation3D *rotation)
{
  v17.x  = v3.vector.f64[0];
  v17.y  = v3.vector.f64[2];
  v17.double z = v4;
  long long v15 = v5;
  long long v16 = v6;
  SPPose3DMake(&v17, v3, (uint64_t)&v15, v12);
  long long v8 = v12[1];
  long long v9 = v12[2];
  double v10 = v13;
  double v11 = v14;
  *(_OWORD *)&retstr->position.x  = v12[0];
  *(_OWORD *)&retstr->position.vector.f64[2]  = v8;
  retstr->rotation.vector.f64[2]  = v10;
  retstr->rotation.vector.f64[3]  = v11;
  *(_OWORD *)retstr->rotation.vector.f64  = v9;
}

__n128 SPPose3DMake@<Q0>(SPPoint3D *a1@<X0>, SPRotation3D a2@<0:Q0, 16:Q1>, uint64_t a3@<X1>, _OWORD *a4@<X8>)
{
  long long v4 = *(_OWORD *)&a1->vector.f64[2];
  *a4  = *(_OWORD *)&a1->x;
  a4[1]  = v4;
  __n128 result = *(__n128 *)a3;
  long long v6 = *(_OWORD *)(a3 + 16);
  a4[2]  = *(_OWORD *)a3;
  a4[3]  = v6;
  return result;
}

double SPPose3D.init(position:rotation:)@<D0>(uint64_t a1@<X8>, __n128 a2@<Q0>, __n128 a3@<Q1>, __n128 a4@<Q2>, __n128 a5@<Q3>)
{
  v15[0]  = a2;
  v15[1]  = a3;
  v14[0]  = a4;
  v14[1]  = a5;
  SPPose3DMakeWithVector((uint64_t)v15, (uint64_t)v14, (uint64_t)v11);
  double result = *(double *)v11;
  long long v7 = v11[1];
  long long v8 = v11[2];
  uint64_t v9 = v12;
  uint64_t v10 = v13;
  *(_OWORD *)a1  = v11[0];
  *(_OWORD *)(a1 + 16)  = v7;
  *(void *)(a1 + 48)  = v9;
  *(void *)(a1 + 56)  = v10;
  *(_OWORD *)(a1 + 32)  = v8;
  return result;
}

__n128 SPPose3DMakeWithVector@<Q0>(uint64_t a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  uint64_t v3 = *(void *)(a1 + 16);
  *(_OWORD *)a3  = *(_OWORD *)a1;
  *(void *)(a3 + 16)  = v3;
  __n128 result = *(__n128 *)a2;
  long long v5 = *(_OWORD *)(a2 + 16);
  *(_OWORD *)(a3 + 32)  = *(_OWORD *)a2;
  *(_OWORD *)(a3 + 48)  = v5;
  return result;
}

double SPPose3D.init(position:rotation:)@<D0>(uint64_t a1@<X8>, float32x4_t a2@<Q0>, float32x4_t a3@<Q1>)
{
  v13[0]  = vcvtq_f64_f32(*(float32x2_t *)a2.f32);
  v13[1]  = vcvt_hight_f64_f32(a2);
  v12[0]  = vcvtq_f64_f32(*(float32x2_t *)a3.f32);
  v12[1]  = vcvt_hight_f64_f32(a3);
  SPPose3DMakeWithVector((uint64_t)v13, (uint64_t)v12, (uint64_t)v9);
  double result = *(double *)v9;
  long long v5 = v9[1];
  long long v6 = v9[2];
  uint64_t v7 = v10;
  uint64_t v8 = v11;
  *(_OWORD *)a1  = v9[0];
  *(_OWORD *)(a1 + 16)  = v5;
  *(void *)(a1 + 48)  = v7;
  *(void *)(a1 + 56)  = v8;
  *(_OWORD *)(a1 + 32)  = v6;
  return result;
}

void __swiftcall SPPose3D.init(position:target:up:)(SPPose3D *__return_ptr retstr, SPPoint3D *position, SPPoint3D *target, SPVector3D *up)
{
  v22.x  = v4;
  v22.y  = v5;
  v22.double z = v6;
  v21.x  = v7;
  v21.y  = v8;
  v21.double z = v9;
  v20.x  = v10;
  v20.y  = v11;
  v20.double z = v23;
  SPPose3DMakeLookAt(&v22, &v21, &v20, v17);
  long long v13 = v17[1];
  long long v14 = v17[2];
  double v15 = v18;
  double v16 = v19;
  *(_OWORD *)&retstr->position.x  = v17[0];
  *(_OWORD *)&retstr->position.vector.f64[2]  = v13;
  retstr->rotation.vector.f64[2]  = v15;
  retstr->rotation.vector.f64[3]  = v16;
  *(_OWORD *)retstr->rotation.vector.f64  = v14;
}

void SPPose3DMakeLookAt(SPPoint3D *a1@<X0>, SPPoint3D *a2@<X1>, SPVector3D *a3@<X2>, _OWORD *a4@<X8>)
{
  float64x2_t v6 = *(float64x2_t *)&a3->vector.f64[2];
  float64x2_t v7 = vsubq_f64(*(float64x2_t *)&a2->x, *(float64x2_t *)&a1->x);
  float64x2_t v8 = vsubq_f64(*(float64x2_t *)&a2->vector.f64[2], *(float64x2_t *)&a1->vector.f64[2]);
  float64x2_t v9 = vmulq_f64(v7, v7);
  v9.f64[0]  = 1.0 / sqrt(vmulq_f64(v8, v8).f64[0] + vaddvq_f64(v9));
  float64x2_t v10 = vmulq_n_f64(v7, v9.f64[0]);
  float64x2_t v11 = vmulq_f64(v8, v9);
  float64x2_t v12 = vmulq_f64(v6, v6);
  v12.f64[0]  = 1.0 / sqrt(v12.f64[0] + vaddvq_f64(vmulq_f64(*(float64x2_t *)&a3->x, *(float64x2_t *)&a3->x)));
  float64x2_t v13 = vmulq_n_f64(*(float64x2_t *)&a3->x, v12.f64[0]);
  float64x2_t v14 = vmulq_f64(v6, v12);
  v15.f64[0]  = v11.f64[0];
  v15.f64[1]  = v10.f64[0];
  float64x2_t v16 = vmlaq_laneq_f64(vmulq_laneq_f64(vnegq_f64(v11), v13, 1), v14, v10, 1);
  v14.f64[1]  = v13.f64[0];
  float64x2_t v17 = vmlaq_f64(vmulq_f64(v14, vnegq_f64(v10)), v13, v15);
  float64x2_t v18 = vmulq_f64(v17, v17);
  double v19 = vmulq_f64(v16, v16).f64[0];
  v16.f64[1]  = v17.f64[0];
  v18.f64[0]  = 1.0 / sqrt(v18.f64[1] + v19 + v18.f64[0]);
  float64x2_t v20 = vmulq_n_f64(v16, v18.f64[0]);
  float64x2_t v21 = vmulq_laneq_f64(v18, v17, 1);
  float64x2_t v22 = vnegq_f64(v20);
  float64x2_t v23 = vnegq_f64(v21);
  v21.f64[1]  = v20.f64[0];
  float64x2_t v24 = vmlaq_f64(vmulq_f64(v15, v22), v10, v21);
  float64x2_t v25 = vmlaq_laneq_f64(vmulq_laneq_f64(v23, v10, 1), v11, v20, 1);
  float64x2_t v26 = vmulq_f64(v24, v24);
  v20.f64[0]  = vmulq_f64(v25, v25).f64[0];
  v25.f64[1]  = v24.f64[0];
  v26.f64[0]  = 1.0 / sqrt(v26.f64[1] + v20.f64[0] + v26.f64[0]);
  v32[0]  = v22;
  v32[1]  = v23;
  v32[2]  = vmulq_n_f64(v25, v26.f64[0]);
  v32[3]  = vmulq_laneq_f64(v26, v24, 1);
  v32[4]  = v10;
  v32[5]  = v11;
  simd_quaternion((uint64_t)v32, (uint64_t)&v33);
  double v27 = vaddvq_f64(vaddq_f64(vmulq_f64(v33, v33), vmulq_f64(v34, v34)));
  if (v27 == 0.0)
  {
    float64x2_t v28 = (float64x2_t)xmmword_228C1F7A0;
    float64x2_t v29 = 0uLL;
  }
  else
  {
    double v30 = 1.0 / sqrt(v27);
    float64x2_t v28 = vmulq_n_f64(v34, v30);
    float64x2_t v29 = vmulq_n_f64(v33, v30);
  }
  long long v31 = *(_OWORD *)&a1->vector.f64[2];
  *a4  = *(_OWORD *)&a1->x;
  a4[1]  = v31;
  a4[2]  = v29;
  a4[3]  = v28;
}

void __swiftcall SPPose3D.init(forward:up:)(SPPose3D *__return_ptr retstr, SPVector3D *forward, SPVector3D *up)
{
  v18.x  = v3;
  v18.y  = v4;
  v18.double z = v5;
  v17.x  = v6;
  v17.y  = v7;
  v17.double z = v8;
  SPPose3DMakeLookAt(&v18, &v17, v14);
  long long v10 = v14[1];
  long long v11 = v14[2];
  double v12 = v15;
  double v13 = v16;
  *(_OWORD *)&retstr->position.x  = v14[0];
  *(_OWORD *)&retstr->position.vector.f64[2]  = v10;
  retstr->rotation.vector.f64[2]  = v12;
  retstr->rotation.vector.f64[3]  = v13;
  *(_OWORD *)retstr->rotation.vector.f64  = v11;
}

void SPPose3DMakeLookAt(SPVector3D *a1@<X0>, SPVector3D *a2@<X1>, _OWORD *a3@<X8>)
{
  float64x2_t v4 = *(float64x2_t *)&a1->vector.f64[2];
  float64x2_t v5 = *(float64x2_t *)&a2->vector.f64[2];
  float64x2_t v6 = vmulq_f64(v4, v4);
  v6.f64[0]  = 1.0 / sqrt(v6.f64[0] + vaddvq_f64(vmulq_f64(*(float64x2_t *)&a1->x, *(float64x2_t *)&a1->x)));
  float64x2_t v7 = vmulq_n_f64(*(float64x2_t *)&a1->x, v6.f64[0]);
  float64x2_t v8 = vmulq_f64(v4, v6);
  float64x2_t v9 = vmulq_f64(v5, v5);
  v9.f64[0]  = 1.0 / sqrt(v9.f64[0] + vaddvq_f64(vmulq_f64(*(float64x2_t *)&a2->x, *(float64x2_t *)&a2->x)));
  float64x2_t v10 = vmulq_n_f64(*(float64x2_t *)&a2->x, v9.f64[0]);
  float64x2_t v11 = vmulq_f64(v5, v9);
  v9.f64[0]  = v8.f64[0];
  v9.f64[1]  = v7.f64[0];
  float64x2_t v12 = vmlaq_laneq_f64(vmulq_laneq_f64(vnegq_f64(v8), v10, 1), v11, v7, 1);
  v11.f64[1]  = v10.f64[0];
  float64x2_t v13 = vmlaq_f64(vmulq_f64(v11, vnegq_f64(v7)), v10, v9);
  float64x2_t v14 = vmulq_f64(v13, v13);
  double v15 = vmulq_f64(v12, v12).f64[0];
  v12.f64[1]  = v13.f64[0];
  v14.f64[0]  = 1.0 / sqrt(v14.f64[1] + v15 + v14.f64[0]);
  float64x2_t v16 = vmulq_n_f64(v12, v14.f64[0]);
  float64x2_t v17 = vmulq_laneq_f64(v14, v13, 1);
  float64x2_t v18 = vnegq_f64(v16);
  float64x2_t v19 = vnegq_f64(v17);
  v17.f64[1]  = v16.f64[0];
  float64x2_t v20 = vmlaq_f64(vmulq_f64(v9, v18), v7, v17);
  float64x2_t v21 = vmlaq_laneq_f64(vmulq_laneq_f64(v19, v7, 1), v8, v16, 1);
  float64x2_t v22 = vmulq_f64(v20, v20);
  v16.f64[0]  = vmulq_f64(v21, v21).f64[0];
  v21.f64[1]  = v20.f64[0];
  v22.f64[0]  = 1.0 / sqrt(v22.f64[1] + v16.f64[0] + v22.f64[0]);
  v27[0]  = v18;
  v27[1]  = v19;
  v27[2]  = vmulq_n_f64(v21, v22.f64[0]);
  v27[3]  = vmulq_laneq_f64(v22, v20, 1);
  v27[4]  = v7;
  v27[5]  = v8;
  simd_quaternion((uint64_t)v27, (uint64_t)&v28);
  double v23 = vaddvq_f64(vaddq_f64(vmulq_f64(v28, v28), vmulq_f64(v29, v29)));
  if (v23 == 0.0)
  {
    float64x2_t v24 = (float64x2_t)xmmword_228C1F7A0;
    float64x2_t v25 = 0uLL;
  }
  else
  {
    double v26 = 1.0 / sqrt(v23);
    float64x2_t v24 = vmulq_n_f64(v29, v26);
    float64x2_t v25 = vmulq_n_f64(v28, v26);
  }
  *a3  = 0uLL;
  a3[1]  = 0uLL;
  a3[2]  = v25;
  a3[3]  = v24;
}

void __swiftcall SPPose3D.init(transform:)(SPPose3D_optional *__return_ptr retstr, SPAffineTransform3D *transform)
{
  SPPose3D.init(transform:)((uint64_t)transform, (void (*)(float64x2_t *__return_ptr, float64x2_t *))SPPose3DMakeWithAffineTransform, (uint64_t)retstr);
}

void SPPose3DMakeWithAffineTransform(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  float64x2_t v5 = *(float64x2_t *)a1;
  float64x2_t v4 = *(float64x2_t *)(a1 + 16);
  float64x2_t v7 = *(float64x2_t *)(a1 + 32);
  float64x2_t v6 = *(float64x2_t *)(a1 + 48);
  float64x2_t v8 = *(float64x2_t *)(a1 + 64);
  float64x2_t v9 = *(float64x2_t *)(a1 + 80);
  v10.f64[0]  = *(float64_t *)(a1 + 80);
  v10.f64[1]  = *(float64_t *)(a1 + 64);
  v11.f64[0]  = *(float64_t *)(a1 + 48);
  v11.f64[1]  = *(float64_t *)(a1 + 32);
  float64x2_t v12 = vmulq_f64(*(float64x2_t *)a1, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)v9, 8uLL), vnegq_f64(v11)), v10, (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v6, 8uLL)));
  float64x2_t v14 = vmulq_f64(v4, vmlaq_laneq_f64(vmulq_f64(v8, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v7, 1))), v7, v8, 1));
  BOOL v13 = v14.f64[0] + vaddvq_f64(v12) < 0.0;
  v14.f64[0]  = -1.0;
  if (!v13) {
    v14.f64[0]  = 1.0;
  }
  v15.f64[0]  = sqrt(vmulq_f64(v4, v4).f64[0] + vaddvq_f64(vmulq_f64(v5, v5)));
  float64x2_t v16 = vmulq_f64(v9, v9);
  v16.f64[0]  = sqrt(v16.f64[0] + vaddvq_f64(vmulq_f64(v8, v8)));
  v15.f64[1]  = sqrt(vmulq_f64(v6, v6).f64[0] + vaddvq_f64(vmulq_f64(v7, v7)));
  float64x2_t v17 = vmulq_n_f64(v15, v14.f64[0]);
  float64x2_t v18 = vmulq_f64(v16, v14);
  float64x2_t v19 = vdivq_f64(v5, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v17.f64[0], 0));
  float64x2_t v20 = vdivq_f64(v4, v17);
  float64x2_t v21 = (float64x2_t)vdupq_laneq_s64((int64x2_t)v17, 1);
  float64x2_t v22 = vdivq_f64(v7, v21);
  float64x2_t v23 = vdivq_f64(v6, v21);
  float64x2_t v24 = vdivq_f64(v8, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v18.f64[0], 0));
  float64x2_t v25 = vdivq_f64(v9, v18);
  float64x2_t v26 = vmulq_f64(v23, v20);
  float64x2_t v27 = vmulq_f64(v20, v20);
  v27.f64[0]  = v27.f64[0] + vaddvq_f64(vmulq_f64(v19, v19));
  v26.f64[0]  = (v26.f64[0] + vaddvq_f64(vmulq_f64(v22, v19))) / v27.f64[0];
  float64x2_t v28 = vsubq_f64(v22, vmulq_n_f64(v19, v26.f64[0]));
  float64x2_t v29 = vsubq_f64(v23, vmulq_f64(v20, v26));
  float64x2_t v30 = vmulq_f64(v25, v20);
  v30.f64[0]  = (v30.f64[0] + vaddvq_f64(vmulq_f64(v24, v19))) / v27.f64[0];
  float64x2_t v31 = vmulq_n_f64(v19, v30.f64[0]);
  float64x2_t v32 = vsubq_f64(v25, vmulq_f64(v20, v30));
  float64x2_t v33 = vsubq_f64(v24, v31);
  float64x2_t v34 = vmulq_f64(v24, v28);
  v34.f64[0]  = vmulq_f64(v25, v29).f64[0] + vaddvq_f64(v34);
  float64x2_t v35 = vmulq_f64(v29, v29);
  v35.f64[0]  = v35.f64[0] + vaddvq_f64(vmulq_f64(v28, v28));
  v34.f64[0]  = v34.f64[0] / v35.f64[0];
  float64x2_t v36 = vmulq_f64(v29, v34);
  float64x2_t v37 = vsubq_f64(v33, vmulq_n_f64(v28, v34.f64[0]));
  float64x2_t v38 = vsubq_f64(v32, v36);
  v27.f64[0]  = 1.0 / sqrt(v27.f64[0]);
  v35.f64[0]  = 1.0 / sqrt(v35.f64[0]);
  v12.f64[0]  = 1.0 / sqrt(vmulq_f64(v38, v38).f64[0] + vaddvq_f64(vmulq_f64(v37, v37)));
  float64x2_t v49 = 0u;
  float64x2_t v50 = 0u;
  v48[0]  = vmulq_n_f64(v19, v27.f64[0]);
  v48[1]  = vmulq_f64(v20, v27);
  v48[2]  = vmulq_n_f64(v28, v35.f64[0]);
  v48[3]  = vmulq_f64(v29, v35);
  v48[4]  = vmulq_n_f64(v37, v12.f64[0]);
  v48[5]  = vmulq_f64(v38, v12);
  simd_quaternion((uint64_t)v48, (uint64_t)&v49);
  int64x2_t v39 = (int64x2_t)vornq_s8(vmvnq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v49), (int8x16_t)vcgezq_f64(v49))), vorrq_s8((int8x16_t)vcltzq_f64(v50), (int8x16_t)vcgezq_f64(v50)));
  unint64_t v40 = vorrq_s8((int8x16_t)v39, (int8x16_t)vdupq_laneq_s64(v39, 1)).u64[0];
  float64x2_t v41 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if ((v40 & 0x8000000000000000) != 0)
  {
    float64x2_t v44 = v41;
    float64x2_t v43 = v41;
  }
  else
  {
    double v42 = vaddvq_f64(vaddq_f64(vmulq_f64(v49, v49), vmulq_f64(v50, v50)));
    if (v42 == 0.0)
    {
      float64x2_t v43 = (float64x2_t)xmmword_228C1F7A0;
      float64x2_t v44 = 0uLL;
    }
    else
    {
      double v45 = 1.0 / sqrt(v42);
      float64x2_t v43 = vmulq_n_f64(v50, v45);
      float64x2_t v44 = vmulq_n_f64(v49, v45);
    }
  }
  int64x2_t v46 = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v44), (int8x16_t)vcgezq_f64(v44)), (int8x16_t)vceqq_f64(vabsq_f64(v44), v41)), vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v43), (int8x16_t)vcgezq_f64(v43)), (int8x16_t)vceqq_f64(vabsq_f64(v43), v41)));
  if ((vandq_s8((int8x16_t)v46, (int8x16_t)vdupq_laneq_s64(v46, 1)).u64[0] & 0x8000000000000000) != 0)
  {
    uint64_t v47 = *(void *)(a1 + 112);
    *(_OWORD *)a2  = *(_OWORD *)(a1 + 96);
    *(void *)(a2 + 16)  = v47;
    *(void *)(a2 + 24)  = 0;
    *(float64x2_t *)(a2 + 32)  = v44;
  }
  else
  {
    *(_OWORD *)a2  = SPPose3DInvalid_0;
    *(_OWORD *)(a2 + 16)  = unk_228C205B0;
    float64x2_t v43 = (float64x2_t)unk_228C205D0;
    *(_OWORD *)(a2 + 32)  = xmmword_228C205C0;
  }
  *(float64x2_t *)(a2 + 48)  = v43;
}

void __swiftcall SPPose3D.init(transform:)(SPPose3D_optional *__return_ptr retstr, SPProjectiveTransform3D *transform)
{
  SPPose3D.init(transform:)((uint64_t)transform, (void (*)(float64x2_t *__return_ptr, float64x2_t *))SPPose3DMakeWithProjectiveTransform, (uint64_t)retstr);
}

unint64_t SPPose3D.init(transform:)@<X0>(uint64_t a1@<X0>, void (*a2)(float64x2_t *__return_ptr, float64x2_t *)@<X1>, uint64_t a3@<X8>)
{
  long long v4 = *(_OWORD *)(a1 + 16);
  long long v5 = *(_OWORD *)(a1 + 32);
  long long v6 = *(_OWORD *)(a1 + 48);
  long long v7 = *(_OWORD *)(a1 + 64);
  long long v8 = *(_OWORD *)(a1 + 80);
  long long v9 = *(_OWORD *)(a1 + 96);
  long long v10 = *(_OWORD *)(a1 + 112);
  float64x2_t v24 = *(float64x2_t *)a1;
  long long v25 = v4;
  long long v26 = v5;
  long long v27 = v6;
  long long v28 = v7;
  long long v29 = v8;
  long long v30 = v9;
  long long v31 = v10;
  a2(&v20, &v24);
  uint64_t v11 = *((void *)&v21 + 1);
  uint64_t v12 = v21;
  float64_t v13 = v20.f64[1];
  float64_t v14 = v20.f64[0];
  uint64_t v15 = *((void *)&v23 + 1);
  uint64_t v16 = v23;
  uint64_t v17 = *((void *)&v22 + 1);
  uint64_t v18 = v22;
  long long v25 = v21;
  float64x2_t v24 = v20;
  long long v27 = v23;
  long long v26 = v22;
  unint64_t result = SPPose3DIsValid(&v24);
  if (!result)
  {
    float64_t v14 = 0.0;
    float64_t v13 = 0.0;
    uint64_t v12 = 0;
    uint64_t v11 = 0;
    uint64_t v18 = 0;
    uint64_t v17 = 0;
    uint64_t v16 = 0;
    uint64_t v15 = 0;
  }
  *(float64_t *)a3  = v14;
  *(float64_t *)(a3 + 8)  = v13;
  *(void *)(a3 + 16)  = v12;
  *(void *)(a3 + 24)  = v11;
  *(void *)(a3 + 32)  = v18;
  *(void *)(a3 + 40)  = v17;
  *(void *)(a3 + 48)  = v16;
  *(void *)(a3 + 56)  = v15;
  *(unsigned char *)(a3 + 64)  = result ^ 1;
  return result;
}

void SPPose3DMakeWithProjectiveTransform(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  float64x2_t v5 = *(float64x2_t *)a1;
  float64x2_t v4 = *(float64x2_t *)(a1 + 16);
  float64x2_t v7 = *(float64x2_t *)(a1 + 32);
  float64x2_t v6 = *(float64x2_t *)(a1 + 48);
  float64x2_t v8 = *(float64x2_t *)(a1 + 64);
  float64x2_t v9 = *(float64x2_t *)(a1 + 80);
  v10.f64[0]  = *(float64_t *)(a1 + 80);
  v10.f64[1]  = *(float64_t *)(a1 + 64);
  v11.f64[0]  = *(float64_t *)(a1 + 48);
  v11.f64[1]  = *(float64_t *)(a1 + 32);
  float64x2_t v12 = vmulq_f64(*(float64x2_t *)a1, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)v9, 8uLL), vnegq_f64(v11)), v10, (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v6, 8uLL)));
  float64x2_t v14 = vmulq_f64(v4, vmlaq_laneq_f64(vmulq_f64(v8, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v7, 1))), v7, v8, 1));
  BOOL v13 = v14.f64[0] + vaddvq_f64(v12) < 0.0;
  v14.f64[0]  = -1.0;
  if (!v13) {
    v14.f64[0]  = 1.0;
  }
  v15.f64[0]  = sqrt(vmulq_f64(v4, v4).f64[0] + vaddvq_f64(vmulq_f64(v5, v5)));
  float64x2_t v16 = vmulq_f64(v9, v9);
  v16.f64[0]  = sqrt(v16.f64[0] + vaddvq_f64(vmulq_f64(v8, v8)));
  v15.f64[1]  = sqrt(vmulq_f64(v6, v6).f64[0] + vaddvq_f64(vmulq_f64(v7, v7)));
  float64x2_t v17 = vmulq_n_f64(v15, v14.f64[0]);
  float64x2_t v18 = vmulq_f64(v16, v14);
  float64x2_t v19 = vdivq_f64(v5, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v17.f64[0], 0));
  float64x2_t v20 = vdivq_f64(v4, v17);
  float64x2_t v21 = (float64x2_t)vdupq_laneq_s64((int64x2_t)v17, 1);
  float64x2_t v22 = vdivq_f64(v7, v21);
  float64x2_t v23 = vdivq_f64(v6, v21);
  float64x2_t v24 = vdivq_f64(v8, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v18.f64[0], 0));
  float64x2_t v25 = vdivq_f64(v9, v18);
  float64x2_t v26 = vmulq_f64(v23, v20);
  float64x2_t v27 = vmulq_f64(v20, v20);
  v27.f64[0]  = v27.f64[0] + vaddvq_f64(vmulq_f64(v19, v19));
  v26.f64[0]  = (v26.f64[0] + vaddvq_f64(vmulq_f64(v22, v19))) / v27.f64[0];
  float64x2_t v28 = vsubq_f64(v22, vmulq_n_f64(v19, v26.f64[0]));
  float64x2_t v29 = vsubq_f64(v23, vmulq_f64(v20, v26));
  float64x2_t v30 = vmulq_f64(v25, v20);
  v30.f64[0]  = (v30.f64[0] + vaddvq_f64(vmulq_f64(v24, v19))) / v27.f64[0];
  float64x2_t v31 = vmulq_n_f64(v19, v30.f64[0]);
  float64x2_t v32 = vsubq_f64(v25, vmulq_f64(v20, v30));
  float64x2_t v33 = vsubq_f64(v24, v31);
  float64x2_t v34 = vmulq_f64(v24, v28);
  v34.f64[0]  = vmulq_f64(v25, v29).f64[0] + vaddvq_f64(v34);
  float64x2_t v35 = vmulq_f64(v29, v29);
  v35.f64[0]  = v35.f64[0] + vaddvq_f64(vmulq_f64(v28, v28));
  v34.f64[0]  = v34.f64[0] / v35.f64[0];
  float64x2_t v36 = vmulq_f64(v29, v34);
  float64x2_t v37 = vsubq_f64(v33, vmulq_n_f64(v28, v34.f64[0]));
  float64x2_t v38 = vsubq_f64(v32, v36);
  v27.f64[0]  = 1.0 / sqrt(v27.f64[0]);
  v35.f64[0]  = 1.0 / sqrt(v35.f64[0]);
  v12.f64[0]  = 1.0 / sqrt(vmulq_f64(v38, v38).f64[0] + vaddvq_f64(vmulq_f64(v37, v37)));
  float64x2_t v49 = 0u;
  float64x2_t v50 = 0u;
  v48[0]  = vmulq_n_f64(v19, v27.f64[0]);
  v48[1]  = vmulq_f64(v20, v27);
  v48[2]  = vmulq_n_f64(v28, v35.f64[0]);
  v48[3]  = vmulq_f64(v29, v35);
  v48[4]  = vmulq_n_f64(v37, v12.f64[0]);
  v48[5]  = vmulq_f64(v38, v12);
  simd_quaternion((uint64_t)v48, (uint64_t)&v49);
  int64x2_t v39 = (int64x2_t)vornq_s8(vmvnq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v49), (int8x16_t)vcgezq_f64(v49))), vorrq_s8((int8x16_t)vcltzq_f64(v50), (int8x16_t)vcgezq_f64(v50)));
  unint64_t v40 = vorrq_s8((int8x16_t)v39, (int8x16_t)vdupq_laneq_s64(v39, 1)).u64[0];
  float64x2_t v41 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if ((v40 & 0x8000000000000000) != 0)
  {
    float64x2_t v44 = v41;
    float64x2_t v43 = v41;
  }
  else
  {
    double v42 = vaddvq_f64(vaddq_f64(vmulq_f64(v49, v49), vmulq_f64(v50, v50)));
    if (v42 == 0.0)
    {
      float64x2_t v43 = (float64x2_t)xmmword_228C1F7A0;
      float64x2_t v44 = 0uLL;
    }
    else
    {
      double v45 = 1.0 / sqrt(v42);
      float64x2_t v43 = vmulq_n_f64(v50, v45);
      float64x2_t v44 = vmulq_n_f64(v49, v45);
    }
  }
  int64x2_t v46 = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v44), (int8x16_t)vcgezq_f64(v44)), (int8x16_t)vceqq_f64(vabsq_f64(v44), v41)), vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v43), (int8x16_t)vcgezq_f64(v43)), (int8x16_t)vceqq_f64(vabsq_f64(v43), v41)));
  if ((vandq_s8((int8x16_t)v46, (int8x16_t)vdupq_laneq_s64(v46, 1)).u64[0] & 0x8000000000000000) != 0)
  {
    uint64_t v47 = *(void *)(a1 + 112);
    *(_OWORD *)a2  = *(_OWORD *)(a1 + 96);
    *(void *)(a2 + 16)  = v47;
    *(void *)(a2 + 24)  = 0;
    *(float64x2_t *)(a2 + 32)  = v44;
  }
  else
  {
    *(_OWORD *)a2  = SPPose3DInvalid_0;
    *(_OWORD *)(a2 + 16)  = unk_228C205B0;
    float64x2_t v43 = (float64x2_t)unk_228C205D0;
    *(_OWORD *)(a2 + 32)  = xmmword_228C205C0;
  }
  *(float64x2_t *)(a2 + 48)  = v43;
}

double SPPose3D.matrix.getter@<D0>(_OWORD *a1@<X8>)
{
  long long v3 = *(_OWORD *)(v1 + 16);
  long long v4 = *(_OWORD *)(v1 + 32);
  double v5 = *(double *)(v1 + 48);
  double v6 = *(double *)(v1 + 56);
  *(_OWORD *)&v16.position.x  = *(_OWORD *)v1;
  *(_OWORD *)&v16.position.vector.f64[2]  = v3;
  v16.rotation.vector.f64[2]  = v5;
  v16.rotation.vector.f64[3]  = v6;
  *(_OWORD *)v16.rotation.vector.f64  = v4;
  SPPose3DGet4x4Matrix(&v16, (uint64_t)v15);
  double result = *(double *)v15;
  long long v8 = v15[1];
  long long v9 = v15[2];
  long long v10 = v15[3];
  long long v11 = v15[4];
  long long v12 = v15[5];
  long long v13 = v15[6];
  long long v14 = v15[7];
  *a1  = v15[0];
  a1[1]  = v8;
  a1[2]  = v9;
  a1[3]  = v10;
  a1[4]  = v11;
  a1[5]  = v12;
  a1[6]  = v13;
  a1[7]  = v14;
  return result;
}

double SPPose3D.inverse.getter@<D0>(uint64_t a1@<X8>)
{
  long long v3 = *(_OWORD *)(v1 + 16);
  long long v4 = *(_OWORD *)(v1 + 32);
  double v5 = *(double *)(v1 + 48);
  double v6 = *(double *)(v1 + 56);
  *(_OWORD *)&v15.position.x  = *(_OWORD *)v1;
  *(_OWORD *)&v15.position.vector.f64[2]  = v3;
  v15.rotation.vector.f64[2]  = v5;
  v15.rotation.vector.f64[3]  = v6;
  *(_OWORD *)v15.rotation.vector.f64  = v4;
  SPPose3DGetInverse(&v15, (uint64_t)v12);
  double result = *(double *)v12;
  long long v8 = v12[1];
  long long v9 = v12[2];
  uint64_t v10 = v13;
  uint64_t v11 = v14;
  *(_OWORD *)a1  = v12[0];
  *(_OWORD *)(a1 + 16)  = v8;
  *(void *)(a1 + 48)  = v10;
  *(void *)(a1 + 56)  = v11;
  *(_OWORD *)(a1 + 32)  = v9;
  return result;
}

float64x2_t SPPose3DGetInverse@<Q0>(SPPose3D *a1@<X0>, uint64_t a2@<X8>)
{
  float64x2_t v2 = *(float64x2_t *)a1->rotation.vector.f64;
  float64x2_t v3 = *(float64x2_t *)&a1->rotation.quaternion.vector.f64[2];
  float64x2_t v4 = vnegq_f64(v2);
  v2.f64[0]  = 1.0 / vaddvq_f64(vaddq_f64(vmulq_f64(v2, v2), vmulq_f64(v3, v3)));
  float64x2_t v5 = vmulq_n_f64(vmulq_f64(v3, (float64x2_t)xmmword_228C1FC40), v2.f64[0]);
  float64x2_t result = vmulq_n_f64(v4, v2.f64[0]);
  long long v7 = *(_OWORD *)&a1->position.vector.f64[2];
  float64x2_t v8 = vmulq_f64(v5, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v9 = (int8x16_t)vnegq_f64(result);
  float64x2_t v10 = vnegq_f64(*(float64x2_t *)&a1->position.x);
  float64x2_t v11 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)vnegq_f64(v8), 8uLL);
  float64x2_t v12 = vmlsq_lane_f64(vmlsq_lane_f64(vmulq_laneq_f64(v8, v10, 1), v11, a1->position.x, 0), (float64x2_t)vextq_s8((int8x16_t)result, v9, 8uLL), *(double *)&v7, 0);
  float64x2_t v13 = vmlsq_lane_f64(vmlsq_lane_f64(vmulq_laneq_f64(result, v10, 1), (float64x2_t)vextq_s8(v9, (int8x16_t)result, 8uLL), a1->position.x, 0), v11, *(double *)&v7, 0);
  float64x2_t v14 = vnegq_f64(v12);
  float64x2_t v15 = (float64x2_t)vextq_s8((int8x16_t)v13, (int8x16_t)vnegq_f64(v13), 8uLL);
  *(float64x2_t *)a2  = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v12, v5, 1), (float64x2_t)vextq_s8((int8x16_t)v14, (int8x16_t)v12, 8uLL), v5.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v13, result, 1), v15, result.f64[0]));
  *(void *)(a2 + 16)  = *(_OWORD *)&vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v13, v5, 1), v15, v5.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v14, result, 1), (float64x2_t)vextq_s8((int8x16_t)v12, (int8x16_t)v14, 8uLL), result.f64[0]));
  *(float64x2_t *)(a2 + 32)  = result;
  *(float64x2_t *)(a2 + 48)  = v5;
  return result;
}

double static SPPose3D.identity.getter@<D0>(uint64_t a1@<X8>)
{
  double result = 0.0;
  *(_OWORD *)a1  = 0u;
  *(_OWORD *)(a1 + 16)  = 0u;
  *(void *)(a1 + 48)  = 0;
  *(void *)(a1 + 56)  = 0x3FF0000000000000;
  *(_OWORD *)(a1 + 32)  = 0u;
  return result;
}

void __swiftcall SPPose3D.flipped(along:)(SPPose3D *__return_ptr retstr, SPAxis *along)
{
  long long v4 = *(_OWORD *)(v2 + 16);
  long long v5 = *(_OWORD *)(v2 + 32);
  double v6 = *(double *)(v2 + 48);
  double v7 = *(double *)(v2 + 56);
  *(_OWORD *)&v15.position.x  = *(_OWORD *)v2;
  *(_OWORD *)&v15.position.vector.f64[2]  = v4;
  v15.rotation.vector.f64[2]  = v6;
  v15.rotation.vector.f64[3]  = v7;
  *(_OWORD *)v15.rotation.vector.f64  = v5;
  SPPose3DFlip(&v15, along, (uint64_t)v12);
  long long v8 = v12[1];
  long long v9 = v12[2];
  double v10 = v13;
  double v11 = v14;
  *(_OWORD *)&retstr->position.x  = v12[0];
  *(_OWORD *)&retstr->position.vector.f64[2]  = v8;
  retstr->rotation.vector.f64[2]  = v10;
  retstr->rotation.vector.f64[3]  = v11;
  *(_OWORD *)retstr->rotation.vector.f64  = v9;
}

void SPPose3DFlip(SPPose3D *a1@<X0>, unsigned int a2@<W1>, uint64_t a3@<X8>)
{
  if (a2 > 4 || ((1 << a2) & 0x16) == 0)
  {
    long long v44 = *(_OWORD *)&a1->position.vector.f64[2];
    *(_OWORD *)a3  = *(_OWORD *)&a1->position.x;
    *(_OWORD *)(a3 + 16)  = v44;
    long long v45 = *(_OWORD *)a1->rotation.vector.f64;
    long long v46 = *(_OWORD *)&a1->rotation.quaternion.vector.f64[2];
LABEL_48:
    *(_OWORD *)(a3 + 32)  = v45;
    *(_OWORD *)(a3 + 48)  = v46;
    return;
  }
  v133  = *(float64x2_t *)&a1->position.x;
  v135  = *(float64x2_t *)&a1->position.vector.f64[2];
  rotation  = a1->rotation;
  v159  = 0u;
  v160  = 0u;
  v157  = 0u;
  v158  = 0u;
  v155  = 0u;
  v156  = 0u;
  v153  = 0u;
  v154  = 0u;
  v145  = *(_OWORD *)rotation.vector.f64;
  v146  = *(float64x2_t *)&rotation.quaternion.vector.f64[2];
  simd_matrix4x4(rotation.quaternion, (float64x2_t *)&v145, (uint64_t)&v153);
  float64x2_t v6 = v153;
  float64x2_t v5 = v154;
  float64x2_t v8 = v155;
  float64x2_t v7 = v156;
  float64x2_t v10 = v157;
  float64x2_t v9 = v158;
  float64x2_t v12 = *(float64x2_t *)MEMORY[0x263EF8988];
  float64x2_t v11 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v14 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v13 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v17 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v155), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v153)), (int8x16_t)vceqq_f64(v16, v157));
  unint64_t v18 = vandq_s8((int8x16_t)vdupq_laneq_s64(v17, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v13, v156), (int8x16_t)vceqq_f64(v11, v154)), (int8x16_t)vceqq_f64(v15, v158)), (int8x16_t)v17)).u64[0];
  __asm { FMOV            V0.2D, #1.0 }
  int8x16_t v23 = (int8x16_t)vceqzq_f64(v11);
  if ((v18 & 0x8000000000000000) == 0) {
    goto LABEL_5;
  }
  __asm { FMOV            V25.2D, #1.0 }
  int64x2_t v25 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqq_f64(v12, (float64x2_t)xmmword_228C1F7D0)), (int8x16_t)vceqzq_f64(v16));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v25, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqzq_f64(v13), v23), (int8x16_t)vceqq_f64(v15, _Q25)), (int8x16_t)v25)).u64[0] & 0x8000000000000000) == 0)
  {
LABEL_5:
    uint64_t v26 = 0;
    v145  = xmmword_228C1F7D0;
    v146  = 0u;
    v147  = xmmword_228C1F7A0;
    v148  = 0u;
    v149  = 0uLL;
    v150  = _Q0;
    v153  = 0u;
    v154  = 0u;
    v155  = 0u;
    v156  = 0u;
    v157  = 0u;
    v158  = 0u;
    *(void *)&v6.f64[1]  = vextq_s8((int8x16_t)v6, (int8x16_t)v6, 8uLL).u64[0];
    *(void *)&v8.f64[1]  = vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL).u64[0];
    *(void *)&v10.f64[1]  = vextq_s8((int8x16_t)v10, (int8x16_t)v10, 8uLL).u64[0];
    do
    {
      float64x2_t v28 = *(float64x2_t *)((char *)&v145 + v26);
      float64x2_t v27 = *(float64x2_t *)((char *)&v145 + v26 + 16);
      float64x2_t v29 = (float64x2_t *)((char *)&v153 + v26);
      *float64x2_t v29 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v28.f64[0]), v8, v28, 1), v10, v27.f64[0]);
      v29[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v5, v28), v7, v28, 1), v27, v9);
      v26 += 32;
    }
    while (v26 != 96);
    float64x2_t v6 = v153;
    float64x2_t v5 = v154;
    float64x2_t v8 = v155;
    float64x2_t v7 = v156;
    float64x2_t v10 = v157;
    float64x2_t v9 = v158;
  }
  float64x2_t v31 = v133;
  float64x2_t v30 = v135;
  float64x2_t v32 = (float64x2_t)xmmword_228C1F7D0;
  int64x2_t v33 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqq_f64(v12, (float64x2_t)xmmword_228C1F7D0)), (int8x16_t)vceqzq_f64(v16));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v33, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqzq_f64(v13), v23), (int8x16_t)vceqq_f64(v15, _Q0)), (int8x16_t)v33)).u64[0] & 0x8000000000000000) != 0&& (int64x2_t v34 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v8), (int8x16_t)vceqq_f64(v16, v10)), (int8x16_t)vceqq_f64(v12, v6)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v34, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v13, v7), (int8x16_t)vceqq_f64(v15, v9)), (int8x16_t)vceqq_f64(v11, v5)), (int8x16_t)v34)).u64[0] & 0x8000000000000000) != 0))
  {
    float64x2_t v43 = 0uLL;
    float64x2_t v31 = vaddq_f64(v133, (float64x2_t)0);
    float64x2_t v30 = vaddq_f64(v135, (float64x2_t)0);
    float64x2_t v41 = (float64x2_t)xmmword_228C1F7A0;
    float64x2_t v42 = 0uLL;
    float64x2_t v40 = 0uLL;
  }
  else
  {
    int64x2_t v35 = vceqzq_f64(v133);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v35, 1), vandq_s8((int8x16_t)vceqzq_f64(v135), (int8x16_t)v35)).u64[0] & 0x8000000000000000) != 0)
    {
      uint64_t v47 = 0;
      v145  = (__int128)v6;
      v146  = v5;
      v147  = (__int128)v8;
      v148  = v7;
      v149  = v10;
      v150  = v9;
      v153  = 0u;
      v154  = 0u;
      v155  = 0u;
      v156  = 0u;
      v157  = 0u;
      v158  = 0u;
      do
      {
        float64x2_t v49 = *(float64x2_t *)((char *)&v145 + v47);
        float64x2_t v48 = *(float64x2_t *)((char *)&v145 + v47 + 16);
        float64x2_t v50 = (float64x2_t *)((char *)&v153 + v47);
        *float64x2_t v50 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)xmmword_228C1F7D0, v49.f64[0]), (float64x2_t)xmmword_228C1F7A0, v49, 1), (float64x2_t)0, v48.f64[0]);
        v50[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v49, (float64x2_t)0), (float64x2_t)0, v49, 1), _Q0, v48);
        v47 += 32;
      }
      while (v47 != 96);
      float64x2_t v32 = v153;
      float64x2_t v40 = v154;
      float64x2_t v41 = v155;
      float64x2_t v42 = v156;
      float64x2_t v43 = v157;
      _Q0  = v158;
    }
    else
    {
      uint64_t v36 = 0;
      v30.f64[1]  = 1.0;
      v145  = (__int128)v6;
      v146  = (float64x2_t)*(unint64_t *)&v5.f64[0];
      v147  = (__int128)v8;
      v148  = (float64x2_t)*(unint64_t *)&v7.f64[0];
      v149  = v10;
      v150  = (float64x2_t)*(unint64_t *)&v9.f64[0];
      v151  = 0uLL;
      v152  = xmmword_228C1F7A0;
      v153  = 0u;
      v154  = 0u;
      v155  = 0u;
      v156  = 0u;
      v157  = 0u;
      v158  = 0u;
      v159  = 0u;
      v160  = 0u;
      do
      {
        float64x2_t v38 = *(float64x2_t *)((char *)&v145 + v36);
        float64x2_t v37 = *(float64x2_t *)((char *)&v145 + v36 + 16);
        int64x2_t v39 = (float64x2_t *)((char *)&v153 + v36);
        *int64x2_t v39 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)xmmword_228C1F7D0, v38.f64[0]), (float64x2_t)xmmword_228C1F7A0, v38, 1), (float64x2_t)0, v37.f64[0]), v133, v37, 1);
        v39[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v38.f64[0]), (float64x2_t)0, v38, 1), (float64x2_t)xmmword_228C1F7D0, v37.f64[0]), v30, v37, 1);
        v36 += 32;
      }
      while (v36 != 128);
      float64x2_t v32 = v153;
      float64x2_t v40 = v154;
      float64x2_t v41 = v155;
      float64x2_t v42 = v156;
      float64x2_t v43 = v157;
      _Q0  = v158;
      float64x2_t v31 = v159;
      float64x2_t v30 = v160;
    }
  }
  _VF  = __OFSUB__(a2, 1);
  if (a2 == 1)
  {
    unsigned int v52 = 0;
  }
  else
  {
    _VF  = __OFSUB__(a2, 4);
    if (a2 == 4)
    {
      unsigned int v52 = 2;
    }
    else
    {
      _VF  = __OFSUB__(a2, 2);
      if (a2 != 2)
      {
LABEL_39:
        v134  = v31;
        v136  = v30.f64[0];
        goto LABEL_40;
      }
      unsigned int v52 = 1;
    }
  }
  v143.f64[1]  = 0.0;
  v144  = 0u;
  v141  = 0u;
  v139.f64[0]  = 0.0;
  v140  = 0u;
  v138  = 0u;
  v137  = (float64x2_t)0x3FF0000000000000uLL;
  v139.f64[1]  = 1.0;
  v142  = (float64x2_t)0x3FF0000000000000uLL;
  v137.f64[4 * v52 + v52]  = -1.0;
  v143.f64[0]  = 0.0;
  float64x2_t v54 = v137;
  float64x2_t v53 = v138;
  float64x2_t v56 = v139;
  float64x2_t v55 = v140;
  float64x2_t v58 = v141;
  float64x2_t v57 = v142;
  float64x2_t v60 = v143;
  float64x2_t v59 = v144;
  int64x2_t v61 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v139), (int8x16_t)vceqq_f64(v12, v137)), (int8x16_t)vceqq_f64(v16, v141));
  unint64_t v62 = vandq_s8((int8x16_t)vdupq_laneq_s64(v61, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v13, v140), (int8x16_t)vceqq_f64(v11, v138)), (int8x16_t)vceqq_f64(v15, v142)), (int8x16_t)v61)).u64[0];
  if ((v62 & 0x8000000000000000) != 0
    && (int64x2_t v63 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v41), (int8x16_t)vceqq_f64(v16, v43)), (int8x16_t)vceqq_f64(v12, v32)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v63, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v13, v42), (int8x16_t)vceqq_f64(v15, _Q0)), (int8x16_t)vceqq_f64(v11, v40)), (int8x16_t)v63)).u64[0] & 0x8000000000000000) != 0))
  {
    v74  = vaddq_f64(v31, v143);
    float64x2_t v43 = v141;
    _Q0  = v142;
    float64x2_t v30 = vaddq_f64(v30, v144);
    float64x2_t v41 = v139;
    float64x2_t v42 = v140;
    float64x2_t v32 = v137;
    float64x2_t v40 = v138;
  }
  else
  {
    v64  = vceqzq_f64(v143);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v64, 1), vandq_s8((int8x16_t)vceqzq_f64(v144), (int8x16_t)v64)).u64[0] & 0x8000000000000000) != 0
      && (v65  = vceqzq_f64(v31),
          (vandq_s8((int8x16_t)vdupq_laneq_s64(v65, 1), vandq_s8((int8x16_t)vceqzq_f64(v30), (int8x16_t)v65)).u64[0] & 0x8000000000000000) != 0))
    {
      v122  = 0;
      v145  = (__int128)v32;
      v146  = v40;
      v147  = (__int128)v41;
      v148  = v42;
      v149  = v43;
      v150  = _Q0;
      v153  = 0u;
      v154  = 0u;
      v155  = 0u;
      v156  = 0u;
      v157  = 0u;
      v158  = 0u;
      v123.f64[0]  = v137.f64[0];
      *(void *)&v123.f64[1]  = vextq_s8((int8x16_t)v137, (int8x16_t)v137, 8uLL).u64[0];
      v124.f64[0]  = v139.f64[0];
      *(void *)&v124.f64[1]  = vextq_s8((int8x16_t)v139, (int8x16_t)v139, 8uLL).u64[0];
      v125.f64[0]  = v141.f64[0];
      *(void *)&v125.f64[1]  = vextq_s8((int8x16_t)v141, (int8x16_t)v141, 8uLL).u64[0];
      do
      {
        v127  = *(float64x2_t *)((char *)&v145 + v122);
        v126  = *(float64x2_t *)((char *)&v145 + v122 + 16);
        v128  = (float64x2_t *)((char *)&v153 + v122);
        *v128  = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v123, v127.f64[0]), v124, v127, 1), v125, v126.f64[0]);
        v128[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v53, v127), v55, v127, 1), v126, v57);
        v122 += 32;
        _VF  = __OFSUB__(v122, 96);
      }
      while (v122 != 96);
      float64x2_t v32 = v153;
      float64x2_t v40 = v154;
      float64x2_t v41 = v155;
      float64x2_t v42 = v156;
      v74  = v60;
      float64x2_t v30 = v59;
      float64x2_t v43 = v157;
      _Q0  = v158;
    }
    else
    {
      uint64_t v66 = 0;
      float64x2_t v67 = (float64x2_t)*(unint64_t *)&v138.f64[0];
      float64x2_t v68 = (float64x2_t)*(unint64_t *)&v140.f64[0];
      float64x2_t v69 = (float64x2_t)*(unint64_t *)&v142.f64[0];
      v70.f64[0]  = v144.f64[0];
      v70.f64[1]  = 1.0;
      v30.f64[1]  = 1.0;
      v145  = (__int128)v32;
      v146  = (float64x2_t)*(unint64_t *)&v40.f64[0];
      v147  = (__int128)v41;
      v148  = (float64x2_t)*(unint64_t *)&v42.f64[0];
      v149  = v43;
      v150  = (float64x2_t)*(unint64_t *)&_Q0.f64[0];
      v151  = v31;
      v152  = (__int128)v30;
      v153  = 0u;
      v154  = 0u;
      v155  = 0u;
      v156  = 0u;
      v157  = 0u;
      v158  = 0u;
      v159  = 0u;
      v160  = 0u;
      do
      {
        float64x2_t v72 = *(float64x2_t *)((char *)&v145 + v66);
        float64x2_t v71 = *(float64x2_t *)((char *)&v145 + v66 + 16);
        v73  = (float64x2_t *)((char *)&v153 + v66);
        *v73  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v54, v72.f64[0]), v56, v72, 1), v58, v71.f64[0]), v60, v71, 1);
        v73[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v67, v72.f64[0]), v68, v72, 1), v69, v71.f64[0]), v70, v71, 1);
        v66 += 32;
        _VF  = __OFSUB__(v66, 128);
      }
      while (v66 != 128);
      float64x2_t v32 = v153;
      float64x2_t v40 = v154;
      float64x2_t v41 = v155;
      float64x2_t v42 = v156;
      float64x2_t v43 = v157;
      _Q0  = v158;
      v74  = v159;
      float64x2_t v30 = v160;
    }
  }
  v75  = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v13, v42), (int8x16_t)vceqq_f64(v15, _Q0)), (int8x16_t)vceqq_f64(v11, v40));
  v76  = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v41), (int8x16_t)vceqq_f64(v16, v43)), (int8x16_t)vceqq_f64(v12, v32));
  if (((vandq_s8((int8x16_t)vdupq_laneq_s64(v76, 1), vandq_s8(v75, (int8x16_t)v76)).u64[0] & v62 & 0x8000000000000000) != 0) != _VF)
  {
    float64x2_t v31 = vaddq_f64(v60, v74);
    *(void *)&v30.f64[0]  = *(_OWORD *)&vaddq_f64(v59, v30);
    goto LABEL_39;
  }
  v77  = vceqzq_f64(v74);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v77, 1), vandq_s8((int8x16_t)vceqzq_f64(v30), (int8x16_t)v77)).u64[0] & 0x8000000000000000) != 0
    && (v78  = vceqzq_f64(v60),
        (vandq_s8((int8x16_t)vdupq_laneq_s64(v78, 1), vandq_s8((int8x16_t)vceqzq_f64(v59), (int8x16_t)v78)).u64[0] & 0x8000000000000000) != 0))
  {
    v134  = v74;
    v136  = v30.f64[0];
    v129  = 0;
    v145  = (__int128)v54;
    v146  = v53;
    v147  = (__int128)v56;
    v148  = v55;
    v149  = v58;
    v150  = v57;
    v153  = 0u;
    v154  = 0u;
    v155  = 0u;
    v156  = 0u;
    v157  = 0u;
    v158  = 0u;
    *(void *)&v32.f64[1]  = vextq_s8((int8x16_t)v32, (int8x16_t)v32, 8uLL).u64[0];
    *(void *)&v41.f64[1]  = vextq_s8((int8x16_t)v41, (int8x16_t)v41, 8uLL).u64[0];
    *(void *)&v43.f64[1]  = vextq_s8((int8x16_t)v43, (int8x16_t)v43, 8uLL).u64[0];
    do
    {
      v131  = *(float64x2_t *)((char *)&v145 + v129);
      v130  = *(float64x2_t *)((char *)&v145 + v129 + 16);
      v132  = (float64x2_t *)((char *)&v153 + v129);
      *v132  = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v32, v131.f64[0]), v41, v131, 1), v43, v130.f64[0]);
      v132[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v40, v131), v42, v131, 1), v130, _Q0);
      v129 += 32;
    }
    while (v129 != 96);
    float64x2_t v32 = v153;
    float64x2_t v40 = v154;
    float64x2_t v41 = v155;
    float64x2_t v42 = v156;
    float64x2_t v43 = v157;
    _Q0  = v158;
  }
  else
  {
    v79  = 0;
    v40.f64[1]  = 0.0;
    v42.f64[1]  = 0.0;
    _Q0.f64[1]  = 0.0;
    v30.f64[1]  = 1.0;
    v59.f64[1]  = 1.0;
    v145  = (__int128)v54;
    v146  = (float64x2_t)*(unint64_t *)&v53.f64[0];
    v147  = (__int128)v56;
    v148  = (float64x2_t)*(unint64_t *)&v55.f64[0];
    v149  = v58;
    v150  = (float64x2_t)*(unint64_t *)&v57.f64[0];
    v151  = v60;
    v152  = (__int128)v59;
    v153  = 0u;
    v154  = 0u;
    v155  = 0u;
    v156  = 0u;
    v157  = 0u;
    v158  = 0u;
    v159  = 0u;
    v160  = 0u;
    do
    {
      v81  = *(float64x2_t *)((char *)&v145 + v79);
      v80  = *(float64x2_t *)((char *)&v145 + v79 + 16);
      v82  = (float64x2_t *)((char *)&v153 + v79);
      *v82  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v32, v81.f64[0]), v41, v81, 1), v43, v80.f64[0]), v74, v80, 1);
      v82[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v40, v81.f64[0]), v42, v81, 1), _Q0, v80.f64[0]), v30, v80, 1);
      v79 += 32;
    }
    while (v79 != 128);
    float64x2_t v32 = v153;
    float64x2_t v40 = v154;
    float64x2_t v41 = v155;
    float64x2_t v42 = v156;
    float64x2_t v43 = v157;
    _Q0  = v158;
    v134  = v159;
    v136  = v160.f64[0];
  }
LABEL_40:
  v83.f64[0]  = _Q0.f64[0];
  v83.f64[1]  = v43.f64[0];
  v84.f64[0]  = v42.f64[0];
  v84.f64[1]  = v41.f64[0];
  v85  = vmulq_f64(v32, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v43, (int8x16_t)_Q0, 8uLL), vnegq_f64(v84)), v83, (float64x2_t)vextq_s8((int8x16_t)v41, (int8x16_t)v42, 8uLL)));
  v86  = vmulq_f64(v40, vmlaq_laneq_f64(vmulq_f64(v43, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v41, 1))), v41, v43, 1));
  _NF  = v86.f64[0] + vaddvq_f64(v85) < 0.0;
  v86.f64[0]  = -1.0;
  if (!_NF) {
    v86.f64[0]  = 1.0;
  }
  v87.f64[0]  = sqrt(vmulq_f64(v40, v40).f64[0] + vaddvq_f64(vmulq_f64(v32, v32)));
  v88  = vmulq_f64(_Q0, _Q0);
  v88.f64[0]  = sqrt(v88.f64[0] + vaddvq_f64(vmulq_f64(v43, v43)));
  v87.f64[1]  = sqrt(vmulq_f64(v42, v42).f64[0] + vaddvq_f64(vmulq_f64(v41, v41)));
  v89  = vmulq_n_f64(v87, v86.f64[0]);
  v90  = vmulq_f64(v88, v86);
  v91  = vdivq_f64(v32, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v89.f64[0], 0));
  v92  = vdivq_f64(v40, v89);
  v93  = (float64x2_t)vdupq_laneq_s64((int64x2_t)v89, 1);
  v94  = vdivq_f64(v41, v93);
  v95  = vdivq_f64(v42, v93);
  v96  = vdivq_f64(v43, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v90.f64[0], 0));
  v97  = vdivq_f64(_Q0, v90);
  v98  = vmulq_f64(v95, v92);
  v99  = vmulq_f64(v92, v92);
  v99.f64[0]  = v99.f64[0] + vaddvq_f64(vmulq_f64(v91, v91));
  v98.f64[0]  = (v98.f64[0] + vaddvq_f64(vmulq_f64(v94, v91))) / v99.f64[0];
  v100  = vmulq_f64(v92, v98);
  v101  = vsubq_f64(v94, vmulq_n_f64(v91, v98.f64[0]));
  v102  = vsubq_f64(v95, v100);
  v103  = vmulq_f64(v97, v92);
  v103.f64[0]  = (v103.f64[0] + vaddvq_f64(vmulq_f64(v96, v91))) / v99.f64[0];
  v104  = vmulq_n_f64(v91, v103.f64[0]);
  v105  = vsubq_f64(v97, vmulq_f64(v92, v103));
  v106  = vsubq_f64(v96, v104);
  v107  = vmulq_f64(v97, v102);
  v107.f64[0]  = v107.f64[0] + vaddvq_f64(vmulq_f64(v96, v101));
  v108  = vmulq_f64(v102, v102);
  v108.f64[0]  = v108.f64[0] + vaddvq_f64(vmulq_f64(v101, v101));
  v107.f64[0]  = v107.f64[0] / v108.f64[0];
  v109  = vmulq_f64(v102, v107);
  v110  = vsubq_f64(v106, vmulq_n_f64(v101, v107.f64[0]));
  v111  = vsubq_f64(v105, v109);
  v99.f64[0]  = 1.0 / sqrt(v99.f64[0]);
  v108.f64[0]  = 1.0 / sqrt(v108.f64[0]);
  v85.f64[0]  = 1.0 / sqrt(vmulq_f64(v111, v111).f64[0] + vaddvq_f64(vmulq_f64(v110, v110)));
  v145  = 0u;
  v146  = 0u;
  v153  = vmulq_n_f64(v91, v99.f64[0]);
  v154  = vmulq_f64(v92, v99);
  v155  = vmulq_n_f64(v101, v108.f64[0]);
  v156  = vmulq_f64(v102, v108);
  v157  = vmulq_n_f64(v110, v85.f64[0]);
  v158  = vmulq_f64(v111, v85);
  simd_quaternion((uint64_t)&v153, (uint64_t)&v145);
  v112  = (int64x2_t)vornq_s8(vmvnq_s8(vorrq_s8((int8x16_t)vcltzq_f64((float64x2_t)v145), (int8x16_t)vcgezq_f64((float64x2_t)v145))), vorrq_s8((int8x16_t)vcltzq_f64(v146), (int8x16_t)vcgezq_f64(v146)));
  v113  = vorrq_s8((int8x16_t)v112, (int8x16_t)vdupq_laneq_s64(v112, 1)).u64[0];
  v114  = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if ((v113 & 0x8000000000000000) != 0)
  {
    v119  = v114;
    v118  = v114;
    v117  = v134;
    v116  = v136;
  }
  else
  {
    v115  = vaddvq_f64(vaddq_f64(vmulq_f64((float64x2_t)v145, (float64x2_t)v145), vmulq_f64(v146, v146)));
    v117  = v134;
    v116  = v136;
    v118  = (float64x2_t)xmmword_228C1F7A0;
    v119  = 0uLL;
    if (v115 != 0.0)
    {
      v120  = 1.0 / sqrt(v115);
      v118  = vmulq_n_f64(v146, v120);
      v119  = vmulq_n_f64((float64x2_t)v145, v120);
    }
  }
  v121  = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v119), (int8x16_t)vcgezq_f64(v119)), (int8x16_t)vceqq_f64(vabsq_f64(v119), v114)), vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v118), (int8x16_t)vcgezq_f64(v118)), (int8x16_t)vceqq_f64(vabsq_f64(v118), v114)));
  if ((vandq_s8((int8x16_t)v121, (int8x16_t)vdupq_laneq_s64(v121, 1)).u64[0] & 0x8000000000000000) == 0)
  {
    *(_OWORD *)a3  = SPPose3DInvalid_0;
    *(_OWORD *)(a3 + 16)  = unk_228C205B0;
    long long v45 = xmmword_228C205C0;
    long long v46 = unk_228C205D0;
    goto LABEL_48;
  }
  *(float64x2_t *)a3  = v117;
  *(float64_t *)(a3 + 16)  = v116;
  *(void *)(a3 + 24)  = 0;
  *(float64x2_t *)(a3 + 32)  = v119;
  *(float64x2_t *)(a3 + 48)  = v118;
}

Swift::Void __swiftcall SPPose3D.flip(along:)(SPAxis *along)
{
  long long v2 = *(_OWORD *)(v1 + 16);
  long long v3 = *(_OWORD *)(v1 + 32);
  double v4 = *(double *)(v1 + 48);
  double v5 = *(double *)(v1 + 56);
  *(_OWORD *)&v13.position.x  = *(_OWORD *)v1;
  *(_OWORD *)&v13.position.vector.f64[2]  = v2;
  v13.rotation.vector.f64[2]  = v4;
  v13.rotation.vector.f64[3]  = v5;
  *(_OWORD *)v13.rotation.vector.f64  = v3;
  SPPose3DFlip(&v13, along, (uint64_t)v10);
  long long v6 = v10[1];
  long long v7 = v10[2];
  uint64_t v8 = v11;
  uint64_t v9 = v12;
  *(_OWORD *)uint64_t v1 = v10[0];
  *(_OWORD *)(v1 + 16)  = v6;
  *(void *)(v1 + 48)  = v8;
  *(void *)(v1 + 56)  = v9;
  *(_OWORD *)(v1 + 32)  = v7;
}

void __swiftcall SPPose3D.rotated(by:)(SPPose3D *__return_ptr retstr, SPRotation3D *by)
{
  long long v5 = *(_OWORD *)(v2 + 16);
  long long v6 = *(_OWORD *)(v2 + 32);
  double v7 = *(double *)(v2 + 48);
  double v8 = *(double *)(v2 + 56);
  *(_OWORD *)&v18.position.x  = *(_OWORD *)v2;
  *(_OWORD *)&v18.position.vector.f64[2]  = v5;
  v18.rotation.vector.f64[2]  = v7;
  v18.rotation.vector.f64[3]  = v8;
  *(_OWORD *)v18.rotation.vector.f64  = v6;
  float64x2_t v16 = *(float64x2_t *)v3.vector.f64;
  long long v17 = *(_OWORD *)&v3.quaternion.vector.f64[2];
  SPPose3DRotate(&v18, v3, &v16, (uint64_t)v13);
  long long v9 = v13[1];
  long long v10 = v13[2];
  double v11 = v14;
  double v12 = v15;
  *(_OWORD *)&retstr->position.x  = v13[0];
  *(_OWORD *)&retstr->position.vector.f64[2]  = v9;
  retstr->rotation.vector.f64[2]  = v11;
  retstr->rotation.vector.f64[3]  = v12;
  *(_OWORD *)retstr->rotation.vector.f64  = v10;
}

float64x2_t SPPose3DRotate@<Q0>(SPPose3D *a1@<X0>, SPRotation3D a2@<0:Q0, 16:Q1>, float64x2_t *a3@<X1>, uint64_t a4@<X8>)
{
  float64x2_t v5 = *(float64x2_t *)a1->rotation.vector.f64;
  float64x2_t v4 = *(float64x2_t *)&a1->rotation.quaternion.vector.f64[2];
  float64x2_t v6 = a3[1];
  float64x2_t v7 = vnegq_f64(v5);
  float64x2_t v8 = (float64x2_t)vextq_s8((int8x16_t)v4, (int8x16_t)vnegq_f64(v4), 8uLL);
  float64x2_t v9 = vmlaq_n_f64(vmulq_laneq_f64(v7, *a3, 1), (float64x2_t)vextq_s8((int8x16_t)v5, (int8x16_t)v7, 8uLL), a3->f64[0]);
  float64x2_t v10 = vmlaq_n_f64(vmulq_laneq_f64(v4, *a3, 1), v8, a3->f64[0]);
  long long v11 = *(_OWORD *)&a1->position.vector.f64[2];
  *(_OWORD *)a4  = *(_OWORD *)&a1->position.x;
  *(_OWORD *)(a4 + 16)  = v11;
  float64x2_t result = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v4, v6, 1), v8, v6.f64[0]), v9);
  *(float64x2_t *)(a4 + 32)  = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v5, v6, 1), (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v5, 8uLL), v6.f64[0]), v10);
  *(float64x2_t *)(a4 + 48)  = result;
  return result;
}

{
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  long long v11;
  float64x2_t result;

  float64x2_t v5 = *(float64x2_t *)a1->rotation.vector.f64;
  float64x2_t v4 = *(float64x2_t *)&a1->rotation.quaternion.vector.f64[2];
  float64x2_t v6 = a3[1];
  float64x2_t v7 = vnegq_f64(v5);
  float64x2_t v8 = (float64x2_t)vextq_s8((int8x16_t)v4, (int8x16_t)vnegq_f64(v4), 8uLL);
  float64x2_t v9 = vmlaq_n_f64(vmulq_laneq_f64(v7, *a3, 1), (float64x2_t)vextq_s8((int8x16_t)v5, (int8x16_t)v7, 8uLL), a3->f64[0]);
  float64x2_t v10 = vmlaq_n_f64(vmulq_laneq_f64(v4, *a3, 1), v8, a3->f64[0]);
  long long v11 = *(_OWORD *)&a1->position.vector.f64[2];
  *(_OWORD *)a4  = *(_OWORD *)&a1->position.x;
  *(_OWORD *)(a4 + 16)  = v11;
  float64x2_t result = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v4, v6, 1), v8, v6.f64[0]), v9);
  *(float64x2_t *)(a4 + 32)  = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v5, v6, 1), (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v5, 8uLL), v6.f64[0]), v10);
  *(float64x2_t *)(a4 + 48)  = result;
  return result;
}

void __swiftcall SPPose3D.rotated(by:)(SPPose3D *__return_ptr retstr, simd_quatd *by)
{
  long long v5 = *(_OWORD *)(v2 + 16);
  long long v6 = *(_OWORD *)(v2 + 32);
  double v7 = *(double *)(v2 + 48);
  double v8 = *(double *)(v2 + 56);
  *(_OWORD *)&v18.position.x  = *(_OWORD *)v2;
  *(_OWORD *)&v18.position.vector.f64[2]  = v5;
  v18.rotation.vector.f64[2]  = v7;
  v18.rotation.vector.f64[3]  = v8;
  *(_OWORD *)v18.rotation.vector.f64  = v6;
  float64x2_t v16 = *(float64x2_t *)v3.vector.f64;
  long long v17 = *(_OWORD *)&v3.vector.f64[2];
  SPPose3DRotateByQuaternion(&v18, v3, &v16, (uint64_t)v13);
  long long v9 = v13[1];
  long long v10 = v13[2];
  double v11 = v14;
  double v12 = v15;
  *(_OWORD *)&retstr->position.x  = v13[0];
  *(_OWORD *)&retstr->position.vector.f64[2]  = v9;
  retstr->rotation.vector.f64[2]  = v11;
  retstr->rotation.vector.f64[3]  = v12;
  *(_OWORD *)retstr->rotation.vector.f64  = v10;
}

float64x2_t SPPose3DRotateByQuaternion@<Q0>(SPPose3D *a1@<X0>, simd_quatd a2@<0:Q0, 16:Q1>, float64x2_t *a3@<X1>, uint64_t a4@<X8>)
{
  float64x2_t v4 = a3[1];
  float64x2_t v6 = *(float64x2_t *)a1->rotation.vector.f64;
  float64x2_t v5 = *(float64x2_t *)&a1->rotation.quaternion.vector.f64[2];
  float64x2_t v7 = vnegq_f64(v6);
  float64x2_t v8 = (float64x2_t)vextq_s8((int8x16_t)v5, (int8x16_t)vnegq_f64(v5), 8uLL);
  float64x2_t v9 = vmlaq_n_f64(vmulq_laneq_f64(v7, *a3, 1), (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v7, 8uLL), a3->f64[0]);
  float64x2_t v10 = vmlaq_n_f64(vmulq_laneq_f64(v5, *a3, 1), v8, a3->f64[0]);
  float64x2_t v11 = vmlaq_n_f64(vmulq_laneq_f64(v5, v4, 1), v8, v4.f64[0]);
  float64x2_t result = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v6, v4, 1), (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v6, 8uLL), v4.f64[0]), v10);
  long long v13 = *(_OWORD *)&a1->position.vector.f64[2];
  *(_OWORD *)a4  = *(_OWORD *)&a1->position.x;
  *(_OWORD *)(a4 + 16)  = v13;
  *(float64x2_t *)(a4 + 32)  = result;
  *(float64x2_t *)(a4 + 48)  = vaddq_f64(v11, v9);
  return result;
}

{
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t result;
  long long v13;

  float64x2_t v4 = a3[1];
  float64x2_t v6 = *(float64x2_t *)a1->rotation.vector.f64;
  float64x2_t v5 = *(float64x2_t *)&a1->rotation.quaternion.vector.f64[2];
  float64x2_t v7 = vnegq_f64(v6);
  float64x2_t v8 = (float64x2_t)vextq_s8((int8x16_t)v5, (int8x16_t)vnegq_f64(v5), 8uLL);
  float64x2_t v9 = vmlaq_n_f64(vmulq_laneq_f64(v7, *a3, 1), (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v7, 8uLL), a3->f64[0]);
  float64x2_t v10 = vmlaq_n_f64(vmulq_laneq_f64(v5, *a3, 1), v8, a3->f64[0]);
  float64x2_t v11 = vmlaq_n_f64(vmulq_laneq_f64(v5, v4, 1), v8, v4.f64[0]);
  float64x2_t result = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v6, v4, 1), (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v6, 8uLL), v4.f64[0]), v10);
  long long v13 = *(_OWORD *)&a1->position.vector.f64[2];
  *(_OWORD *)a4  = *(_OWORD *)&a1->position.x;
  *(_OWORD *)(a4 + 16)  = v13;
  *(float64x2_t *)(a4 + 32)  = result;
  *(float64x2_t *)(a4 + 48)  = vaddq_f64(v11, v9);
  return result;
}

double protocol witness for Rotatable3D.rotated(by:) in conformance SPPose3D@<D0>(uint64_t a1@<X8>, SPRotation3D a2@<Q1:Q0>)
{
  long long v4 = *(_OWORD *)(v2 + 16);
  long long v5 = *(_OWORD *)(v2 + 32);
  double v6 = *(double *)(v2 + 48);
  double v7 = *(double *)(v2 + 56);
  *(_OWORD *)&v17.position.x  = *(_OWORD *)v2;
  *(_OWORD *)&v17.position.vector.f64[2]  = v4;
  v17.rotation.vector.f64[2]  = v6;
  v17.rotation.vector.f64[3]  = v7;
  *(_OWORD *)v17.rotation.vector.f64  = v5;
  v16[0]  = *(float64x2_t *)a2.vector.f64;
  v16[1]  = *(float64x2_t *)&a2.quaternion.vector.f64[2];
  SPPose3DRotate(&v17, a2, v16, (uint64_t)v13);
  double result = *(double *)v13;
  long long v9 = v13[1];
  long long v10 = v13[2];
  uint64_t v11 = v14;
  uint64_t v12 = v15;
  *(_OWORD *)a1  = v13[0];
  *(_OWORD *)(a1 + 16)  = v9;
  *(void *)(a1 + 48)  = v11;
  *(void *)(a1 + 56)  = v12;
  *(_OWORD *)(a1 + 32)  = v10;
  return result;
}

double protocol witness for Rotatable3D.rotated(by:) in conformance SPPose3D@<D0>(uint64_t a1@<X8>, simd_quatd a2@<Q1:Q0>)
{
  long long v4 = *(_OWORD *)(v2 + 16);
  long long v5 = *(_OWORD *)(v2 + 32);
  double v6 = *(double *)(v2 + 48);
  double v7 = *(double *)(v2 + 56);
  *(_OWORD *)&v17.position.x  = *(_OWORD *)v2;
  *(_OWORD *)&v17.position.vector.f64[2]  = v4;
  v17.rotation.vector.f64[2]  = v6;
  v17.rotation.vector.f64[3]  = v7;
  *(_OWORD *)v17.rotation.vector.f64  = v5;
  v16[0]  = *(float64x2_t *)a2.vector.f64;
  v16[1]  = *(float64x2_t *)&a2.vector.f64[2];
  SPPose3DRotateByQuaternion(&v17, a2, v16, (uint64_t)v13);
  double result = *(double *)v13;
  long long v9 = v13[1];
  long long v10 = v13[2];
  uint64_t v11 = v14;
  uint64_t v12 = v15;
  *(_OWORD *)a1  = v13[0];
  *(_OWORD *)(a1 + 16)  = v9;
  *(void *)(a1 + 48)  = v11;
  *(void *)(a1 + 56)  = v12;
  *(_OWORD *)(a1 + 32)  = v10;
  return result;
}

unint64_t static SPPose3D.== infix(_:_:)(uint64_t a1, double *a2)
{
  long long v2 = *(_OWORD *)a2;
  long long v3 = *((_OWORD *)a2 + 1);
  long long v4 = *((_OWORD *)a2 + 2);
  double v5 = a2[6];
  double v6 = a2[7];
  long long v7 = *(_OWORD *)(a1 + 16);
  long long v8 = *(_OWORD *)(a1 + 32);
  double v9 = *(double *)(a1 + 48);
  double v10 = *(double *)(a1 + 56);
  *(_OWORD *)&v13.position.x  = *(_OWORD *)a1;
  *(_OWORD *)&v13.position.vector.f64[2]  = v7;
  v13.rotation.vector.f64[2]  = v9;
  v13.rotation.vector.f64[3]  = v10;
  *(_OWORD *)v13.rotation.vector.f64  = v8;
  *(_OWORD *)&v12.position.x  = v2;
  *(_OWORD *)&v12.position.vector.f64[2]  = v3;
  v12.rotation.vector.f64[2]  = v5;
  v12.rotation.vector.f64[3]  = v6;
  *(_OWORD *)v12.rotation.vector.f64  = v4;
  return SPPose3DEqualToPose(&v13, &v12);
}

unint64_t SPPose3DEqualToPose(SPPose3D *a1, SPPose3D *a2)
{
  int64x2_t v2 = vceqq_f64(*(float64x2_t *)&a1->position.x, *(float64x2_t *)&a2->position.x);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v2, 1), vandq_s8((int8x16_t)vceqq_f64(*(float64x2_t *)&a1->position.vector.f64[2], *(float64x2_t *)&a2->position.vector.f64[2]), (int8x16_t)v2)).u64[0] & 0x8000000000000000) == 0)return 0; {
  float64x2_t v4 = *(float64x2_t *)a1->rotation.vector.f64;
  }
  float64x2_t v5 = *(float64x2_t *)&a1->rotation.quaternion.vector.f64[2];
  float64x2_t v6 = *(float64x2_t *)a2->rotation.vector.f64;
  float64x2_t v7 = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  int64x2_t v8 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(v4, v6), (int8x16_t)vceqq_f64(v5, v7));
  if ((vandq_s8((int8x16_t)v8, (int8x16_t)vdupq_laneq_s64(v8, 1)).u64[0] & 0x8000000000000000) != 0) {
    return 1;
  }
  int64x2_t v9 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(v4, vnegq_f64(v6)), (int8x16_t)vceqq_f64(v5, vnegq_f64(v7)));
  return vandq_s8((int8x16_t)v9, (int8x16_t)vdupq_laneq_s64(v9, 1)).u64[0] >> 63;
}

Swift::Bool __swiftcall SPPose3D.isApproximatelyEqual(to:tolerance:)(SPPose3D *to, Swift::Double tolerance)
{
  long long v3 = *(_OWORD *)&to->position.x;
  long long v4 = *(_OWORD *)&to->position.vector.f64[2];
  long long v5 = *(_OWORD *)to->rotation.vector.f64;
  double v6 = to->rotation.vector.f64[2];
  double v7 = to->rotation.vector.f64[3];
  long long v8 = *(_OWORD *)(v2 + 16);
  long long v9 = *(_OWORD *)(v2 + 32);
  double v10 = *(double *)(v2 + 48);
  double v11 = *(double *)(v2 + 56);
  *(_OWORD *)&v14.position.x  = *(_OWORD *)v2;
  *(_OWORD *)&v14.position.vector.f64[2]  = v8;
  v14.rotation.vector.f64[2]  = v10;
  v14.rotation.vector.f64[3]  = v11;
  *(_OWORD *)v14.rotation.vector.f64  = v9;
  *(_OWORD *)&v13.position.x  = v3;
  *(_OWORD *)&v13.position.vector.f64[2]  = v4;
  v13.rotation.vector.f64[2]  = v6;
  v13.rotation.vector.f64[3]  = v7;
  *(_OWORD *)v13.rotation.vector.f64  = v5;
  return SPPose3DAlmostEqualToPose(&v14, &v13, tolerance);
}

uint64_t SPPose3DAlmostEqualToPose(SPPose3D *a1, SPPose3D *a2, double a3)
{
  float64x2_t v3 = *(float64x2_t *)a1->rotation.vector.f64;
  float64x2_t v4 = *(float64x2_t *)a2->rotation.vector.f64;
  BOOL v7 = vabdd_f64(v3.f64[0], v4.f64[0]) < a3
    && (float64x2_t v5 = vsubq_f64(v3, v4),
        float64x2_t v6 = vsubq_f64(*(float64x2_t *)&a1->rotation.quaternion.vector.f64[2], *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2]), v5.f64[0] = v6.f64[0], *(int32x2_t *)&v5.f64[0] = vmovn_s64(vcgtq_f64((float64x2_t)vdupq_lane_s64(*(uint64_t *)&a3, 0), vabsq_f64(v5))), (HIDWORD(v5.f64[0]) & LODWORD(v5.f64[0]) & 1) != 0)&& fabs(v6.f64[1]) < a3;
  BOOL v8 = vabdd_f64(a1->position.x, a2->position.x) < a3
    && vabdd_f64(a1->position.y, a2->position.y) < a3
    && vabdd_f64(a1->position.z, a2->position.z) < a3;
  return v7 & v8;
}

uint64_t SPPose3D.isIdentity.getter()
{
  long long v1 = *(_OWORD *)(v0 + 16);
  long long v2 = *(_OWORD *)(v0 + 32);
  double v3 = *(double *)(v0 + 48);
  double v4 = *(double *)(v0 + 56);
  *(_OWORD *)&v6.position.x  = *(_OWORD *)v0;
  *(_OWORD *)&v6.position.vector.f64[2]  = v1;
  v6.rotation.vector.f64[2]  = v3;
  v6.rotation.vector.f64[3]  = v4;
  *(_OWORD *)v6.rotation.vector.f64  = v2;
  return SPPose3DIsIdentity(&v6);
}

uint64_t SPPose3DIsIdentity(SPPose3D *a1)
{
  long long v1 = *(_OWORD *)a1->rotation.vector.f64;
  if (fabs(*(double *)&v1) >= 0.0000000149011612)
  {
    BOOL v3 = 0;
  }
  else
  {
    double v2 = fabs(a1->rotation.vector.f64[2]);
    BOOL v3 = fabs(a1->rotation.vector.f64[3] + -1.0) < 0.0000000149011612;
    if (fabs(*((double *)&v1 + 1)) >= 0.0000000149011612 || v2 >= 0.0000000149011612) {
      BOOL v3 = 0;
    }
  }
  double v5 = fabs(a1->position.y);
  BOOL v7 = fabs(a1->position.x) < 0.0000000149011612 && v5 < 0.0000000149011612 && fabs(a1->position.z) < 0.0000000149011612;
  return v3 & v7;
}

unint64_t protocol witness for static Equatable.== infix(_:_:) in conformance SPPose3D(uint64_t a1, double *a2)
{
  long long v2 = *(_OWORD *)(a1 + 16);
  long long v3 = *(_OWORD *)(a1 + 32);
  double v4 = *(double *)(a1 + 48);
  double v5 = *(double *)(a1 + 56);
  long long v6 = *(_OWORD *)a2;
  long long v7 = *((_OWORD *)a2 + 1);
  long long v8 = *((_OWORD *)a2 + 2);
  double v9 = a2[6];
  double v10 = a2[7];
  *(_OWORD *)&v13.position.x  = *(_OWORD *)a1;
  *(_OWORD *)&v13.position.vector.f64[2]  = v2;
  v13.rotation.vector.f64[2]  = v4;
  v13.rotation.vector.f64[3]  = v5;
  *(_OWORD *)v13.rotation.vector.f64  = v3;
  *(_OWORD *)&v12.position.x  = v6;
  *(_OWORD *)&v12.position.vector.f64[2]  = v7;
  v12.rotation.vector.f64[2]  = v9;
  v12.rotation.vector.f64[3]  = v10;
  *(_OWORD *)v12.rotation.vector.f64  = v8;
  return SPPose3DEqualToPose(&v13, &v12);
}

void SPPose3D.hash(into:)()
{
  specialized SIMD.hash(into:)(*(__n128 *)v0, *(double *)(v0 + 16));
  v1.i64[0]  = 0;
  float64x2_t v2 = *(float64x2_t *)(v0 + 32);
  int8x16_t v3 = (int8x16_t)vdupq_lane_s64(vcgtq_s64(v1, (int64x2_t)v2).i64[0], 0);
  __n128 v4 = (__n128)vbslq_s8(v3, (int8x16_t)vnegq_f64(v2), (int8x16_t)v2);
  __n128 v5 = (__n128)vbslq_s8(v3, (int8x16_t)vnegq_f64(*(float64x2_t *)(v0 + 48)), *(int8x16_t *)(v0 + 48));

  specialized SIMD.hash(into:)(v4, v5);
}

Swift::Int SPPose3D.hashValue.getter()
{
  Hasher.init(_seed:)();
  specialized SIMD.hash(into:)(*(__n128 *)v0, *(double *)(v0 + 16));
  v1.i64[0]  = 0;
  float64x2_t v2 = *(float64x2_t *)(v0 + 32);
  int8x16_t v3 = (int8x16_t)vdupq_lane_s64(vcgtq_s64(v1, (int64x2_t)v2).i64[0], 0);
  specialized SIMD.hash(into:)((__n128)vbslq_s8(v3, (int8x16_t)vnegq_f64(v2), (int8x16_t)v2), (__n128)vbslq_s8(v3, (int8x16_t)vnegq_f64(*(float64x2_t *)(v0 + 48)), *(int8x16_t *)(v0 + 48)));
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance SPPose3D()
{
  float64x2_t v2 = *(float64x2_t *)(v0 + 32);
  float64x2_t v3 = *(float64x2_t *)(v0 + 48);
  specialized SIMD.hash(into:)(*(__n128 *)v0, *(double *)(v0 + 16));
  *(float64x2_t *)v1.vector.f64  = v2;
  if ((*(void *)&v2.f64[0] & 0x8000000000000000) != 0)
  {
    v5[0]  = v2;
    *(float64x2_t *)v1.vector.f64  = v3;
    v5[1]  = v3;
    simd_negate(v1, v5, (float64x2_t *)&v4);
    simd_quatd v1 = v4;
  }
  else
  {
    *(float64x2_t *)&v1.vector.f64[2]  = v3;
  }

  specialized SIMD.hash(into:)(*(__n128 *)v1.vector.f64, *(__n128 *)&v1.vector.f64[2]);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPPose3D()
{
  float64x2_t v5 = v0[1];
  float64x2_t v6 = v0[3];
  __n128 v3 = *(__n128 *)v0;
  float64x2_t v4 = v0[2];
  Hasher.init(_seed:)();
  specialized SIMD.hash(into:)(v3, v5.f64[0]);
  *(float64x2_t *)v1.vector.f64  = v4;
  if ((*(void *)&v4.f64[0] & 0x8000000000000000) != 0)
  {
    v8[0]  = v4;
    *(float64x2_t *)v1.vector.f64  = v6;
    v8[1]  = v6;
    simd_negate(v1, v8, (float64x2_t *)&v7);
    simd_quatd v1 = v7;
  }
  else
  {
    *(float64x2_t *)&v1.vector.f64[2]  = v6;
  }
  specialized SIMD.hash(into:)(*(__n128 *)v1.vector.f64, *(__n128 *)&v1.vector.f64[2]);
  return Hasher._finalize()();
}

uint64_t protocol witness for static Equatable.== infix(_:_:) in conformance SPPose3D.CodingKeys(char *a1, char *a2)
{
  return specialized == infix<A>(_:_:)(*a1, *a2);
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPPose3D.CodingKeys()
{
  return Hasher._finalize()();
}

uint64_t protocol witness for Hashable.hash(into:) in conformance SPPose3D.CodingKeys()
{
  String.hash(into:)();

  return swift_bridgeObjectRelease();
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPPose3D.CodingKeys()
{
  return Hasher._finalize()();
}

uint64_t protocol witness for RawRepresentable.init(rawValue:) in conformance SPPose3D.CodingKeys@<X0>(Swift::String *a1@<X0>, char *a2@<X8>)
{
  Swift::Int v3 = _findStringSwitchCase(cases:string:)((Swift::OpaquePointer)&outlined read-only object #0 of SPPose3D.CodingKeys.init(rawValue:), *a1);
  uint64_t result = swift_bridgeObjectRelease();
  if (v3 == 1) {
    char v5 = 1;
  }
  else {
    char v5 = 2;
  }
  if (!v3) {
    char v5 = 0;
  }
  *a2  = v5;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance SPPose3D.CodingKeys(uint64_t *a1@<X8>)
{
  uint64_t v2 = 0x6E6F697469736F70;
  if (*v1) {
    uint64_t v2 = 0x6E6F697461746F72;
  }
  *a1  = v2;
  a1[1]  = 0xE800000000000000;
}

uint64_t protocol witness for CodingKey.stringValue.getter in conformance SPPose3D.CodingKeys()
{
  if (*v0) {
    return 0x6E6F697461746F72;
  }
  else {
    return 0x6E6F697469736F70;
  }
}

uint64_t protocol witness for CodingKey.init(stringValue:) in conformance SPPose3D.CodingKeys@<X0>(Swift::String string@<0:X0, 8:X1>, char *a2@<X8>)
{
  object  = string._object;
  v3._countAndFlagsBits  = string._countAndFlagsBits;
  v3._object  = object;
  Swift::Int v5 = _findStringSwitchCase(cases:string:)((Swift::OpaquePointer)&outlined read-only object #0 of SPPose3D.CodingKeys.init(rawValue:), v3);
  uint64_t result = swift_bridgeObjectRelease();
  if (v5 == 1) {
    char v7 = 1;
  }
  else {
    char v7 = 2;
  }
  if (!v5) {
    char v7 = 0;
  }
  *a2  = v7;
  return result;
}

void protocol witness for CodingKey.init(intValue:) in conformance SPPose3D.CodingKeys(unsigned char *a1@<X8>)
{
  *a1  = 2;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPPose3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys();

  return MEMORY[0x270FA00B0](a1, v2);
}

uint64_t protocol witness for CustomDebugStringConvertible.debugDescription.getter in conformance SPPose3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys();

  return MEMORY[0x270FA00B8](a1, v2);
}

uint64_t SPPose3D.encode(to:)(void *a1)
{
  Swift::String v3 = v1;
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedEncodingContainer<SPPose3D.CodingKeys>);
  uint64_t v6 = *(void *)(v5 - 8);
  MEMORY[0x270FA5388](v5);
  long long v8 = (char *)&v12 - ((v7 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys();
  dispatch thunk of Encoder.container<A>(keyedBy:)();
  long long v9 = v3[1];
  long long v14 = *v3;
  long long v15 = v9;
  char v13 = 0;
  type metadata accessor for SPPoint3D(0);
  _sSo9SPPoint3DaABSE7SpatialWlTm_0(&lazy protocol witness table cache variable for type SPPoint3D and conformance SPPoint3D, type metadata accessor for SPPoint3D);
  KeyedEncodingContainer.encode<A>(_:forKey:)();
  if (!v2)
  {
    long long v10 = v3[3];
    long long v14 = v3[2];
    long long v15 = v10;
    char v13 = 1;
    type metadata accessor for SPRotation3D(0);
    _sSo9SPPoint3DaABSE7SpatialWlTm_0(&lazy protocol witness table cache variable for type SPRotation3D and conformance SPRotation3D, type metadata accessor for SPRotation3D);
    KeyedEncodingContainer.encode<A>(_:forKey:)();
  }
  return (*(uint64_t (**)(char *, uint64_t))(v6 + 8))(v8, v5);
}

double SPPose3D.init(from:)@<D0>(void *a1@<X0>, _OWORD *a2@<X8>)
{
  specialized SPPose3D.init(from:)(a1, (uint64_t)v7);
  if (!v2)
  {
    long long v5 = v7[1];
    *a2  = v7[0];
    a2[1]  = v5;
    double result = *(double *)&v8;
    long long v6 = v9;
    a2[2]  = v8;
    a2[3]  = v6;
  }
  return result;
}

double protocol witness for Decodable.init(from:) in conformance SPPose3D@<D0>(void *a1@<X0>, _OWORD *a2@<X8>)
{
  specialized SPPose3D.init(from:)(a1, (uint64_t)v7);
  if (!v2)
  {
    long long v5 = v7[1];
    *a2  = v7[0];
    a2[1]  = v5;
    double result = *(double *)&v8;
    long long v6 = v9;
    a2[2]  = v8;
    a2[3]  = v6;
  }
  return result;
}

uint64_t protocol witness for Encodable.encode(to:) in conformance SPPose3D(void *a1)
{
  return SPPose3D.encode(to:)(a1);
}

uint64_t SPPose3D.description.getter()
{
  _StringGuts.grow(_:)(28);
  swift_bridgeObjectRelease();
  _StringGuts.grow(_:)(21);
  v0._countAndFlagsBits  = 540702760;
  v0._object  = (void *)0xE400000000000000;
  String.append(_:)(v0);
  Double.write<A>(to:)();
  v1._countAndFlagsBits  = 0x203A79202CLL;
  v1._object  = (void *)0xE500000000000000;
  String.append(_:)(v1);
  Double.write<A>(to:)();
  v2._countAndFlagsBits  = 0x203A7A202CLL;
  v2._object  = (void *)0xE500000000000000;
  String.append(_:)(v2);
  Double.write<A>(to:)();
  v3._countAndFlagsBits  = 41;
  v3._object  = (void *)0xE100000000000000;
  String.append(_:)(v3);
  v4._countAndFlagsBits  = 0;
  v4._object  = (void *)0xE000000000000000;
  String.append(_:)(v4);
  swift_bridgeObjectRelease();
  v5._countAndFlagsBits  = 0x697461746F72202CLL;
  v5._object  = (void *)0xEC000000203A6E6FLL;
  String.append(_:)(v5);
  v10._countAndFlagsBits  = 0;
  v10._object  = (void *)0xE000000000000000;
  _StringGuts.grow(_:)(16);
  v6._countAndFlagsBits  = 0x6E72657461757128;
  v6._object  = (void *)0xED0000203A6E6F69;
  String.append(_:)(v6);
  type metadata accessor for simd_quatd(0);
  _print_unlocked<A, B>(_:_:)();
  v7._countAndFlagsBits  = 41;
  v7._object  = (void *)0xE100000000000000;
  String.append(_:)(v7);
  String.append(_:)(v10);
  swift_bridgeObjectRelease();
  v8._countAndFlagsBits  = 41;
  v8._object  = (void *)0xE100000000000000;
  String.append(_:)(v8);
  return 0x6F697469736F7028;
}

uint64_t SPPose3D.customMirror.getter()
{
  uint64_t v1 = type metadata accessor for Mirror.AncestorRepresentation();
  uint64_t v2 = *(void *)(v1 - 8);
  MEMORY[0x270FA5388](v1);
  Swift::String v4 = (char *)v21 - ((v3 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName((uint64_t *)&demangling cache variable for type metadata for Mirror.DisplayStyle?);
  MEMORY[0x270FA5388](v5 - 8);
  Swift::String v7 = (char *)v21 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  long long v8 = v0[1];
  v21[0]  = *v0;
  v21[1]  = v8;
  long long v9 = v0[3];
  v21[2]  = v0[2];
  v21[3]  = v9;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<(String, Any)>);
  uint64_t v10 = swift_allocObject();
  *(_OWORD *)(v10 + 16)  = xmmword_228C202E0;
  *(void *)(v10 + 32)  = 0x6E6F697469736F70;
  *(void *)(v10 + 40)  = 0xE800000000000000;
  type metadata accessor for SPPoint3D(0);
  *(void *)(v10 + 72)  = v11;
  uint64_t v12 = swift_allocObject();
  *(void *)(v10 + 48)  = v12;
  long long v13 = v0[1];
  *(_OWORD *)(v12 + 16)  = *v0;
  *(_OWORD *)(v12 + 32)  = v13;
  *(void *)(v10 + 80)  = 0x6E6F697461746F72;
  *(void *)(v10 + 88)  = 0xE800000000000000;
  type metadata accessor for SPRotation3D(0);
  *(void *)(v10 + 120)  = v14;
  uint64_t v15 = swift_allocObject();
  *(void *)(v10 + 96)  = v15;
  long long v16 = v0[3];
  *(_OWORD *)(v15 + 16)  = v0[2];
  *(_OWORD *)(v15 + 32)  = v16;
  uint64_t v17 = *MEMORY[0x263F8E808];
  uint64_t v18 = type metadata accessor for Mirror.DisplayStyle();
  uint64_t v19 = *(void *)(v18 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v19 + 104))(v7, v17, v18);
  (*(void (**)(char *, void, uint64_t, uint64_t))(v19 + 56))(v7, 0, 1, v18);
  (*(void (**)(char *, void, uint64_t))(v2 + 104))(v4, *MEMORY[0x263F8E830], v1);
  type metadata accessor for SPPose3D(0);
  return Mirror.init<A>(_:children:displayStyle:ancestorRepresentation:)();
}

float64x2_t simd_negate@<Q0>(simd_quatd a1@<0:Q0, 16:Q1>, float64x2_t *a2@<X0>, float64x2_t *a3@<X8>)
{
  float64x2_t result = vnegq_f64(a2[1]);
  *a3  = vnegq_f64(*a2);
  a3[1]  = result;
  return result;
}

unint64_t lazy protocol witness table accessor for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys()
{
  unint64_t result = lazy protocol witness table cache variable for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys);
  }
  return result;
}

uint64_t specialized SPPose3D.init(from:)@<X0>(void *a1@<X0>, uint64_t a2@<X8>)
{
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedDecodingContainer<SPPose3D.CodingKeys>);
  uint64_t v6 = *(void *)(v5 - 8);
  MEMORY[0x270FA5388](v5);
  long long v8 = (char *)&v15 - ((v7 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPPose3D.CodingKeys and conformance SPPose3D.CodingKeys();
  dispatch thunk of Decoder.container<A>(keyedBy:)();
  if (v2) {
    return __swift_destroy_boxed_opaque_existential_1(a1);
  }
  type metadata accessor for SPPoint3D(0);
  LOBYTE(v18[0])  = 0;
  _sSo9SPPoint3DaABSE7SpatialWlTm_0(&lazy protocol witness table cache variable for type SPPoint3D and conformance SPPoint3D, type metadata accessor for SPPoint3D);
  KeyedDecodingContainer.decode<A>(_:forKey:)();
  *(void *)&long long v17 = v31;
  double v9 = v29;
  *(void *)&long long v16 = v30;
  double v10 = v28;
  type metadata accessor for SPRotation3D(0);
  LOBYTE(v18[0])  = 1;
  _sSo9SPPoint3DaABSE7SpatialWlTm_0(&lazy protocol witness table cache variable for type SPRotation3D and conformance SPRotation3D, type metadata accessor for SPRotation3D);
  KeyedDecodingContainer.decode<A>(_:forKey:)();
  *(_OWORD *)v32.vector.f64  = v25;
  v24.x  = v10;
  v24.y  = v9;
  *(void *)&v24.double z = v16;
  *(void *)&v24.vector.f64[3]  = v17;
  uint64_t v22 = v26;
  uint64_t v23 = v27;
  long long v21 = v25;
  SPPose3DMake(&v24, v32, (uint64_t)&v21, v18);
  long long v16 = v18[0];
  long long v15 = v18[1];
  long long v17 = v18[2];
  uint64_t v11 = v19;
  uint64_t v12 = v20;
  (*(void (**)(char *, uint64_t))(v6 + 8))(v8, v5);
  uint64_t result = __swift_destroy_boxed_opaque_existential_1(a1);
  long long v14 = v15;
  *(_OWORD *)a2  = v16;
  *(_OWORD *)(a2 + 16)  = v14;
  *(void *)(a2 + 48)  = v11;
  *(void *)(a2 + 56)  = v12;
  *(_OWORD *)(a2 + 32)  = v17;
  return result;
}

uint64_t sub_228BFB444()
{
  return MEMORY[0x270FA0238](v0, 48, 15);
}

uint64_t base witness table accessor for Equatable in SPPose3D()
{
  return _sSo9SPPoint3DaABSE7SpatialWlTm_0(&lazy protocol witness table cache variable for type SPPose3D and conformance SPPose3D, type metadata accessor for SPPose3D);
}

uint64_t getEnumTagSinglePayload for SPPose3D.CodingKeys(unsigned __int8 *a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0xFF) {
    goto LABEL_17;
  }
  if (a2 + 1 >= 0xFFFF00) {
    int v2 = 4;
  }
  else {
    int v2 = 2;
  }
  if ((a2 + 1) >> 8 < 0xFF) {
    int v3 = 1;
  }
  else {
    int v3 = v2;
  }
  if (v3 == 4)
  {
    int v4 = *(_DWORD *)(a1 + 1);
    if (v4) {
      return (*a1 | (v4 << 8)) - 1;
    }
  }
  else
  {
    if (v3 == 2)
    {
      int v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1)) {
        goto LABEL_17;
      }
      return (*a1 | (v4 << 8)) - 1;
    }
    int v4 = a1[1];
    if (a1[1]) {
      return (*a1 | (v4 << 8)) - 1;
    }
  }
LABEL_17:
  unsigned int v6 = *a1;
  BOOL v7 = v6 >= 2;
  int v8 = v6 - 2;
  if (!v7) {
    int v8 = -1;
  }
  return (v8 + 1);
}

unsigned char *storeEnumTagSinglePayload for SPPose3D.CodingKeys(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 1 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFF) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFE)
  {
    unsigned int v6 = ((a2 - 255) >> 8) + 1;
    *uint64_t result = a2 + 1;
    switch(v5)
    {
      case 1:
        result[1]  = v6;
        break;
      case 2:
        *(_WORD *)(result + 1)  = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x228BFB5F8);
      case 4:
        *(_DWORD *)(result + 1)  = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1]  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1)  = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1)  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 1;
        break;
    }
  }
  return result;
}

unsigned char *destructiveInjectEnumTag for SPPose3D.CodingKeys(unsigned char *result, char a2)
{
  *uint64_t result = a2 & 1;
  return result;
}

ValueMetadata *type metadata accessor for SPPose3D.CodingKeys()
{
  return &type metadata for SPPose3D.CodingKeys;
}

uint64_t _sSo9SPPoint3DaABSE7SpatialWlTm_0(unint64_t *a1, void (*a2)(uint64_t))
{
  uint64_t result = *a1;
  if (!result)
  {
    a2(255);
    uint64_t result = swift_getWitnessTable();
    atomic_store(result, a1);
  }
  return result;
}

double simd_matrix4x4@<D0>(simd_quatd a1@<0:Q0, 16:Q1>, float64x2_t *a2@<X0>, uint64_t a3@<X8>)
{
  _Q1  = *a2;
  _Q0  = a2[1];
  _D2  = a2->f64[1];
  __asm { FMLS            D3, D0, V0.D[0] }
  _D5  = a2[1].f64[1];
  __asm { FMLA            D3, D5, V0.D[1] }
  double v13 = vmlad_n_f64(vmuld_lane_f64(_Q0.f64[0], _Q0, 1), _D2, a2->f64[0]);
  v14.f64[0]  = vmuld_lane_f64(_D2, _Q0, 1);
  double v15 = vmlad_n_f64(-(_D2 * _D5), _Q0.f64[0], a2->f64[0]);
  *((double *)&_Q3 + 1)  = v13 + v13;
  *(_OWORD *)a3  = _Q3;
  *(_OWORD *)(a3 + 16)  = COERCE_UNSIGNED_INT64(v15 + v15);
  *(double *)&_Q3  = vmlad_n_f64(-(_Q0.f64[0] * _D5), _D2, _Q1.f64[0]);
  *(double *)&_Q3  = *(double *)&_Q3 + *(double *)&_Q3;
  __asm
  {
    FMLA            D4, D2, V1.D[1]
    FMLA            D4, D5, V0.D[1]
    FMLS            D4, D1, V1.D[0]
    FMLA            D6, D0, V1.D[1]
  }
  *((void *)&_Q3 + 1)  = _D4;
  *(double *)&unint64_t v20 = _D6 + _D6;
  float64_t v21 = -(_Q1.f64[0] * _D5);
  float64x2_t v22 = (float64x2_t)vzip1q_s64((int64x2_t)_Q1, (int64x2_t)_Q0);
  __asm
  {
    FMLS            D5, D1, V1.D[0]
    FMLS            D5, D2, V1.D[1]
  }
  _Q1.f64[0]  = _Q0.f64[0];
  v14.f64[1]  = v21;
  *(_OWORD *)(a3 + 32)  = _Q3;
  *(_OWORD *)(a3 + 48)  = v20;
  float64x2_t v25 = vmlaq_f64(v14, v22, _Q1);
  *(float64x2_t *)(a3 + 64)  = vaddq_f64(v25, v25);
  *(_OWORD *)(a3 + 80)  = _D5;
  *(void *)(a3 + 96)  = 0;
  *(void *)(a3 + 104)  = 0;
  double result = 0.0;
  *(_OWORD *)(a3 + 112)  = xmmword_228C1F7A0;
  return result;
}

double SPSphericalCoordinates3D.init(x:y:z:)(float64x2_t a1, float64_t a2, double a3)
{
  a1.f64[1]  = a2;
  v5[0]  = a1;
  v5[1]  = (float64x2_t)*(unint64_t *)&a3;
  SPSphericalCoordinates3DMakeWithCartesianVector(v5, (uint64_t)&v4);
  return v4;
}

void SPSphericalCoordinates3DMakeWithCartesianVector(float64x2_t *a1@<X0>, uint64_t a2@<X8>)
{
  float64x2_t v6 = *a1;
  float64x2_t v7 = a1[1];
  double v3 = sqrt(vmulq_f64(v7, v7).f64[0] + vaddvq_f64(vmulq_f64(v6, v6)));
  long double v4 = asin(a1->f64[1] / v3);
  long double v5 = atan(v6.f64[0] / v7.f64[0]);
  *(double *)a2  = v3;
  *(long double *)(a2 + 8)  = v4;
  *(long double *)(a2 + 16)  = v5;
  *(void *)(a2 + 24)  = 0x3FF0000000000000;
}

double SPSphericalCoordinates3D.init(_:)(float64x2_t a1, float64x2_t a2)
{
  v4[0]  = a1;
  v4[1]  = a2;
  SPSphericalCoordinates3DMakeWithCartesianVector(v4, (uint64_t)&v3);
  return v3;
}

double SPSphericalCoordinates3D.init(_:)(double a1, double a2, double a3)
{
  return SPSphericalCoordinates3D.init(_:)((void (*)(double *__return_ptr, void *))SPSphericalCoordinates3DMakeWithCartesianPoint, a1, a2, a3);
}

{
  return SPSphericalCoordinates3D.init(_:)((void (*)(double *__return_ptr, void *))SPSphericalCoordinates3DMakeWithCartesianVector, a1, a2, a3);
}

void SPSphericalCoordinates3DMakeWithCartesianPoint(SPPoint3D *a1@<X0>, uint64_t a2@<X8>)
{
  *(SPPoint3D *)&v6.x  = *(SPPoint3D *)&a1->x;
  double v3 = sqrt(vmulq_f64(*(float64x2_t *)&v6.vector.f64[2], *(float64x2_t *)&v6.vector.f64[2]).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)&v6.x, *(float64x2_t *)&v6.x)));
  long double v4 = asin(a1->y / v3);
  long double v5 = atan(v6.x / v6.z);
  *(double *)a2  = v3;
  *(long double *)(a2 + 8)  = v4;
  *(long double *)(a2 + 16)  = v5;
  *(void *)(a2 + 24)  = 0x3FF0000000000000;
}

double SPSphericalCoordinates3D.init(_:)(void (*a1)(double *__return_ptr, void *), double a2, double a3, double a4)
{
  *(double *)SPPoint3D v6 = a2;
  *(double *)&v6[1]  = a3;
  *(double *)&v6[2]  = a4;
  a1(&v5, v6);
  return v5;
}

void SPSphericalCoordinates3DMakeWithCartesianVector(SPVector3D *a1@<X0>, uint64_t a2@<X8>)
{
  *(SPVector3D *)&v6.x  = *(SPVector3D *)&a1->x;
  double v3 = sqrt(vmulq_f64(*(float64x2_t *)&v6.vector.f64[2], *(float64x2_t *)&v6.vector.f64[2]).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)&v6.x, *(float64x2_t *)&v6.x)));
  long double v4 = asin(a1->y / v3);
  long double v5 = atan(v6.x / v6.z);
  *(double *)a2  = v3;
  *(long double *)(a2 + 8)  = v4;
  *(long double *)(a2 + 16)  = v5;
  *(void *)(a2 + 24)  = 0x3FF0000000000000;
}

double SPPoint3D.init(_:)(double a1, double a2, double a3, double a4)
{
  return SPPoint3D.init(_:)((void (*)(double *__return_ptr, void *))SPPoint3DMakeWithSphericalCoordinates, a1, a2, a3, a4);
}

double SPPoint3DMakeWithSphericalCoordinates@<D0>(double *a1@<X0>, double *a2@<X8>)
{
  double v4 = *a1;
  __double2 v5 = __sincos_stret(a1[1]);
  __double2 v6 = __sincos_stret(a1[2]);
  double result = v4 * v5.__cosval * v6.__sinval;
  *a2  = result;
  a2[1]  = v4 * v5.__sinval;
  a2[2]  = v4 * v5.__cosval * v6.__cosval;
  return result;
}

double SPVector3D.init(_:)(double a1, double a2, double a3, double a4)
{
  return SPPoint3D.init(_:)((void (*)(double *__return_ptr, void *))SPVector3DMakeWithSphericalCoordinates, a1, a2, a3, a4);
}

double SPPoint3D.init(_:)(void (*a1)(double *__return_ptr, void *), double a2, double a3, double a4, double a5)
{
  *(double *)float64x2_t v7 = a2;
  *(double *)&v7[1]  = a3;
  *(double *)&v7[2]  = a4;
  *(double *)&v7[3]  = a5;
  a1(&v6, v7);
  return v6;
}

double SPVector3DMakeWithSphericalCoordinates@<D0>(double *a1@<X0>, double *a2@<X8>)
{
  double v3 = *a1;
  double v4 = a1[2];
  __double2 v5 = __sincos_stret(a1[1]);
  __double2 v6 = __sincos_stret(v4);
  double result = v3 * v5.__cosval * v6.__sinval;
  *a2  = result;
  a2[1]  = v3 * v5.__sinval;
  a2[2]  = v3 * v5.__cosval * v6.__cosval;
  return result;
}

BOOL static SPSphericalCoordinates3D.== infix(_:_:)(double a1, double a2, double a3, double a4, double a5, double a6, double a7)
{
  BOOL v7 = a1 == a5;
  if (a2 != a6) {
    BOOL v7 = 0;
  }
  return a3 == a7 && v7;
}

void SPSphericalCoordinates3D.hash(into:)(__n128 a1, double a2, double a3)
{
  a1.n128_f64[1]  = a2;
  specialized SIMD.hash(into:)(a1, a3);
}

Swift::Int SPSphericalCoordinates3D.hashValue.getter(double a1, double a2, double a3)
{
  Hasher.init(_seed:)();
  v3.n128_f64[0]  = a1;
  v3.n128_f64[1]  = a2;
  specialized SIMD.hash(into:)(v3, a3);
  return Hasher._finalize()();
}

uint64_t protocol witness for static Equatable.== infix(_:_:) in conformance SPSphericalCoordinates3D.CodingKeys(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return specialized == infix<A>(_:_:)(*a1, *a2);
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPSphericalCoordinates3D.CodingKeys()
{
  return Hasher._finalize()();
}

uint64_t protocol witness for Hashable.hash(into:) in conformance SPSphericalCoordinates3D.CodingKeys()
{
  String.hash(into:)();

  return swift_bridgeObjectRelease();
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPSphericalCoordinates3D.CodingKeys()
{
  return Hasher._finalize()();
}

unint64_t protocol witness for RawRepresentable.init(rawValue:) in conformance SPSphericalCoordinates3D.CodingKeys@<X0>(Swift::String *a1@<X0>, unsigned char *a2@<X8>)
{
  unint64_t result = specialized SPSphericalCoordinates3D.CodingKeys.init(rawValue:)(*a1);
  *a2  = result;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance SPSphericalCoordinates3D.CodingKeys(uint64_t *a1@<X8>)
{
  int v2 = *v1;
  unint64_t v3 = 0xE600000000000000;
  unint64_t v4 = 0xEB000000006E6F69;
  uint64_t v5 = 0x74616E696C636E69;
  if (v2 != 1)
  {
    uint64_t v5 = 0x6874756D697A61;
    unint64_t v4 = 0xE700000000000000;
  }
  BOOL v6 = v2 == 0;
  if (*v1) {
    uint64_t v7 = v5;
  }
  else {
    uint64_t v7 = 0x737569646172;
  }
  if (!v6) {
    unint64_t v3 = v4;
  }
  *a1  = v7;
  a1[1]  = v3;
}

uint64_t protocol witness for CodingKey.stringValue.getter in conformance SPSphericalCoordinates3D.CodingKeys()
{
  uint64_t v1 = 0x74616E696C636E69;
  if (*v0 != 1) {
    uint64_t v1 = 0x6874756D697A61;
  }
  if (*v0) {
    return v1;
  }
  else {
    return 0x737569646172;
  }
}

unint64_t protocol witness for CodingKey.init(stringValue:) in conformance SPSphericalCoordinates3D.CodingKeys@<X0>(Swift::String a1@<X1:X0>, unsigned char *a2@<X8>)
{
  unint64_t result = specialized SPSphericalCoordinates3D.CodingKeys.init(rawValue:)(a1);
  *a2  = result;
  return result;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPSphericalCoordinates3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys();

  return MEMORY[0x270FA00B0](a1, v2);
}

uint64_t protocol witness for CustomDebugStringConvertible.debugDescription.getter in conformance SPSphericalCoordinates3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys();

  return MEMORY[0x270FA00B8](a1, v2);
}

uint64_t SPSphericalCoordinates3D.encode(to:)(void *a1, double a2, double a3, double a4)
{
  uint64_t v8 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedEncodingContainer<SPSphericalCoordinates3D.CodingKeys>);
  uint64_t v9 = *(void *)(v8 - 8);
  MEMORY[0x270FA5388](v8);
  uint64_t v11 = (char *)&v13 - ((v10 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys();
  dispatch thunk of Encoder.container<A>(keyedBy:)();
  LOBYTE(v14)  = 0;
  KeyedEncodingContainer.encode(_:forKey:)();
  if (!v4)
  {
    double v14 = a3;
    HIBYTE(v13)  = 1;
    type metadata accessor for SPAngle(0);
    lazy protocol witness table accessor for type SPAngle and conformance SPAngle(&lazy protocol witness table cache variable for type SPAngle and conformance SPAngle, type metadata accessor for SPAngle);
    KeyedEncodingContainer.encode<A>(_:forKey:)();
    double v14 = a4;
    HIBYTE(v13)  = 2;
    KeyedEncodingContainer.encode<A>(_:forKey:)();
  }
  return (*(uint64_t (**)(char *, uint64_t))(v9 + 8))(v11, v8);
}

double SPSphericalCoordinates3D.init(from:)(void *a1)
{
  return specialized SPSphericalCoordinates3D.init(from:)(a1);
}

void protocol witness for Decodable.init(from:) in conformance SPSphericalCoordinates3D(void *a1@<X0>, uint64_t a2@<X8>)
{
  double v4 = specialized SPSphericalCoordinates3D.init(from:)(a1);
  if (!v2)
  {
    *(double *)a2  = v4;
    *(void *)(a2 + 8)  = v5;
    *(void *)(a2 + 16)  = v6;
    *(void *)(a2 + 24)  = v7;
  }
}

uint64_t protocol witness for Encodable.encode(to:) in conformance SPSphericalCoordinates3D(void *a1)
{
  return SPSphericalCoordinates3D.encode(to:)(a1, *v1, v1[1], v1[2]);
}

uint64_t SPSphericalCoordinates3D.description.getter()
{
  _StringGuts.grow(_:)(42);
  v0._countAndFlagsBits  = 0x3A73756964617228;
  v0._object  = (void *)0xE900000000000020;
  String.append(_:)(v0);
  Double.write<A>(to:)();
  v1._countAndFlagsBits  = 0x6E696C636E69202CLL;
  v1._object  = (void *)0xEF203A6E6F697461;
  String.append(_:)(v1);
  v2._countAndFlagsBits  = 0x736E616964617228;
  v2._object  = (void *)0xEA0000000000203ALL;
  String.append(_:)(v2);
  Double.write<A>(to:)();
  v3._countAndFlagsBits  = 41;
  v3._object  = (void *)0xE100000000000000;
  String.append(_:)(v3);
  v4._countAndFlagsBits  = 0;
  v4._object  = (void *)0xE000000000000000;
  String.append(_:)(v4);
  swift_bridgeObjectRelease();
  v5._countAndFlagsBits  = 0x74756D697A61202CLL;
  v5._object  = (void *)0xEB00000000203A68;
  String.append(_:)(v5);
  v6._countAndFlagsBits  = 0x736E616964617228;
  v6._object  = (void *)0xEA0000000000203ALL;
  String.append(_:)(v6);
  Double.write<A>(to:)();
  v7._countAndFlagsBits  = 41;
  v7._object  = (void *)0xE100000000000000;
  String.append(_:)(v7);
  v8._countAndFlagsBits  = 0;
  v8._object  = (void *)0xE000000000000000;
  String.append(_:)(v8);
  swift_bridgeObjectRelease();
  v9._countAndFlagsBits  = 41;
  v9._object  = (void *)0xE100000000000000;
  String.append(_:)(v9);
  return 0;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPSphericalCoordinates3D()
{
  return SPSphericalCoordinates3D.description.getter();
}

uint64_t SPSphericalCoordinates3D.customMirror.getter(double a1, double a2, double a3, double a4)
{
  uint64_t v8 = type metadata accessor for Mirror.AncestorRepresentation();
  uint64_t v9 = *(void *)(v8 - 8);
  MEMORY[0x270FA5388](v8);
  uint64_t v11 = (char *)v22 - ((v10 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v12 = __swift_instantiateConcreteTypeFromMangledName((uint64_t *)&demangling cache variable for type metadata for Mirror.DisplayStyle?);
  MEMORY[0x270FA5388](v12 - 8);
  double v14 = (char *)v22 - ((v13 + 15) & 0xFFFFFFFFFFFFFFF0);
  *(double *)float64x2_t v22 = a1;
  *(double *)&v22[1]  = a2;
  *(double *)&v22[2]  = a3;
  *(double *)&v22[3]  = a4;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<(String, Any)>);
  uint64_t v15 = swift_allocObject();
  *(_OWORD *)(v15 + 16)  = xmmword_228C1FC50;
  *(void *)(v15 + 32)  = 0x737569646172;
  *(void *)(v15 + 40)  = 0xE600000000000000;
  uint64_t v16 = MEMORY[0x263F8D538];
  *(double *)(v15 + 48)  = a1;
  *(void *)(v15 + 72)  = v16;
  *(void *)(v15 + 80)  = 0x74616E696C636E69;
  *(void *)(v15 + 88)  = 0xEB000000006E6F69;
  type metadata accessor for SPAngle(0);
  *(double *)(v15 + 96)  = a2;
  *(void *)(v15 + 120)  = v17;
  *(void *)(v15 + 128)  = 0x6874756D697A61;
  *(void *)(v15 + 136)  = 0xE700000000000000;
  *(void *)(v15 + 168)  = v17;
  *(double *)(v15 + 144)  = a3;
  uint64_t v18 = *MEMORY[0x263F8E808];
  uint64_t v19 = type metadata accessor for Mirror.DisplayStyle();
  uint64_t v20 = *(void *)(v19 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v20 + 104))(v14, v18, v19);
  (*(void (**)(char *, void, uint64_t, uint64_t))(v20 + 56))(v14, 0, 1, v19);
  (*(void (**)(char *, void, uint64_t))(v9 + 104))(v11, *MEMORY[0x263F8E830], v8);
  type metadata accessor for SPSphericalCoordinates3D(0);
  return Mirror.init<A>(_:children:displayStyle:ancestorRepresentation:)();
}

uint64_t protocol witness for CustomReflectable.customMirror.getter in conformance SPSphericalCoordinates3D()
{
  return SPSphericalCoordinates3D.customMirror.getter(*v0, v0[1], v0[2], v0[3]);
}

unint64_t lazy protocol witness table accessor for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys()
{
  unint64_t result = lazy protocol witness table cache variable for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys);
  }
  return result;
}

unint64_t specialized SPSphericalCoordinates3D.CodingKeys.init(rawValue:)(Swift::String string)
{
  object  = string._object;
  v2._countAndFlagsBits  = string._countAndFlagsBits;
  v2._object  = object;
  unint64_t v3 = _findStringSwitchCase(cases:string:)((Swift::OpaquePointer)&outlined read-only object #0 of SPSphericalCoordinates3D.CodingKeys.init(rawValue:), v2);
  swift_bridgeObjectRelease();
  if (v3 >= 3) {
    return 3;
  }
  else {
    return v3;
  }
}

double specialized SPSphericalCoordinates3D.init(from:)(void *a1)
{
  uint64_t v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedDecodingContainer<SPSphericalCoordinates3D.CodingKeys>);
  uint64_t v5 = *(void *)(v4 - 8);
  MEMORY[0x270FA5388](v4);
  Swift::String v7 = (char *)&v16 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPSphericalCoordinates3D.CodingKeys and conformance SPSphericalCoordinates3D.CodingKeys();
  dispatch thunk of Decoder.container<A>(keyedBy:)();
  if (!v1)
  {
    LOBYTE(v16)  = 1;
    KeyedDecodingContainer.decode(_:forKey:)();
    double v9 = v8;
    LOBYTE(v16)  = 2;
    KeyedDecodingContainer.decode(_:forKey:)();
    double v11 = v10;
    LOBYTE(v16)  = 0;
    KeyedDecodingContainer.decode(_:forKey:)();
    v13.radians  = v9;
    v14.radians  = v11;
    SPSphericalCoordinates3DMake(v15, v13, v14, (uint64_t)&v16);
    double v2 = v16;
    (*(void (**)(char *, uint64_t))(v5 + 8))(v7, v4);
  }
  __swift_destroy_boxed_opaque_existential_1(a1);
  return v2;
}

uint64_t base witness table accessor for Equatable in SPSphericalCoordinates3D()
{
  return lazy protocol witness table accessor for type SPAngle and conformance SPAngle(&lazy protocol witness table cache variable for type SPSphericalCoordinates3D and conformance SPSphericalCoordinates3D, type metadata accessor for SPSphericalCoordinates3D);
}

uint64_t lazy protocol witness table accessor for type SPAngle and conformance SPAngle(unint64_t *a1, void (*a2)(uint64_t))
{
  uint64_t result = *a1;
  if (!result)
  {
    a2(255);
    uint64_t result = swift_getWitnessTable();
    atomic_store(result, a1);
  }
  return result;
}

unsigned char *storeEnumTagSinglePayload for SPSphericalCoordinates3D.CodingKeys(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 2 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 2) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFE) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFD)
  {
    unsigned int v6 = ((a2 - 254) >> 8) + 1;
    *uint64_t result = a2 + 2;
    switch(v5)
    {
      case 1:
        result[1]  = v6;
        break;
      case 2:
        *(_WORD *)(result + 1)  = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x228BFCB0CLL);
      case 4:
        *(_DWORD *)(result + 1)  = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1]  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1)  = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1)  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 2;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for SPSphericalCoordinates3D.CodingKeys()
{
  return &type metadata for SPSphericalCoordinates3D.CodingKeys;
}

void SPSphericalCoordinates3DMake(double a1@<D0>, SPAngle a2@<0:D1>, SPAngle a3@<0:D2>, uint64_t a4@<X8>)
{
  *(double *)a4  = a1;
  *(SPAngle *)(a4 + 8)  = a2;
  *(SPAngle *)(a4 + 16)  = a3;
  *(void *)(a4 + 24)  = 0x3FF0000000000000;
}

BOOL static SPAngle.== infix(_:_:)(double a1, double a2)
{
  return a1 == a2;
}

double SPAngleMakeWithDegrees(double a1)
{
  return a1 / 180.0 * 3.14159265;
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance SPAngle(double *a1, double *a2)
{
  return *a1 == *a2;
}

void SPAngle.hash(into:)(double a1)
{
  if ((*(void *)&a1 & 0x7FFFFFFFFFFFFFFFLL) != 0) {
    Swift::UInt64 v1 = *(void *)&a1;
  }
  else {
    Swift::UInt64 v1 = 0;
  }
  Hasher._combine(_:)(v1);
}

Swift::Int SPAngle.hashValue.getter(double a1)
{
  Hasher.init(_seed:)();
  if ((*(void *)&a1 & 0x7FFFFFFFFFFFFFFFLL) != 0) {
    Swift::UInt64 v2 = *(void *)&a1;
  }
  else {
    Swift::UInt64 v2 = 0;
  }
  Hasher._combine(_:)(v2);
  return Hasher._finalize()();
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPAngle()
{
  Swift::UInt64 v1 = *v0;
  Hasher.init(_seed:)();
  if ((v1 & 0x7FFFFFFFFFFFFFFFLL) != 0) {
    Swift::UInt64 v2 = v1;
  }
  else {
    Swift::UInt64 v2 = 0;
  }
  Hasher._combine(_:)(v2);
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance SPAngle()
{
  if ((*v0 & 0x7FFFFFFFFFFFFFFFLL) != 0) {
    Swift::UInt64 v1 = *v0;
  }
  else {
    Swift::UInt64 v1 = 0;
  }
  Hasher._combine(_:)(v1);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPAngle()
{
  Swift::UInt64 v1 = *v0;
  Hasher.init(_seed:)();
  if ((v1 & 0x7FFFFFFFFFFFFFFFLL) != 0) {
    Swift::UInt64 v2 = v1;
  }
  else {
    Swift::UInt64 v2 = 0;
  }
  Hasher._combine(_:)(v2);
  return Hasher._finalize()();
}

uint64_t SPAngle.encode(to:)(void *a1)
{
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  dispatch thunk of Encoder.singleValueContainer()();
  __swift_mutable_project_boxed_opaque_existential_1((uint64_t)v2, v2[3]);
  dispatch thunk of SingleValueEncodingContainer.encode(_:)();
  return __swift_destroy_boxed_opaque_existential_1(v2);
}

uint64_t __swift_mutable_project_boxed_opaque_existential_1(uint64_t a1, uint64_t a2)
{
  if ((*(_DWORD *)(*(void *)(a2 - 8) + 80) & 0x20000) != 0)
  {
    swift_makeBoxUnique();
    return v2;
  }
  return result;
}

double SPAngle.init(from:)(void *a1)
{
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  dispatch thunk of Decoder.singleValueContainer()();
  if (v1)
  {
    __swift_destroy_boxed_opaque_existential_1(a1);
  }
  else
  {
    __swift_project_boxed_opaque_existential_1(v6, v6[3]);
    dispatch thunk of SingleValueDecodingContainer.decode(_:)();
    double v2 = v4;
    __swift_destroy_boxed_opaque_existential_1(v6);
    __swift_destroy_boxed_opaque_existential_1(a1);
  }
  return v2;
}

uint64_t protocol witness for Decodable.init(from:) in conformance SPAngle@<X0>(void *a1@<X0>, void *a2@<X8>)
{
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  dispatch thunk of Decoder.singleValueContainer()();
  if (v2) {
    return __swift_destroy_boxed_opaque_existential_1(a1);
  }
  __swift_project_boxed_opaque_existential_1(v8, v8[3]);
  dispatch thunk of SingleValueDecodingContainer.decode(_:)();
  uint64_t v6 = v5;
  __swift_destroy_boxed_opaque_existential_1(v8);
  uint64_t result = __swift_destroy_boxed_opaque_existential_1(a1);
  *a2  = v6;
  return result;
}

uint64_t protocol witness for Encodable.encode(to:) in conformance SPAngle(void *a1)
{
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  dispatch thunk of Encoder.singleValueContainer()();
  __swift_mutable_project_boxed_opaque_existential_1((uint64_t)v2, v2[3]);
  dispatch thunk of SingleValueEncodingContainer.encode(_:)();
  return __swift_destroy_boxed_opaque_existential_1(v2);
}

uint64_t SPAngle.description.getter()
{
  v0._countAndFlagsBits  = 0x736E616964617228;
  v0._object  = (void *)0xEA0000000000203ALL;
  String.append(_:)(v0);
  Double.write<A>(to:)();
  v1._countAndFlagsBits  = 41;
  v1._object  = (void *)0xE100000000000000;
  String.append(_:)(v1);
  return 0;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPAngle()
{
  v0._countAndFlagsBits  = 0x736E616964617228;
  v0._object  = (void *)0xEA0000000000203ALL;
  String.append(_:)(v0);
  Double.write<A>(to:)();
  v1._countAndFlagsBits  = 41;
  v1._object  = (void *)0xE100000000000000;
  String.append(_:)(v1);
  return 0;
}

uint64_t SPAngle.customMirror.getter(double a1)
{
  uint64_t v2 = type metadata accessor for Mirror.AncestorRepresentation();
  uint64_t v3 = *(void *)(v2 - 8);
  MEMORY[0x270FA5388](v2);
  uint64_t v5 = (char *)v15 - ((v4 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v6 = __swift_instantiateConcreteTypeFromMangledName((uint64_t *)&demangling cache variable for type metadata for Mirror.DisplayStyle?);
  MEMORY[0x270FA5388](v6 - 8);
  double v8 = (char *)v15 - ((v7 + 15) & 0xFFFFFFFFFFFFFFF0);
  *(double *)&v15[1]  = a1;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<(String, Any)>);
  uint64_t v9 = swift_allocObject();
  *(_OWORD *)(v9 + 16)  = xmmword_228C20870;
  uint64_t v10 = MEMORY[0x263F8D538];
  *(void *)(v9 + 32)  = 0x736E6169646172;
  *(void *)(v9 + 40)  = 0xE700000000000000;
  *(void *)(v9 + 72)  = v10;
  *(double *)(v9 + 48)  = a1;
  uint64_t v11 = *MEMORY[0x263F8E808];
  uint64_t v12 = type metadata accessor for Mirror.DisplayStyle();
  uint64_t v13 = *(void *)(v12 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v13 + 104))(v8, v11, v12);
  (*(void (**)(char *, void, uint64_t, uint64_t))(v13 + 56))(v8, 0, 1, v12);
  (*(void (**)(char *, void, uint64_t))(v3 + 104))(v5, *MEMORY[0x263F8E830], v2);
  type metadata accessor for SPAngle(0);
  return Mirror.init<A>(_:children:displayStyle:ancestorRepresentation:)();
}

uint64_t protocol witness for CustomReflectable.customMirror.getter in conformance SPAngle()
{
  return SPAngle.customMirror.getter(*v0);
}

void SPAngleNormalize(SPAngle a1)
{
}

unint64_t lazy protocol witness table accessor for type SPAngle and conformance SPAngle()
{
  unint64_t result = lazy protocol witness table cache variable for type SPAngle and conformance SPAngle;
  if (!lazy protocol witness table cache variable for type SPAngle and conformance SPAngle)
  {
    type metadata accessor for SPAngle(255);
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPAngle and conformance SPAngle);
  }
  return result;
}

uint64_t SPProjectiveTransform3D.inverse.getter@<X0>(uint64_t a1@<X8>)
{
  long long v3 = *(_OWORD *)(v1 + 16);
  long long v4 = *(_OWORD *)(v1 + 32);
  long long v5 = *(_OWORD *)(v1 + 48);
  long long v6 = *(_OWORD *)(v1 + 64);
  long long v7 = *(_OWORD *)(v1 + 80);
  long long v8 = *(_OWORD *)(v1 + 96);
  long long v9 = *(_OWORD *)(v1 + 112);
  float64x2_t v27 = *(float64x2_t *)v1;
  long long v28 = v3;
  long long v29 = v4;
  long long v30 = v5;
  long long v31 = v6;
  long long v32 = v7;
  long long v33 = v8;
  long long v34 = v9;
  SPProjectiveTransform3DInverted(&v27, &v19);
  long long v17 = v22;
  long long v18 = v20;
  float64x2_t v27 = v19;
  long long v28 = v20;
  long long v13 = v21;
  float64x2_t v14 = v19;
  long long v29 = v21;
  long long v30 = v22;
  long long v15 = v26;
  long long v16 = v24;
  long long v31 = v23;
  long long v32 = v24;
  long long v11 = v25;
  long long v12 = v23;
  long long v33 = v25;
  long long v34 = v26;
  if (SPProjectiveTransform3DIsValid(&v27))
  {
    float64x2_t v19 = v14;
    long long v20 = v18;
    long long v21 = v13;
    long long v22 = v17;
    long long v23 = v12;
    long long v24 = v16;
    long long v25 = v11;
    long long v26 = v15;
    _sSo19SPAffineTransform3DaSgWOi_((uint64_t)&v19);
  }
  else
  {
    _sSo19SPAffineTransform3DaSgWOi0_((uint64_t)&v19);
  }
  outlined init with take of SPProjectiveTransform3D?((uint64_t)&v19, (uint64_t)&v27);
  return outlined init with take of SPProjectiveTransform3D?((uint64_t)&v27, a1);
}

uint64_t outlined init with take of SPProjectiveTransform3D?(uint64_t a1, uint64_t a2)
{
  uint64_t v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for SPProjectiveTransform3D?);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(void *)(v4 - 8) + 32))(a2, a1, v4);
  return a2;
}

double SPProjectiveTransform3D.translation.getter()
{
  return SPProjectiveTransform3D.translation.getter((void (*)(double *__return_ptr, _OWORD *))SPProjectiveTransform3DGetTranslation);
}

__n128 SPProjectiveTransform3DGetTranslation@<Q0>(__n128 *a1@<X0>, __n128 *a2@<X8>)
{
  __n128 result = a1[6];
  unint64_t v3 = a1[7].n128_u64[0];
  *a2  = result;
  a2[1].n128_u64[0]  = v3;
  return result;
}

double SPProjectiveTransform3D.translation.setter(double a1, double a2, double a3)
{
  v5.x  = a1;
  v5.y  = a2;
  v5.double z = a3;
  *(void *)&double result = SPProjectiveTransform3DSetTranslation(v3, &v5).n128_u64[0];
  return result;
}

void (*SPProjectiveTransform3D.translation.modify(__n128 **a1))(double **a1)
{
  unint64_t v3 = (__n128 *)malloc(0x28uLL);
  *a1  = v3;
  v3[2].n128_u64[0]  = (unint64_t)v1;
  __n128 v4 = v1[1];
  __n128 v5 = v1[2];
  __n128 v6 = v1[3];
  __n128 v7 = v1[4];
  __n128 v8 = v1[5];
  __n128 v9 = v1[6];
  __n128 v10 = v1[7];
  v12[0]  = *v1;
  v12[1]  = v4;
  v12[2]  = v5;
  v12[3]  = v6;
  v12[4]  = v7;
  v12[5]  = v8;
  v12[6]  = v9;
  v12[7]  = v10;
  SPProjectiveTransform3DGetTranslation(v12, v3);
  return SPProjectiveTransform3D.translation.modify;
}

void SPProjectiveTransform3D.translation.modify(double **a1)
{
  uint64_t v1 = *a1;
  double v3 = (*a1)[2];
  double v2 = (*a1)[3];
  double v5 = **a1;
  double v4 = (*a1)[1];
  __n128 v6 = (SPProjectiveTransform3D *)*((void *)*a1 + 4);
  v7.x  = v5;
  v7.y  = v4;
  v7.double z = v3;
  v7.vector.f64[3]  = v2;
  SPProjectiveTransform3DSetTranslation(v6, &v7);

  free(v1);
}

void __swiftcall SPProjectiveTransform3D.init()(SPProjectiveTransform3D *__return_ptr retstr)
{
  SPPoint3DMake(1.0, 1.0, 1.0, &v11.width);
  SPProjectiveTransform3DMakeScale(&v11, (uint64_t)v10, v2);
  long long v3 = v10[1];
  long long v4 = v10[2];
  long long v5 = v10[3];
  long long v6 = v10[4];
  long long v7 = v10[5];
  long long v8 = v10[6];
  long long v9 = v10[7];
  *(_OWORD *)retstr->matrix.columns[0].f64  = v10[0];
  *(_OWORD *)&retstr->matrix.columns[0].f64[2]  = v3;
  *(_OWORD *)retstr->matrix.columns[1].f64  = v4;
  *(_OWORD *)&retstr->matrix.columns[1].f64[2]  = v5;
  *(_OWORD *)retstr->matrix.columns[2].f64  = v6;
  *(_OWORD *)&retstr->matrix.columns[2].f64[2]  = v7;
  *(_OWORD *)retstr->matrix.columns[3].f64  = v8;
  *(_OWORD *)&retstr->matrix.columns[3].f64[2]  = v9;
}

double SPProjectiveTransform3DMakeScale@<D0>(SPSize3D *a1@<X0>, uint64_t a2@<X8>, __n128 a3@<Q3>)
{
  *(void *)&long long v3 = 0;
  *((void *)&v3 + 1)  = *(void *)&a1->height;
  a3.n128_u64[0]  = *(void *)&a1->depth;
  *(_OWORD *)a2  = *(unint64_t *)&a1->width;
  *(_OWORD *)(a2 + 16)  = 0u;
  *(_OWORD *)(a2 + 32)  = v3;
  *(_OWORD *)(a2 + 48)  = 0u;
  *(_OWORD *)(a2 + 64)  = 0u;
  *(__n128 *)(a2 + 80)  = a3;
  double result = 0.0;
  *(_OWORD *)(a2 + 96)  = 0u;
  *(_OWORD *)(a2 + 112)  = xmmword_228C1F7A0;
  return result;
}

#error "228BFDA78: call analysis failed (funcsize=41)"

float64x2_t *SPProjectiveTransform3DMake@<X0>(SPSize3D *a1@<X0>, SPRotation3D a2@<0:Q0, 16:Q1>, SPVector3D *a3@<X1>, long long *a4@<X2>, uint64_t a5@<X8>, __n128 a6@<Q2>, float64x2_t a7, long long a8, long long a9, long long a10, long long a11, long long a12, long long a13, long long a14, float64x2_t a15, long long a16, long long a17, long long a18, long long a19, __n128 a20,long long a21,long long a22,float64x2_t a23,long long a24,long long a25,long long a26,float64x2_t a27,long long a28,long long a29,long long a30)
{
  width  = a1->width;
  *(void *)&long long v32 = 0;
  *((void *)&v32 + 1)  = *(void *)&a1->height;
  a6.n128_u64[0]  = *(void *)&a1->depth;
  _Q6  = *(float64x2_t *)&a3->x;
  _Q4  = *(float64x2_t *)&a3->vector.f64[2];
  _D7  = a3->y;
  __asm { FMLS            D3, D4, V4.D[0] }
  _D18  = a3->vector.f64[3];
  __asm { FMLA            D3, D18, V4.D[1] }
  double v43 = vmlad_n_f64(vmuld_lane_f64(_Q4.f64[0], _Q4, 1), _D7, a3->x);
  double v44 = vmlad_n_f64(-(_D7 * _D18), _Q4.f64[0], a3->x);
  _Q3.f64[1]  = v43 + v43;
  *(double *)&unint64_t v45 = v44 + v44;
  double v46 = vmlad_n_f64(-(_Q4.f64[0] * _D18), _D7, a3->x);
  *(double *)&long long v47 = v46 + v46;
  __asm
  {
    FMLA            D17, D7, V6.D[1]
    FMLA            D17, D18, V4.D[1]
    FMLS            D17, D6, V6.D[0]
    FMLA            D19, D4, V6.D[1]
  }
  *((void *)&v47 + 1)  = _D17;
  *(double *)&unint64_t v52 = _D19 + _D19;
  double v53 = -(a3->x * _D18);
  float64x2_t v54 = (float64x2_t)vzip1q_s64(*(int64x2_t *)&a3->x, (int64x2_t)_Q4);
  __asm
  {
    FMLS            D18, D6, V6.D[0]
    FMLS            D18, D7, V6.D[1]
  }
  _Q6.f64[0]  = a3->z;
  _Q4.f64[0]  = vmuld_lane_f64(_D7, _Q4, 1);
  _Q4.f64[1]  = v53;
  unint64_t v57 = _D18;
  __asm { FMOV            V18.2D, #1.0 }
  *(void *)&_Q18  = *((void *)a4 + 2);
  long long v69 = _Q18;
  long long v59 = *a4;
  long long v60 = *(_OWORD *)(MEMORY[0x263EF8990] + 48);
  a9  = *(_OWORD *)(MEMORY[0x263EF8990] + 32);
  a10  = v60;
  long long v61 = *(_OWORD *)(MEMORY[0x263EF8990] + 80);
  a11  = *(_OWORD *)(MEMORY[0x263EF8990] + 64);
  a12  = v61;
  long long v62 = *(_OWORD *)(MEMORY[0x263EF8990] + 16);
  vars0  = v59;
  a7  = *(float64x2_t *)MEMORY[0x263EF8990];
  a8  = v62;
  *(_OWORD *)a5  = 0u;
  *(_OWORD *)(a5 + 16)  = 0u;
  *(_OWORD *)(a5 + 32)  = 0u;
  *(_OWORD *)(a5 + 48)  = 0u;
  float64x2_t v63 = vmlaq_f64(_Q4, v54, _Q6);
  *(_OWORD *)(a5 + 64)  = 0u;
  *(_OWORD *)(a5 + 80)  = 0u;
  *(_OWORD *)(a5 + 96)  = 0u;
  *(_OWORD *)(a5 + 112)  = 0u;
  a23  = _Q3;
  a24  = v45;
  a25  = v47;
  a26  = v52;
  a27  = vaddq_f64(v63, v63);
  a28  = v57;
  a29  = 0u;
  a30  = xmmword_228C1F7A0;
  a15  = (float64x2_t)*(unint64_t *)&width;
  a16  = 0u;
  a17  = v32;
  a18  = 0u;
  a19  = 0u;
  a20  = a6;
  a21  = 0u;
  a22  = xmmword_228C1F7A0;
  SPProjectiveTransform3DConcatenation(&a23, &a15, (float64x2_t *)a5);
  a13  = vars0;
  a14  = v69;
  v64  = *(_OWORD *)(a5 + 80);
  a27  = *(float64x2_t *)(a5 + 64);
  a28  = v64;
  v65  = *(_OWORD *)(a5 + 112);
  a29  = *(_OWORD *)(a5 + 96);
  a30  = v65;
  long long v66 = *(_OWORD *)(a5 + 16);
  a23  = *(float64x2_t *)a5;
  a24  = v66;
  long long v67 = *(_OWORD *)(a5 + 48);
  a25  = *(_OWORD *)(a5 + 32);
  a26  = v67;
  return SPProjectiveTransform3DConcatenation(&a7, &a23, (float64x2_t *)a5);
}

#error "228BFDCA4: call analysis failed (funcsize=28)"

#error "228BFDD10: call analysis failed (funcsize=27)"

float64x2_t *SPProjectiveTransform3DMakeWithPose@<X0>(SPPose3D *a1@<X0>, uint64_t a2@<X8>, float64x2_t a3, long long a4, long long a5, long long a6, long long a7, long long a8, long long a9, long long a10, long long a11, long long a12, long long a13, long long a14, long long a15, long long a16, long long a17, long long a18, float64x2_t a19, long long a20,long long a21,long long a22,float64x2_t a23,long long a24,long long a25,long long a26)
{
  long long v27 = *(_OWORD *)&a1->position.vector.f64[2];
  vars0  = *(_OWORD *)&a1->position.x;
  _Q2  = *(float64x2_t *)a1->rotation.vector.f64;
  _Q1  = *(float64x2_t *)&a1->rotation.quaternion.vector.f64[2];
  _D3  = a1->rotation.vector.f64[1];
  __asm { FMLS            D0, D1, V1.D[0] }
  _D7  = a1->rotation.vector.f64[3];
  __asm { FMLA            D0, D7, V1.D[1] }
  double v38 = vmlad_n_f64(vmuld_lane_f64(_Q1.f64[0], _Q1, 1), _D3, _Q2.f64[0]);
  double v39 = vmlad_n_f64(-(_D3 * _D7), _Q1.f64[0], _Q2.f64[0]);
  _Q0.f64[1]  = v38 + v38;
  *(double *)&unint64_t v40 = v39 + v39;
  double v41 = vmlad_n_f64(-(_Q1.f64[0] * _D7), _D3, _Q2.f64[0]);
  *(double *)&long long v42 = v41 + v41;
  __asm
  {
    FMLA            D6, D3, V2.D[1]
    FMLA            D6, D7, V1.D[1]
    FMLS            D6, D2, V2.D[0]
    FMLA            D16, D1, V2.D[1]
  }
  *((void *)&v42 + 1)  = _D6;
  *(double *)&unint64_t v47 = _D16 + _D16;
  float64_t v48 = -(_Q2.f64[0] * _D7);
  float64x2_t v49 = (float64x2_t)vzip1q_s64((int64x2_t)_Q2, (int64x2_t)_Q1);
  __asm
  {
    FMLS            D7, D2, V2.D[0]
    FMLS            D7, D3, V2.D[1]
  }
  _Q2.f64[0]  = a1->rotation.vector.f64[2];
  _Q1.f64[0]  = vmuld_lane_f64(_D3, _Q1, 1);
  _Q1.f64[1]  = v48;
  *((void *)&v27 + 1)  = 1.0;
  long long v61 = v27;
  long long v52 = *(_OWORD *)(MEMORY[0x263EF8990] + 48);
  a5  = *(_OWORD *)(MEMORY[0x263EF8990] + 32);
  a6  = v52;
  long long v53 = *(_OWORD *)(MEMORY[0x263EF8990] + 80);
  a7  = *(_OWORD *)(MEMORY[0x263EF8990] + 64);
  a8  = v53;
  long long v54 = *(_OWORD *)(MEMORY[0x263EF8990] + 16);
  a3  = *(float64x2_t *)MEMORY[0x263EF8990];
  a4  = v54;
  *(_OWORD *)a2  = 0u;
  *(_OWORD *)(a2 + 16)  = 0u;
  *(_OWORD *)(a2 + 32)  = 0u;
  *(_OWORD *)(a2 + 48)  = 0u;
  float64x2_t v55 = vmlaq_f64(_Q1, v49, _Q2);
  *(_OWORD *)(a2 + 64)  = 0u;
  *(_OWORD *)(a2 + 80)  = 0u;
  *(_OWORD *)(a2 + 96)  = 0u;
  *(_OWORD *)(a2 + 112)  = 0u;
  a19  = _Q0;
  a20  = v40;
  a21  = v42;
  a22  = v47;
  a23  = vaddq_f64(v55, v55);
  a24  = _D7;
  a25  = 0u;
  a26  = xmmword_228C1F7A0;
  a11  = xmmword_228C1F7D0;
  a12  = 0u;
  a13  = xmmword_228C1F7A0;
  a14  = 0u;
  a15  = 0u;
  a16  = xmmword_228C1F7D0;
  a17  = 0u;
  a18  = xmmword_228C1F7A0;
  SPProjectiveTransform3DConcatenation(&a19, (float64x2_t *)&a11, (float64x2_t *)a2);
  a9  = vars0;
  a10  = v61;
  long long v56 = *(_OWORD *)(a2 + 80);
  a23  = *(float64x2_t *)(a2 + 64);
  a24  = v56;
  long long v57 = *(_OWORD *)(a2 + 112);
  a25  = *(_OWORD *)(a2 + 96);
  a26  = v57;
  long long v58 = *(_OWORD *)(a2 + 16);
  a19  = *(float64x2_t *)a2;
  a20  = v58;
  long long v59 = *(_OWORD *)(a2 + 48);
  a21  = *(_OWORD *)(a2 + 32);
  a22  = v59;
  return SPProjectiveTransform3DConcatenation(&a3, &a19, (float64x2_t *)a2);
}

#error "228BFDF24: call analysis failed (funcsize=29)"

float64x2_t *SPProjectiveTransform3DMakeWithScaledPose@<X0>(long long *a1@<X0>, uint64_t a2@<X8>, float64x2_t a3, long long a4, long long a5, long long a6, long long a7, long long a8, long long a9, long long a10, float64x2_t a11, long long a12, long long a13, long long a14, long long a15, long long a16, long long a17, long long a18, float64x2_t a19, long long a20,long long a21,long long a22,float64x2_t a23,long long a24,long long a25,long long a26)
{
  _Q4  = (int64x2_t)a1[2];
  _Q3  = (float64x2_t)a1[3];
  *(void *)&long long v29 = 0;
  *((void *)&v29 + 1)  = *((void *)a1 + 8);
  _D6  = *((double *)a1 + 5);
  __asm { FMLS            D2, D3, V3.D[0] }
  _D18  = *((double *)a1 + 7);
  __asm { FMLA            D2, D18, V3.D[1] }
  double v38 = vmlad_n_f64(vmuld_lane_f64(_Q3.f64[0], _Q3, 1), _D6, *(double *)_Q4.i64);
  double v39 = vmlad_n_f64(-(_D6 * _D18), _Q3.f64[0], *(double *)_Q4.i64);
  _Q2.f64[1]  = v38 + v38;
  *(double *)&unint64_t v40 = v39 + v39;
  v41.f64[0]  = vmuld_lane_f64(_D6, _Q3, 1);
  double v42 = vmlad_n_f64(-(_Q3.f64[0] * _D18), _D6, *(double *)_Q4.i64);
  *(double *)&long long v43 = v42 + v42;
  __asm
  {
    FMLA            D17, D6, V4.D[1]
    FMLA            D17, D18, V3.D[1]
    FMLS            D17, D4, V4.D[0]
    FMLA            D19, D3, V4.D[1]
  }
  *((void *)&v43 + 1)  = _D17;
  double v48 = -(*(double *)_Q4.i64 * _D18);
  __asm
  {
    FMLS            D18, D4, V4.D[0]
    FMLS            D18, D6, V4.D[1]
  }
  v51.f64[1]  = _D6;
  v51.f64[0]  = *((float64_t *)a1 + 6);
  v41.f64[1]  = v48;
  unint64_t v52 = _D18;
  long long v53 = a1[1];
  *((void *)&v53 + 1)  = 1.0;
  long long v63 = v53;
  vars0  = *a1;
  long long v54 = *(_OWORD *)(MEMORY[0x263EF8990] + 48);
  a5  = *(_OWORD *)(MEMORY[0x263EF8990] + 32);
  a6  = v54;
  long long v55 = *(_OWORD *)(MEMORY[0x263EF8990] + 80);
  a7  = *(_OWORD *)(MEMORY[0x263EF8990] + 64);
  a8  = v55;
  long long v56 = *(_OWORD *)(MEMORY[0x263EF8990] + 16);
  a3  = *(float64x2_t *)MEMORY[0x263EF8990];
  a4  = v56;
  *(_OWORD *)a2  = 0u;
  *(_OWORD *)(a2 + 16)  = 0u;
  *(_OWORD *)(a2 + 32)  = 0u;
  *(_OWORD *)(a2 + 48)  = 0u;
  float64x2_t v57 = vmlaq_f64(v41, (float64x2_t)vzip1q_s64(_Q4, (int64x2_t)_Q3), v51);
  *(_OWORD *)(a2 + 64)  = 0u;
  *(_OWORD *)(a2 + 80)  = 0u;
  *(_OWORD *)(a2 + 96)  = 0u;
  *(_OWORD *)(a2 + 112)  = 0u;
  a19  = _Q2;
  a20  = v40;
  a21  = v43;
  a22  = COERCE_UNSIGNED_INT64(_D19 + _D19);
  a23  = vaddq_f64(v57, v57);
  a24  = v52;
  a25  = 0u;
  a26  = xmmword_228C1F7A0;
  a11  = (float64x2_t)*((unint64_t *)&v29 + 1);
  a12  = 0u;
  a13  = v29;
  a14  = 0u;
  a15  = 0u;
  a16  = *((unint64_t *)&v29 + 1);
  a17  = 0u;
  a18  = xmmword_228C1F7A0;
  SPProjectiveTransform3DConcatenation(&a19, &a11, (float64x2_t *)a2);
  a9  = vars0;
  a10  = v63;
  long long v58 = *(_OWORD *)(a2 + 80);
  a23  = *(float64x2_t *)(a2 + 64);
  a24  = v58;
  long long v59 = *(_OWORD *)(a2 + 112);
  a25  = *(_OWORD *)(a2 + 96);
  a26  = v59;
  long long v60 = *(_OWORD *)(a2 + 16);
  a19  = *(float64x2_t *)a2;
  a20  = v60;
  long long v61 = *(_OWORD *)(a2 + 48);
  a21  = *(_OWORD *)(a2 + 32);
  a22  = v61;
  return SPProjectiveTransform3DConcatenation(&a3, &a19, (float64x2_t *)a2);
}

Swift::Bool __swiftcall SPProjectiveTransform3D.isApproximatelyEqual(to:tolerance:)(SPProjectiveTransform3D *to, Swift::Double tolerance)
{
  float64x2_t v3 = *(float64x2_t *)to->matrix.columns[0].f64;
  float64x2_t v4 = *(float64x2_t *)&to->matrix.columns[0].f64[2];
  float64x2_t v5 = *(float64x2_t *)to->matrix.columns[1].f64;
  float64x2_t v6 = *(float64x2_t *)&to->matrix.columns[1].f64[2];
  float64x2_t v7 = *(float64x2_t *)to->matrix.columns[2].f64;
  float64x2_t v8 = *(float64x2_t *)&to->matrix.columns[2].f64[2];
  float64x2_t v9 = *(float64x2_t *)to->matrix.columns[3].f64;
  float64x2_t v10 = *(float64x2_t *)&to->matrix.columns[3].f64[2];
  float64x2_t v11 = v2[1];
  float64x2_t v12 = v2[2];
  float64x2_t v13 = v2[3];
  float64x2_t v14 = v2[4];
  float64x2_t v15 = v2[5];
  float64x2_t v16 = v2[6];
  float64x2_t v17 = v2[7];
  v20[0]  = *v2;
  v20[1]  = v11;
  v20[2]  = v12;
  v20[3]  = v13;
  v20[4]  = v14;
  v20[5]  = v15;
  v20[6]  = v16;
  v20[7]  = v17;
  v19[0]  = v3;
  v19[1]  = v4;
  v19[2]  = v5;
  v19[3]  = v6;
  v19[4]  = v7;
  v19[5]  = v8;
  v19[6]  = v9;
  v19[7]  = v10;
  return SPProjectiveTransform3DAlmostEqualToTransform(v20, v19, tolerance);
}

unint64_t SPProjectiveTransform3DAlmostEqualToTransform(float64x2_t *a1, float64x2_t *a2, double a3)
{
  float64x2_t v3 = (float64x2_t)vdupq_lane_s64(*(uint64_t *)&a3, 0);
  int64x2_t v4 = (int64x2_t)vandq_s8(vandq_s8(vandq_s8((int8x16_t)vcgeq_f64(v3, vabdq_f64(a1[2], a2[2])), (int8x16_t)vcgeq_f64(v3, vabdq_f64(*a1, *a2))), vandq_s8((int8x16_t)vcgeq_f64(v3, vabdq_f64(a1[4], a2[4])), (int8x16_t)vcgeq_f64(v3, vabdq_f64(a1[6], a2[6])))), vandq_s8(vandq_s8((int8x16_t)vcgeq_f64(v3, vabdq_f64(a1[3], a2[3])), (int8x16_t)vcgeq_f64(v3, vabdq_f64(a1[1], a2[1]))), vandq_s8((int8x16_t)vcgeq_f64(v3, vabdq_f64(a1[5], a2[5])), (int8x16_t)vcgeq_f64(v3, vabdq_f64(a1[7], a2[7])))));
  return vandq_s8((int8x16_t)v4, (int8x16_t)vdupq_laneq_s64(v4, 1)).u64[0] >> 63;
}

void __swiftcall SPProjectiveTransform3D.flipped(along:)(SPProjectiveTransform3D *__return_ptr retstr, SPAxis *along)
{
  float64x2_t v4 = v2[1];
  float64x2_t v5 = v2[2];
  float64x2_t v6 = v2[3];
  float64x2_t v7 = v2[4];
  float64x2_t v8 = v2[5];
  float64x2_t v9 = v2[6];
  float64x2_t v10 = v2[7];
  v19[0]  = *v2;
  v19[1]  = v4;
  v19[2]  = v5;
  v19[3]  = v6;
  v19[4]  = v7;
  v19[5]  = v8;
  v19[6]  = v9;
  v19[7]  = v10;
  SPProjectiveTransform3DFlip(v19, (int)along, v18);
  float64x2_t v11 = v18[1];
  float64x2_t v12 = v18[2];
  float64x2_t v13 = v18[3];
  float64x2_t v14 = v18[4];
  float64x2_t v15 = v18[5];
  float64x2_t v16 = v18[6];
  float64x2_t v17 = v18[7];
  *(float64x2_t *)retstr->matrix.columns[0].f64  = v18[0];
  *(float64x2_t *)&retstr->matrix.columns[0].f64[2]  = v11;
  *(float64x2_t *)retstr->matrix.columns[1].f64  = v12;
  *(float64x2_t *)&retstr->matrix.columns[1].f64[2]  = v13;
  *(float64x2_t *)retstr->matrix.columns[2].f64  = v14;
  *(float64x2_t *)&retstr->matrix.columns[2].f64[2]  = v15;
  *(float64x2_t *)retstr->matrix.columns[3].f64  = v16;
  *(float64x2_t *)&retstr->matrix.columns[3].f64[2]  = v17;
}

float64x2_t *SPProjectiveTransform3DFlip@<X0>(float64x2_t *result@<X0>, int a2@<W1>, float64x2_t *a3@<X8>)
{
  if (result[1].f64[1] != 0.0 || result[3].f64[1] != 0.0 || result[5].f64[1] != 0.0 || result[7].f64[1] != 1.0)
  {
    a3[4]  = (float64x2_t)xmmword_228C20CA0;
    a3[5]  = (float64x2_t)unk_228C20CB0;
    a3[6]  = (float64x2_t)xmmword_228C20CC0;
    a3[7]  = (float64x2_t)unk_228C20CD0;
    *a3  = (float64x2_t)SPProjectiveTransform3DInvalid_0;
    a3[1]  = (float64x2_t)unk_228C20C70;
    a3[2]  = (float64x2_t)xmmword_228C20C80;
    a3[3]  = (float64x2_t)unk_228C20C90;
    return result;
  }
  float64x2_t v3 = (float64x2_t)xmmword_228C1FC60;
  float64x2_t v4 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if (vabdd_f64(0.0, result[1].f64[1]) >= 0.0000000149011612
    || vabdd_f64(0.0, result[3].f64[1]) >= 0.0000000149011612
    || (float64x2_t v4 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL), vabdd_f64(0.0, result[5].f64[1]) >= 0.0000000149011612))
  {
    float64x2_t v5 = v4;
    float64x2_t v6 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v7 = v4;
    float64x2_t v8 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v9 = v4;
    float64x2_t v10 = (float64x2_t)xmmword_228C1FC60;
  }
  else
  {
    float64x2_t v5 = v4;
    float64x2_t v6 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v7 = v4;
    float64x2_t v8 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v9 = v4;
    float64x2_t v10 = (float64x2_t)xmmword_228C1FC60;
    if (vabdd_f64(1.0, result[7].f64[1]) < 0.0000000149011612)
    {
      float64x2_t v4 = *result;
      float64x2_t v5 = result[2];
      float64x2_t v3 = result[1];
      float64x2_t v7 = result[4];
      float64x2_t v6 = result[3];
      float64x2_t v8 = result[5];
      float64x2_t v10 = result[7];
      float64x2_t v9 = result[6];
    }
  }
  BOOL v11 = __OFSUB__(a2, 1);
  if (a2 == 1)
  {
    unsigned int v12 = 0;
LABEL_18:
    v65.f64[1]  = 0.0;
    float64x2_t v66 = 0u;
    float64x2_t v63 = 0u;
    v61.f64[0]  = 0.0;
    float64x2_t v62 = 0u;
    float64x2_t v60 = 0u;
    float64x2_t v59 = (float64x2_t)0x3FF0000000000000uLL;
    v61.f64[1]  = 1.0;
    v64  = (float64x2_t)0x3FF0000000000000uLL;
    v59.f64[4 * v12 + v12]  = -1.0;
    v65.f64[0]  = 0.0;
    float64x2_t v14 = v59;
    float64x2_t v13 = v60;
    float64x2_t v16 = v61;
    float64x2_t v15 = v62;
    float64x2_t v18 = v63;
    float64x2_t v17 = v64;
    float64x2_t v20 = v65;
    float64x2_t v19 = v66;
    float64x2_t v22 = *(float64x2_t *)MEMORY[0x263EF8988];
    float64x2_t v21 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
    float64x2_t v24 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
    float64x2_t v23 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
    float64x2_t v26 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
    float64x2_t v25 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
    int64x2_t v27 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v24, v61), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v59)), (int8x16_t)vceqq_f64(v26, v63));
    unint64_t v28 = vandq_s8((int8x16_t)vdupq_laneq_s64(v27, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v23, v62), (int8x16_t)vceqq_f64(v21, v60)), (int8x16_t)vceqq_f64(v25, v64)), (int8x16_t)v27)).u64[0];
    if ((v28 & 0x8000000000000000) != 0
      && (int64x2_t v29 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v24, v5), (int8x16_t)vceqq_f64(v22, v4)), (int8x16_t)vceqq_f64(v26, v7)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v29, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v23, v6), (int8x16_t)vceqq_f64(v21, v3)), (int8x16_t)vceqq_f64(v25, v8)), (int8x16_t)v29)).u64[0] & 0x8000000000000000) != 0))
    {
      float64x2_t v9 = vaddq_f64(v9, v65);
      float64x2_t v7 = v63;
      float64x2_t v8 = v64;
      float64x2_t v10 = vaddq_f64(v10, v66);
      float64x2_t v5 = v61;
      float64x2_t v6 = v62;
      float64x2_t v4 = v59;
      float64x2_t v3 = v60;
    }
    else
    {
      int64x2_t v30 = vceqzq_f64(v65);
      if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v30, 1), vandq_s8((int8x16_t)vceqzq_f64(v66), (int8x16_t)v30)).u64[0] & 0x8000000000000000) != 0
        && (int64x2_t v31 = vceqzq_f64(v9),
            (vandq_s8((int8x16_t)vdupq_laneq_s64(v31, 1), vandq_s8((int8x16_t)vceqzq_f64(v10), (int8x16_t)v31)).u64[0] & 0x8000000000000000) != 0))
      {
        uint64_t v48 = 0;
        float64x2_t v67 = v4;
        float64x2_t v68 = v3;
        float64x2_t v69 = v5;
        float64x2_t v70 = v6;
        float64x2_t v71 = v7;
        float64x2_t v72 = v8;
        v75  = 0u;
        v76  = 0u;
        v77  = 0u;
        v78  = 0u;
        v79  = 0u;
        v80  = 0u;
        v49.f64[0]  = v59.f64[0];
        *(void *)&v49.f64[1]  = vextq_s8((int8x16_t)v59, (int8x16_t)v59, 8uLL).u64[0];
        v50.f64[0]  = v61.f64[0];
        *(void *)&v50.f64[1]  = vextq_s8((int8x16_t)v61, (int8x16_t)v61, 8uLL).u64[0];
        v51.f64[0]  = v63.f64[0];
        *(void *)&v51.f64[1]  = vextq_s8((int8x16_t)v63, (int8x16_t)v63, 8uLL).u64[0];
        do
        {
          float64x2_t v53 = *(float64x2_t *)((char *)&v67 + v48);
          float64x2_t v52 = *(float64x2_t *)((char *)&v67 + v48 + 16);
          long long v54 = (float64x2_t *)((char *)&v75 + v48);
          *long long v54 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v49, v53.f64[0]), v50, v53, 1), v51, v52.f64[0]);
          v54[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v13, v53), v15, v53, 1), v52, v17);
          v48 += 32;
          BOOL v11 = __OFSUB__(v48, 96);
        }
        while (v48 != 96);
        float64x2_t v4 = v75;
        float64x2_t v3 = v76;
        float64x2_t v5 = v77;
        float64x2_t v6 = v78;
        float64x2_t v9 = v20;
        float64x2_t v10 = v19;
        float64x2_t v7 = v79;
        float64x2_t v8 = v80;
      }
      else
      {
        uint64_t v32 = 0;
        float64x2_t v33 = (float64x2_t)*(unint64_t *)&v60.f64[0];
        float64x2_t v34 = (float64x2_t)*(unint64_t *)&v62.f64[0];
        float64x2_t v35 = (float64x2_t)*(unint64_t *)&v64.f64[0];
        v36.f64[0]  = v66.f64[0];
        v36.f64[1]  = 1.0;
        v10.f64[1]  = 1.0;
        float64x2_t v67 = v4;
        float64x2_t v68 = (float64x2_t)*(unint64_t *)&v3.f64[0];
        float64x2_t v69 = v5;
        float64x2_t v70 = (float64x2_t)*(unint64_t *)&v6.f64[0];
        float64x2_t v71 = v7;
        float64x2_t v72 = (float64x2_t)*(unint64_t *)&v8.f64[0];
        v73  = v9;
        v74  = v10;
        v75  = 0u;
        v76  = 0u;
        v77  = 0u;
        v78  = 0u;
        v79  = 0u;
        v80  = 0u;
        v81  = 0u;
        v82  = 0u;
        do
        {
          float64x2_t v38 = *(float64x2_t *)((char *)&v67 + v32);
          float64x2_t v37 = *(float64x2_t *)((char *)&v67 + v32 + 16);
          double v39 = (float64x2_t *)((char *)&v75 + v32);
          *double v39 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v14, v38.f64[0]), v16, v38, 1), v18, v37.f64[0]), v20, v37, 1);
          v39[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v33, v38.f64[0]), v34, v38, 1), v35, v37.f64[0]), v36, v37, 1);
          v32 += 32;
          BOOL v11 = __OFSUB__(v32, 128);
        }
        while (v32 != 128);
        float64x2_t v4 = v75;
        float64x2_t v3 = v76;
        float64x2_t v5 = v77;
        float64x2_t v6 = v78;
        float64x2_t v7 = v79;
        float64x2_t v8 = v80;
        float64x2_t v9 = v81;
        float64x2_t v10 = v82;
      }
    }
    int8x16_t v40 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v23, v6), (int8x16_t)vceqq_f64(v25, v8)), (int8x16_t)vceqq_f64(v21, v3));
    int64x2_t v41 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v24, v5), (int8x16_t)vceqq_f64(v26, v7)), (int8x16_t)vceqq_f64(v22, v4));
    if (((vandq_s8((int8x16_t)vdupq_laneq_s64(v41, 1), vandq_s8(v40, (int8x16_t)v41)).u64[0] & v28 & 0x8000000000000000) != 0) != v11)
    {
      float64x2_t v9 = vaddq_f64(v20, v9);
      *(void *)&v10.f64[0]  = *(_OWORD *)&vaddq_f64(v19, v10);
    }
    else
    {
      int64x2_t v42 = vceqzq_f64(v9);
      if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v42, 1), vandq_s8((int8x16_t)vceqzq_f64(v10), (int8x16_t)v42)).u64[0] & 0x8000000000000000) != 0
        && (int64x2_t v43 = vceqzq_f64(v20),
            (vandq_s8((int8x16_t)vdupq_laneq_s64(v43, 1), vandq_s8((int8x16_t)vceqzq_f64(v19), (int8x16_t)v43)).u64[0] & 0x8000000000000000) != 0))
      {
        uint64_t v55 = 0;
        float64x2_t v67 = v14;
        float64x2_t v68 = v13;
        float64x2_t v69 = v16;
        float64x2_t v70 = v15;
        float64x2_t v71 = v18;
        float64x2_t v72 = v17;
        v75  = 0u;
        v76  = 0u;
        v77  = 0u;
        v78  = 0u;
        v79  = 0u;
        v80  = 0u;
        *(void *)&v4.f64[1]  = vextq_s8((int8x16_t)v4, (int8x16_t)v4, 8uLL).u64[0];
        *(void *)&v5.f64[1]  = vextq_s8((int8x16_t)v5, (int8x16_t)v5, 8uLL).u64[0];
        *(void *)&v7.f64[1]  = vextq_s8((int8x16_t)v7, (int8x16_t)v7, 8uLL).u64[0];
        do
        {
          float64x2_t v57 = *(float64x2_t *)((char *)&v67 + v55);
          float64x2_t v56 = *(float64x2_t *)((char *)&v67 + v55 + 16);
          long long v58 = (float64x2_t *)((char *)&v75 + v55);
          *long long v58 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v4, v57.f64[0]), v5, v57, 1), v7, v56.f64[0]);
          v58[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v3, v57), v6, v57, 1), v56, v8);
          v55 += 32;
        }
        while (v55 != 96);
        float64x2_t v4 = v75;
        v3.f64[0]  = v76.f64[0];
        float64x2_t v5 = v77;
        v6.f64[0]  = v78.f64[0];
        float64x2_t v7 = v79;
        v8.f64[0]  = v80.f64[0];
      }
      else
      {
        uint64_t v44 = 0;
        v3.f64[1]  = 0.0;
        v6.f64[1]  = 0.0;
        v8.f64[1]  = 0.0;
        v10.f64[1]  = 1.0;
        v19.f64[1]  = 1.0;
        float64x2_t v67 = v14;
        float64x2_t v68 = (float64x2_t)*(unint64_t *)&v13.f64[0];
        float64x2_t v69 = v16;
        float64x2_t v70 = (float64x2_t)*(unint64_t *)&v15.f64[0];
        float64x2_t v71 = v18;
        float64x2_t v72 = (float64x2_t)*(unint64_t *)&v17.f64[0];
        v73  = v20;
        v74  = v19;
        v75  = 0u;
        v76  = 0u;
        v77  = 0u;
        v78  = 0u;
        v79  = 0u;
        v80  = 0u;
        v81  = 0u;
        v82  = 0u;
        do
        {
          float64x2_t v46 = *(float64x2_t *)((char *)&v67 + v44);
          float64x2_t v45 = *(float64x2_t *)((char *)&v67 + v44 + 16);
          unint64_t v47 = (float64x2_t *)((char *)&v75 + v44);
          *unint64_t v47 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v4, v46.f64[0]), v5, v46, 1), v7, v45.f64[0]), v9, v45, 1);
          v47[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v3, v46.f64[0]), v6, v46, 1), v8, v45.f64[0]), v10, v45, 1);
          v44 += 32;
        }
        while (v44 != 128);
        float64x2_t v4 = v75;
        v3.f64[0]  = v76.f64[0];
        float64x2_t v5 = v77;
        v6.f64[0]  = v78.f64[0];
        float64x2_t v7 = v79;
        v8.f64[0]  = v80.f64[0];
        float64x2_t v9 = v81;
        v10.f64[0]  = v82.f64[0];
      }
    }
    goto LABEL_32;
  }
  BOOL v11 = __OFSUB__(a2, 4);
  if (a2 == 4)
  {
    unsigned int v12 = 2;
    goto LABEL_18;
  }
  BOOL v11 = __OFSUB__(a2, 2);
  if (a2 == 2)
  {
    unsigned int v12 = 1;
    goto LABEL_18;
  }
LABEL_32:
  v10.f64[1]  = 1.0;
  *a3  = v4;
  a3[1]  = (float64x2_t)*(unint64_t *)&v3.f64[0];
  a3[2]  = v5;
  a3[3]  = (float64x2_t)*(unint64_t *)&v6.f64[0];
  a3[4]  = v7;
  a3[5]  = (float64x2_t)*(unint64_t *)&v8.f64[0];
  a3[6]  = v9;
  a3[7]  = v10;
  return result;
}

Swift::Void __swiftcall SPProjectiveTransform3D.flip(along:)(SPAxis *along)
{
  float64x2_t v2 = v1[1];
  float64x2_t v3 = v1[2];
  float64x2_t v4 = v1[3];
  float64x2_t v5 = v1[4];
  float64x2_t v6 = v1[5];
  float64x2_t v7 = v1[6];
  float64x2_t v8 = v1[7];
  v17[0]  = *v1;
  v17[1]  = v2;
  v17[2]  = v3;
  v17[3]  = v4;
  v17[4]  = v5;
  v17[5]  = v6;
  v17[6]  = v7;
  v17[7]  = v8;
  SPProjectiveTransform3DFlip(v17, (int)along, v16);
  float64x2_t v9 = v16[1];
  float64x2_t v10 = v16[2];
  float64x2_t v11 = v16[3];
  float64x2_t v12 = v16[4];
  float64x2_t v13 = v16[5];
  float64x2_t v14 = v16[6];
  float64x2_t v15 = v16[7];
  float64x2_t *v1 = v16[0];
  v1[1]  = v9;
  v1[2]  = v10;
  v1[3]  = v11;
  v1[4]  = v12;
  v1[5]  = v13;
  v1[6]  = v14;
  v1[7]  = v15;
}

void __swiftcall SPProjectiveTransform3D.scaledBy(x:y:z:)(SPProjectiveTransform3D *__return_ptr retstr, Swift::Double x, Swift::Double y, Swift::Double z)
{
  float64x2_t v6 = v4[1];
  float64x2_t v7 = v4[2];
  float64x2_t v8 = v4[3];
  float64x2_t v9 = v4[4];
  float64x2_t v10 = v4[5];
  float64x2_t v11 = v4[6];
  float64x2_t v12 = v4[7];
  v21[0]  = *v4;
  v21[1]  = v6;
  v21[2]  = v7;
  v21[3]  = v8;
  v21[4]  = v9;
  v21[5]  = v10;
  v21[6]  = v11;
  v21[7]  = v12;
  SPProjectiveTransform3DScaleBy(v21, v20, *(unint64_t *)&x, y, *(unint64_t *)&z);
  float64x2_t v13 = v20[1];
  float64x2_t v14 = v20[2];
  float64x2_t v15 = v20[3];
  float64x2_t v16 = v20[4];
  float64x2_t v17 = v20[5];
  float64x2_t v18 = v20[6];
  float64x2_t v19 = v20[7];
  *(float64x2_t *)retstr->matrix.columns[0].f64  = v20[0];
  *(float64x2_t *)&retstr->matrix.columns[0].f64[2]  = v13;
  *(float64x2_t *)retstr->matrix.columns[1].f64  = v14;
  *(float64x2_t *)&retstr->matrix.columns[1].f64[2]  = v15;
  *(float64x2_t *)retstr->matrix.columns[2].f64  = v16;
  *(float64x2_t *)&retstr->matrix.columns[2].f64[2]  = v17;
  *(float64x2_t *)retstr->matrix.columns[3].f64  = v18;
  *(float64x2_t *)&retstr->matrix.columns[3].f64[2]  = v19;
}

float64x2_t *SPProjectiveTransform3DScaleBy@<X0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X8>, unint64_t a3@<D0>, float64_t a4@<D1>, unint64_t a5@<D2>)
{
  v6.f64[0]  = 0.0;
  v6.f64[1]  = a4;
  float64x2_t v7 = a1[5];
  v13[4]  = a1[4];
  v13[5]  = v7;
  float64x2_t v8 = a1[7];
  v13[6]  = a1[6];
  v13[7]  = v8;
  float64x2_t v9 = a1[1];
  v13[0]  = *a1;
  v13[1]  = v9;
  float64x2_t v10 = a1[3];
  v13[2]  = a1[2];
  v13[3]  = v10;
  v12[0]  = (float64x2_t)a3;
  v12[1]  = 0u;
  v12[2]  = v6;
  memset(&v12[3], 0, 32);
  v12[5]  = (float64x2_t)a5;
  v12[6]  = 0u;
  v12[7]  = (float64x2_t)xmmword_228C1F7A0;
  return SPProjectiveTransform3DConcatenation(v13, v12, a2);
}

{
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v12[8];
  float64x2_t v13[8];

  v6.f64[0]  = 0.0;
  v6.f64[1]  = a4;
  float64x2_t v7 = a1[5];
  v13[4]  = a1[4];
  v13[5]  = v7;
  float64x2_t v8 = a1[7];
  v13[6]  = a1[6];
  v13[7]  = v8;
  float64x2_t v9 = a1[1];
  v13[0]  = *a1;
  v13[1]  = v9;
  float64x2_t v10 = a1[3];
  v13[2]  = a1[2];
  v13[3]  = v10;
  v12[0]  = (float64x2_t)a3;
  v12[1]  = 0u;
  v12[2]  = v6;
  memset(&v12[3], 0, 32);
  v12[5]  = (float64x2_t)a5;
  v12[6]  = 0u;
  v12[7]  = (float64x2_t)xmmword_228C1F7A0;
  return SPProjectiveTransform3DConcatenation(v13, v12, a2);
}

Swift::Bool __swiftcall SPProjectiveTransform3D.isUniform(overDimensions:)(Spatial::Dimension3DSet overDimensions)
{
  unint64_t v2 = *(void *)overDimensions.rawValue;
  if ((*(void *)overDimensions.rawValue & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_5;
  }
  if (HIDWORD(v2))
  {
LABEL_5:
    __break(1u);
    return overDimensions.rawValue;
  }
  float64x2_t v3 = v1[1];
  float64x2_t v5 = v1[2];
  float64x2_t v4 = v1[3];
  float64x2_t v7 = v1[4];
  float64x2_t v6 = v1[5];
  float64x2_t v9 = v1[6];
  float64x2_t v8 = v1[7];
  v11[0]  = *v1;
  v11[1]  = v3;
  v11[2]  = v5;
  v11[3]  = v4;
  v11[4]  = v7;
  v11[5]  = v6;
  v11[6]  = v9;
  v11[7]  = v8;
  LOBYTE(overDimensions.rawValue)  = SPProjectiveTransform3DIsUniformOverDimensions(v11, v2);
  return overDimensions.rawValue;
}

BOOL SPProjectiveTransform3DIsUniformOverDimensions(float64x2_t *a1, int a2)
{
  BOOL v2 = 0;
  if (a1[1].f64[1] == 0.0 && a1[3].f64[1] == 0.0 && a1[5].f64[1] == 0.0)
  {
    if (a1[7].f64[1] == 1.0)
    {
      float64x2_t v4 = (float64x2_t)xmmword_228C1FC60;
      float64x2_t v5 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
      if (vabdd_f64(0.0, a1[1].f64[1]) >= 0.0000000149011612
        || vabdd_f64(0.0, a1[3].f64[1]) >= 0.0000000149011612
        || (float64x2_t v5 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL), vabdd_f64(0.0, a1[5].f64[1]) >= 0.0000000149011612))
      {
        float64x2_t v6 = v5;
        float64x2_t v7 = (float64x2_t)xmmword_228C1FC60;
        float64x2_t v8 = v5;
        float64x2_t v9 = (float64x2_t)xmmword_228C1FC60;
      }
      else
      {
        float64x2_t v6 = v5;
        float64x2_t v7 = (float64x2_t)xmmword_228C1FC60;
        float64x2_t v8 = v5;
        float64x2_t v9 = (float64x2_t)xmmword_228C1FC60;
        if (vabdd_f64(1.0, a1[7].f64[1]) < 0.0000000149011612)
        {
          float64x2_t v5 = *a1;
          float64x2_t v6 = a1[2];
          float64x2_t v4 = a1[1];
          float64x2_t v7 = a1[3];
          float64x2_t v9 = a1[5];
          float64x2_t v8 = a1[4];
        }
      }
      v10.f64[0]  = v9.f64[0];
      v10.f64[1]  = v8.f64[0];
      v11.f64[0]  = v7.f64[0];
      v11.f64[1]  = v6.f64[0];
      float64x2_t v13 = vmulq_f64(v5, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)v9, 8uLL), vnegq_f64(v11)), v10, (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v7, 8uLL)));
      BOOL v12 = vmulq_f64(v4, vmlaq_laneq_f64(vmulq_f64(v8, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v6, 1))), v6, v8, 1)).f64[0]+ vaddvq_f64(v13) < 0.0;
      v13.f64[0]  = -1.0;
      if (!v12) {
        v13.f64[0]  = 1.0;
      }
      v14.f64[0]  = sqrt(vmulq_f64(v4, v4).f64[0] + vaddvq_f64(vmulq_f64(v5, v5)));
      float64x2_t v15 = vmulq_f64(v9, v9);
      v15.f64[0]  = sqrt(v15.f64[0] + vaddvq_f64(vmulq_f64(v8, v8)));
      v14.f64[1]  = sqrt(vmulq_f64(v7, v7).f64[0] + vaddvq_f64(vmulq_f64(v6, v6)));
      *(void *)&double v22 = *(_OWORD *)&vmulq_f64(v15, v13);
      float64x2_t v23 = vmulq_n_f64(v14, v13.f64[0]);
      if (a2 == 7)
      {
        float64x2_t v24 = v5;
        float64x2_t v25 = v4;
        float64x2_t v26 = v6;
        float64x2_t v27 = v7;
        float64x2_t v28 = v8;
        float64x2_t v29 = v9;
        simd_quaternion((uint64_t)&v24, (uint64_t)&v30);
        long double v16 = atan2(sqrt(vmulq_f64(v31, v31).f64[0] + vaddvq_f64(vmulq_f64(v30, v30))), v31.f64[1]);
        double v17 = remainder(v16 + v16, 1.57079633);
        BOOL v2 = 0;
        if (fabs(v17) < 0.0000000149011612 && v23.f64[0] == v23.f64[1]) {
          return v23.f64[1] == v22;
        }
      }
      else
      {
        float64x2_t v24 = v5;
        float64x2_t v25 = v4;
        float64x2_t v26 = v6;
        float64x2_t v27 = v7;
        float64x2_t v28 = v8;
        float64x2_t v29 = v9;
        simd_quaternion((uint64_t)&v24, (uint64_t)&v30);
        long double v18 = atan2(sqrt(vmulq_f64(v31, v31).f64[0] + vaddvq_f64(vmulq_f64(v30, v30))), v31.f64[1]);
        double v19 = fabs(remainder(v18 + v18, 1.57079633));
        BOOL v2 = v19 < 0.0000000149011612;
        switch(a2)
        {
          case 6:
            LODWORD(v2)  = v19 < 0.0000000149011612;
            BOOL v20 = v23.f64[1] == v22;
            break;
          case 5:
            LODWORD(v2)  = v19 < 0.0000000149011612;
            BOOL v20 = v23.f64[0] == v22;
            break;
          case 3:
            LODWORD(v2)  = v19 < 0.0000000149011612;
            BOOL v20 = v23.f64[0] == v23.f64[1];
            break;
          default:
            return v2;
        }
        return v20 && v2;
      }
    }
    else
    {
      return 0;
    }
  }
  return v2;
}

uint64_t SPProjectiveTransform3D.init(shear:)@<X0>(uint64_t a1@<X0>, _OWORD *a2@<X8>, float64x2_t a3@<Q0>, float64x2_t a4@<Q1>)
{
  int v5 = *(unsigned __int8 *)(a1 + 16);
  if (v5 == 2)
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    uint64_t v6 = 4;
    goto LABEL_7;
  }
  if (v5 == 1)
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    uint64_t v6 = 2;
LABEL_7:
    uint64_t result = SPProjectiveTransform3DMakeShear(v6, a3, a4, v15);
    long long v8 = v15[1];
    long long v10 = v15[2];
    long long v9 = v15[3];
    long long v12 = v15[4];
    long long v11 = v15[5];
    long long v14 = v15[6];
    long long v13 = v15[7];
    *a2  = v15[0];
    a2[1]  = v8;
    a2[2]  = v10;
    a2[3]  = v9;
    a2[4]  = v12;
    a2[5]  = v11;
    a2[6]  = v14;
    a2[7]  = v13;
    return result;
  }
  if (!*(unsigned char *)(a1 + 16))
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    uint64_t v6 = 1;
    goto LABEL_7;
  }
  uint64_t result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t SPProjectiveTransform3DMakeShear@<X0>(uint64_t result@<X0>, float64x2_t a2@<Q0>, float64x2_t a3@<Q1>, long long *a4@<X8>)
{
  __asm { FMOV            V7.2D, #1.0 }
  switch(result)
  {
    case 4:
      a2.f64[1]  = a3.f64[0];
      float64x2_t v9 = (float64x2_t)xmmword_228C1F7D0;
      float64x2_t v12 = 0uLL;
      _Q5  = _Q7;
      a3  = 0uLL;
      float64x2_t v10 = (float64x2_t)xmmword_228C1F7A0;
      break;
    case 2:
      __asm { FMOV            V5.2D, #1.0 }
      v10.f64[1]  = _Q5.f64[1];
      v10.f64[0]  = a2.f64[0];
      float64x2_t v9 = (float64x2_t)xmmword_228C1F7D0;
      a2  = 0uLL;
      float64x2_t v12 = a3;
      a3  = 0uLL;
      break;
    case 1:
      v9.f64[0]  = _Q7.f64[0];
      v9.f64[1]  = a2.f64[0];
      float64x2_t v10 = (float64x2_t)xmmword_228C1F7A0;
      a2  = 0uLL;
      _Q5  = _Q7;
      float64x2_t v12 = 0uLL;
      break;
    default:
      long long v28 = xmmword_228C1F7D0;
      long long v19 = xmmword_228C1F7A0;
      unint64_t v27 = 0;
      unint64_t v29 = 0;
      long long v32 = 0uLL;
      uint64_t v18 = 0x3FF0000000000000;
LABEL_14:
      long long v30 = 0uLL;
      *(void *)&long long v31 = 0;
      goto LABEL_15;
  }
  float64x2_t v13 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v14 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v17 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  uint64_t v18 = 0x3FF0000000000000;
  long long v19 = xmmword_228C1F7A0;
  int64x2_t v20 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], (float64x2_t)xmmword_228C1F7D0)), (int8x16_t)vceqzq_f64(v16));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v20, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqzq_f64(v14), (int8x16_t)vceqzq_f64(v13)), (int8x16_t)vceqq_f64(v17, _Q7)), (int8x16_t)v20)).u64[0] & 0x8000000000000000) != 0)
  {
    int64x2_t v21 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, v10), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v9)), (int8x16_t)vceqq_f64(v16, a2));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v21, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v12), (int8x16_t)vceqq_f64(v13, a3)), (int8x16_t)vceqq_f64(v17, _Q5)), (int8x16_t)v21)).u64[0] & 0x8000000000000000) != 0)
    {
      unint64_t v27 = 0;
      long long v28 = xmmword_228C1F7D0;
      unint64_t v29 = 0;
      long long v32 = 0uLL;
      goto LABEL_14;
    }
  }
  uint64_t v22 = 0;
  v33[0]  = v9;
  v33[1]  = a3;
  v33[2]  = v10;
  v33[3]  = v12;
  v33[4]  = a2;
  v33[5]  = _Q5;
  long long v34 = 0u;
  long long v35 = 0u;
  long long v36 = 0u;
  long long v37 = 0u;
  __asm { FMOV            V1.2D, #1.0 }
  long long v38 = 0u;
  long long v39 = 0u;
  do
  {
    float64x2_t v25 = (float64x2_t)v33[v22];
    float64x2_t v24 = (float64x2_t)v33[v22 + 1];
    float64x2_t v26 = (float64x2_t *)((char *)&v34 + v22 * 16);
    *float64x2_t v26 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)xmmword_228C1F7D0, v25.f64[0]), (float64x2_t)xmmword_228C1F7A0, v25, 1), (float64x2_t)0, v24.f64[0]);
    v26[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v25, (float64x2_t)0), (float64x2_t)0, v25, 1), _Q1, v24);
    v22 += 2;
  }
  while (v22 != 6);
  long long v28 = v34;
  unint64_t v27 = v35;
  long long v19 = v36;
  unint64_t v29 = v37;
  long long v30 = 0uLL;
  *(void *)&long long v31 = 0;
  long long v32 = v38;
  uint64_t v18 = v39;
LABEL_15:
  *((void *)&v31 + 1)  = 1.0;
  *a4  = v28;
  a4[1]  = v27;
  a4[2]  = v19;
  a4[3]  = v29;
  a4[4]  = v32;
  a4[5]  = (unint64_t)v18;
  a4[6]  = v30;
  a4[7]  = v31;
  return result;
}

uint64_t SPProjectiveTransform3D.sheared(_:)@<X0>(uint64_t a1@<X0>, float64x2_t *a2@<X8>, float64x2_t a3@<Q0>, float64x2_t a4@<Q1>)
{
  long long v6 = v4[1];
  long long v8 = v4[2];
  long long v7 = v4[3];
  long long v10 = v4[4];
  long long v9 = v4[5];
  long long v12 = v4[6];
  long long v11 = v4[7];
  int v13 = *(unsigned __int8 *)(a1 + 16);
  if (v13 == 2)
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    long long v24 = *v4;
    long long v25 = v6;
    long long v26 = v8;
    long long v27 = v7;
    long long v28 = v10;
    long long v29 = v9;
    long long v30 = v12;
    long long v31 = v11;
    int v14 = 4;
    goto LABEL_7;
  }
  if (v13 == 1)
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    long long v24 = *v4;
    long long v25 = v6;
    long long v26 = v8;
    long long v27 = v7;
    long long v28 = v10;
    long long v29 = v9;
    long long v30 = v12;
    long long v31 = v11;
    int v14 = 2;
LABEL_7:
    uint64_t result = (uint64_t)SPProjectiveTransform3DShear(&v24, v14, v23, a3, a4);
    float64x2_t v16 = v23[1];
    float64x2_t v18 = v23[2];
    float64x2_t v17 = v23[3];
    float64x2_t v20 = v23[4];
    float64x2_t v19 = v23[5];
    float64x2_t v22 = v23[6];
    float64x2_t v21 = v23[7];
    *a2  = v23[0];
    a2[1]  = v16;
    a2[2]  = v18;
    a2[3]  = v17;
    a2[4]  = v20;
    a2[5]  = v19;
    a2[6]  = v22;
    a2[7]  = v21;
    return result;
  }
  if (!*(unsigned char *)(a1 + 16))
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    long long v24 = *v4;
    long long v25 = v6;
    long long v26 = v8;
    long long v27 = v7;
    long long v28 = v10;
    long long v29 = v9;
    long long v30 = v12;
    long long v31 = v11;
    int v14 = 1;
    goto LABEL_7;
  }
  uint64_t result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

float64x2_t *SPProjectiveTransform3DShear@<X0>(long long *a1@<X0>, int a2@<W1>, float64x2_t *a3@<X8>, float64x2_t a4@<Q0>, float64x2_t a5@<Q1>)
{
  __asm { FMOV            V7.2D, #1.0 }
  switch(a2)
  {
    case 4:
      a4.f64[1]  = a5.f64[0];
      float64x2_t v10 = (float64x2_t)xmmword_228C1F7D0;
      float64x2_t v13 = 0uLL;
      _Q5  = _Q7;
      a5  = 0uLL;
      float64x2_t v11 = (float64x2_t)xmmword_228C1F7A0;
      break;
    case 2:
      __asm { FMOV            V5.2D, #1.0 }
      v11.f64[1]  = _Q5.f64[1];
      v11.f64[0]  = a4.f64[0];
      float64x2_t v10 = (float64x2_t)xmmword_228C1F7D0;
      a4  = 0uLL;
      float64x2_t v13 = a5;
      a5  = 0uLL;
      break;
    case 1:
      v10.f64[0]  = _Q7.f64[0];
      v10.f64[1]  = a4.f64[0];
      float64x2_t v11 = (float64x2_t)xmmword_228C1F7A0;
      a4  = 0uLL;
      _Q5  = _Q7;
      float64x2_t v13 = 0uLL;
      break;
    default:
      long long v29 = xmmword_228C1F7D0;
      long long v20 = xmmword_228C1F7A0;
      unint64_t v28 = 0;
      unint64_t v30 = 0;
      float64x2_t v33 = 0uLL;
      uint64_t v19 = 0x3FF0000000000000;
LABEL_14:
      long long v31 = 0uLL;
      *(void *)&long long v32 = 0;
      goto LABEL_15;
  }
  float64x2_t v14 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v17 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v18 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  uint64_t v19 = 0x3FF0000000000000;
  long long v20 = xmmword_228C1F7A0;
  int64x2_t v21 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v16, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], (float64x2_t)xmmword_228C1F7D0)), (int8x16_t)vceqzq_f64(v17));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v21, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqzq_f64(v15), (int8x16_t)vceqzq_f64(v14)), (int8x16_t)vceqq_f64(v18, _Q7)), (int8x16_t)v21)).u64[0] & 0x8000000000000000) != 0)
  {
    int64x2_t v22 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v16, v11), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v10)), (int8x16_t)vceqq_f64(v17, a4));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v22, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, v13), (int8x16_t)vceqq_f64(v14, a5)), (int8x16_t)vceqq_f64(v18, _Q5)), (int8x16_t)v22)).u64[0] & 0x8000000000000000) != 0)
    {
      unint64_t v28 = 0;
      long long v29 = xmmword_228C1F7D0;
      unint64_t v30 = 0;
      float64x2_t v33 = 0uLL;
      goto LABEL_14;
    }
  }
  uint64_t v23 = 0;
  long long v39 = (__int128)v10;
  float64x2_t v40 = a5;
  long long v41 = (__int128)v11;
  float64x2_t v42 = v13;
  float64x2_t v43 = a4;
  float64x2_t v44 = _Q5;
  long long v47 = 0u;
  long long v48 = 0u;
  long long v49 = 0u;
  long long v50 = 0u;
  __asm { FMOV            V1.2D, #1.0 }
  float64x2_t v51 = 0u;
  long long v52 = 0u;
  do
  {
    float64x2_t v26 = *(float64x2_t *)((char *)&v39 + v23);
    float64x2_t v25 = *(float64x2_t *)((char *)&v39 + v23 + 16);
    long long v27 = (float64x2_t *)((char *)&v47 + v23);
    *long long v27 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)xmmword_228C1F7D0, v26.f64[0]), (float64x2_t)xmmword_228C1F7A0, v26, 1), (float64x2_t)0, v25.f64[0]);
    v27[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v26, (float64x2_t)0), (float64x2_t)0, v26, 1), _Q1, v25);
    v23 += 32;
  }
  while (v23 != 96);
  long long v29 = v47;
  unint64_t v28 = v48;
  long long v20 = v49;
  unint64_t v30 = v50;
  long long v31 = 0uLL;
  *(void *)&long long v32 = 0;
  float64x2_t v33 = v51;
  uint64_t v19 = v52;
LABEL_15:
  *((void *)&v32 + 1)  = 1.0;
  long long v34 = a1[5];
  float64x2_t v51 = (float64x2_t)a1[4];
  long long v52 = v34;
  long long v35 = a1[7];
  long long v53 = a1[6];
  long long v54 = v35;
  long long v36 = a1[1];
  long long v47 = *a1;
  long long v48 = v36;
  long long v37 = a1[3];
  long long v49 = a1[2];
  long long v50 = v37;
  long long v39 = v29;
  float64x2_t v40 = (float64x2_t)v28;
  long long v41 = v20;
  float64x2_t v42 = (float64x2_t)v30;
  float64x2_t v43 = v33;
  float64x2_t v44 = (float64x2_t)(unint64_t)v19;
  long long v45 = v31;
  long long v46 = v32;
  return SPProjectiveTransform3DConcatenation((float64x2_t *)&v47, (float64x2_t *)&v39, a3);
}

unint64_t SPProjectiveTransform3D.rotation.getter@<X0>(uint64_t a1@<X8>)
{
  long long v3 = *(_OWORD *)(v1 + 16);
  long long v4 = *(_OWORD *)(v1 + 32);
  long long v5 = *(_OWORD *)(v1 + 48);
  long long v6 = *(_OWORD *)(v1 + 64);
  long long v7 = *(_OWORD *)(v1 + 80);
  long long v8 = *(_OWORD *)(v1 + 96);
  long long v9 = *(_OWORD *)(v1 + 112);
  float64x2_t v17 = *(float64x2_t *)v1;
  long long v18 = v3;
  long long v19 = v4;
  long long v20 = v5;
  long long v21 = v6;
  long long v22 = v7;
  long long v23 = v8;
  long long v24 = v9;
  SPProjectiveTransform3DGetRotation(&v17, &v15);
  uint64_t v10 = *((void *)&v16 + 1);
  uint64_t v11 = v16;
  float64_t v12 = v15.f64[1];
  float64_t v13 = v15.f64[0];
  long long v18 = v16;
  float64x2_t v17 = v15;
  unint64_t result = SPRotation3DIsValid(&v17);
  if (!result)
  {
    float64_t v13 = 0.0;
    float64_t v12 = 0.0;
    uint64_t v11 = 0;
    uint64_t v10 = 0;
  }
  *(float64_t *)a1  = v13;
  *(float64_t *)(a1 + 8)  = v12;
  *(void *)(a1 + 16)  = v11;
  *(void *)(a1 + 24)  = v10;
  *(unsigned char *)(a1 + 32)  = result ^ 1;
  return result;
}

void SPProjectiveTransform3DGetRotation(float64x2_t *a1@<X0>, float64x2_t *a2@<X8>)
{
  float64x2_t v4 = *a1;
  float64x2_t v3 = a1[1];
  float64x2_t v6 = a1[2];
  float64x2_t v5 = a1[3];
  float64x2_t v7 = a1[4];
  float64x2_t v8 = a1[5];
  v9.f64[0]  = a1[5].f64[0];
  v9.f64[1]  = a1[4].f64[0];
  v10.f64[0]  = a1[3].f64[0];
  v10.f64[1]  = a1[2].f64[0];
  float64x2_t v11 = vmulq_f64(*a1, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v8, 8uLL), vnegq_f64(v10)), v9, (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v5, 8uLL)));
  float64x2_t v13 = vmulq_f64(v3, vmlaq_laneq_f64(vmulq_f64(v7, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v6, 1))), v6, v7, 1));
  BOOL v12 = v13.f64[0] + vaddvq_f64(v11) < 0.0;
  v13.f64[0]  = -1.0;
  if (!v12) {
    v13.f64[0]  = 1.0;
  }
  v14.f64[0]  = sqrt(vmulq_f64(v3, v3).f64[0] + vaddvq_f64(vmulq_f64(v4, v4)));
  float64x2_t v15 = vmulq_f64(v8, v8);
  v15.f64[0]  = sqrt(v15.f64[0] + vaddvq_f64(vmulq_f64(v7, v7)));
  v14.f64[1]  = sqrt(vmulq_f64(v5, v5).f64[0] + vaddvq_f64(vmulq_f64(v6, v6)));
  float64x2_t v16 = vmulq_n_f64(v14, v13.f64[0]);
  float64x2_t v17 = vmulq_f64(v15, v13);
  float64x2_t v18 = vdivq_f64(v4, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v16.f64[0], 0));
  float64x2_t v19 = vdivq_f64(v3, v16);
  float64x2_t v20 = (float64x2_t)vdupq_laneq_s64((int64x2_t)v16, 1);
  float64x2_t v21 = vdivq_f64(v6, v20);
  float64x2_t v22 = vdivq_f64(v5, v20);
  float64x2_t v23 = vdivq_f64(v7, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v17.f64[0], 0));
  float64x2_t v24 = vdivq_f64(v8, v17);
  float64x2_t v25 = vmulq_f64(v22, v19);
  float64x2_t v26 = vmulq_f64(v19, v19);
  v26.f64[0]  = v26.f64[0] + vaddvq_f64(vmulq_f64(v18, v18));
  v25.f64[0]  = (v25.f64[0] + vaddvq_f64(vmulq_f64(v21, v18))) / v26.f64[0];
  float64x2_t v27 = vsubq_f64(v21, vmulq_n_f64(v18, v25.f64[0]));
  float64x2_t v28 = vsubq_f64(v22, vmulq_f64(v19, v25));
  float64x2_t v29 = vmulq_f64(v24, v19);
  v29.f64[0]  = (v29.f64[0] + vaddvq_f64(vmulq_f64(v23, v18))) / v26.f64[0];
  float64x2_t v30 = vmulq_n_f64(v18, v29.f64[0]);
  float64x2_t v31 = vsubq_f64(v24, vmulq_f64(v19, v29));
  float64x2_t v32 = vsubq_f64(v23, v30);
  float64x2_t v33 = vmulq_f64(v23, v27);
  v33.f64[0]  = vmulq_f64(v24, v28).f64[0] + vaddvq_f64(v33);
  float64x2_t v34 = vmulq_f64(v28, v28);
  v34.f64[0]  = v34.f64[0] + vaddvq_f64(vmulq_f64(v27, v27));
  v33.f64[0]  = v33.f64[0] / v34.f64[0];
  float64x2_t v35 = vmulq_f64(v28, v33);
  float64x2_t v36 = vsubq_f64(v32, vmulq_n_f64(v27, v33.f64[0]));
  float64x2_t v37 = vsubq_f64(v31, v35);
  v26.f64[0]  = 1.0 / sqrt(v26.f64[0]);
  v34.f64[0]  = 1.0 / sqrt(v34.f64[0]);
  v11.f64[0]  = 1.0 / sqrt(vmulq_f64(v37, v37).f64[0] + vaddvq_f64(vmulq_f64(v36, v36)));
  float64x2_t v44 = 0u;
  float64x2_t v45 = 0u;
  v43[0]  = vmulq_n_f64(v18, v26.f64[0]);
  v43[1]  = vmulq_f64(v19, v26);
  v43[2]  = vmulq_n_f64(v27, v34.f64[0]);
  v43[3]  = vmulq_f64(v28, v34);
  v43[4]  = vmulq_n_f64(v36, v11.f64[0]);
  v43[5]  = vmulq_f64(v37, v11);
  simd_quaternion((uint64_t)v43, (uint64_t)&v44);
  int64x2_t v38 = (int64x2_t)vornq_s8(vmvnq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v44), (int8x16_t)vcgezq_f64(v44))), vorrq_s8((int8x16_t)vcltzq_f64(v45), (int8x16_t)vcgezq_f64(v45)));
  if ((vorrq_s8((int8x16_t)v38, (int8x16_t)vdupq_laneq_s64(v38, 1)).u64[0] & 0x8000000000000000) != 0)
  {
    float64x2_t v41 = (float64x2_t)SPRotation3DInvalid;
    float64x2_t v40 = (float64x2_t)unk_228C20CF0;
  }
  else
  {
    double v39 = vaddvq_f64(vaddq_f64(vmulq_f64(v44, v44), vmulq_f64(v45, v45)));
    if (v39 == 0.0)
    {
      float64x2_t v40 = (float64x2_t)xmmword_228C1F7A0;
      float64x2_t v41 = 0uLL;
    }
    else
    {
      double v42 = 1.0 / sqrt(v39);
      float64x2_t v40 = vmulq_n_f64(v45, v42);
      float64x2_t v41 = vmulq_n_f64(v44, v42);
    }
  }
  *a2  = v41;
  a2[1]  = v40;
}

unint64_t SPRotation3DIsValid(float64x2_t *a1)
{
  float64x2_t v1 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  int64x2_t v2 = (int64x2_t)vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(*a1), (int8x16_t)vcgezq_f64(*a1)), (int8x16_t)vceqq_f64(vabsq_f64(*a1), v1)), vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(a1[1]), (int8x16_t)vcgezq_f64(a1[1])), (int8x16_t)vceqq_f64(vabsq_f64(a1[1]), v1)));
  return vandq_s8((int8x16_t)v2, (int8x16_t)vdupq_laneq_s64(v2, 1)).u64[0] >> 63;
}

BOOL SPProjectiveTransform3D.scale.getter@<W0>(uint64_t a1@<X8>)
{
  long long v3 = v1[1];
  long long v4 = v1[2];
  long long v5 = v1[3];
  long long v6 = v1[4];
  long long v7 = v1[5];
  long long v8 = v1[6];
  long long v9 = v1[7];
  *(_OWORD *)&v16.width  = *v1;
  *(_OWORD *)&v16.vector.f64[2]  = v3;
  long long v17 = v4;
  long long v18 = v5;
  long long v19 = v6;
  long long v20 = v7;
  long long v21 = v8;
  long long v22 = v9;
  SPProjectiveTransform3DGetScale((float64x2_t *)&v16, (float64x2_t *)&v15);
  double v10 = v15.vector.f64[3];
  depth  = v15.depth;
  height  = v15.height;
  width  = v15.width;
  *(SPSize3D *)&v16.width  = *(SPSize3D *)&v15.width;
  BOOL result = SPSize3DIsValid(&v16);
  if (!result)
  {
    width  = 0.0;
    height  = 0.0;
    depth  = 0.0;
    double v10 = 0.0;
  }
  *(double *)a1  = width;
  *(double *)(a1 + 8)  = height;
  *(double *)(a1 + 16)  = depth;
  *(double *)(a1 + 24)  = v10;
  *(unsigned char *)(a1 + 32)  = !result;
  return result;
}

float64_t SPProjectiveTransform3DGetScale@<D0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X8>)
{
  float64x2_t v3 = a1[2];
  float64x2_t v2 = a1[3];
  float64x2_t v4 = a1[4];
  float64x2_t v5 = a1[5];
  float64x2_t v6 = (float64x2_t)vextq_s8((int8x16_t)v4, (int8x16_t)v5, 8uLL);
  *(void *)&double v7 = *(_OWORD *)&vmulq_f64(v5, v5);
  v5.f64[1]  = a1[4].f64[0];
  float64x2_t v8 = (float64x2_t)vextq_s8((int8x16_t)v3, (int8x16_t)v2, 8uLL);
  *(void *)&double v9 = *(_OWORD *)&vmulq_f64(v2, v2);
  v2.f64[1]  = a1[2].f64[0];
  float64x2_t v10 = a1[1];
  float64x2_t v12 = vmulq_f64(*a1, vmlaq_f64(vmulq_f64(v6, vnegq_f64(v2)), v5, v8));
  BOOL v11 = vmulq_f64(v10, vmlaq_laneq_f64(vmulq_f64(v4, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v3, 1))), v3, v4, 1)).f64[0]+ vaddvq_f64(v12) < 0.0;
  v12.f64[0]  = -1.0;
  if (!v11) {
    v12.f64[0]  = 1.0;
  }
  v13.f64[0]  = sqrt(vmulq_f64(v10, v10).f64[0] + vaddvq_f64(vmulq_f64(*a1, *a1)));
  float64x2_t v14 = vmulq_f64(v4, v4);
  v14.f64[0]  = sqrt(v7 + vaddvq_f64(v14));
  v13.f64[1]  = sqrt(v9 + vaddvq_f64(vmulq_f64(v3, v3)));
  float64x2_t v15 = vmulq_n_f64(v13, v12.f64[0]);
  *(void *)&float64_t result = *(_OWORD *)&vmulq_f64(v14, v12);
  *a2  = v15;
  a2[1].f64[0]  = result;
  return result;
}

BOOL SPSize3DIsValid(SPSize3D *a1)
{
  return a1->width != INFINITY || a1->height != INFINITY || a1->depth != INFINITY;
}

float64_t protocol witness for Rotatable3D.rotated(by:) in conformance SPProjectiveTransform3D@<D0>(float64x2_t *a1@<X8>, float64x2_t a2@<Q0>, float64x2_t a3@<Q1>)
{
  float64x2_t v5 = v3[1];
  float64x2_t v6 = v3[2];
  float64x2_t v7 = v3[3];
  float64x2_t v8 = v3[4];
  float64x2_t v9 = v3[5];
  float64x2_t v10 = v3[6];
  float64x2_t v11 = v3[7];
  v22[0]  = *v3;
  v22[1]  = v5;
  v22[2]  = v6;
  v22[3]  = v7;
  v22[4]  = v8;
  v22[5]  = v9;
  v22[6]  = v10;
  v22[7]  = v11;
  v21[0]  = a2;
  v21[1]  = a3;
  SPProjectiveTransform3DRotateByQuaternion(v22, v21, v20);
  float64_t result = v20[0].f64[0];
  float64x2_t v13 = v20[1];
  float64x2_t v14 = v20[2];
  float64x2_t v15 = v20[3];
  float64x2_t v16 = v20[4];
  float64x2_t v17 = v20[5];
  float64x2_t v18 = v20[6];
  float64x2_t v19 = v20[7];
  *a1  = v20[0];
  a1[1]  = v13;
  a1[2]  = v14;
  a1[3]  = v15;
  a1[4]  = v16;
  a1[5]  = v17;
  a1[6]  = v18;
  a1[7]  = v19;
  return result;
}

float64_t protocol witness for Scalable3D.scaledBy(x:y:z:) in conformance SPProjectiveTransform3D@<D0>(float64x2_t *a1@<X8>, unint64_t a2@<D0>, float64_t a3@<D1>, unint64_t a4@<D2>)
{
  float64x2_t v6 = v4[1];
  float64x2_t v7 = v4[2];
  float64x2_t v8 = v4[3];
  float64x2_t v9 = v4[4];
  float64x2_t v10 = v4[5];
  float64x2_t v11 = v4[6];
  float64x2_t v12 = v4[7];
  v22[0]  = *v4;
  v22[1]  = v6;
  v22[2]  = v7;
  v22[3]  = v8;
  v22[4]  = v9;
  v22[5]  = v10;
  v22[6]  = v11;
  v22[7]  = v12;
  SPProjectiveTransform3DScaleBy(v22, v21, a2, a3, a4);
  float64_t result = v21[0].f64[0];
  float64x2_t v14 = v21[1];
  float64x2_t v15 = v21[2];
  float64x2_t v16 = v21[3];
  float64x2_t v17 = v21[4];
  float64x2_t v18 = v21[5];
  float64x2_t v19 = v21[6];
  float64x2_t v20 = v21[7];
  *a1  = v21[0];
  a1[1]  = v14;
  a1[2]  = v15;
  a1[3]  = v16;
  a1[4]  = v17;
  a1[5]  = v18;
  a1[6]  = v19;
  a1[7]  = v20;
  return result;
}

double protocol witness for Scalable3D.scaled(by:) in conformance SPProjectiveTransform3D@<D0>(_OWORD *a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>)
{
  return protocol witness for Translatable3D.translated(by:) in conformance SPProjectiveTransform3D((void (*)(_OWORD *__return_ptr, _OWORD *, void *))SPProjectiveTransform3DScaleBySize, a1, a2, a3, a4);
}

double protocol witness for Translatable3D.translated(by:) in conformance SPProjectiveTransform3D@<D0>(void (*a1)(_OWORD *__return_ptr, _OWORD *, void *)@<X2>, _OWORD *a2@<X8>, double a3@<D0>, double a4@<D1>, double a5@<D2>)
{
  long long v7 = v5[1];
  long long v8 = v5[2];
  long long v9 = v5[3];
  long long v10 = v5[4];
  long long v11 = v5[5];
  long long v12 = v5[6];
  long long v13 = v5[7];
  v24[0]  = *v5;
  v24[1]  = v7;
  v24[2]  = v8;
  v24[3]  = v9;
  v24[4]  = v10;
  v24[5]  = v11;
  v24[6]  = v12;
  v24[7]  = v13;
  *(double *)float64x2_t v23 = a3;
  *(double *)&v23[1]  = a4;
  *(double *)&v23[2]  = a5;
  a1(v22, v24, v23);
  double result = *(double *)v22;
  long long v15 = v22[1];
  long long v16 = v22[2];
  long long v17 = v22[3];
  long long v18 = v22[4];
  long long v19 = v22[5];
  long long v20 = v22[6];
  long long v21 = v22[7];
  *a2  = v22[0];
  a2[1]  = v15;
  a2[2]  = v16;
  a2[3]  = v17;
  a2[4]  = v18;
  a2[5]  = v19;
  a2[6]  = v20;
  a2[7]  = v21;
  return result;
}

float64_t protocol witness for Scalable3D.uniformlyScaled(by:) in conformance SPProjectiveTransform3D@<D0>(float64x2_t *a1@<X8>, unint64_t a2@<D0>)
{
  float64x2_t v4 = v2[1];
  float64x2_t v5 = v2[2];
  float64x2_t v6 = v2[3];
  float64x2_t v7 = v2[4];
  float64x2_t v8 = v2[5];
  float64x2_t v9 = v2[6];
  float64x2_t v10 = v2[7];
  v20[0]  = *v2;
  v20[1]  = v4;
  v20[2]  = v5;
  v20[3]  = v6;
  v20[4]  = v7;
  v20[5]  = v8;
  v20[6]  = v9;
  v20[7]  = v10;
  SPProjectiveTransform3DScaleUniform(v20, v19, a2);
  float64_t result = v19[0].f64[0];
  float64x2_t v12 = v19[1];
  float64x2_t v13 = v19[2];
  float64x2_t v14 = v19[3];
  float64x2_t v15 = v19[4];
  float64x2_t v16 = v19[5];
  float64x2_t v17 = v19[6];
  float64x2_t v18 = v19[7];
  *a1  = v19[0];
  a1[1]  = v12;
  a1[2]  = v13;
  a1[3]  = v14;
  a1[4]  = v15;
  a1[5]  = v16;
  a1[6]  = v17;
  a1[7]  = v18;
  return result;
}

double protocol witness for Shearable3D.sheared(_:) in conformance SPProjectiveTransform3D@<D0>(uint64_t a1@<X0>, uint64_t a2@<X8>, float64x2_t a3@<Q0>, float64x2_t a4@<Q1>)
{
  SPProjectiveTransform3D.sheared(_:)(a1, v10, a3, a4);
  long long v5 = v14;
  *(_OWORD *)(a2 + 64)  = v13;
  *(_OWORD *)(a2 + 80)  = v5;
  long long v6 = v16;
  *(_OWORD *)(a2 + 96)  = v15;
  *(_OWORD *)(a2 + 112)  = v6;
  float64x2_t v7 = v10[1];
  *(float64x2_t *)a2  = v10[0];
  *(float64x2_t *)(a2 + 16)  = v7;
  double result = *(double *)&v11;
  long long v9 = v12;
  *(_OWORD *)(a2 + 32)  = v11;
  *(_OWORD *)(a2 + 48)  = v9;
  return result;
}

unint64_t static SPProjectiveTransform3D.== infix(_:_:)(float64x2_t *a1, float64x2_t *a2)
{
  int64x2_t v2 = (int64x2_t)vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(a1[2], a2[2]), (int8x16_t)vceqq_f64(*a1, *a2)), vandq_s8((int8x16_t)vceqq_f64(a1[4], a2[4]), (int8x16_t)vceqq_f64(a1[6], a2[6]))), vandq_s8(vandq_s8((int8x16_t)vceqq_f64(a1[3], a2[3]), (int8x16_t)vceqq_f64(a1[1], a2[1])), vandq_s8((int8x16_t)vceqq_f64(a1[5], a2[5]), (int8x16_t)vceqq_f64(a1[7], a2[7]))));
  return vandq_s8((int8x16_t)v2, (int8x16_t)vdupq_laneq_s64(v2, 1)).u64[0] >> 63;
}

unint64_t simd_equal(float64x2_t *a1, float64x2_t *a2)
{
  int64x2_t v2 = (int64x2_t)vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(a1[2], a2[2]), (int8x16_t)vceqq_f64(*a1, *a2)), vandq_s8((int8x16_t)vceqq_f64(a1[4], a2[4]), (int8x16_t)vceqq_f64(a1[6], a2[6]))), vandq_s8(vandq_s8((int8x16_t)vceqq_f64(a1[3], a2[3]), (int8x16_t)vceqq_f64(a1[1], a2[1])), vandq_s8((int8x16_t)vceqq_f64(a1[5], a2[5]), (int8x16_t)vceqq_f64(a1[7], a2[7]))));
  return vandq_s8((int8x16_t)v2, (int8x16_t)vdupq_laneq_s64(v2, 1)).u64[0] >> 63;
}

{
  int64x2_t v2;

  int64x2_t v2 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(a1[2], a2[2]), (int8x16_t)vceqq_f64(*a1, *a2)), vandq_s8((int8x16_t)vceqq_f64(a1[4], a2[4]), (int8x16_t)vceqq_f64(a1[6], a2[6])));
  return vandq_s8((int8x16_t)vdupq_laneq_s64(v2, 1), vandq_s8((int8x16_t)vshlq_n_s64((int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(a1[3], a2[3]), (int8x16_t)vceqq_f64(a1[1], a2[1])), vandq_s8((int8x16_t)vceqq_f64(a1[5], a2[5]), (int8x16_t)vceqq_f64(a1[7], a2[7]))), 0x3FuLL), (int8x16_t)v2)).u64[0] >> 63;
}

unint64_t protocol witness for static Equatable.== infix(_:_:) in conformance SPProjectiveTransform3D(float64x2_t *a1, float64x2_t *a2)
{
  float64x2_t v2 = a1[1];
  float64x2_t v3 = a1[2];
  float64x2_t v4 = a1[3];
  float64x2_t v5 = a1[4];
  float64x2_t v6 = a1[5];
  float64x2_t v7 = a1[6];
  float64x2_t v8 = a1[7];
  float64x2_t v9 = *a2;
  float64x2_t v10 = a2[1];
  float64x2_t v11 = a2[2];
  float64x2_t v12 = a2[3];
  float64x2_t v13 = a2[4];
  float64x2_t v14 = a2[5];
  float64x2_t v15 = a2[6];
  float64x2_t v16 = a2[7];
  v19[0]  = *a1;
  v19[1]  = v2;
  v19[2]  = v3;
  v19[3]  = v4;
  v19[4]  = v5;
  v19[5]  = v6;
  v19[6]  = v7;
  v19[7]  = v8;
  v18[0]  = v9;
  v18[1]  = v10;
  v18[2]  = v11;
  _OWORD v18[3] = v12;
  v18[4]  = v13;
  v18[5]  = v14;
  v18[6]  = v15;
  v18[7]  = v16;
  return simd_equal(v19, v18);
}

void SPProjectiveTransform3D.hash(into:)()
{
  __n128 v1 = v0[2];
  __n128 v2 = v0[3];
  __n128 v3 = v0[4];
  __n128 v4 = v0[5];
  __n128 v5 = v0[6];
  __n128 v6 = v0[7];
  specialized SIMD.hash(into:)(*v0, v0[1]);
  specialized SIMD.hash(into:)(v1, v2);
  specialized SIMD.hash(into:)(v3, v4);

  specialized SIMD.hash(into:)(v5, v6);
}

Swift::Int SPProjectiveTransform3D.hashValue.getter()
{
  Hasher.init(_seed:)();
  __n128 v2 = v0[2];
  __n128 v3 = v0[3];
  __n128 v4 = v0[4];
  __n128 v5 = v0[5];
  __n128 v6 = v0[6];
  __n128 v7 = v0[7];
  specialized SIMD.hash(into:)(*v0, v0[1]);
  specialized SIMD.hash(into:)(v2, v3);
  specialized SIMD.hash(into:)(v4, v5);
  specialized SIMD.hash(into:)(v6, v7);
  return Hasher._finalize()();
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPProjectiveTransform3D()
{
  __n128 v2 = *v0;
  __n128 v3 = v0[1];
  __n128 v4 = v0[2];
  __n128 v5 = v0[3];
  __n128 v6 = v0[4];
  __n128 v7 = v0[5];
  __n128 v8 = v0[6];
  __n128 v9 = v0[7];
  Hasher.init(_seed:)();
  specialized SIMD.hash(into:)(v2, v3);
  specialized SIMD.hash(into:)(v4, v5);
  specialized SIMD.hash(into:)(v6, v7);
  specialized SIMD.hash(into:)(v8, v9);
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance SPProjectiveTransform3D()
{
  __n128 v1 = v0[2];
  __n128 v2 = v0[3];
  __n128 v3 = v0[4];
  __n128 v4 = v0[5];
  __n128 v5 = v0[6];
  __n128 v6 = v0[7];
  specialized SIMD.hash(into:)(*v0, v0[1]);
  specialized SIMD.hash(into:)(v1, v2);
  specialized SIMD.hash(into:)(v3, v4);

  specialized SIMD.hash(into:)(v5, v6);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPProjectiveTransform3D()
{
  __n128 v2 = *v0;
  __n128 v3 = v0[1];
  __n128 v4 = v0[2];
  __n128 v5 = v0[3];
  __n128 v6 = v0[4];
  __n128 v7 = v0[5];
  __n128 v8 = v0[6];
  __n128 v9 = v0[7];
  Hasher.init(_seed:)();
  specialized SIMD.hash(into:)(v2, v3);
  specialized SIMD.hash(into:)(v4, v5);
  specialized SIMD.hash(into:)(v6, v7);
  specialized SIMD.hash(into:)(v8, v9);
  return Hasher._finalize()();
}

uint64_t protocol witness for static Equatable.== infix(_:_:) in conformance SPProjectiveTransform3D.CodingKeys(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return specialized == infix<A>(_:_:)(*a1, *a2);
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPProjectiveTransform3D.CodingKeys()
{
  return Hasher._finalize()();
}

uint64_t protocol witness for Hashable.hash(into:) in conformance SPProjectiveTransform3D.CodingKeys()
{
  String.hash(into:)();

  return swift_bridgeObjectRelease();
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPProjectiveTransform3D.CodingKeys()
{
  return Hasher._finalize()();
}

unint64_t protocol witness for RawRepresentable.init(rawValue:) in conformance SPProjectiveTransform3D.CodingKeys@<X0>(Swift::String *a1@<X0>, unsigned char *a2@<X8>)
{
  unint64_t result = specialized SPProjectiveTransform3D.CodingKeys.init(rawValue:)(*a1);
  *a2  = result;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance SPProjectiveTransform3D.CodingKeys(void *a1@<X8>)
{
  *a1  = ((unint64_t)*v1 << 48) + 0x306E6D756C6F63;
  a1[1]  = 0xE700000000000000;
}

unint64_t protocol witness for CodingKey.stringValue.getter in conformance SPProjectiveTransform3D.CodingKeys()
{
  return ((unint64_t)*v0 << 48) + 0x306E6D756C6F63;
}

unint64_t protocol witness for CodingKey.init(stringValue:) in conformance SPProjectiveTransform3D.CodingKeys@<X0>(Swift::String a1@<X1:X0>, unsigned char *a2@<X8>)
{
  unint64_t result = specialized SPProjectiveTransform3D.CodingKeys.init(rawValue:)(a1);
  *a2  = result;
  return result;
}

void protocol witness for CodingKey.init(intValue:) in conformance SPProjectiveTransform3D.CodingKeys(unsigned char *a1@<X8>)
{
  *a1  = 4;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPProjectiveTransform3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys();

  return MEMORY[0x270FA00B0](a1, v2);
}

uint64_t protocol witness for CustomDebugStringConvertible.debugDescription.getter in conformance SPProjectiveTransform3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys();

  return MEMORY[0x270FA00B8](a1, v2);
}

uint64_t SPProjectiveTransform3D.encode(to:)(void *a1)
{
  __n128 v3 = v1;
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedEncodingContainer<SPProjectiveTransform3D.CodingKeys>);
  uint64_t v6 = *(void *)(v5 - 8);
  MEMORY[0x270FA5388](v5);
  __n128 v8 = (char *)&v15 - ((v7 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys();
  dispatch thunk of Encoder.container<A>(keyedBy:)();
  long long v9 = *v3;
  long long v10 = v3[1];
  long long v11 = v3[2];
  long long v19 = v3[3];
  long long v20 = v11;
  long long v12 = v3[4];
  long long v17 = v3[5];
  long long v18 = v12;
  long long v13 = v3[6];
  long long v15 = v3[7];
  long long v16 = v13;
  long long v22 = v9;
  long long v23 = v10;
  char v21 = 0;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for SIMD4<Double>);
  lazy protocol witness table accessor for type SIMD4<Double> and conformance SIMD4<A>(&lazy protocol witness table cache variable for type SIMD4<Double> and conformance SIMD4<A>);
  KeyedEncodingContainer.encode<A>(_:forKey:)();
  if (!v2)
  {
    long long v22 = v20;
    long long v23 = v19;
    char v21 = 1;
    KeyedEncodingContainer.encode<A>(_:forKey:)();
    long long v22 = v18;
    long long v23 = v17;
    char v21 = 2;
    KeyedEncodingContainer.encode<A>(_:forKey:)();
    long long v22 = v16;
    long long v23 = v15;
    char v21 = 3;
    KeyedEncodingContainer.encode<A>(_:forKey:)();
  }
  return (*(uint64_t (**)(char *, uint64_t))(v6 + 8))(v8, v5);
}

double SPProjectiveTransform3D.init(from:)@<D0>(void *a1@<X0>, _OWORD *a2@<X8>)
{
  specialized SPProjectiveTransform3D.init(from:)(a1, v9);
  if (!v2)
  {
    long long v5 = v13;
    a2[4]  = v12;
    a2[5]  = v5;
    long long v6 = v15;
    a2[6]  = v14;
    a2[7]  = v6;
    long long v7 = v9[1];
    *a2  = v9[0];
    a2[1]  = v7;
    double result = *(double *)&v10;
    long long v8 = v11;
    a2[2]  = v10;
    a2[3]  = v8;
  }
  return result;
}

double protocol witness for Decodable.init(from:) in conformance SPProjectiveTransform3D@<D0>(void *a1@<X0>, _OWORD *a2@<X8>)
{
  specialized SPProjectiveTransform3D.init(from:)(a1, v9);
  if (!v2)
  {
    long long v5 = v13;
    a2[4]  = v12;
    a2[5]  = v5;
    long long v6 = v15;
    a2[6]  = v14;
    a2[7]  = v6;
    long long v7 = v9[1];
    *a2  = v9[0];
    a2[1]  = v7;
    double result = *(double *)&v10;
    long long v8 = v11;
    a2[2]  = v10;
    a2[3]  = v8;
  }
  return result;
}

uint64_t protocol witness for Encodable.encode(to:) in conformance SPProjectiveTransform3D(void *a1)
{
  return SPProjectiveTransform3D.encode(to:)(a1);
}

uint64_t SPProjectiveTransform3D.description.getter()
{
  v0._countAndFlagsBits  = 0x3A78697274616D28;
  v0._object  = (void *)0xE900000000000020;
  String.append(_:)(v0);
  type metadata accessor for simd_double4x4(0);
  _print_unlocked<A, B>(_:_:)();
  v1._countAndFlagsBits  = 41;
  v1._object  = (void *)0xE100000000000000;
  String.append(_:)(v1);
  return 0;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPProjectiveTransform3D()
{
  v0._countAndFlagsBits  = 0x3A78697274616D28;
  v0._object  = (void *)0xE900000000000020;
  String.append(_:)(v0);
  type metadata accessor for simd_double4x4(0);
  _print_unlocked<A, B>(_:_:)();
  v1._countAndFlagsBits  = 41;
  v1._object  = (void *)0xE100000000000000;
  String.append(_:)(v1);
  return 0;
}

uint64_t SPProjectiveTransform3D.customMirror.getter()
{
  uint64_t v1 = type metadata accessor for Mirror.AncestorRepresentation();
  uint64_t v2 = *(void *)(v1 - 8);
  MEMORY[0x270FA5388](v1);
  __n128 v4 = (char *)v23 - ((v3 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName((uint64_t *)&demangling cache variable for type metadata for Mirror.DisplayStyle?);
  MEMORY[0x270FA5388](v5 - 8);
  long long v7 = (char *)v23 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  long long v8 = v0[5];
  v23[4]  = v0[4];
  v23[5]  = v8;
  long long v9 = v0[7];
  v23[6]  = v0[6];
  v23[7]  = v9;
  long long v10 = v0[1];
  v23[0]  = *v0;
  v23[1]  = v10;
  long long v11 = v0[3];
  v23[2]  = v0[2];
  v23[3]  = v11;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<(String, Any)>);
  uint64_t v12 = swift_allocObject();
  *(_OWORD *)(v12 + 16)  = xmmword_228C20870;
  *(void *)(v12 + 32)  = 0x78697274616DLL;
  *(void *)(v12 + 40)  = 0xE600000000000000;
  type metadata accessor for simd_double4x4(0);
  *(void *)(v12 + 72)  = v13;
  long long v14 = (_OWORD *)swift_allocObject();
  *(void *)(v12 + 48)  = v14;
  long long v15 = v0[5];
  v14[5]  = v0[4];
  v14[6]  = v15;
  long long v16 = v0[7];
  v14[7]  = v0[6];
  v14[8]  = v16;
  long long v17 = v0[1];
  v14[1]  = *v0;
  v14[2]  = v17;
  long long v18 = v0[3];
  v14[3]  = v0[2];
  v14[4]  = v18;
  uint64_t v19 = *MEMORY[0x263F8E808];
  uint64_t v20 = type metadata accessor for Mirror.DisplayStyle();
  uint64_t v21 = *(void *)(v20 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v21 + 104))(v7, v19, v20);
  (*(void (**)(char *, void, uint64_t, uint64_t))(v21 + 56))(v7, 0, 1, v20);
  (*(void (**)(char *, void, uint64_t))(v2 + 104))(v4, *MEMORY[0x263F8E830], v1);
  type metadata accessor for SPProjectiveTransform3D(0);
  return Mirror.init<A>(_:children:displayStyle:ancestorRepresentation:)();
}

double SPProjectiveTransform3D.scaleComponent.getter()
{
  return SPProjectiveTransform3D.translation.getter((void (*)(double *__return_ptr, _OWORD *))SPProjectiveTransform3DGetScale);
}

double SPProjectiveTransform3D.translation.getter(void (*a1)(double *__return_ptr, _OWORD *))
{
  long long v2 = v1[1];
  long long v3 = v1[2];
  long long v4 = v1[3];
  long long v5 = v1[4];
  long long v6 = v1[5];
  long long v7 = v1[6];
  long long v8 = v1[7];
  v11[0]  = *v1;
  v11[1]  = v2;
  v11[2]  = v3;
  v11[3]  = v4;
  v11[4]  = v5;
  v11[5]  = v6;
  v11[6]  = v7;
  v11[7]  = v8;
  a1(&v10, v11);
  return v10;
}

unint64_t lazy protocol witness table accessor for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys()
{
  unint64_t result = lazy protocol witness table cache variable for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys);
  }
  return result;
}

uint64_t __swift_instantiateConcreteTypeFromMangledNameAbstract(uint64_t *a1)
{
  uint64_t result = *a1;
  if (result < 0)
  {
    uint64_t result = swift_getTypeByMangledNameInContextInMetadataState2();
    *a1  = result;
  }
  return result;
}

unint64_t specialized SPProjectiveTransform3D.CodingKeys.init(rawValue:)(Swift::String string)
{
  object  = string._object;
  v2._countAndFlagsBits  = string._countAndFlagsBits;
  v2._object  = object;
  unint64_t v3 = _findStringSwitchCase(cases:string:)((Swift::OpaquePointer)&outlined read-only object #0 of SPProjectiveTransform3D.CodingKeys.init(rawValue:), v2);
  swift_bridgeObjectRelease();
  if (v3 >= 4) {
    return 4;
  }
  else {
    return v3;
  }
}

uint64_t specialized SPProjectiveTransform3D.init(from:)@<X0>(void *a1@<X0>, _OWORD *a2@<X8>)
{
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedDecodingContainer<SPProjectiveTransform3D.CodingKeys>);
  uint64_t v6 = *(void *)(v5 - 8);
  MEMORY[0x270FA5388](v5);
  long long v8 = (char *)&v20 - ((v7 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPProjectiveTransform3D.CodingKeys and conformance SPProjectiveTransform3D.CodingKeys();
  dispatch thunk of Decoder.container<A>(keyedBy:)();
  if (v2) {
    return __swift_destroy_boxed_opaque_existential_1(a1);
  }
  float64x2_t v28 = a2;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<SIMD4<Double>>);
  long long v9 = (_OWORD *)swift_allocObject();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for SIMD4<Double>);
  char v32 = 0;
  lazy protocol witness table accessor for type SIMD4<Double> and conformance SIMD4<A>(&lazy protocol witness table cache variable for type SIMD4<Double> and conformance SIMD4<A>);
  KeyedDecodingContainer.decode<A>(_:forKey:)();
  char v31 = 1;
  KeyedDecodingContainer.decode<A>(_:forKey:)();
  char v30 = 2;
  KeyedDecodingContainer.decode<A>(_:forKey:)();
  char v29 = 3;
  KeyedDecodingContainer.decode<A>(_:forKey:)();
  long long v10 = v9[2];
  long long v11 = v9[3];
  long long v26 = v9[4];
  long long v27 = v10;
  long long v22 = v9[5];
  long long v23 = v11;
  long long v12 = v9[6];
  long long v13 = v9[7];
  long long v24 = v9[8];
  long long v25 = v12;
  long long v20 = v9[9];
  long long v21 = v13;
  swift_setDeallocating();
  swift_deallocClassInstance();
  (*(void (**)(char *, uint64_t))(v6 + 8))(v8, v5);
  uint64_t result = __swift_destroy_boxed_opaque_existential_1(a1);
  long long v15 = v28;
  long long v16 = v23;
  *float64x2_t v28 = v27;
  v15[1]  = v16;
  long long v17 = v22;
  v15[2]  = v26;
  v15[3]  = v17;
  long long v18 = v21;
  v15[4]  = v25;
  v15[5]  = v18;
  long long v19 = v20;
  v15[6]  = v24;
  v15[7]  = v19;
  return result;
}

uint64_t sub_228C00C98()
{
  return MEMORY[0x270FA0238](v0, 144, 15);
}

unint64_t lazy protocol witness table accessor for type SPProjectiveTransform3D and conformance SPProjectiveTransform3D()
{
  unint64_t result = lazy protocol witness table cache variable for type SPProjectiveTransform3D and conformance SPProjectiveTransform3D;
  if (!lazy protocol witness table cache variable for type SPProjectiveTransform3D and conformance SPProjectiveTransform3D)
  {
    type metadata accessor for SPProjectiveTransform3D(255);
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPProjectiveTransform3D and conformance SPProjectiveTransform3D);
  }
  return result;
}

double sub_228C00D04@<D0>(__n128 *a1@<X0>, __n128 *a2@<X8>)
{
  __n128 v2 = a1[1];
  __n128 v3 = a1[2];
  __n128 v4 = a1[3];
  __n128 v5 = a1[4];
  __n128 v6 = a1[5];
  __n128 v7 = a1[6];
  __n128 v8 = a1[7];
  v10[0]  = *a1;
  v10[1]  = v2;
  v10[2]  = v3;
  v10[3]  = v4;
  v10[4]  = v5;
  v10[5]  = v6;
  v10[6]  = v7;
  v10[7]  = v8;
  *(void *)&double result = SPProjectiveTransform3DGetTranslation(v10, a2).n128_u64[0];
  return result;
}

double sub_228C00D48(double *a1, SPProjectiveTransform3D *a2)
{
  long long v2 = *(_OWORD *)a1;
  double v3 = a1[3];
  v5.double z = a1[2];
  v5.vector.f64[3]  = v3;
  *(_OWORD *)&v5.x  = v2;
  *(void *)&double result = SPProjectiveTransform3DSetTranslation(a2, &v5).n128_u64[0];
  return result;
}

uint64_t getEnumTagSinglePayload for SPProjectiveTransform3D.CodingKeys(unsigned __int8 *a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 < 0xFD) {
    goto LABEL_17;
  }
  if (a2 + 3 >= 0xFFFF00) {
    int v2 = 4;
  }
  else {
    int v2 = 2;
  }
  if ((a2 + 3) >> 8 < 0xFF) {
    int v3 = 1;
  }
  else {
    int v3 = v2;
  }
  if (v3 == 4)
  {
    int v4 = *(_DWORD *)(a1 + 1);
    if (v4) {
      return (*a1 | (v4 << 8)) - 3;
    }
  }
  else
  {
    if (v3 == 2)
    {
      int v4 = *(unsigned __int16 *)(a1 + 1);
      if (!*(_WORD *)(a1 + 1)) {
        goto LABEL_17;
      }
      return (*a1 | (v4 << 8)) - 3;
    }
    int v4 = a1[1];
    if (a1[1]) {
      return (*a1 | (v4 << 8)) - 3;
    }
  }
LABEL_17:
  unsigned int v6 = *a1;
  BOOL v7 = v6 >= 4;
  int v8 = v6 - 4;
  if (!v7) {
    int v8 = -1;
  }
  return (v8 + 1);
}

unsigned char *storeEnumTagSinglePayload for SPProjectiveTransform3D.CodingKeys(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 3 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 3) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFD) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFC)
  {
    unsigned int v6 = ((a2 - 253) >> 8) + 1;
    *double result = a2 + 3;
    switch(v5)
    {
      case 1:
        result[1]  = v6;
        break;
      case 2:
        *(_WORD *)(result + 1)  = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x228C00EE0);
      case 4:
        *(_DWORD *)(result + 1)  = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1]  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1)  = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1)  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *double result = a2 + 3;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for SPProjectiveTransform3D.CodingKeys()
{
  return &type metadata for SPProjectiveTransform3D.CodingKeys;
}

float64x2_t *SPProjectiveTransform3DScaleUniform@<X0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X8>, unint64_t a3@<D0>)
{
  v3.f64[0]  = 0.0;
  *(void *)&v3.f64[1]  = a3;
  float64x2_t v4 = a1[5];
  v10[4]  = a1[4];
  v10[5]  = v4;
  float64x2_t v5 = a1[7];
  v10[6]  = a1[6];
  v10[7]  = v5;
  float64x2_t v6 = a1[1];
  v10[0]  = *a1;
  v10[1]  = v6;
  float64x2_t v7 = a1[3];
  v10[2]  = a1[2];
  v10[3]  = v7;
  v9[0]  = (float64x2_t)a3;
  v9[1]  = 0u;
  v9[2]  = v3;
  memset(&v9[3], 0, 32);
  v9[5]  = (float64x2_t)a3;
  v9[6]  = 0u;
  v9[7]  = (float64x2_t)xmmword_228C1F7A0;
  return SPProjectiveTransform3DConcatenation(v10, v9, a2);
}

{
  float64x2_t v3;
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v9[8];
  float64x2_t v10[8];

  v3.f64[0]  = 0.0;
  *(void *)&v3.f64[1]  = a3;
  float64x2_t v4 = a1[5];
  v10[4]  = a1[4];
  v10[5]  = v4;
  float64x2_t v5 = a1[7];
  v10[6]  = a1[6];
  v10[7]  = v5;
  float64x2_t v6 = a1[1];
  v10[0]  = *a1;
  v10[1]  = v6;
  float64x2_t v7 = a1[3];
  v10[2]  = a1[2];
  v10[3]  = v7;
  v9[0]  = (float64x2_t)a3;
  v9[1]  = 0u;
  v9[2]  = v3;
  memset(&v9[3], 0, 32);
  v9[5]  = (float64x2_t)a3;
  v9[6]  = 0u;
  v9[7]  = (float64x2_t)xmmword_228C1F7A0;
  return SPProjectiveTransform3DConcatenation(v10, v9, a2);
}

float64x2_t *SPProjectiveTransform3DScaleBySize@<X0>(float64x2_t *a1@<X0>, unint64_t *a2@<X1>, float64x2_t *a3@<X8>, float64x2_t a4@<Q2>)
{
  unint64_t v4 = *a2;
  v5.f64[0]  = 0.0;
  *(void *)&v5.f64[1]  = a2[1];
  *(void *)&a4.f64[0]  = a2[2];
  float64x2_t v6 = a1[5];
  v12[4]  = a1[4];
  v12[5]  = v6;
  float64x2_t v7 = a1[7];
  v12[6]  = a1[6];
  v12[7]  = v7;
  float64x2_t v8 = a1[1];
  v12[0]  = *a1;
  v12[1]  = v8;
  float64x2_t v9 = a1[3];
  v12[2]  = a1[2];
  v12[3]  = v9;
  v11[0]  = (float64x2_t)v4;
  v11[1]  = 0u;
  v11[2]  = v5;
  memset(&v11[3], 0, 32);
  v11[5]  = a4;
  v11[6]  = 0u;
  v11[7]  = (float64x2_t)xmmword_228C1F7A0;
  return SPProjectiveTransform3DConcatenation(v12, v11, a3);
}

{
  unint64_t v4;
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v11[8];
  float64x2_t v12[8];

  unint64_t v4 = *a2;
  v5.f64[0]  = 0.0;
  *(void *)&v5.f64[1]  = a2[1];
  *(void *)&a4.f64[0]  = a2[2];
  float64x2_t v6 = a1[5];
  v12[4]  = a1[4];
  v12[5]  = v6;
  float64x2_t v7 = a1[7];
  v12[6]  = a1[6];
  v12[7]  = v7;
  float64x2_t v8 = a1[1];
  v12[0]  = *a1;
  v12[1]  = v8;
  float64x2_t v9 = a1[3];
  v12[2]  = a1[2];
  v12[3]  = v9;
  v11[0]  = (float64x2_t)v4;
  v11[1]  = 0u;
  v11[2]  = v5;
  memset(&v11[3], 0, 32);
  v11[5]  = a4;
  v11[6]  = 0u;
  v11[7]  = (float64x2_t)xmmword_228C1F7A0;
  return SPProjectiveTransform3DConcatenation(v12, v11, a3);
}

float64x2_t *SPProjectiveTransform3DRotateByQuaternion@<X0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>)
{
  _Q2  = *a2;
  _Q1  = a2[1];
  _D4  = a2->f64[1];
  __asm { FMLS            D0, D1, V1.D[0] }
  _D7  = a2[1].f64[1];
  __asm { FMLA            D0, D7, V1.D[1] }
  double v13 = vmlad_n_f64(vmuld_lane_f64(_Q1.f64[0], _Q1, 1), _D4, a2->f64[0]);
  double v14 = v13 + v13;
  v15.f64[0]  = vmuld_lane_f64(_D4, _Q1, 1);
  double v16 = vmlad_n_f64(-(_D4 * _D7), _Q1.f64[0], a2->f64[0]);
  _Q0.f64[1]  = v14;
  *(double *)&unint64_t v17 = v16 + v16;
  double v18 = vmlad_n_f64(-(_Q1.f64[0] * _D7), _D4, a2->f64[0]);
  v19.f64[0]  = v18 + v18;
  __asm
  {
    FMLA            D6, D4, V2.D[1]
    FMLA            D6, D7, V1.D[1]
    FMLS            D6, D2, V2.D[0]
    FMLA            D18, D1, V2.D[1]
  }
  v19.f64[1]  = _D6;
  *(double *)&unint64_t v24 = _D18 + _D18;
  float64_t v25 = -(a2->f64[0] * _D7);
  __asm
  {
    FMLS            D7, D2, V2.D[0]
    FMLS            D7, D4, V2.D[1]
  }
  v28.f64[1]  = _D4;
  v28.f64[0]  = a2[1].f64[0];
  v15.f64[1]  = v25;
  unint64_t v29 = _D7;
  float64x2_t v30 = (float64x2_t)vzip1q_s64(*(int64x2_t *)a2, (int64x2_t)_Q1);
  float64x2_t v31 = a1[5];
  v41[4]  = a1[4];
  v41[5]  = v31;
  float64x2_t v32 = a1[7];
  v41[6]  = a1[6];
  v41[7]  = v32;
  float64x2_t v33 = a1[1];
  v41[0]  = *a1;
  v41[1]  = v33;
  float64x2_t v34 = a1[3];
  float64x2_t v35 = vmlaq_f64(v15, v30, v28);
  v41[2]  = a1[2];
  v41[3]  = v34;
  v37[0]  = _Q0;
  v37[1]  = (float64x2_t)v17;
  v37[2]  = v19;
  v37[3]  = (float64x2_t)v24;
  v37[4]  = vaddq_f64(v35, v35);
  v37[5]  = (float64x2_t)v29;
  uint64_t v38 = 0;
  uint64_t v39 = 0;
  long long v40 = xmmword_228C1F7A0;
  return SPProjectiveTransform3DConcatenation(v41, v37, a3);
}

float64x2_t *SPProjectiveTransform3DTranslate@<X0>(uint64_t a1@<X0>, float64x2_t *a2@<X8>, uint64_t a3, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, float64x2_t a15, long long a16, long long a17, long long a18, long long a19, long long a20,long long a21,long long a22)
{
  __asm { FMOV            V1.2D, #1.0 }
  float64x2_t v27 = *(float64x2_t *)(MEMORY[0x263EF8990] + 16);
  var10[0]  = *(float64x2_t *)MEMORY[0x263EF8990];
  var10[1]  = v27;
  long long v28 = *(_OWORD *)(a1 + 80);
  a19  = *(_OWORD *)(a1 + 64);
  a20  = v28;
  long long v29 = *(_OWORD *)(a1 + 112);
  a21  = *(_OWORD *)(a1 + 96);
  a22  = v29;
  long long v30 = *(_OWORD *)(a1 + 16);
  a15  = *(float64x2_t *)a1;
  a16  = v30;
  long long v31 = *(_OWORD *)(a1 + 48);
  a17  = *(_OWORD *)(a1 + 32);
  a18  = v31;
  return SPProjectiveTransform3DConcatenation(&a15, var10, a2);
}

{
  float64x2_t v27;
  long long v28;
  long long v29;
  long long v30;
  long long v31;
  float64x2_t var10[2];

  __asm { FMOV            V1.2D, #1.0 }
  float64x2_t v27 = *(float64x2_t *)(MEMORY[0x263EF8990] + 16);
  var10[0]  = *(float64x2_t *)MEMORY[0x263EF8990];
  var10[1]  = v27;
  long long v28 = *(_OWORD *)(a1 + 80);
  a19  = *(_OWORD *)(a1 + 64);
  a20  = v28;
  long long v29 = *(_OWORD *)(a1 + 112);
  a21  = *(_OWORD *)(a1 + 96);
  a22  = v29;
  long long v30 = *(_OWORD *)(a1 + 16);
  a15  = *(float64x2_t *)a1;
  a16  = v30;
  long long v31 = *(_OWORD *)(a1 + 48);
  a17  = *(_OWORD *)(a1 + 32);
  a18  = v31;
  return SPProjectiveTransform3DConcatenation(&a15, var10, a2);
}

uint64_t lazy protocol witness table accessor for type SIMD4<Double> and conformance SIMD4<A>(unint64_t *a1)
{
  uint64_t result = *a1;
  if (!result)
  {
    __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for SIMD4<Double>);
    uint64_t result = swift_getWitnessTable();
    atomic_store(result, a1);
  }
  return result;
}

void __swiftcall SPRect3D.applying(_:)(SPRect3D *__return_ptr retstr, SPPose3D *a2)
{
  long long v4 = *(_OWORD *)&a2->position.x;
  long long v5 = *(_OWORD *)&a2->position.vector.f64[2];
  long long v6 = *(_OWORD *)a2->rotation.vector.f64;
  double v7 = a2->rotation.vector.f64[2];
  double v8 = a2->rotation.vector.f64[3];
  long long v9 = *(_OWORD *)(v2 + 16);
  long long v10 = *(_OWORD *)(v2 + 32);
  double v11 = *(double *)(v2 + 48);
  double v12 = *(double *)(v2 + 56);
  *(_OWORD *)&v21.origin.x  = *(_OWORD *)v2;
  *(_OWORD *)&v21.origin.vector.f64[2]  = v9;
  v21.size.depth  = v11;
  v21.size.vector.f64[3]  = v12;
  *(_OWORD *)&v21.size.width  = v10;
  *(_OWORD *)&v20.position.x  = v4;
  *(_OWORD *)&v20.position.vector.f64[2]  = v5;
  v20.rotation.vector.f64[2]  = v7;
  v20.rotation.vector.f64[3]  = v8;
  *(_OWORD *)v20.rotation.vector.f64  = v6;
  SPRect3DApplyPose(&v21, &v20, v17);
  float64x2_t v13 = v17[1];
  float64x2_t v14 = v17[2];
  double v15 = v18;
  double v16 = v19;
  *(float64x2_t *)&retstr->origin.x  = v17[0];
  *(float64x2_t *)&retstr->origin.vector.f64[2]  = v13;
  retstr->size.depth  = v15;
  retstr->size.vector.f64[3]  = v16;
  *(float64x2_t *)&retstr->size.width  = v14;
}

void __swiftcall SPRect3D.unapplying(_:)(SPRect3D *__return_ptr retstr, SPAffineTransform3D *a2)
{
  SPRect3D.unapplying(_:)((float64x2_t *)a2, (void (*)(float64x2_t *__return_ptr, void *, float64x2_t *))SPRect3DUnapplyAffineTransform, &retstr->origin.x);
}

float64x2_t SPRect3DUnapplyAffineTransform@<Q0>(uint64_t a1@<X0>, uint64_t a2@<X1>, float64x2_t *a3@<X8>, float64x2_t a4@<Q3>)
{
  v91  = *MEMORY[0x263EF8340];
  int64x2_t v7 = *(int64x2_t *)a2;
  float64x2_t v8 = *(float64x2_t *)(a2 + 16);
  float64x2_t v10 = *(float64x2_t *)(a2 + 32);
  float64x2_t v9 = *(float64x2_t *)(a2 + 48);
  int64x2_t v11 = *(int64x2_t *)(a2 + 64);
  int8x16_t v12 = *(int8x16_t *)(a2 + 80);
  double v13 = *(double *)(a2 + 40);
  double v14 = *(double *)a2 + v13 + *(double *)v12.i64;
  v73  = *(float64x2_t *)a2;
  v74  = (float64x2_t)v11;
  float64x2_t v71 = v10;
  float64x2_t v72 = v8;
  int8x16_t v69 = v12;
  int8x16_t v70 = (int8x16_t)v9;
  if (v14 >= 0.0)
  {
    double v31 = sqrt(v14 + 1.0);
    double v32 = v31 + v31;
    v36.f64[0]  = 1.0 / v32 * vsubq_f64(v9, (float64x2_t)vdupq_laneq_s64(v11, 1)).f64[0];
    double v37 = (*(double *)v11.i64 - v8.f64[0]) * (1.0 / v32);
    a4.f64[0]  = 1.0 / v32 * vsubq_f64((float64x2_t)vdupq_laneq_s64(v7, 1), v10).f64[0];
    double v38 = v32 * 0.25;
  }
  else if (*(double *)v7.i64 < v13 || *(double *)v7.i64 < *(double *)v12.i64)
  {
    double v16 = 1.0 - *(double *)v7.i64;
    BOOL v17 = v13 < *(double *)v12.i64;
    double v18 = sqrt(1.0 - *(double *)v7.i64 - v13 + *(double *)v12.i64);
    double v19 = v18 + v18;
    double v20 = 1.0 / v19;
    double v21 = (v8.f64[0] + *(double *)v11.i64) * (1.0 / v19);
    double v22 = vaddq_f64(v9, (float64x2_t)vdupq_laneq_s64(v11, 1)).f64[0];
    double v23 = 1.0 / v19 * v22;
    float64x2_t v24 = (float64x2_t)vdupq_laneq_s64(v7, 1);
    a4.f64[0]  = v19 * 0.25;
    double v25 = v20 * vsubq_f64(v24, v10).f64[0];
    double v26 = sqrt(v13 + v16 - *(double *)v12.i64);
    double v27 = v26 + v26;
    double v28 = 1.0 / v27 * vaddq_f64(v24, v10).f64[0];
    double v29 = v27 * 0.25;
    double v30 = 1.0 / v27 * v22;
    double v38 = (*(double *)v11.i64 - v8.f64[0]) * (1.0 / v27);
    if (v17) {
      v36.f64[0]  = v21;
    }
    else {
      v36.f64[0]  = v28;
    }
    if (v17) {
      double v37 = v23;
    }
    else {
      double v37 = v29;
    }
    if (v17) {
      double v38 = v25;
    }
    else {
      a4.f64[0]  = v30;
    }
  }
  else
  {
    double v33 = sqrt(*(double *)v7.i64 + 1.0 - v13 - *(double *)v12.i64);
    double v34 = v33 + v33;
    double v35 = 1.0 / v34;
    v36.f64[0]  = v34 * 0.25;
    double v37 = v35 * vaddq_f64((float64x2_t)vdupq_laneq_s64(v7, 1), v10).f64[0];
    a4.f64[0]  = (v8.f64[0] + *(double *)v11.i64) * v35;
    double v38 = v35 * vsubq_f64(v9, (float64x2_t)vdupq_laneq_s64(v11, 1)).f64[0];
  }
  v36.f64[1]  = v37;
  long double v39 = atan2(sqrt(vmulq_f64(a4, a4).f64[0] + vaddvq_f64(vmulq_f64(v36, v36))), v38);
  if (fabs(remainder(v39 + v39, 1.57079633)) >= 0.0000000149011612)
  {
    *a3  = (float64x2_t)SPRect3DNull;
    a3[1]  = (float64x2_t)unk_228C20FF0;
    result.f64[0]  = 0.0;
    a3[2]  = (float64x2_t)xmmword_228C21000;
    a3[3]  = (float64x2_t)unk_228C21010;
  }
  else
  {
    *(void *)&v41.f64[0]  = v69.i64[0];
    v41.f64[1]  = v74.f64[0];
    *(void *)&v42.f64[0]  = v70.i64[0];
    v42.f64[1]  = v71.f64[0];
    if (vmulq_f64(v72, vmlaq_laneq_f64(vmulq_f64(v74, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v71, 1))), v71, v74, 1)).f64[0]+ vaddvq_f64(vmulq_f64(v73, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v74, v69, 8uLL), vnegq_f64(v42)), v41, (float64x2_t)vextq_s8((int8x16_t)v71, v70, 8uLL)))) == 0.0)
    {
      float64x2_t v43 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
      float64x2_t v45 = v43;
      float64x2_t v44 = (float64x2_t)xmmword_228C1FC60;
      float64x2_t v46 = (float64x2_t)xmmword_228C1FC60;
      float64x2_t v47 = v43;
      float64x2_t v48 = (float64x2_t)xmmword_228C1FC60;
      float64x2_t v50 = v43;
      float64x2_t v49 = (float64x2_t)xmmword_228C1FC60;
    }
    else
    {
      float64x2_t v67 = *(float64x2_t *)(a2 + 96);
      float64x2_t v68 = *(float64x2_t *)(a2 + 112);
      v75  = v73;
      v76  = v72;
      v77  = v71;
      v78  = v70;
      v79  = v74;
      v80  = v69;
      __invert_d3();
      float64x2_t v43 = 0u;
      float64x2_t v44 = 0u;
      float64x2_t v45 = 0u;
      float64x2_t v46 = 0u;
      float64x2_t v47 = 0u;
      float64x2_t v48 = 0u;
      float64x2_t v49 = vnegq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v67, (float64x2_t)0), (float64x2_t)0, v67, 1), v68, (float64x2_t)0));
      float64x2_t v50 = vnegq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v67.f64[0]), (float64x2_t)0, v67, 1), (float64x2_t)0, v68.f64[0]));
    }
    uint64_t v51 = 0;
    float64x2_t v52 = *(float64x2_t *)(a1 + 32);
    double v53 = *(double *)(a1 + 48);
    v89  = 0u;
    *(_OWORD *)v90  = 0u;
    v87  = 0u;
    v88  = 0u;
    v85  = 0u;
    v86  = 0u;
    v83  = 0u;
    v84  = 0u;
    v81  = 0u;
    v82  = 0u;
    v79  = 0u;
    v80  = 0u;
    v77  = 0u;
    v78  = 0u;
    v75  = 0u;
    v76  = 0u;
    do
    {
      long long v54 = (_OWORD *)&v75.f64[v51];
      long long v55 = *(_OWORD *)(a1 + 16);
      *long long v54 = *(_OWORD *)a1;
      v54[1]  = v55;
      v51 += 4;
    }
    while (v51 != 32);
    uint64_t v56 = 0;
    v77.f64[1]  = v52.f64[1] + v77.f64[1];
    v79  = vaddq_f64(v52, v79);
    *(double *)&v81  = v52.f64[0] + *(double *)&v81;
    *(double *)&v84  = v53 + *(double *)&v84;
    *((double *)&v85 + 1)  = v52.f64[1] + *((double *)&v85 + 1);
    *(double *)&v86  = v53 + *(double *)&v86;
    v87  = vaddq_f64(v52, v87);
    *(double *)&v88  = v53 + *(double *)&v88;
    *(double *)&v89  = v52.f64[0] + *(double *)&v89;
    v90[0]  = v53 + v90[0];
    *(void *)&v43.f64[1]  = vextq_s8((int8x16_t)v43, (int8x16_t)v43, 8uLL).u64[0];
    *(void *)&v45.f64[1]  = vextq_s8((int8x16_t)v45, (int8x16_t)v45, 8uLL).u64[0];
    *(void *)&v47.f64[1]  = vextq_s8((int8x16_t)v47, (int8x16_t)v47, 8uLL).u64[0];
    *(void *)&v50.f64[1]  = vextq_s8((int8x16_t)v50, (int8x16_t)v50, 8uLL).u64[0];
    do
    {
      float64x2_t v57 = (char *)&v75 + v56;
      float64x2_t v59 = *(float64x2_t *)((char *)&v75 + v56);
      float64x2_t v58 = *(float64x2_t *)((char *)&v75 + v56 + 16);
      *(float64x2_t *)float64x2_t v57 = vaddq_f64(v50, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v43, v59.f64[0]), v45, v59, 1), v47, v58.f64[0]));
      *((void *)v57 + 2)  = *(_OWORD *)&vaddq_f64(v49, vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v44, v59), v46, v59, 1), v58, v48));
      v56 += 32;
    }
    while (v56 != 256);
    uint64_t v60 = 0;
    a3[2]  = 0u;
    a3[3]  = 0u;
    a3[1]  = 0u;
    float64x2_t v61 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    v62.f64[0]  = INFINITY;
    float64x2_t v63 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
    v64.f64[0]  = -INFINITY;
    do
    {
      v65  = *(float64x2_t *)((char *)&v75 + v60);
      long long v66 = *(long long *)((char *)&v75 + v60 + 16);
      float64x2_t v61 = vminnmq_f64(v61, v65);
      float64x2_t v62 = vminnmq_f64((float64x2_t)*(unint64_t *)&v62.f64[0], (float64x2_t)(unint64_t)v66);
      float64x2_t v63 = vmaxnmq_f64(v63, v65);
      v64  = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v64.f64[0], (float64x2_t)(unint64_t)v66);
      v60 += 32;
    }
    while (v60 != 256);
    *a3  = v61;
    a3[1].f64[0]  = v62.f64[0];
    float64x2_t result = vsubq_f64(v63, v61);
    a3[2]  = result;
    *(void *)&a3[3].f64[0]  = *(_OWORD *)&vsubq_f64(v64, v62);
  }
  return result;
}

{
  int64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  int64x2_t v11;
  int8x16_t v12;
  double v13;
  double v14;
  double v16;
  BOOL v17;
  double v18;
  double v19;
  double v20;
  double v21;
  double v22;
  double v23;
  float64x2_t v24;
  double v25;
  double v26;
  double v27;
  double v28;
  double v29;
  double v30;
  double v31;
  double v32;
  double v33;
  double v34;
  double v35;
  float64x2_t v36;
  double v37;
  double v38;
  long double v39;
  float64x2_t result;
  float64x2_t v41;
  float64x2_t v42;
  float64x2_t v43;
  float64x2_t v44;
  float64x2_t v45;
  float64x2_t v46;
  float64x2_t v47;
  float64x2_t v48;
  float64x2_t v49;
  float64x2_t v50;
  uint64_t v51;
  float64x2_t v52;
  double v53;
  _OWORD *v54;
  long long v55;
  uint64_t v56;
  char *v57;
  float64x2_t v58;
  float64x2_t v59;
  uint64_t v60;
  float64x2_t v61;
  float64x2_t v62;
  float64x2_t v63;
  float64x2_t v64;
  float64x2_t v65;
  long long v66;
  float64x2_t v67;
  float64x2_t v68;
  int8x16_t v69;
  int8x16_t v70;
  float64x2_t v71;
  float64x2_t v72;
  float64x2_t v73;
  float64x2_t v74;
  float64x2_t v75;
  float64x2_t v76;
  float64x2_t v77;
  int8x16_t v78;
  float64x2_t v79;
  int8x16_t v80;
  long long v81;
  long long v82;
  long long v83;
  long long v84;
  long long v85;
  long long v86;
  float64x2_t v87;
  long long v88;
  long long v89;
  double v90[3];
  uint64_t v91;

  v91  = *MEMORY[0x263EF8340];
  int64x2_t v7 = *(int64x2_t *)a2;
  float64x2_t v8 = *(float64x2_t *)(a2 + 16);
  float64x2_t v10 = *(float64x2_t *)(a2 + 32);
  float64x2_t v9 = *(float64x2_t *)(a2 + 48);
  int64x2_t v11 = *(int64x2_t *)(a2 + 64);
  int8x16_t v12 = *(int8x16_t *)(a2 + 80);
  double v13 = *(double *)(a2 + 40);
  double v14 = *(double *)a2 + v13 + *(double *)v12.i64;
  v73  = *(float64x2_t *)a2;
  v74  = (float64x2_t)v11;
  float64x2_t v71 = v10;
  float64x2_t v72 = v8;
  int8x16_t v69 = v12;
  int8x16_t v70 = (int8x16_t)v9;
  if (v14 >= 0.0)
  {
    double v31 = sqrt(v14 + 1.0);
    double v32 = v31 + v31;
    v36.f64[0]  = 1.0 / v32 * vsubq_f64(v9, (float64x2_t)vdupq_laneq_s64(v11, 1)).f64[0];
    double v37 = (*(double *)v11.i64 - v8.f64[0]) * (1.0 / v32);
    a4.f64[0]  = 1.0 / v32 * vsubq_f64((float64x2_t)vdupq_laneq_s64(v7, 1), v10).f64[0];
    double v38 = v32 * 0.25;
  }
  else if (*(double *)v7.i64 < v13 || *(double *)v7.i64 < *(double *)v12.i64)
  {
    double v16 = 1.0 - *(double *)v7.i64;
    BOOL v17 = v13 < *(double *)v12.i64;
    double v18 = sqrt(1.0 - *(double *)v7.i64 - v13 + *(double *)v12.i64);
    double v19 = v18 + v18;
    double v20 = 1.0 / v19;
    double v21 = (v8.f64[0] + *(double *)v11.i64) * (1.0 / v19);
    double v22 = vaddq_f64(v9, (float64x2_t)vdupq_laneq_s64(v11, 1)).f64[0];
    double v23 = 1.0 / v19 * v22;
    float64x2_t v24 = (float64x2_t)vdupq_laneq_s64(v7, 1);
    a4.f64[0]  = v19 * 0.25;
    double v25 = v20 * vsubq_f64(v24, v10).f64[0];
    double v26 = sqrt(v13 + v16 - *(double *)v12.i64);
    double v27 = v26 + v26;
    double v28 = 1.0 / v27 * vaddq_f64(v24, v10).f64[0];
    double v29 = v27 * 0.25;
    double v30 = 1.0 / v27 * v22;
    double v38 = (*(double *)v11.i64 - v8.f64[0]) * (1.0 / v27);
    if (v17) {
      v36.f64[0]  = v21;
    }
    else {
      v36.f64[0]  = v28;
    }
    if (v17) {
      double v37 = v23;
    }
    else {
      double v37 = v29;
    }
    if (v17) {
      double v38 = v25;
    }
    else {
      a4.f64[0]  = v30;
    }
  }
  else
  {
    double v33 = sqrt(*(double *)v7.i64 + 1.0 - v13 - *(double *)v12.i64);
    double v34 = v33 + v33;
    double v35 = 1.0 / v34;
    v36.f64[0]  = v34 * 0.25;
    double v37 = v35 * vaddq_f64((float64x2_t)vdupq_laneq_s64(v7, 1), v10).f64[0];
    a4.f64[0]  = (v8.f64[0] + *(double *)v11.i64) * v35;
    double v38 = v35 * vsubq_f64(v9, (float64x2_t)vdupq_laneq_s64(v11, 1)).f64[0];
  }
  v36.f64[1]  = v37;
  long double v39 = atan2(sqrt(vmulq_f64(a4, a4).f64[0] + vaddvq_f64(vmulq_f64(v36, v36))), v38);
  if (fabs(remainder(v39 + v39, 1.57079633)) >= 0.0000000149011612)
  {
    *a3  = (float64x2_t)SPRect3DNull_0;
    a3[1]  = (float64x2_t)unk_228C21830;
    result.f64[0]  = 0.0;
    a3[2]  = (float64x2_t)xmmword_228C21840;
    a3[3]  = (float64x2_t)unk_228C21850;
  }
  else
  {
    *(void *)&v41.f64[0]  = v69.i64[0];
    v41.f64[1]  = v74.f64[0];
    *(void *)&v42.f64[0]  = v70.i64[0];
    v42.f64[1]  = v71.f64[0];
    if (vmulq_f64(v72, vmlaq_laneq_f64(vmulq_f64(v74, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v71, 1))), v71, v74, 1)).f64[0]+ vaddvq_f64(vmulq_f64(v73, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v74, v69, 8uLL), vnegq_f64(v42)), v41, (float64x2_t)vextq_s8((int8x16_t)v71, v70, 8uLL)))) == 0.0)
    {
      float64x2_t v43 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
      float64x2_t v45 = v43;
      float64x2_t v44 = (float64x2_t)xmmword_228C1FC60;
      float64x2_t v46 = (float64x2_t)xmmword_228C1FC60;
      float64x2_t v47 = v43;
      float64x2_t v48 = (float64x2_t)xmmword_228C1FC60;
      float64x2_t v50 = v43;
      float64x2_t v49 = (float64x2_t)xmmword_228C1FC60;
    }
    else
    {
      float64x2_t v67 = *(float64x2_t *)(a2 + 96);
      float64x2_t v68 = *(float64x2_t *)(a2 + 112);
      v75  = v73;
      v76  = v72;
      v77  = v71;
      v78  = v70;
      v79  = v74;
      v80  = v69;
      __invert_d3();
      float64x2_t v43 = 0u;
      float64x2_t v44 = 0u;
      float64x2_t v45 = 0u;
      float64x2_t v46 = 0u;
      float64x2_t v47 = 0u;
      float64x2_t v48 = 0u;
      float64x2_t v49 = vnegq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v67, (float64x2_t)0), (float64x2_t)0, v67, 1), v68, (float64x2_t)0));
      float64x2_t v50 = vnegq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v67.f64[0]), (float64x2_t)0, v67, 1), (float64x2_t)0, v68.f64[0]));
    }
    uint64_t v51 = 0;
    float64x2_t v52 = *(float64x2_t *)(a1 + 32);
    double v53 = *(double *)(a1 + 48);
    v89  = 0u;
    *(_OWORD *)v90  = 0u;
    v87  = 0u;
    v88  = 0u;
    v85  = 0u;
    v86  = 0u;
    v83  = 0u;
    v84  = 0u;
    v81  = 0u;
    v82  = 0u;
    v79  = 0u;
    v80  = 0u;
    v77  = 0u;
    v78  = 0u;
    v75  = 0u;
    v76  = 0u;
    do
    {
      long long v54 = (_OWORD *)&v75.f64[v51];
      long long v55 = *(_OWORD *)(a1 + 16);
      *long long v54 = *(_OWORD *)a1;
      v54[1]  = v55;
      v51 += 4;
    }
    while (v51 != 32);
    uint64_t v56 = 0;
    v77.f64[1]  = v52.f64[1] + v77.f64[1];
    v79  = vaddq_f64(v52, v79);
    *(double *)&v81  = v52.f64[0] + *(double *)&v81;
    *(double *)&v84  = v53 + *(double *)&v84;
    *((double *)&v85 + 1)  = v52.f64[1] + *((double *)&v85 + 1);
    *(double *)&v86  = v53 + *(double *)&v86;
    v87  = vaddq_f64(v52, v87);
    *(double *)&v88  = v53 + *(double *)&v88;
    *(double *)&v89  = v52.f64[0] + *(double *)&v89;
    v90[0]  = v53 + v90[0];
    *(void *)&v43.f64[1]  = vextq_s8((int8x16_t)v43, (int8x16_t)v43, 8uLL).u64[0];
    *(void *)&v45.f64[1]  = vextq_s8((int8x16_t)v45, (int8x16_t)v45, 8uLL).u64[0];
    *(void *)&v47.f64[1]  = vextq_s8((int8x16_t)v47, (int8x16_t)v47, 8uLL).u64[0];
    *(void *)&v50.f64[1]  = vextq_s8((int8x16_t)v50, (int8x16_t)v50, 8uLL).u64[0];
    do
    {
      float64x2_t v57 = (char *)&v75 + v56;
      float64x2_t v59 = *(float64x2_t *)((char *)&v75 + v56);
      float64x2_t v58 = *(float64x2_t *)((char *)&v75 + v56 + 16);
      *(float64x2_t *)float64x2_t v57 = vaddq_f64(v50, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v43, v59.f64[0]), v45, v59, 1), v47, v58.f64[0]));
      *((void *)v57 + 2)  = *(_OWORD *)&vaddq_f64(v49, vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v44, v59), v46, v59, 1), v58, v48));
      v56 += 32;
    }
    while (v56 != 256);
    uint64_t v60 = 0;
    a3[2]  = 0u;
    a3[3]  = 0u;
    a3[1]  = 0u;
    float64x2_t v61 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    v62.f64[0]  = INFINITY;
    float64x2_t v63 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
    v64.f64[0]  = -INFINITY;
    do
    {
      v65  = *(float64x2_t *)((char *)&v75 + v60);
      long long v66 = *(long long *)((char *)&v75 + v60 + 16);
      float64x2_t v61 = vminnmq_f64(v61, v65);
      float64x2_t v62 = vminnmq_f64((float64x2_t)*(unint64_t *)&v62.f64[0], (float64x2_t)(unint64_t)v66);
      float64x2_t v63 = vmaxnmq_f64(v63, v65);
      v64  = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v64.f64[0], (float64x2_t)(unint64_t)v66);
      v60 += 32;
    }
    while (v60 != 256);
    *a3  = v61;
    a3[1].f64[0]  = v62.f64[0];
    float64x2_t result = vsubq_f64(v63, v61);
    a3[2]  = result;
    *(void *)&a3[3].f64[0]  = *(_OWORD *)&vsubq_f64(v64, v62);
  }
  return result;
}

BOOL SPRect3DIsNull(float64x2_t *a1, double a2, float64x2_t a3, double a4, double a5, double a6, float64x2_t a7)
{
  a3.f64[0]  = a1[3].f64[0];
  float64x2_t v7 = (float64x2_t)vceqzq_f64(a3);
  int64x2_t v8 = vceqzq_f64(a1[2]);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v8, 1), vandq_s8((int8x16_t)v7, (int8x16_t)v8)).u64[0] & 0x8000000000000000) == 0) {
    return 0;
  }
  v7.f64[0]  = a1[1].f64[0];
  a7.f64[0]  = INFINITY;
  int64x2_t v10 = (int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(*a1), (int8x16_t)vcgezq_f64(*a1)), (int8x16_t)vceqq_f64(vabsq_f64(*a1), (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL)));
  return vandq_s8((int8x16_t)vdupq_laneq_s64(v10, 1), vandq_s8((int8x16_t)vshlq_n_s64((int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v7), (int8x16_t)vcgezq_f64(v7)), (int8x16_t)vceqq_f64(vabsq_f64(v7), a7)), 0x3FuLL), (int8x16_t)v10)).i64[0] >= 0;
}

void __swiftcall SPRect3D.unapplying(_:)(SPRect3D *__return_ptr retstr, SPProjectiveTransform3D *a2)
{
  SPRect3D.unapplying(_:)((float64x2_t *)a2, (void (*)(float64x2_t *__return_ptr, void *, float64x2_t *))SPRect3DUnapplyProjectiveTransform, &retstr->origin.x);
}

BOOL SPRect3D.unapplying(_:)@<W0>(float64x2_t *a1@<X0>, void (*a2)(float64x2_t *__return_ptr, void *, float64x2_t *)@<X1>, float64_t *a3@<X8>)
{
  float64x2_t v5 = *a1;
  float64x2_t v6 = a1[1];
  float64x2_t v7 = a1[2];
  float64x2_t v8 = a1[3];
  float64x2_t v9 = a1[4];
  float64x2_t v10 = a1[5];
  float64x2_t v11 = a1[6];
  float64x2_t v12 = a1[7];
  uint64_t v13 = *((void *)v3 + 3);
  double v34 = v3[4];
  double v35 = v3[5];
  double v36 = *v3;
  uint64_t v37 = *((void *)v3 + 6);
  double v38 = v3[1];
  uint64_t v39 = *((void *)v3 + 7);
  uint64_t v53 = *((void *)v3 + 2);
  uint64_t v33 = v53;
  uint64_t v54 = v13;
  *(double *)float64x2_t v52 = v36;
  *(double *)&v52[1]  = v38;
  uint64_t v57 = v37;
  uint64_t v58 = v39;
  double v55 = v34;
  double v56 = v35;
  float64x2_t v44 = v5;
  float64x2_t v45 = v6;
  float64x2_t v46 = v7;
  float64x2_t v47 = v8;
  float64x2_t v48 = v9;
  float64x2_t v49 = v10;
  float64x2_t v50 = v11;
  float64x2_t v51 = v12;
  a2(&v40, v52, &v44);
  float64x2_t v14 = v40;
  unsigned long long v15 = v41;
  float64x2_t v16 = v42;
  unsigned long long v17 = v43;
  float64x2_t v45 = (float64x2_t)v41;
  float64x2_t v44 = v40;
  float64x2_t v47 = (float64x2_t)v43;
  float64x2_t v46 = v42;
  BOOL result = SPRect3DIsNull(&v44, v18, v19, v20, v21, v22, v23);
  uint64_t v25 = v33;
  float64_t v26 = v34;
  if (result)
  {
    uint64_t v27 = v13;
  }
  else
  {
    uint64_t v27 = v15 >> 64;
    uint64_t v25 = v15;
  }
  *((void *)a3 + 2)  = v25;
  *((void *)a3 + 3)  = v27;
  float64_t v28 = v36;
  if (!result) {
    float64_t v28 = v14.f64[0];
  }
  float64_t v29 = v38;
  uint64_t v30 = v39;
  if (!result)
  {
    float64_t v29 = v14.f64[1];
    float64_t v26 = v16.f64[0];
  }
  *a3  = v28;
  a3[1]  = v29;
  float64_t v31 = v35;
  if (!result) {
    float64_t v31 = v16.f64[1];
  }
  uint64_t v32 = v37;
  if (!result)
  {
    uint64_t v30 = v17 >> 64;
    uint64_t v32 = v17;
  }
  *((void *)a3 + 6)  = v32;
  *((void *)a3 + 7)  = v30;
  a3[4]  = v26;
  a3[5]  = v31;
  return result;
}

float64x2_t SPRect3DUnapplyProjectiveTransform@<Q0>(uint64_t a1@<X0>, uint64_t a2@<X1>, float64x2_t *a3@<X8>, float64x2_t result@<Q0>, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15, uint64_t a16, uint64_t a17, uint64_t a18, uint64_t a19, uint64_t a20,uint64_t a21,uint64_t a22,uint64_t a23,uint64_t a24,uint64_t a25,uint64_t a26,uint64_t a27,uint64_t a28,uint64_t a29,uint64_t a30,float64x2_t a31,float64x2_t a32,float64x2_t a33,float64x2_t a34,float64x2_t a35,float64x2_t a36,float64x2_t a37,float64x2_t a38,long long a39,long long a40,long long a41,long long a42,float64x2_t a43,long long a44,long long a45,long long a46,uint64_t a47,uint64_t a48)
{
  a48  = *MEMORY[0x263EF8340];
  double v49 = *(double *)(a2 + 24);
  if (v49 != 0.0
    || (double v50 = *(double *)(a2 + 56), v50 != 0.0)
    || ((double v51 = *(double *)(a2 + 88), v51 == 0.0) ? (v52 = *(double *)(a2 + 120) == 1.0) : (v52 = 0), !v52))
  {
LABEL_36:
    *a3  = (float64x2_t)SPRect3DNull;
    a3[1]  = (float64x2_t)unk_228C20FF0;
    result.f64[0]  = 0.0;
    a3[2]  = (float64x2_t)xmmword_228C21000;
    a3[3]  = (float64x2_t)unk_228C21010;
    return result;
  }
  double v54 = vabdd_f64(0.0, v49);
  float64x2_t v55 = (float64x2_t)xmmword_228C1FC60;
  int64x2_t v56 = vdupq_n_s64(0x7FF0000000000000uLL);
  v127  = *(float64x2_t *)a2;
  if (v54 < 0.0000000149011612 && vabdd_f64(0.0, v50) < 0.0000000149011612)
  {
    if (vabdd_f64(0.0, v51) < 0.0000000149011612)
    {
      int64x2_t v56 = *(int64x2_t *)a2;
      float64x2_t v55 = *(float64x2_t *)(a2 + 16);
      float64x2_t v57 = *(float64x2_t *)(a2 + 32);
      float64x2_t v58 = *(float64x2_t *)(a2 + 48);
      int64x2_t v59 = *(int64x2_t *)(a2 + 64);
      double v60 = *(double *)(a2 + 80);
      goto LABEL_14;
    }
    int64x2_t v56 = vdupq_n_s64(0x7FF0000000000000uLL);
  }
  float64x2_t v57 = (float64x2_t)v56;
  float64x2_t v58 = (float64x2_t)xmmword_228C1FC60;
  int64x2_t v59 = v56;
  double v60 = INFINITY;
LABEL_14:
  double v61 = *(double *)v56.i64 + v57.f64[1] + v60;
  v132  = *(float64x2_t *)(a2 + 48);
  v133  = *(float64x2_t *)(a2 + 16);
  v130  = *(float64x2_t *)(a2 + 80);
  v131  = *(float64x2_t *)(a2 + 112);
  vars0  = *(float64x2_t *)(a2 + 32);
  v129  = *(float64x2_t *)(a2 + 64);
  if (v61 >= 0.0)
  {
    v80  = sqrt(v61 + 1.0);
    v81  = v80 + v80;
    v82  = 1.0 / (v80 + v80);
    v86.f64[0]  = v82 * vsubq_f64(v58, (float64x2_t)vdupq_laneq_s64(v59, 1)).f64[0];
    v87  = (*(double *)v59.i64 - v55.f64[0]) * v82;
    float64x2_t v55 = vsubq_f64((float64x2_t)vdupq_laneq_s64(v56, 1), v57);
    v55.f64[0]  = v82 * v55.f64[0];
    v88  = v81 * 0.25;
  }
  else if (*(double *)v56.i64 < v57.f64[1] || *(double *)v56.i64 < v60)
  {
    double v63 = 1.0 - *(double *)v56.i64;
    v64  = v57.f64[1] < v60;
    v65  = sqrt(1.0 - *(double *)v56.i64 - v57.f64[1] + v60);
    double v66 = v65 + v65;
    double v67 = 1.0 / v66;
    double v68 = (v55.f64[0] + *(double *)v59.i64) * (1.0 / v66);
    double v69 = vaddq_f64(v58, (float64x2_t)vdupq_laneq_s64(v59, 1)).f64[0];
    double v70 = 1.0 / v66 * v69;
    float64x2_t v71 = (float64x2_t)vdupq_laneq_s64(v56, 1);
    double v72 = v66 * 0.25;
    v73  = v67 * vsubq_f64(v71, v57).f64[0];
    v74  = sqrt(v57.f64[1] + v63 - v60);
    v75  = v74 + v74;
    v76  = 1.0 / v75 * vaddq_f64(v71, v57).f64[0];
    v77  = v75 * 0.25;
    v78  = 1.0 / v75 * v69;
    v79  = (*(double *)v59.i64 - v55.f64[0]) * (1.0 / v75);
    if (v64) {
      v86.f64[0]  = v68;
    }
    else {
      v86.f64[0]  = v76;
    }
    if (v64) {
      v87  = v70;
    }
    else {
      v87  = v77;
    }
    if (v64) {
      v55.f64[0]  = v72;
    }
    else {
      v55.f64[0]  = v78;
    }
    if (v64) {
      v88  = v73;
    }
    else {
      v88  = v79;
    }
  }
  else
  {
    v83  = sqrt(*(double *)v56.i64 + 1.0 - v57.f64[1] - v60);
    v84  = v83 + v83;
    v85  = 1.0 / v84;
    v86.f64[0]  = v84 * 0.25;
    v87  = v85 * vaddq_f64((float64x2_t)vdupq_laneq_s64(v56, 1), v57).f64[0];
    v55.f64[0]  = (v55.f64[0] + *(double *)v59.i64) * v85;
    v88  = v85 * vsubq_f64(v58, (float64x2_t)vdupq_laneq_s64(v59, 1)).f64[0];
  }
  v86.f64[1]  = v87;
  v126  = *(float64x2_t *)(a2 + 96);
  v89  = atan2(sqrt(vmulq_f64(v55, v55).f64[0] + vaddvq_f64(vmulq_f64(v86, v86))), v88);
  if (fabs(remainder(v89 + v89, 1.57079633)) >= 0.0000000149011612) {
    goto LABEL_36;
  }
  v90  = (float64x2_t)vextq_s8((int8x16_t)vars0, (int8x16_t)v132, 8uLL);
  v91  = (float64x2_t)vextq_s8((int8x16_t)v132, (int8x16_t)vars0, 8uLL);
  v92  = (float64x2_t)vextq_s8((int8x16_t)v126, (int8x16_t)v131, 8uLL);
  v93  = (float64x2_t)vextq_s8((int8x16_t)v131, (int8x16_t)v126, 8uLL);
  v94  = (float64x2_t)vextq_s8((int8x16_t)v129, (int8x16_t)v130, 8uLL);
  v95  = (float64x2_t)vextq_s8((int8x16_t)v130, (int8x16_t)v129, 8uLL);
  v96  = vnegq_f64(v95);
  v97  = vnegq_f64(v94);
  v98  = vmlaq_f64(vmulq_f64(v126, v97), v92, v129);
  v99  = vmlaq_f64(vmulq_f64(v132, vmlaq_f64(vmulq_f64(v93, v97), v92, v95)), vmlaq_f64(vmulq_f64(v131, v96), v93, v130), v90);
  v100  = (int64x2_t)vmulq_f64(v133, vmlaq_f64(vmlaq_f64(vmulq_f64(vars0, vmlaq_f64(vmulq_f64(v92, v96), v93, v94)), v98, v91), vmlaq_f64(vmulq_f64(v93, vnegq_f64(v129)), v126, v95), v90));
  v101  = (int64x2_t)vmulq_f64(v127, vmlaq_f64(v99, vmlaq_f64(vmulq_f64(v92, vnegq_f64(v130)), v131, v94), v91));
  if (vaddvq_f64(vsubq_f64((float64x2_t)vzip1q_s64(v101, v100), (float64x2_t)vzip2q_s64(v101, v100))) == 0.0)
  {
    v102  = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    v103  = v102;
    v104  = v102;
    v105  = v102;
    v106  = v102;
    v107  = v102;
    v108  = v102;
    v109  = v102;
  }
  else
  {
    a31  = v127;
    a32  = v133;
    a33  = vars0;
    a34  = v132;
    a35  = v129;
    a36  = v130;
    a37  = v126;
    a38  = v131;
    __invert_d4();
    v102  = 0u;
    v103  = 0u;
    v104  = 0u;
    v105  = 0u;
    v106  = 0u;
    v107  = 0u;
    v108  = 0u;
    v109  = 0u;
  }
  v110  = 0;
  v111  = *(float64x2_t *)(a1 + 32);
  v112  = *(double *)(a1 + 48);
  a45  = 0u;
  a46  = 0u;
  a43  = 0u;
  a44  = 0u;
  a41  = 0u;
  a42  = 0u;
  a39  = 0u;
  a40  = 0u;
  a37  = 0u;
  a38  = 0u;
  a35  = 0u;
  a36  = 0u;
  a33  = 0u;
  a34  = 0u;
  a31  = 0u;
  a32  = 0u;
  do
  {
    v113  = (_OWORD *)&a31.f64[v110];
    v114  = *(_OWORD *)(a1 + 16);
    *v113  = *(_OWORD *)a1;
    v113[1]  = v114;
    v110 += 4;
  }
  while (v110 != 32);
  v115  = 0;
  a33.f64[1]  = v111.f64[1] + a33.f64[1];
  a35  = vaddq_f64(v111, a35);
  a37.f64[0]  = v111.f64[0] + a37.f64[0];
  *(double *)&a40  = v112 + *(double *)&a40;
  *((double *)&a41 + 1)  = v111.f64[1] + *((double *)&a41 + 1);
  *(double *)&a42  = v112 + *(double *)&a42;
  a43  = vaddq_f64(v111, a43);
  *(double *)&a44  = v112 + *(double *)&a44;
  *(double *)&a45  = v111.f64[0] + *(double *)&a45;
  *(double *)&a46  = v112 + *(double *)&a46;
  do
  {
    v116  = (char *)&a31 + v115;
    v118  = *(float64x2_t *)((char *)&a31 + v115);
    v117  = *(long long *)((char *)&a31 + v115 + 16);
    *(float64x2_t *)v116  = vaddq_f64(v108, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v102, v118.f64[0]), v104, v118, 1), v106, *(double *)&v117));
    *((void *)v116 + 2)  = *(_OWORD *)&vaddq_f64(v109, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v103, v118.f64[0]), v105, v118, 1), v107, *(double *)&v117));
    v115 += 32;
  }
  while (v115 != 256);
  v119  = 0;
  a3[2]  = 0u;
  a3[3]  = 0u;
  a3[1]  = 0u;
  v120  = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v121.f64[0]  = INFINITY;
  v122  = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
  v123.f64[0]  = -INFINITY;
  do
  {
    v124  = *(float64x2_t *)((char *)&a31 + v119);
    v125  = *(long long *)((char *)&a31 + v119 + 16);
    v120  = vminnmq_f64(v120, v124);
    v121  = vminnmq_f64((float64x2_t)*(unint64_t *)&v121.f64[0], (float64x2_t)(unint64_t)v125);
    v122  = vmaxnmq_f64(v122, v124);
    v123  = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v123.f64[0], (float64x2_t)(unint64_t)v125);
    v119 += 32;
  }
  while (v119 != 256);
  *a3  = v120;
  a3[1].f64[0]  = v121.f64[0];
  BOOL result = vsubq_f64(v122, v120);
  a3[2]  = result;
  *(void *)&a3[3].f64[0]  = *(_OWORD *)&vsubq_f64(v123, v121);
  return result;
}

{
  double v49;
  double v50;
  double v51;
  BOOL v52;
  double v54;
  float64x2_t v55;
  int64x2_t v56;
  float64x2_t v57;
  float64x2_t v58;
  int64x2_t v59;
  double v60;
  double v61;
  double v63;
  BOOL v64;
  double v65;
  double v66;
  double v67;
  double v68;
  double v69;
  double v70;
  float64x2_t v71;
  double v72;
  double v73;
  double v74;
  double v75;
  double v76;
  double v77;
  double v78;
  double v79;
  double v80;
  double v81;
  double v82;
  double v83;
  double v84;
  double v85;
  float64x2_t v86;
  double v87;
  double v88;
  long double v89;
  float64x2_t v90;
  float64x2_t v91;
  float64x2_t v92;
  float64x2_t v93;
  float64x2_t v94;
  float64x2_t v95;
  float64x2_t v96;
  float64x2_t v97;
  float64x2_t v98;
  float64x2_t v99;
  int64x2_t v100;
  int64x2_t v101;
  float64x2_t v102;
  float64x2_t v103;
  float64x2_t v104;
  float64x2_t v105;
  float64x2_t v106;
  float64x2_t v107;
  float64x2_t v108;
  float64x2_t v109;
  uint64_t v110;
  float64x2_t v111;
  double v112;
  _OWORD *v113;
  long long v114;
  uint64_t v115;
  char *v116;
  long long v117;
  float64x2_t v118;
  uint64_t v119;
  float64x2_t v120;
  float64x2_t v121;
  float64x2_t v122;
  float64x2_t v123;
  float64x2_t v124;
  long long v125;
  float64x2_t v126;
  float64x2_t v127;
  float64x2_t vars0;
  float64x2_t v129;
  float64x2_t v130;
  float64x2_t v131;
  float64x2_t v132;
  float64x2_t v133;

  a48  = *MEMORY[0x263EF8340];
  double v49 = *(double *)(a2 + 24);
  if (v49 != 0.0
    || (double v50 = *(double *)(a2 + 56), v50 != 0.0)
    || ((double v51 = *(double *)(a2 + 88), v51 == 0.0) ? (v52 = *(double *)(a2 + 120) == 1.0) : (v52 = 0), !v52))
  {
LABEL_36:
    *a3  = (float64x2_t)SPRect3DNull_0;
    a3[1]  = (float64x2_t)unk_228C21830;
    result.f64[0]  = 0.0;
    a3[2]  = (float64x2_t)xmmword_228C21840;
    a3[3]  = (float64x2_t)unk_228C21850;
    return result;
  }
  double v54 = vabdd_f64(0.0, v49);
  float64x2_t v55 = (float64x2_t)xmmword_228C1FC60;
  int64x2_t v56 = vdupq_n_s64(0x7FF0000000000000uLL);
  v127  = *(float64x2_t *)a2;
  if (v54 < 0.0000000149011612 && vabdd_f64(0.0, v50) < 0.0000000149011612)
  {
    if (vabdd_f64(0.0, v51) < 0.0000000149011612)
    {
      int64x2_t v56 = *(int64x2_t *)a2;
      float64x2_t v55 = *(float64x2_t *)(a2 + 16);
      float64x2_t v57 = *(float64x2_t *)(a2 + 32);
      float64x2_t v58 = *(float64x2_t *)(a2 + 48);
      int64x2_t v59 = *(int64x2_t *)(a2 + 64);
      double v60 = *(double *)(a2 + 80);
      goto LABEL_14;
    }
    int64x2_t v56 = vdupq_n_s64(0x7FF0000000000000uLL);
  }
  float64x2_t v57 = (float64x2_t)v56;
  float64x2_t v58 = (float64x2_t)xmmword_228C1FC60;
  int64x2_t v59 = v56;
  double v60 = INFINITY;
LABEL_14:
  double v61 = *(double *)v56.i64 + v57.f64[1] + v60;
  v132  = *(float64x2_t *)(a2 + 48);
  v133  = *(float64x2_t *)(a2 + 16);
  v130  = *(float64x2_t *)(a2 + 80);
  v131  = *(float64x2_t *)(a2 + 112);
  vars0  = *(float64x2_t *)(a2 + 32);
  v129  = *(float64x2_t *)(a2 + 64);
  if (v61 >= 0.0)
  {
    v80  = sqrt(v61 + 1.0);
    v81  = v80 + v80;
    v82  = 1.0 / (v80 + v80);
    v86.f64[0]  = v82 * vsubq_f64(v58, (float64x2_t)vdupq_laneq_s64(v59, 1)).f64[0];
    v87  = (*(double *)v59.i64 - v55.f64[0]) * v82;
    float64x2_t v55 = vsubq_f64((float64x2_t)vdupq_laneq_s64(v56, 1), v57);
    v55.f64[0]  = v82 * v55.f64[0];
    v88  = v81 * 0.25;
  }
  else if (*(double *)v56.i64 < v57.f64[1] || *(double *)v56.i64 < v60)
  {
    double v63 = 1.0 - *(double *)v56.i64;
    v64  = v57.f64[1] < v60;
    v65  = sqrt(1.0 - *(double *)v56.i64 - v57.f64[1] + v60);
    double v66 = v65 + v65;
    double v67 = 1.0 / v66;
    double v68 = (v55.f64[0] + *(double *)v59.i64) * (1.0 / v66);
    double v69 = vaddq_f64(v58, (float64x2_t)vdupq_laneq_s64(v59, 1)).f64[0];
    double v70 = 1.0 / v66 * v69;
    float64x2_t v71 = (float64x2_t)vdupq_laneq_s64(v56, 1);
    double v72 = v66 * 0.25;
    v73  = v67 * vsubq_f64(v71, v57).f64[0];
    v74  = sqrt(v57.f64[1] + v63 - v60);
    v75  = v74 + v74;
    v76  = 1.0 / v75 * vaddq_f64(v71, v57).f64[0];
    v77  = v75 * 0.25;
    v78  = 1.0 / v75 * v69;
    v79  = (*(double *)v59.i64 - v55.f64[0]) * (1.0 / v75);
    if (v64) {
      v86.f64[0]  = v68;
    }
    else {
      v86.f64[0]  = v76;
    }
    if (v64) {
      v87  = v70;
    }
    else {
      v87  = v77;
    }
    if (v64) {
      v55.f64[0]  = v72;
    }
    else {
      v55.f64[0]  = v78;
    }
    if (v64) {
      v88  = v73;
    }
    else {
      v88  = v79;
    }
  }
  else
  {
    v83  = sqrt(*(double *)v56.i64 + 1.0 - v57.f64[1] - v60);
    v84  = v83 + v83;
    v85  = 1.0 / v84;
    v86.f64[0]  = v84 * 0.25;
    v87  = v85 * vaddq_f64((float64x2_t)vdupq_laneq_s64(v56, 1), v57).f64[0];
    v55.f64[0]  = (v55.f64[0] + *(double *)v59.i64) * v85;
    v88  = v85 * vsubq_f64(v58, (float64x2_t)vdupq_laneq_s64(v59, 1)).f64[0];
  }
  v86.f64[1]  = v87;
  v126  = *(float64x2_t *)(a2 + 96);
  v89  = atan2(sqrt(vmulq_f64(v55, v55).f64[0] + vaddvq_f64(vmulq_f64(v86, v86))), v88);
  if (fabs(remainder(v89 + v89, 1.57079633)) >= 0.0000000149011612) {
    goto LABEL_36;
  }
  v90  = (float64x2_t)vextq_s8((int8x16_t)vars0, (int8x16_t)v132, 8uLL);
  v91  = (float64x2_t)vextq_s8((int8x16_t)v132, (int8x16_t)vars0, 8uLL);
  v92  = (float64x2_t)vextq_s8((int8x16_t)v126, (int8x16_t)v131, 8uLL);
  v93  = (float64x2_t)vextq_s8((int8x16_t)v131, (int8x16_t)v126, 8uLL);
  v94  = (float64x2_t)vextq_s8((int8x16_t)v129, (int8x16_t)v130, 8uLL);
  v95  = (float64x2_t)vextq_s8((int8x16_t)v130, (int8x16_t)v129, 8uLL);
  v96  = vnegq_f64(v95);
  v97  = vnegq_f64(v94);
  v98  = vmlaq_f64(vmulq_f64(v126, v97), v92, v129);
  v99  = vmlaq_f64(vmulq_f64(v132, vmlaq_f64(vmulq_f64(v93, v97), v92, v95)), vmlaq_f64(vmulq_f64(v131, v96), v93, v130), v90);
  v100  = (int64x2_t)vmulq_f64(v133, vmlaq_f64(vmlaq_f64(vmulq_f64(vars0, vmlaq_f64(vmulq_f64(v92, v96), v93, v94)), v98, v91), vmlaq_f64(vmulq_f64(v93, vnegq_f64(v129)), v126, v95), v90));
  v101  = (int64x2_t)vmulq_f64(v127, vmlaq_f64(v99, vmlaq_f64(vmulq_f64(v92, vnegq_f64(v130)), v131, v94), v91));
  if (vaddvq_f64(vsubq_f64((float64x2_t)vzip1q_s64(v101, v100), (float64x2_t)vzip2q_s64(v101, v100))) == 0.0)
  {
    v102  = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    v103  = v102;
    v104  = v102;
    v105  = v102;
    v106  = v102;
    v107  = v102;
    v108  = v102;
    v109  = v102;
  }
  else
  {
    a31  = v127;
    a32  = v133;
    a33  = vars0;
    a34  = v132;
    a35  = v129;
    a36  = v130;
    a37  = v126;
    a38  = v131;
    __invert_d4();
    v102  = 0u;
    v103  = 0u;
    v104  = 0u;
    v105  = 0u;
    v106  = 0u;
    v107  = 0u;
    v108  = 0u;
    v109  = 0u;
  }
  v110  = 0;
  v111  = *(float64x2_t *)(a1 + 32);
  v112  = *(double *)(a1 + 48);
  a45  = 0u;
  a46  = 0u;
  a43  = 0u;
  a44  = 0u;
  a41  = 0u;
  a42  = 0u;
  a39  = 0u;
  a40  = 0u;
  a37  = 0u;
  a38  = 0u;
  a35  = 0u;
  a36  = 0u;
  a33  = 0u;
  a34  = 0u;
  a31  = 0u;
  a32  = 0u;
  do
  {
    v113  = (_OWORD *)&a31.f64[v110];
    v114  = *(_OWORD *)(a1 + 16);
    *v113  = *(_OWORD *)a1;
    v113[1]  = v114;
    v110 += 4;
  }
  while (v110 != 32);
  v115  = 0;
  a33.f64[1]  = v111.f64[1] + a33.f64[1];
  a35  = vaddq_f64(v111, a35);
  a37.f64[0]  = v111.f64[0] + a37.f64[0];
  *(double *)&a40  = v112 + *(double *)&a40;
  *((double *)&a41 + 1)  = v111.f64[1] + *((double *)&a41 + 1);
  *(double *)&a42  = v112 + *(double *)&a42;
  a43  = vaddq_f64(v111, a43);
  *(double *)&a44  = v112 + *(double *)&a44;
  *(double *)&a45  = v111.f64[0] + *(double *)&a45;
  *(double *)&a46  = v112 + *(double *)&a46;
  do
  {
    v116  = (char *)&a31 + v115;
    v118  = *(float64x2_t *)((char *)&a31 + v115);
    v117  = *(long long *)((char *)&a31 + v115 + 16);
    *(float64x2_t *)v116  = vaddq_f64(v108, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v102, v118.f64[0]), v104, v118, 1), v106, *(double *)&v117));
    *((void *)v116 + 2)  = *(_OWORD *)&vaddq_f64(v109, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v103, v118.f64[0]), v105, v118, 1), v107, *(double *)&v117));
    v115 += 32;
  }
  while (v115 != 256);
  v119  = 0;
  a3[2]  = 0u;
  a3[3]  = 0u;
  a3[1]  = 0u;
  v120  = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v121.f64[0]  = INFINITY;
  v122  = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
  v123.f64[0]  = -INFINITY;
  do
  {
    v124  = *(float64x2_t *)((char *)&a31 + v119);
    v125  = *(long long *)((char *)&a31 + v119 + 16);
    v120  = vminnmq_f64(v120, v124);
    v121  = vminnmq_f64((float64x2_t)*(unint64_t *)&v121.f64[0], (float64x2_t)(unint64_t)v125);
    v122  = vmaxnmq_f64(v122, v124);
    v123  = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v123.f64[0], (float64x2_t)(unint64_t)v125);
    v119 += 32;
  }
  while (v119 != 256);
  *a3  = v120;
  a3[1].f64[0]  = v121.f64[0];
  BOOL result = vsubq_f64(v122, v120);
  a3[2]  = result;
  *(void *)&a3[3].f64[0]  = *(_OWORD *)&vsubq_f64(v123, v121);
  return result;
}

void __swiftcall SPRect3D.unapplying(_:)(SPRect3D *__return_ptr retstr, SPPose3D *a2)
{
  long long v4 = *(_OWORD *)&a2->position.x;
  long long v5 = *(_OWORD *)&a2->position.vector.f64[2];
  long long v6 = *(_OWORD *)a2->rotation.vector.f64;
  double v7 = a2->rotation.vector.f64[2];
  double v8 = a2->rotation.vector.f64[3];
  double v9 = *(double *)(v2 + 24);
  long long v32 = *(_OWORD *)(v2 + 32);
  double v33 = *(double *)v2;
  double v34 = *(double *)(v2 + 48);
  double v35 = *(double *)(v2 + 8);
  double v36 = *(double *)(v2 + 56);
  v39.origin.double z = *(double *)(v2 + 16);
  double z = v39.origin.z;
  v39.origin.vector.f64[3]  = v9;
  v39.origin.x  = v33;
  v39.origin.y  = v35;
  v39.size.depth  = v34;
  v39.size.vector.f64[3]  = v36;
  *(_OWORD *)&v39.size.width  = v32;
  *(_OWORD *)&v38.position.x  = v4;
  *(_OWORD *)&v38.position.vector.f64[2]  = v5;
  v38.rotation.vector.f64[2]  = v7;
  v38.rotation.vector.f64[3]  = v8;
  *(_OWORD *)v38.rotation.vector.f64  = v6;
  *(void *)&double v10 = *(_OWORD *)&SPRect3DUnapplyPose(&v39, &v38, (float64x2_t *)&v37);
  x  = v37.origin.x;
  long long v13 = *(_OWORD *)&v37.origin.vector.f64[1];
  double v12 = v37.origin.vector.f64[3];
  width  = v37.size.width;
  long long v16 = *(_OWORD *)&v37.size.vector.f64[1];
  double v15 = v37.size.vector.f64[3];
  SPRect3D v39 = v37;
  BOOL v22 = SPRect3DIsNull((float64x2_t *)&v39, v10, v17, v18, v19, v20, v21);
  double v23 = z;
  double v24 = *(double *)&v32;
  if (v22)
  {
    double v25 = v9;
  }
  else
  {
    double v23 = *((double *)&v13 + 1);
    double v25 = v12;
  }
  retstr->origin.double z = v23;
  retstr->origin.vector.f64[3]  = v25;
  double v26 = v33;
  if (!v22) {
    double v26 = x;
  }
  double v27 = v35;
  double v28 = v36;
  if (!v22)
  {
    double v27 = *(double *)&v13;
    double v24 = width;
  }
  retstr->origin.x  = v26;
  retstr->origin.y  = v27;
  double v29 = *((double *)&v32 + 1);
  if (!v22) {
    double v29 = *(double *)&v16;
  }
  double v30 = v34;
  if (!v22)
  {
    double v30 = *((double *)&v16 + 1);
    double v28 = v15;
  }
  retstr->size.depth  = v30;
  retstr->size.vector.f64[3]  = v28;
  retstr->size.width  = v24;
  retstr->size.height  = v29;
}

float64x2_t SPRect3DUnapplyPose@<Q0>(SPRect3D *a1@<X0>, SPPose3D *a2@<X1>, float64x2_t *a3@<X8>)
{
  v65  = *MEMORY[0x263EF8340];
  float64x2_t v48 = vmulq_f64(*(float64x2_t *)&a2->rotation.quaternion.vector.f64[2], *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2]);
  float64x2_t v49 = *(float64x2_t *)a2->rotation.vector.f64;
  float64x2_t v46 = vmulq_f64(v49, v49);
  float64x2_t v47 = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  result.f64[0]  = atan2(sqrt(v48.f64[0] + vaddvq_f64(v46)), a2->rotation.vector.f64[3]);
  if (fabs(result.f64[0] + result.f64[0]) <= 0.0000000149011612)
  {
    uint64_t v7 = 0;
    long long v63 = 0u;
    *(_OWORD *)v64  = 0u;
    float64x2_t v61 = 0u;
    long long v62 = 0u;
    long long v59 = 0u;
    long long v60 = 0u;
    long long v57 = 0u;
    long long v58 = 0u;
    long long v55 = 0u;
    long long v56 = 0u;
    float64x2_t v53 = 0u;
    long long v54 = 0u;
    long long v51 = 0u;
    long long v52 = 0u;
    memset(v50, 0, sizeof(v50));
    float64x2_t v8 = *(float64x2_t *)&a1->size.width;
    depth  = a1->size.depth;
    do
    {
      double v10 = &v50[v7];
      long long v11 = *(_OWORD *)&a1->origin.vector.f64[2];
      *double v10 = *(_OWORD *)&a1->origin.x;
      v10[1]  = v11;
      v7 += 2;
    }
    while (v7 != 16);
    uint64_t v12 = 0;
    *((double *)&v51 + 1)  = v8.f64[1] + *((double *)&v51 + 1);
    float64x2_t v53 = vaddq_f64(v8, v53);
    *(double *)&long long v55 = v8.f64[0] + *(double *)&v55;
    *(double *)&long long v58 = depth + *(double *)&v58;
    *((double *)&v59 + 1)  = v8.f64[1] + *((double *)&v59 + 1);
    *(double *)&long long v60 = depth + *(double *)&v60;
    float64x2_t v61 = vaddq_f64(v8, v61);
    *(double *)&long long v62 = depth + *(double *)&v62;
    *(double *)&long long v63 = v8.f64[0] + *(double *)&v63;
    v64[0]  = depth + v64[0];
    long long v13 = *(_OWORD *)&a2->position.vector.f64[2];
    double v14 = 1.0 / vaddvq_f64(vaddq_f64(v46, v48));
    float64x2_t v15 = vmulq_n_f64(vmulq_f64(v47, (float64x2_t)xmmword_228C1FC40), v14);
    float64x2_t v16 = vmulq_n_f64(vnegq_f64(v49), v14);
    float64x2_t v17 = vmulq_f64(v15, (float64x2_t)xmmword_228C1FC40);
    int8x16_t v18 = (int8x16_t)vnegq_f64(v16);
    float64x2_t v19 = vnegq_f64(*(float64x2_t *)&a2->position.x);
    float64x2_t v20 = (float64x2_t)vextq_s8((int8x16_t)v17, (int8x16_t)vnegq_f64(v17), 8uLL);
    float64x2_t v21 = (float64x2_t)vextq_s8(v18, (int8x16_t)v16, 8uLL);
    float64x2_t v22 = (float64x2_t)vextq_s8((int8x16_t)v16, v18, 8uLL);
    float64x2_t v23 = vmlsq_lane_f64(vmlsq_lane_f64(vmulq_laneq_f64(v17, v19, 1), v20, a2->position.x, 0), v22, *(double *)&v13, 0);
    float64x2_t v24 = vmlsq_lane_f64(vmlsq_lane_f64(vmulq_laneq_f64(v16, v19, 1), v21, a2->position.x, 0), v20, *(double *)&v13, 0);
    float64x2_t v25 = (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v16.f64[0], 0);
    float64x2_t v26 = vnegq_f64(v23);
    float64x2_t v27 = (float64x2_t)vextq_s8((int8x16_t)v24, (int8x16_t)vnegq_f64(v24), 8uLL);
    float64x2_t v28 = vmlaq_n_f64(vmulq_laneq_f64(v24, v16, 1), v27, v16.f64[0]);
    float64x2_t v29 = (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v15.f64[0], 0);
    float64x2_t v30 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v24, v15, 1), v27, v15.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v26, v16, 1), (float64x2_t)vextq_s8((int8x16_t)v23, (int8x16_t)v26, 8uLL), v16.f64[0]));
    float64x2_t v31 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v23, v15, 1), (float64x2_t)vextq_s8((int8x16_t)v26, (int8x16_t)v23, 8uLL), v15.f64[0]), v28);
    do
    {
      long long v32 = &v50[v12];
      float64x2_t v34 = (float64x2_t)v50[v12];
      long long v33 = v50[v12 + 1];
      float64x2_t v35 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v16, v34, 1), v21, v34.f64[0]), v20, *(double *)&v33);
      float64x2_t v36 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v17, v34, 1), v20, v34.f64[0]), v22, *(double *)&v33);
      float64x2_t v37 = vnegq_f64(v36);
      float64x2_t v38 = (float64x2_t)vextq_s8((int8x16_t)v35, (int8x16_t)vnegq_f64(v35), 8uLL);
      *long long v32 = vaddq_f64(vaddq_f64(vmlaq_f64(vmulq_laneq_f64(v36, v15, 1), (float64x2_t)vextq_s8((int8x16_t)v37, (int8x16_t)v36, 8uLL), v29), vmlaq_f64(vmulq_laneq_f64(v35, v16, 1), v38, v25)), v31);
      *((void *)v32 + 2)  = *(_OWORD *)&vaddq_f64(vaddq_f64(vmlaq_f64(vmulq_laneq_f64(v35, v15, 1), v38, v29), vmlaq_f64(vmulq_laneq_f64(v37, v16, 1), (float64x2_t)vextq_s8((int8x16_t)v36, (int8x16_t)v37, 8uLL), v25)), v30);
      v12 += 2;
    }
    while (v12 != 16);
    uint64_t v39 = 0;
    a3[2]  = 0u;
    a3[3]  = 0u;
    a3[1]  = 0u;
    float64x2_t v40 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    v41.f64[0]  = INFINITY;
    float64x2_t v42 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
    v43.f64[0]  = -INFINITY;
    do
    {
      float64x2_t v44 = (float64x2_t)v50[v39];
      long long v45 = v50[v39 + 1];
      float64x2_t v40 = vminnmq_f64(v40, v44);
      float64x2_t v41 = vminnmq_f64((float64x2_t)*(unint64_t *)&v41.f64[0], (float64x2_t)(unint64_t)v45);
      float64x2_t v42 = vmaxnmq_f64(v42, v44);
      float64x2_t v43 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v43.f64[0], (float64x2_t)(unint64_t)v45);
      v39 += 2;
    }
    while (v39 != 16);
    *a3  = v40;
    a3[1].f64[0]  = v41.f64[0];
    float64x2_t result = vsubq_f64(v42, v40);
    a3[2]  = result;
    *(void *)&a3[3].f64[0]  = *(_OWORD *)&vsubq_f64(v43, v41);
  }
  else
  {
    *a3  = (float64x2_t)SPRect3DNull;
    a3[1]  = (float64x2_t)unk_228C20FF0;
    result.f64[0]  = 0.0;
    a3[2]  = (float64x2_t)xmmword_228C21000;
    a3[3]  = (float64x2_t)unk_228C21010;
  }
  return result;
}

{
  float64x2_t result;
  uint64_t v7;
  float64x2_t v8;
  double depth;
  _OWORD *v10;
  long long v11;
  uint64_t v12;
  long long v13;
  double v14;
  float64x2_t v15;
  float64x2_t v16;
  float64x2_t v17;
  int8x16_t v18;
  float64x2_t v19;
  float64x2_t v20;
  float64x2_t v21;
  float64x2_t v22;
  float64x2_t v23;
  float64x2_t v24;
  float64x2_t v25;
  float64x2_t v26;
  float64x2_t v27;
  float64x2_t v28;
  float64x2_t v29;
  float64x2_t v30;
  float64x2_t v31;
  _OWORD *v32;
  long long v33;
  float64x2_t v34;
  float64x2_t v35;
  float64x2_t v36;
  float64x2_t v37;
  float64x2_t v38;
  uint64_t v39;
  float64x2_t v40;
  float64x2_t v41;
  float64x2_t v42;
  float64x2_t v43;
  float64x2_t v44;
  long long v45;
  float64x2_t v46;
  float64x2_t v47;
  float64x2_t v48;
  float64x2_t v49;
  _OWORD v50[2];
  long long v51;
  long long v52;
  float64x2_t v53;
  long long v54;
  long long v55;
  long long v56;
  long long v57;
  long long v58;
  long long v59;
  long long v60;
  float64x2_t v61;
  long long v62;
  long long v63;
  double v64[3];
  uint64_t v65;

  v65  = *MEMORY[0x263EF8340];
  float64x2_t v48 = vmulq_f64(*(float64x2_t *)&a2->rotation.quaternion.vector.f64[2], *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2]);
  float64x2_t v49 = *(float64x2_t *)a2->rotation.vector.f64;
  float64x2_t v46 = vmulq_f64(v49, v49);
  float64x2_t v47 = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  result.f64[0]  = atan2(sqrt(v48.f64[0] + vaddvq_f64(v46)), a2->rotation.vector.f64[3]);
  if (fabs(result.f64[0] + result.f64[0]) <= 0.0000000149011612)
  {
    uint64_t v7 = 0;
    long long v63 = 0u;
    *(_OWORD *)v64  = 0u;
    float64x2_t v61 = 0u;
    long long v62 = 0u;
    long long v59 = 0u;
    long long v60 = 0u;
    long long v57 = 0u;
    long long v58 = 0u;
    long long v55 = 0u;
    long long v56 = 0u;
    float64x2_t v53 = 0u;
    long long v54 = 0u;
    long long v51 = 0u;
    long long v52 = 0u;
    memset(v50, 0, sizeof(v50));
    float64x2_t v8 = *(float64x2_t *)&a1->size.width;
    depth  = a1->size.depth;
    do
    {
      double v10 = &v50[v7];
      long long v11 = *(_OWORD *)&a1->origin.vector.f64[2];
      *double v10 = *(_OWORD *)&a1->origin.x;
      v10[1]  = v11;
      v7 += 2;
    }
    while (v7 != 16);
    uint64_t v12 = 0;
    *((double *)&v51 + 1)  = v8.f64[1] + *((double *)&v51 + 1);
    float64x2_t v53 = vaddq_f64(v8, v53);
    *(double *)&long long v55 = v8.f64[0] + *(double *)&v55;
    *(double *)&long long v58 = depth + *(double *)&v58;
    *((double *)&v59 + 1)  = v8.f64[1] + *((double *)&v59 + 1);
    *(double *)&long long v60 = depth + *(double *)&v60;
    float64x2_t v61 = vaddq_f64(v8, v61);
    *(double *)&long long v62 = depth + *(double *)&v62;
    *(double *)&long long v63 = v8.f64[0] + *(double *)&v63;
    v64[0]  = depth + v64[0];
    long long v13 = *(_OWORD *)&a2->position.vector.f64[2];
    double v14 = 1.0 / vaddvq_f64(vaddq_f64(v46, v48));
    float64x2_t v15 = vmulq_n_f64(vmulq_f64(v47, (float64x2_t)xmmword_228C1FC40), v14);
    float64x2_t v16 = vmulq_n_f64(vnegq_f64(v49), v14);
    float64x2_t v17 = vmulq_f64(v15, (float64x2_t)xmmword_228C1FC40);
    int8x16_t v18 = (int8x16_t)vnegq_f64(v16);
    float64x2_t v19 = vnegq_f64(*(float64x2_t *)&a2->position.x);
    float64x2_t v20 = (float64x2_t)vextq_s8((int8x16_t)v17, (int8x16_t)vnegq_f64(v17), 8uLL);
    float64x2_t v21 = (float64x2_t)vextq_s8(v18, (int8x16_t)v16, 8uLL);
    float64x2_t v22 = (float64x2_t)vextq_s8((int8x16_t)v16, v18, 8uLL);
    float64x2_t v23 = vmlsq_lane_f64(vmlsq_lane_f64(vmulq_laneq_f64(v17, v19, 1), v20, a2->position.x, 0), v22, *(double *)&v13, 0);
    float64x2_t v24 = vmlsq_lane_f64(vmlsq_lane_f64(vmulq_laneq_f64(v16, v19, 1), v21, a2->position.x, 0), v20, *(double *)&v13, 0);
    float64x2_t v25 = (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v16.f64[0], 0);
    float64x2_t v26 = vnegq_f64(v23);
    float64x2_t v27 = (float64x2_t)vextq_s8((int8x16_t)v24, (int8x16_t)vnegq_f64(v24), 8uLL);
    float64x2_t v28 = vmlaq_n_f64(vmulq_laneq_f64(v24, v16, 1), v27, v16.f64[0]);
    float64x2_t v29 = (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v15.f64[0], 0);
    float64x2_t v30 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v24, v15, 1), v27, v15.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v26, v16, 1), (float64x2_t)vextq_s8((int8x16_t)v23, (int8x16_t)v26, 8uLL), v16.f64[0]));
    float64x2_t v31 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v23, v15, 1), (float64x2_t)vextq_s8((int8x16_t)v26, (int8x16_t)v23, 8uLL), v15.f64[0]), v28);
    do
    {
      long long v32 = &v50[v12];
      float64x2_t v34 = (float64x2_t)v50[v12];
      long long v33 = v50[v12 + 1];
      float64x2_t v35 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v16, v34, 1), v21, v34.f64[0]), v20, *(double *)&v33);
      float64x2_t v36 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v17, v34, 1), v20, v34.f64[0]), v22, *(double *)&v33);
      float64x2_t v37 = vnegq_f64(v36);
      float64x2_t v38 = (float64x2_t)vextq_s8((int8x16_t)v35, (int8x16_t)vnegq_f64(v35), 8uLL);
      *long long v32 = vaddq_f64(vaddq_f64(vmlaq_f64(vmulq_laneq_f64(v36, v15, 1), (float64x2_t)vextq_s8((int8x16_t)v37, (int8x16_t)v36, 8uLL), v29), vmlaq_f64(vmulq_laneq_f64(v35, v16, 1), v38, v25)), v31);
      *((void *)v32 + 2)  = *(_OWORD *)&vaddq_f64(vaddq_f64(vmlaq_f64(vmulq_laneq_f64(v35, v15, 1), v38, v29), vmlaq_f64(vmulq_laneq_f64(v37, v16, 1), (float64x2_t)vextq_s8((int8x16_t)v36, (int8x16_t)v37, 8uLL), v25)), v30);
      v12 += 2;
    }
    while (v12 != 16);
    uint64_t v39 = 0;
    a3[2]  = 0u;
    a3[3]  = 0u;
    a3[1]  = 0u;
    float64x2_t v40 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    v41.f64[0]  = INFINITY;
    float64x2_t v42 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
    v43.f64[0]  = -INFINITY;
    do
    {
      float64x2_t v44 = (float64x2_t)v50[v39];
      long long v45 = v50[v39 + 1];
      float64x2_t v40 = vminnmq_f64(v40, v44);
      float64x2_t v41 = vminnmq_f64((float64x2_t)*(unint64_t *)&v41.f64[0], (float64x2_t)(unint64_t)v45);
      float64x2_t v42 = vmaxnmq_f64(v42, v44);
      float64x2_t v43 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v43.f64[0], (float64x2_t)(unint64_t)v45);
      v39 += 2;
    }
    while (v39 != 16);
    *a3  = v40;
    a3[1].f64[0]  = v41.f64[0];
    float64x2_t result = vsubq_f64(v42, v40);
    a3[2]  = result;
    *(void *)&a3[3].f64[0]  = *(_OWORD *)&vsubq_f64(v43, v41);
  }
  else
  {
    *a3  = (float64x2_t)SPRect3DNull_0;
    a3[1]  = (float64x2_t)unk_228C21830;
    result.f64[0]  = 0.0;
    a3[2]  = (float64x2_t)xmmword_228C21840;
    a3[3]  = (float64x2_t)unk_228C21850;
  }
  return result;
}

double protocol witness for static Primitive3D.zero.getter in conformance SPRect3D@<D0>(_OWORD *a1@<X8>)
{
  double result = 0.0;
  a1[2]  = 0u;
  a1[3]  = 0u;
  *a1  = 0u;
  a1[1]  = 0u;
  return result;
}

void protocol witness for static Primitive3D.infinity.getter in conformance SPRect3D(void *a1@<X8>)
{
  a1[2]  = 0x7FF0000000000000;
  a1[3]  = 0;
  *a1  = 0x7FF0000000000000;
  a1[1]  = 0x7FF0000000000000;
  a1[6]  = 0x7FF0000000000000;
  a1[7]  = 0;
  a1[4]  = 0x7FF0000000000000;
  a1[5]  = 0x7FF0000000000000;
}

uint64_t protocol witness for Primitive3D.isZero.getter in conformance SPRect3D(uint64_t a1, uint64_t a2)
{
  return protocol witness for Primitive3D.isZero.getter in conformance SPRect3D(a1, a2, (uint64_t (*)(_OWORD *))SPRect3DIsZero);
}

uint64_t protocol witness for Primitive3D.isFinite.getter in conformance SPRect3D(uint64_t a1, uint64_t a2)
{
  return protocol witness for Primitive3D.isZero.getter in conformance SPRect3D(a1, a2, (uint64_t (*)(_OWORD *))SPRect3DIsFinite);
}

uint64_t protocol witness for Primitive3D.isNaN.getter in conformance SPRect3D(uint64_t a1, uint64_t a2)
{
  return protocol witness for Primitive3D.isZero.getter in conformance SPRect3D(a1, a2, (uint64_t (*)(_OWORD *))SPRect3DIsNaN);
}

uint64_t protocol witness for Primitive3D.isZero.getter in conformance SPRect3D(uint64_t a1, uint64_t a2, uint64_t (*a3)(_OWORD *))
{
  long long v4 = *(_OWORD *)(v3 + 16);
  long long v5 = *(_OWORD *)(v3 + 32);
  uint64_t v6 = *(void *)(v3 + 48);
  uint64_t v7 = *(void *)(v3 + 56);
  v9[0]  = *(_OWORD *)v3;
  v9[1]  = v4;
  uint64_t v10 = v6;
  uint64_t v11 = v7;
  v9[2]  = v5;
  return a3(v9);
}

double protocol witness for Primitive3D.applying(_:) in conformance SPRect3D@<D0>(long long *a1@<X0>, uint64_t a2@<X8>)
{
  return protocol witness for Primitive3D.applying(_:) in conformance SPRect3D(a1, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRect3DApplyAffineTransform, a2);
}

{
  return protocol witness for Primitive3D.applying(_:) in conformance SPRect3D(a1, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRect3DApplyProjectiveTransform, a2);
}

double protocol witness for Primitive3D.applying(_:) in conformance SPRect3D@<D0>(long long *a1@<X0>, void (*a2)(_OWORD *__return_ptr, _OWORD *, _OWORD *)@<X3>, uint64_t a3@<X8>)
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  long long v13 = *(_OWORD *)(v3 + 16);
  long long v14 = *(_OWORD *)(v3 + 32);
  uint64_t v15 = *(void *)(v3 + 48);
  uint64_t v16 = *(void *)(v3 + 56);
  v26[0]  = *(_OWORD *)v3;
  v26[1]  = v13;
  uint64_t v27 = v15;
  uint64_t v28 = v16;
  v26[2]  = v14;
  v25[0]  = v5;
  v25[1]  = v6;
  v25[2]  = v7;
  v25[3]  = v8;
  double v25[4] = v9;
  v25[5]  = v10;
  v25[6]  = v11;
  v25[7]  = v12;
  a2(v22, v26, v25);
  double result = *(double *)v22;
  long long v18 = v22[1];
  long long v19 = v22[2];
  uint64_t v20 = v23;
  uint64_t v21 = v24;
  *(_OWORD *)a3  = v22[0];
  *(_OWORD *)(a3 + 16)  = v18;
  *(void *)(a3 + 48)  = v20;
  *(void *)(a3 + 56)  = v21;
  *(_OWORD *)(a3 + 32)  = v19;
  return result;
}

float64_t protocol witness for Primitive3D.applying(_:) in conformance SPRect3D@<D0>(double *a1@<X0>, uint64_t a2@<X8>)
{
  long long v4 = *(_OWORD *)a1;
  long long v5 = *((_OWORD *)a1 + 1);
  long long v6 = *((_OWORD *)a1 + 2);
  double v7 = a1[6];
  double v8 = a1[7];
  long long v9 = *(_OWORD *)(v2 + 16);
  long long v10 = *(_OWORD *)(v2 + 32);
  double v11 = *(double *)(v2 + 48);
  double v12 = *(double *)(v2 + 56);
  *(_OWORD *)&v22.origin.x  = *(_OWORD *)v2;
  *(_OWORD *)&v22.origin.vector.f64[2]  = v9;
  v22.size.depth  = v11;
  v22.size.vector.f64[3]  = v12;
  *(_OWORD *)&v22.size.width  = v10;
  *(_OWORD *)&v21.position.x  = v4;
  *(_OWORD *)&v21.position.vector.f64[2]  = v5;
  v21.rotation.vector.f64[2]  = v7;
  v21.rotation.vector.f64[3]  = v8;
  *(_OWORD *)v21.rotation.vector.f64  = v6;
  SPRect3DApplyPose(&v22, &v21, v18);
  float64_t result = v18[0].f64[0];
  float64x2_t v14 = v18[1];
  float64x2_t v15 = v18[2];
  uint64_t v16 = v19;
  uint64_t v17 = v20;
  *(float64x2_t *)a2  = v18[0];
  *(float64x2_t *)(a2 + 16)  = v14;
  *(void *)(a2 + 48)  = v16;
  *(void *)(a2 + 56)  = v17;
  *(float64x2_t *)(a2 + 32)  = v15;
  return result;
}

BOOL protocol witness for Primitive3D.unapplying(_:) in conformance SPRect3D@<W0>(float64x2_t *a1@<X0>, float64_t *a2@<X8>)
{
  return protocol witness for Primitive3D.unapplying(_:) in conformance SPRect3D(a1, (void (*)(float64x2_t *__return_ptr, void *, float64x2_t *))SPRect3DUnapplyAffineTransform, a2);
}

{
  return protocol witness for Primitive3D.unapplying(_:) in conformance SPRect3D(a1, (void (*)(float64x2_t *__return_ptr, void *, float64x2_t *))SPRect3DUnapplyProjectiveTransform, a2);
}

BOOL protocol witness for Primitive3D.unapplying(_:) in conformance SPRect3D@<W0>(float64x2_t *a1@<X0>, void (*a2)(float64x2_t *__return_ptr, void *, float64x2_t *)@<X3>, float64_t *a3@<X8>)
{
  float64x2_t v5 = *a1;
  float64x2_t v6 = a1[1];
  float64x2_t v7 = a1[2];
  float64x2_t v8 = a1[3];
  float64x2_t v9 = a1[4];
  float64x2_t v10 = a1[5];
  float64x2_t v11 = a1[6];
  float64x2_t v12 = a1[7];
  uint64_t v13 = *((void *)v3 + 3);
  double v34 = v3[4];
  double v35 = v3[5];
  double v36 = *v3;
  uint64_t v37 = *((void *)v3 + 6);
  double v38 = v3[1];
  uint64_t v39 = *((void *)v3 + 7);
  uint64_t v53 = *((void *)v3 + 2);
  uint64_t v33 = v53;
  uint64_t v54 = v13;
  *(double *)long long v52 = v36;
  *(double *)&v52[1]  = v38;
  uint64_t v57 = v37;
  uint64_t v58 = v39;
  double v55 = v34;
  double v56 = v35;
  float64x2_t v44 = v5;
  float64x2_t v45 = v6;
  float64x2_t v46 = v7;
  float64x2_t v47 = v8;
  float64x2_t v48 = v9;
  float64x2_t v49 = v10;
  float64x2_t v50 = v11;
  float64x2_t v51 = v12;
  a2(&v40, v52, &v44);
  float64x2_t v14 = v40;
  unsigned long long v15 = v41;
  float64x2_t v16 = v42;
  unsigned long long v17 = v43;
  float64x2_t v45 = (float64x2_t)v41;
  float64x2_t v44 = v40;
  float64x2_t v47 = (float64x2_t)v43;
  float64x2_t v46 = v42;
  BOOL result = SPRect3DIsNull(&v44, v18, v19, v20, v21, v22, v23);
  uint64_t v25 = v33;
  float64_t v26 = v34;
  if (result)
  {
    uint64_t v27 = v13;
  }
  else
  {
    uint64_t v27 = v15 >> 64;
    uint64_t v25 = v15;
  }
  *((void *)a3 + 2)  = v25;
  *((void *)a3 + 3)  = v27;
  float64_t v28 = v36;
  if (!result) {
    float64_t v28 = v14.f64[0];
  }
  float64_t v29 = v38;
  uint64_t v30 = v39;
  if (!result)
  {
    float64_t v29 = v14.f64[1];
    float64_t v26 = v16.f64[0];
  }
  *a3  = v28;
  a3[1]  = v29;
  float64_t v31 = v35;
  if (!result) {
    float64_t v31 = v16.f64[1];
  }
  uint64_t v32 = v37;
  if (!result)
  {
    uint64_t v30 = v17 >> 64;
    uint64_t v32 = v17;
  }
  *((void *)a3 + 6)  = v32;
  *((void *)a3 + 7)  = v30;
  a3[4]  = v26;
  a3[5]  = v31;
  return result;
}

BOOL protocol witness for Primitive3D.unapplying(_:) in conformance SPRect3D@<W0>(double *a1@<X0>, double *a2@<X8>)
{
  long long v4 = *(_OWORD *)a1;
  long long v5 = *((_OWORD *)a1 + 1);
  long long v6 = *((_OWORD *)a1 + 2);
  double v7 = a1[6];
  double v8 = a1[7];
  double v9 = *(double *)(v2 + 24);
  long long v32 = *(_OWORD *)(v2 + 32);
  double v33 = *(double *)v2;
  double v34 = *(double *)(v2 + 48);
  double v35 = *(double *)(v2 + 8);
  double v36 = *(double *)(v2 + 56);
  v39.origin.double z = *(double *)(v2 + 16);
  double z = v39.origin.z;
  v39.origin.vector.f64[3]  = v9;
  v39.origin.x  = v33;
  v39.origin.y  = v35;
  v39.size.depth  = v34;
  v39.size.vector.f64[3]  = v36;
  *(_OWORD *)&v39.size.width  = v32;
  *(_OWORD *)&v38.position.x  = v4;
  *(_OWORD *)&v38.position.vector.f64[2]  = v5;
  v38.rotation.vector.f64[2]  = v7;
  v38.rotation.vector.f64[3]  = v8;
  *(_OWORD *)v38.rotation.vector.f64  = v6;
  *(void *)&double v10 = *(_OWORD *)&SPRect3DUnapplyPose(&v39, &v38, (float64x2_t *)&v37);
  x  = v37.origin.x;
  long long v13 = *(_OWORD *)&v37.origin.vector.f64[1];
  double v12 = v37.origin.vector.f64[3];
  width  = v37.size.width;
  long long v16 = *(_OWORD *)&v37.size.vector.f64[1];
  double v15 = v37.size.vector.f64[3];
  SPRect3D v39 = v37;
  BOOL result = SPRect3DIsNull((float64x2_t *)&v39, v10, v17, v18, v19, v20, v21);
  double v23 = z;
  double v24 = *(double *)&v32;
  if (result)
  {
    double v25 = v9;
  }
  else
  {
    double v23 = *((double *)&v13 + 1);
    double v25 = v12;
  }
  a2[2]  = v23;
  a2[3]  = v25;
  double v26 = v33;
  if (!result) {
    double v26 = x;
  }
  double v27 = v35;
  double v28 = v36;
  if (!result)
  {
    double v27 = *(double *)&v13;
    double v24 = width;
  }
  *a2  = v26;
  a2[1]  = v27;
  uint64_t v29 = *((void *)&v32 + 1);
  if (!result) {
    uint64_t v29 = v16;
  }
  double v30 = v34;
  if (!result)
  {
    double v30 = *((double *)&v16 + 1);
    double v28 = v15;
  }
  a2[6]  = v30;
  a2[7]  = v28;
  a2[4]  = v24;
  *((void *)a2 + 5)  = v29;
  return result;
}

float64_t SPRect3D.applying(_:)@<D0>(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  float64x2_t v4 = *(float64x2_t *)a1;
  float64x2_t v5 = *(float64x2_t *)(a1 + 16);
  float64x2_t v6 = *(float64x2_t *)(a1 + 32);
  uint64_t v7 = *(void *)(a1 + 48);
  uint64_t v8 = *(void *)(a1 + 56);
  uint64_t v9 = *(void *)(a1 + 64);
  float64x2_t v10 = *(float64x2_t *)(v2 + 16);
  long long v11 = *(_OWORD *)(v2 + 32);
  uint64_t v12 = *(void *)(v2 + 48);
  uint64_t v13 = *(void *)(v2 + 56);
  v26[0]  = *(_OWORD *)v2;
  v26[1]  = v10;
  uint64_t v27 = v12;
  uint64_t v28 = v13;
  v26[2]  = v11;
  v22[0]  = v4;
  v22[1]  = v5;
  uint64_t v23 = v7;
  uint64_t v24 = v8;
  v22[2]  = v6;
  uint64_t v25 = v9;
  SPRect3DApplyScaledPose((uint64_t)v26, v22, v19, v10);
  float64_t result = v19[0].f64[0];
  float64x2_t v15 = v19[1];
  float64x2_t v16 = v19[2];
  uint64_t v17 = v20;
  uint64_t v18 = v21;
  *(float64x2_t *)a2  = v19[0];
  *(float64x2_t *)(a2 + 16)  = v15;
  *(void *)(a2 + 48)  = v17;
  *(void *)(a2 + 56)  = v18;
  *(float64x2_t *)(a2 + 32)  = v16;
  return result;
}

float64x2_t SPRect3DApplyScaledPose@<Q0>(uint64_t a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>, float64x2_t a4@<Q5>)
{
  uint64_t v4 = 0;
  uint64_t v52 = *MEMORY[0x263EF8340];
  long long v50 = 0u;
  *(_OWORD *)float64x2_t v51 = 0u;
  float64x2_t v48 = 0u;
  long long v49 = 0u;
  long long v46 = 0u;
  long long v47 = 0u;
  long long v44 = 0u;
  long long v45 = 0u;
  long long v42 = 0u;
  long long v43 = 0u;
  float64x2_t v40 = 0u;
  long long v41 = 0u;
  long long v38 = 0u;
  long long v39 = 0u;
  memset(v37, 0, sizeof(v37));
  float64x2_t v5 = *(float64x2_t *)(a1 + 32);
  double v6 = *(double *)(a1 + 48);
  do
  {
    uint64_t v7 = &v37[v4];
    long long v8 = *(_OWORD *)(a1 + 16);
    *uint64_t v7 = *(_OWORD *)a1;
    v7[1]  = v8;
    v4 += 2;
  }
  while (v4 != 16);
  uint64_t v9 = 0;
  *((double *)&v38 + 1)  = v5.f64[1] + *((double *)&v38 + 1);
  float64x2_t v40 = vaddq_f64(v5, v40);
  *(double *)&long long v42 = v5.f64[0] + *(double *)&v42;
  *(double *)&long long v45 = v6 + *(double *)&v45;
  *((double *)&v46 + 1)  = v5.f64[1] + *((double *)&v46 + 1);
  *(double *)&long long v47 = v6 + *(double *)&v47;
  float64x2_t v48 = vaddq_f64(v5, v48);
  *(double *)&long long v49 = v6 + *(double *)&v49;
  *(double *)&long long v50 = v5.f64[0] + *(double *)&v50;
  v51[0]  = v6 + v51[0];
  float64x2_t v10 = a2[2];
  float64x2_t v11 = a2[3];
  float64x2_t v12 = vmulq_f64(v11, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v13 = (int8x16_t)vnegq_f64(v10);
  float64x2_t v15 = *a2;
  float64x2_t v14 = a2[1];
  a4.f64[0]  = a2[4].f64[0];
  float64x2_t v16 = (float64x2_t)vextq_s8((int8x16_t)v12, (int8x16_t)vnegq_f64(v12), 8uLL);
  float64x2_t v17 = (float64x2_t)vextq_s8(v13, (int8x16_t)v10, 8uLL);
  float64x2_t v18 = (float64x2_t)vextq_s8((int8x16_t)v10, v13, 8uLL);
  float64x2_t v19 = (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v10.f64[0], 0);
  float64x2_t v20 = (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v11.f64[0], 0);
  v21.f64[0]  = a4.f64[0];
  v21.f64[1]  = a4.f64[0];
  do
  {
    double v22 = &v37[v9];
    *(void *)&double v23 = *(_OWORD *)&vmulq_f64((float64x2_t)v37[v9 + 1], a4);
    float64x2_t v24 = vmulq_f64((float64x2_t)v37[v9], v21);
    float64x2_t v25 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v10, v24, 1), v17, v24.f64[0]), v16, v23);
    float64x2_t v26 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v12, v24, 1), v16, v24.f64[0]), v18, v23);
    float64x2_t v27 = vnegq_f64(v26);
    float64x2_t v28 = (float64x2_t)vextq_s8((int8x16_t)v25, (int8x16_t)vnegq_f64(v25), 8uLL);
    *double v22 = vaddq_f64(v15, vaddq_f64(vmlaq_f64(vmulq_laneq_f64(v26, v11, 1), (float64x2_t)vextq_s8((int8x16_t)v27, (int8x16_t)v26, 8uLL), v20), vmlaq_f64(vmulq_laneq_f64(v25, v10, 1), v28, v19)));
    *((void *)v22 + 2)  = *(_OWORD *)&vaddq_f64(v14, vaddq_f64(vmlaq_f64(vmulq_laneq_f64(v25, v11, 1), v28, v20), vmlaq_f64(vmulq_laneq_f64(v27, v10, 1), (float64x2_t)vextq_s8((int8x16_t)v26, (int8x16_t)v27, 8uLL), v19)));
    v9 += 2;
  }
  while (v9 != 16);
  uint64_t v29 = 0;
  a3[2]  = 0u;
  a3[3]  = 0u;
  a3[1]  = 0u;
  float64x2_t v30 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v31.f64[0]  = INFINITY;
  float64x2_t v32 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
  v33.f64[0]  = -INFINITY;
  do
  {
    float64x2_t v34 = (float64x2_t)v37[v29];
    long long v35 = v37[v29 + 1];
    float64x2_t v30 = vminnmq_f64(v30, v34);
    float64x2_t v31 = vminnmq_f64((float64x2_t)*(unint64_t *)&v31.f64[0], (float64x2_t)(unint64_t)v35);
    float64x2_t v32 = vmaxnmq_f64(v32, v34);
    float64x2_t v33 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v33.f64[0], (float64x2_t)(unint64_t)v35);
    v29 += 2;
  }
  while (v29 != 16);
  *a3  = v30;
  a3[1].f64[0]  = v31.f64[0];
  float64x2_t result = vsubq_f64(v32, v30);
  a3[2]  = result;
  *(void *)&a3[3].f64[0]  = *(_OWORD *)&vsubq_f64(v33, v31);
  return result;
}

BOOL SPRect3D.unapplying(_:)@<W0>(uint64_t a1@<X0>, float64_t *a2@<X8>)
{
  float64x2_t v4 = *(float64x2_t *)a1;
  unsigned long long v5 = *(_OWORD *)(a1 + 16);
  long long v6 = *(_OWORD *)(a1 + 32);
  uint64_t v7 = *(void *)(a1 + 48);
  uint64_t v8 = *(void *)(a1 + 56);
  uint64_t v9 = *(void *)(a1 + 64);
  uint64_t v10 = *((void *)v2 + 3);
  uint64_t v32 = *((void *)v2 + 4);
  uint64_t v33 = *((void *)v2 + 5);
  double v34 = *v2;
  uint64_t v35 = *((void *)v2 + 6);
  double v36 = v2[1];
  uint64_t v37 = *((void *)v2 + 7);
  uint64_t v50 = *((void *)v2 + 2);
  uint64_t v31 = v50;
  uint64_t v51 = v10;
  *(double *)long long v49 = v34;
  *(double *)&v49[1]  = v36;
  uint64_t v54 = v35;
  uint64_t v55 = v37;
  uint64_t v52 = v32;
  uint64_t v53 = v33;
  float64x2_t v43 = v4;
  unsigned long long v44 = v5;
  uint64_t v46 = v7;
  uint64_t v47 = v8;
  long long v45 = v6;
  uint64_t v48 = v9;
  *(void *)&double v11 = *(_OWORD *)&SPRect3DUnapplyScaledPose((uint64_t)v49, (uint64_t)&v43, &v38);
  float64x2_t v12 = v38;
  unsigned long long v13 = v39;
  long long v14 = v40;
  uint64_t v15 = v41;
  uint64_t v16 = v42;
  unsigned long long v44 = v39;
  float64x2_t v43 = v38;
  uint64_t v46 = v41;
  uint64_t v47 = v42;
  long long v45 = v40;
  BOOL result = SPRect3DIsNull(&v43, v11, v17, v18, v19, v20, v21);
  uint64_t v23 = v31;
  uint64_t v24 = v32;
  if (result)
  {
    uint64_t v25 = v10;
  }
  else
  {
    uint64_t v25 = v13 >> 64;
    uint64_t v23 = v13;
  }
  *((void *)a2 + 2)  = v23;
  *((void *)a2 + 3)  = v25;
  float64_t v26 = v34;
  if (!result) {
    float64_t v26 = v12.f64[0];
  }
  float64_t v27 = v36;
  uint64_t v28 = v37;
  if (!result)
  {
    float64_t v27 = v12.f64[1];
    uint64_t v24 = v14;
  }
  *a2  = v26;
  a2[1]  = v27;
  uint64_t v29 = v33;
  if (!result) {
    uint64_t v29 = *((void *)&v14 + 1);
  }
  uint64_t v30 = v35;
  if (!result)
  {
    uint64_t v30 = v15;
    uint64_t v28 = v16;
  }
  *((void *)a2 + 6)  = v30;
  *((void *)a2 + 7)  = v28;
  *((void *)a2 + 4)  = v24;
  *((void *)a2 + 5)  = v29;
  return result;
}

float64x2_t SPRect3DUnapplyScaledPose@<Q0>(uint64_t a1@<X0>, uint64_t a2@<X1>, float64x2_t *a3@<X8>)
{
  uint64_t v66 = *MEMORY[0x263EF8340];
  float64x2_t v49 = vmulq_f64(*(float64x2_t *)(a2 + 48), *(float64x2_t *)(a2 + 48));
  float64x2_t v50 = *(float64x2_t *)(a2 + 32);
  float64x2_t v47 = vmulq_f64(v50, v50);
  float64x2_t v48 = *(float64x2_t *)(a2 + 48);
  result.f64[0]  = atan2(sqrt(v49.f64[0] + vaddvq_f64(v47)), *(long double *)(a2 + 56));
  if (fabs(result.f64[0] + result.f64[0]) <= 0.0000000149011612)
  {
    uint64_t v7 = 0;
    v64  = 0u;
    *(_OWORD *)v65  = 0u;
    float64x2_t v62 = 0u;
    long long v63 = 0u;
    long long v60 = 0u;
    long long v61 = 0u;
    long long v58 = 0u;
    long long v59 = 0u;
    long long v56 = 0u;
    long long v57 = 0u;
    float64x2_t v54 = 0u;
    long long v55 = 0u;
    long long v52 = 0u;
    long long v53 = 0u;
    memset(v51, 0, sizeof(v51));
    float64x2_t v8 = *(float64x2_t *)(a1 + 32);
    double v9 = *(double *)(a1 + 48);
    do
    {
      uint64_t v10 = &v51[v7];
      long long v11 = *(_OWORD *)(a1 + 16);
      *uint64_t v10 = *(_OWORD *)a1;
      v10[1]  = v11;
      v7 += 2;
    }
    while (v7 != 16);
    uint64_t v12 = 0;
    *((double *)&v52 + 1)  = v8.f64[1] + *((double *)&v52 + 1);
    float64x2_t v54 = vaddq_f64(v8, v54);
    *(double *)&long long v56 = v8.f64[0] + *(double *)&v56;
    *(double *)&long long v59 = v9 + *(double *)&v59;
    *((double *)&v60 + 1)  = v8.f64[1] + *((double *)&v60 + 1);
    *(double *)&long long v61 = v9 + *(double *)&v61;
    float64x2_t v62 = vaddq_f64(v8, v62);
    *(double *)&long long v63 = v9 + *(double *)&v63;
    *(double *)&v64  = v8.f64[0] + *(double *)&v64;
    v65[0]  = v9 + v65[0];
    long long v13 = *(_OWORD *)(a2 + 16);
    v8.f64[0]  = *(float64_t *)(a2 + 64);
    double v14 = 1.0 / vaddvq_f64(vaddq_f64(v47, v49));
    float64x2_t v15 = vmulq_n_f64(vmulq_f64(v48, (float64x2_t)xmmword_228C1FC40), v14);
    float64x2_t v16 = vmulq_n_f64(vnegq_f64(v50), v14);
    float64x2_t v17 = vmulq_f64(v15, (float64x2_t)xmmword_228C1FC40);
    int8x16_t v18 = (int8x16_t)vnegq_f64(v16);
    float64x2_t v19 = vnegq_f64(*(float64x2_t *)a2);
    float64x2_t v20 = (float64x2_t)vextq_s8((int8x16_t)v17, (int8x16_t)vnegq_f64(v17), 8uLL);
    float64x2_t v21 = (float64x2_t)vextq_s8(v18, (int8x16_t)v16, 8uLL);
    float64x2_t v22 = (float64x2_t)vextq_s8((int8x16_t)v16, v18, 8uLL);
    float64x2_t v23 = vmlsq_lane_f64(vmlsq_lane_f64(vmulq_laneq_f64(v17, v19, 1), v20, *(double *)a2, 0), v22, *(double *)&v13, 0);
    float64x2_t v24 = vmlsq_lane_f64(vmlsq_lane_f64(vmulq_laneq_f64(v16, v19, 1), v21, *(double *)a2, 0), v20, *(double *)&v13, 0);
    float64x2_t v25 = (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v16.f64[0], 0);
    float64x2_t v26 = vnegq_f64(v23);
    float64x2_t v27 = (float64x2_t)vextq_s8((int8x16_t)v24, (int8x16_t)vnegq_f64(v24), 8uLL);
    float64x2_t v28 = (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v15.f64[0], 0);
    float64x2_t v29 = vmlaq_n_f64(vmulq_laneq_f64(v24, v16, 1), v27, v16.f64[0]);
    float64x2_t v30 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v24, v15, 1), v27, v15.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v26, v16, 1), (float64x2_t)vextq_s8((int8x16_t)v23, (int8x16_t)v26, 8uLL), v16.f64[0]));
    float64x2_t v31 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v23, v15, 1), (float64x2_t)vextq_s8((int8x16_t)v26, (int8x16_t)v23, 8uLL), v15.f64[0]), v29);
    v32.f64[0]  = v8.f64[0];
    v32.f64[1]  = v8.f64[0];
    do
    {
      uint64_t v33 = &v51[v12];
      float64x2_t v35 = (float64x2_t)v51[v12];
      long long v34 = v51[v12 + 1];
      float64x2_t v36 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v17, v35, 1), v20, v35.f64[0]), v22, *(double *)&v34);
      float64x2_t v37 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v16, v35, 1), v21, v35.f64[0]), v20, *(double *)&v34);
      float64x2_t v38 = vnegq_f64(v36);
      float64x2_t v39 = (float64x2_t)vextq_s8((int8x16_t)v37, (int8x16_t)vnegq_f64(v37), 8uLL);
      *uint64_t v33 = vdivq_f64(vaddq_f64(vaddq_f64(vmlaq_f64(vmulq_laneq_f64(v36, v15, 1), (float64x2_t)vextq_s8((int8x16_t)v38, (int8x16_t)v36, 8uLL), v28), vmlaq_f64(vmulq_laneq_f64(v37, v16, 1), v39, v25)), v31), v32);
      *((void *)v33 + 2)  = *(_OWORD *)&vdivq_f64(vaddq_f64(vaddq_f64(vmlaq_f64(vmulq_laneq_f64(v37, v15, 1), v39, v28), vmlaq_f64(vmulq_laneq_f64(v38, v16, 1), (float64x2_t)vextq_s8((int8x16_t)v36, (int8x16_t)v38, 8uLL), v25)), v30), v8);
      v12 += 2;
    }
    while (v12 != 16);
    uint64_t v40 = 0;
    a3[2]  = 0u;
    a3[3]  = 0u;
    a3[1]  = 0u;
    float64x2_t v41 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    v42.f64[0]  = INFINITY;
    float64x2_t v43 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
    v44.f64[0]  = -INFINITY;
    do
    {
      float64x2_t v45 = (float64x2_t)v51[v40];
      long long v46 = v51[v40 + 1];
      float64x2_t v41 = vminnmq_f64(v41, v45);
      float64x2_t v42 = vminnmq_f64((float64x2_t)*(unint64_t *)&v42.f64[0], (float64x2_t)(unint64_t)v46);
      float64x2_t v43 = vmaxnmq_f64(v43, v45);
      float64x2_t v44 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v44.f64[0], (float64x2_t)(unint64_t)v46);
      v40 += 2;
    }
    while (v40 != 16);
    *a3  = v41;
    a3[1].f64[0]  = v42.f64[0];
    float64x2_t result = vsubq_f64(v43, v41);
    a3[2]  = result;
    *(void *)&a3[3].f64[0]  = *(_OWORD *)&vsubq_f64(v44, v42);
  }
  else
  {
    *a3  = (float64x2_t)SPRect3DNull;
    a3[1]  = (float64x2_t)unk_228C20FF0;
    result.f64[0]  = 0.0;
    a3[2]  = (float64x2_t)xmmword_228C21000;
    a3[3]  = (float64x2_t)unk_228C21010;
  }
  return result;
}

void __swiftcall SPRect3D.scaledBy(x:y:z:)(SPRect3D *__return_ptr retstr, Swift::Double x, Swift::Double y, Swift::Double z)
{
  long long v6 = *(_OWORD *)(v4 + 16);
  long long v7 = *(_OWORD *)(v4 + 32);
  double v8 = *(double *)(v4 + 48);
  double v9 = *(double *)(v4 + 56);
  *(_OWORD *)&v17.origin.x  = *(_OWORD *)v4;
  *(_OWORD *)&v17.origin.vector.f64[2]  = v6;
  v17.size.depth  = v8;
  v17.size.vector.f64[3]  = v9;
  *(_OWORD *)&v17.size.width  = v7;
  SPRect3DScaleBy(&v17, *(float64x2_t *)&x, y, *(float64x2_t *)&z, (uint64_t)v14);
  long long v10 = v14[1];
  long long v11 = v14[2];
  double v12 = v15;
  double v13 = v16;
  *(_OWORD *)&retstr->origin.x  = v14[0];
  *(_OWORD *)&retstr->origin.vector.f64[2]  = v10;
  retstr->size.depth  = v12;
  retstr->size.vector.f64[3]  = v13;
  *(_OWORD *)&retstr->size.width  = v11;
}

float64x2_t SPRect3DScaleBy@<Q0>(SPRect3D *a1@<X0>, float64x2_t a2@<Q0>, float64_t a3@<D1>, float64x2_t a4@<Q2>, uint64_t a5@<X8>)
{
  a2.f64[1]  = a3;
  float64x2_t v5 = vmulq_f64(a2, *(float64x2_t *)&a1->origin.x);
  uint64_t v6 = *(_OWORD *)&vmulq_f64(a4, *(float64x2_t *)&a1->origin.vector.f64[2]);
  float64x2_t result = vmulq_f64(a2, *(float64x2_t *)&a1->size.width);
  uint64_t v8 = *(_OWORD *)&vmulq_f64(a4, *(float64x2_t *)&a1->size.vector.f64[2]);
  *(float64x2_t *)a5  = v5;
  *(void *)(a5 + 16)  = v6;
  *(float64x2_t *)(a5 + 32)  = result;
  *(void *)(a5 + 48)  = v8;
  return result;
}

double protocol witness for Scalable3D.scaledBy(x:y:z:) in conformance SPRect3D@<D0>(uint64_t a1@<X8>, float64x2_t a2@<Q0>, float64_t a3@<D1>, float64x2_t a4@<Q2>)
{
  long long v6 = *(_OWORD *)(v4 + 16);
  long long v7 = *(_OWORD *)(v4 + 32);
  double v8 = *(double *)(v4 + 48);
  double v9 = *(double *)(v4 + 56);
  *(_OWORD *)&v18.origin.x  = *(_OWORD *)v4;
  *(_OWORD *)&v18.origin.vector.f64[2]  = v6;
  v18.size.depth  = v8;
  v18.size.vector.f64[3]  = v9;
  *(_OWORD *)&v18.size.width  = v7;
  SPRect3DScaleBy(&v18, a2, a3, a4, (uint64_t)v15);
  double result = *(double *)v15;
  long long v11 = v15[1];
  long long v12 = v15[2];
  uint64_t v13 = v16;
  uint64_t v14 = v17;
  *(_OWORD *)a1  = v15[0];
  *(_OWORD *)(a1 + 16)  = v11;
  *(void *)(a1 + 48)  = v13;
  *(void *)(a1 + 56)  = v14;
  *(_OWORD *)(a1 + 32)  = v12;
  return result;
}

double protocol witness for Scalable3D.scaled(by:) in conformance SPRect3D@<D0>(uint64_t a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>)
{
  return protocol witness for Translatable3D.translated(by:) in conformance SPRect3D((void (*)(_OWORD *__return_ptr, _OWORD *, void *))SPRect3DScaleBySize, a1, a2, a3, a4);
}

double protocol witness for Translatable3D.translated(by:) in conformance SPRect3D@<D0>(void (*a1)(_OWORD *__return_ptr, _OWORD *, void *)@<X2>, uint64_t a2@<X8>, double a3@<D0>, double a4@<D1>, double a5@<D2>)
{
  long long v7 = *(_OWORD *)(v5 + 16);
  long long v8 = *(_OWORD *)(v5 + 32);
  uint64_t v9 = *(void *)(v5 + 48);
  uint64_t v10 = *(void *)(v5 + 56);
  v20[0]  = *(_OWORD *)v5;
  v20[1]  = v7;
  uint64_t v21 = v9;
  uint64_t v22 = v10;
  v20[2]  = v8;
  *(double *)float64x2_t v19 = a3;
  *(double *)&v19[1]  = a4;
  *(double *)&v19[2]  = a5;
  a1(v16, v20, v19);
  double result = *(double *)v16;
  long long v12 = v16[1];
  long long v13 = v16[2];
  uint64_t v14 = v17;
  uint64_t v15 = v18;
  *(_OWORD *)a2  = v16[0];
  *(_OWORD *)(a2 + 16)  = v12;
  *(void *)(a2 + 48)  = v14;
  *(void *)(a2 + 56)  = v15;
  *(_OWORD *)(a2 + 32)  = v13;
  return result;
}

double protocol witness for Scalable3D.uniformlyScaled(by:) in conformance SPRect3D@<D0>(uint64_t a1@<X8>, float64x2_t a2@<Q0>)
{
  long long v4 = *(_OWORD *)(v2 + 16);
  long long v5 = *(_OWORD *)(v2 + 32);
  double v6 = *(double *)(v2 + 48);
  double v7 = *(double *)(v2 + 56);
  *(_OWORD *)&v16.origin.x  = *(_OWORD *)v2;
  *(_OWORD *)&v16.origin.vector.f64[2]  = v4;
  v16.size.depth  = v6;
  v16.size.vector.f64[3]  = v7;
  *(_OWORD *)&v16.size.width  = v5;
  SPRect3DScaleUniform(&v16, a2, (uint64_t)v13);
  double result = *(double *)v13;
  long long v9 = v13[1];
  long long v10 = v13[2];
  uint64_t v11 = v14;
  uint64_t v12 = v15;
  *(_OWORD *)a1  = v13[0];
  *(_OWORD *)(a1 + 16)  = v9;
  *(void *)(a1 + 48)  = v11;
  *(void *)(a1 + 56)  = v12;
  *(_OWORD *)(a1 + 32)  = v10;
  return result;
}

float64_t protocol witness for Rotatable3D.rotated(by:) in conformance SPRect3D@<D0>(uint64_t a1@<X8>, SPRotation3D a2@<Q1:Q0>, float64x2_t a3@<Q5>, float64x2_t a4@<Q7>)
{
  long long v6 = *(_OWORD *)(v4 + 16);
  long long v7 = *(_OWORD *)(v4 + 32);
  double v8 = *(double *)(v4 + 48);
  double v9 = *(double *)(v4 + 56);
  *(_OWORD *)&v19.origin.x  = *(_OWORD *)v4;
  *(_OWORD *)&v19.origin.vector.f64[2]  = v6;
  v19.size.depth  = v8;
  v19.size.vector.f64[3]  = v9;
  *(_OWORD *)&v19.size.width  = v7;
  v18[0]  = *(float64x2_t *)a2.vector.f64;
  v18[1]  = *(float64x2_t *)&a2.quaternion.vector.f64[2];
  SPRect3DRotate(&v19, a2, v18, v15, a3, a4);
  float64_t result = v15[0].f64[0];
  float64x2_t v11 = v15[1];
  float64x2_t v12 = v15[2];
  uint64_t v13 = v16;
  uint64_t v14 = v17;
  *(float64x2_t *)a1  = v15[0];
  *(float64x2_t *)(a1 + 16)  = v11;
  *(void *)(a1 + 48)  = v13;
  *(void *)(a1 + 56)  = v14;
  *(float64x2_t *)(a1 + 32)  = v12;
  return result;
}

float64_t protocol witness for Rotatable3D.rotated(by:) in conformance SPRect3D@<D0>(uint64_t a1@<X8>, simd_quatd a2@<Q1:Q0>, float64x2_t a3@<Q5>, float64x2_t a4@<Q7>)
{
  long long v6 = *(_OWORD *)(v4 + 16);
  long long v7 = *(_OWORD *)(v4 + 32);
  double v8 = *(double *)(v4 + 48);
  double v9 = *(double *)(v4 + 56);
  *(_OWORD *)&v19.origin.x  = *(_OWORD *)v4;
  *(_OWORD *)&v19.origin.vector.f64[2]  = v6;
  v19.size.depth  = v8;
  v19.size.vector.f64[3]  = v9;
  *(_OWORD *)&v19.size.width  = v7;
  v18.quaternion  = a2;
  SPRect3DRotateByQuaternion(&v19, a2, &v18, v15, a3, a4);
  float64_t result = v15[0].f64[0];
  float64x2_t v11 = v15[1];
  float64x2_t v12 = v15[2];
  uint64_t v13 = v16;
  uint64_t v14 = v17;
  *(float64x2_t *)a1  = v15[0];
  *(float64x2_t *)(a1 + 16)  = v11;
  *(void *)(a1 + 48)  = v13;
  *(void *)(a1 + 56)  = v14;
  *(float64x2_t *)(a1 + 32)  = v12;
  return result;
}

__n128 SPRect3D.sheared(_:)@<Q0>(uint64_t a1@<X0>, __n128 *a2@<X8>, float64x2_t a3@<Q0>, float64x2_t a4@<Q1>)
{
  double v7 = v4[2];
  double v6 = v4[3];
  double v8 = v4[1];
  double v10 = v4[6];
  double v9 = v4[7];
  double v12 = v4[4];
  double v11 = v4[5];
  unint64_t v14 = *(void *)a1;
  unint64_t v13 = *(void *)(a1 + 8);
  int v15 = *(unsigned __int8 *)(a1 + 16);
  if (v15)
  {
    *(void *)&a3.f64[0]  = v14;
    *(void *)&a4.f64[0]  = v13;
    v31.origin.x  = *v4;
    v31.origin.y  = v8;
    v31.origin.double z = v7;
    v31.origin.vector.f64[3]  = v6;
    v31.size.width  = v12;
    v31.size.height  = v11;
    v31.size.depth  = v10;
    v31.size.vector.f64[3]  = v9;
    if (v15 == 1) {
      SPAxis v16 = SPAxisY;
    }
    else {
      SPAxis v16 = SPAxisZ;
    }
  }
  else
  {
    *(void *)&a3.f64[0]  = v14;
    *(void *)&a4.f64[0]  = v13;
    v31.origin.x  = *v4;
    v31.origin.y  = v8;
    v31.origin.double z = v7;
    v31.origin.vector.f64[3]  = v6;
    v31.size.width  = v12;
    v31.size.height  = v11;
    v31.size.depth  = v10;
    v31.size.vector.f64[3]  = v9;
    SPAxis v16 = SPAxisX;
  }
  SPRect3DShear(&v31, v16, a3, a4, (float64x2_t *)&v24);
  unint64_t v17 = v30;
  unint64_t v18 = v29;
  unint64_t v19 = v26;
  unint64_t v20 = v27;
  unint64_t v21 = v25;
  __n128 result = v24;
  unint64_t v23 = v28;
  *a2  = v24;
  a2[1].n128_u64[0]  = v21;
  a2[1].n128_u64[1]  = v19;
  a2[2].n128_u64[0]  = v20;
  a2[2].n128_u64[1]  = v23;
  a2[3].n128_u64[0]  = v18;
  a2[3].n128_u64[1]  = v17;
  return result;
}

float64x2_t SPRect3DShear@<Q0>(SPRect3D *a1@<X0>, SPAxis a2@<W1>, float64x2_t a3@<Q0>, float64x2_t a4@<Q1>, float64x2_t *a5@<X8>)
{
  uint64_t v68 = *MEMORY[0x263EF8340];
  switch(a2)
  {
    case SPAxisZ:
      a3.f64[1]  = a4.f64[0];
      float64x2_t v10 = (float64x2_t)xmmword_228C1F7D0;
      float64x2_t v12 = 0uLL;
      __asm { FMOV            V4.2D, #1.0 }
      a4  = 0uLL;
      float64x2_t v11 = (float64x2_t)xmmword_228C1F7A0;
      break;
    case SPAxisY:
      __asm { FMOV            V4.2D, #1.0 }
      v11.f64[1]  = _Q4.f64[1];
      v11.f64[0]  = a3.f64[0];
      float64x2_t v10 = (float64x2_t)xmmword_228C1F7D0;
      a3  = 0uLL;
      float64x2_t v12 = a4;
      a4  = 0uLL;
      break;
    case SPAxisX:
      __asm { FMOV            V4.2D, #1.0 }
      v10.f64[0]  = _Q4.f64[0];
      v10.f64[1]  = a3.f64[0];
      float64x2_t v11 = (float64x2_t)xmmword_228C1F7A0;
      a3  = 0uLL;
      float64x2_t v12 = 0uLL;
      break;
    default:
      float64x2_t v29 = (float64x2_t)xmmword_228C1F7D0;
      float64x2_t v19 = (float64x2_t)xmmword_228C1F7A0;
      float64x2_t v28 = 0uLL;
      float64x2_t v30 = 0uLL;
      float64x2_t v33 = 0uLL;
      float64x2_t v18 = (float64x2_t)xmmword_228C1F7D0;
LABEL_14:
      float64x2_t v31 = 0uLL;
      float64x2_t v32 = 0uLL;
      goto LABEL_15;
  }
  float64x2_t v13 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v14 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v17 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  float64x2_t v18 = (float64x2_t)xmmword_228C1F7D0;
  float64x2_t v19 = (float64x2_t)xmmword_228C1F7A0;
  __asm { FMOV            V7.2D, #1.0 }
  int64x2_t v21 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], (float64x2_t)xmmword_228C1F7D0)), (int8x16_t)vceqzq_f64(v16));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v21, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqzq_f64(v14), (int8x16_t)vceqzq_f64(v13)), (int8x16_t)vceqq_f64(v17, _Q7)), (int8x16_t)v21)).u64[0] & 0x8000000000000000) != 0)
  {
    int8x16_t v22 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v12), (int8x16_t)vceqq_f64(v13, a4)), (int8x16_t)vceqq_f64(v17, _Q4));
    int64x2_t v23 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, v11), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v10)), (int8x16_t)vceqq_f64(v16, a3));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v23, 1), vandq_s8(v22, (int8x16_t)v23)).u64[0] & 0x8000000000000000) != 0)
    {
      float64x2_t v28 = 0uLL;
      float64x2_t v29 = (float64x2_t)xmmword_228C1F7D0;
      float64x2_t v30 = 0uLL;
      float64x2_t v33 = 0uLL;
      goto LABEL_14;
    }
  }
  uint64_t v24 = 0;
  v51[0]  = v10;
  v51[1]  = a4;
  v51[2]  = v11;
  v51[3]  = v12;
  v51[4]  = a3;
  v51[5]  = _Q4;
  float64x2_t v52 = 0u;
  float64x2_t v53 = 0u;
  float64x2_t v54 = 0u;
  float64x2_t v55 = 0u;
  float64x2_t v56 = 0u;
  float64x2_t v57 = 0u;
  do
  {
    float64x2_t v26 = (float64x2_t)v51[v24];
    float64x2_t v25 = (float64x2_t)v51[v24 + 1];
    unint64_t v27 = (float64x2_t *)((char *)&v52 + v24 * 16);
    *unint64_t v27 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)xmmword_228C1F7D0, v26.f64[0]), (float64x2_t)xmmword_228C1F7A0, v26, 1), (float64x2_t)0, v25.f64[0]);
    v27[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v26, (float64x2_t)0), (float64x2_t)0, v26, 1), _Q7, v25);
    v24 += 2;
  }
  while (v24 != 6);
  float64x2_t v29 = v52;
  float64x2_t v28 = v53;
  float64x2_t v19 = v54;
  float64x2_t v30 = v55;
  float64x2_t v31 = 0uLL;
  float64x2_t v32 = 0uLL;
  float64x2_t v33 = v56;
  float64x2_t v18 = v57;
LABEL_15:
  uint64_t v34 = 0;
  float64x2_t v35 = *(float64x2_t *)&a1->size.width;
  depth  = a1->size.depth;
  long long v66 = 0u;
  *(_OWORD *)double v67 = 0u;
  v64  = 0u;
  v65  = 0u;
  long long v62 = 0u;
  long long v63 = 0u;
  long long v60 = 0u;
  long long v61 = 0u;
  long long v58 = 0u;
  long long v59 = 0u;
  float64x2_t v56 = 0u;
  float64x2_t v57 = 0u;
  float64x2_t v54 = 0u;
  float64x2_t v55 = 0u;
  float64x2_t v52 = 0u;
  float64x2_t v53 = 0u;
  do
  {
    float64x2_t v37 = (_OWORD *)&v52.f64[v34];
    long long v38 = *(_OWORD *)&a1->origin.vector.f64[2];
    *float64x2_t v37 = *(_OWORD *)&a1->origin.x;
    v37[1]  = v38;
    v34 += 4;
  }
  while (v34 != 32);
  uint64_t v39 = 0;
  v54.f64[1]  = v35.f64[1] + v54.f64[1];
  float64x2_t v56 = vaddq_f64(v35, v56);
  *(double *)&long long v58 = v35.f64[0] + *(double *)&v58;
  *(double *)&long long v61 = depth + *(double *)&v61;
  *((double *)&v62 + 1)  = v35.f64[1] + *((double *)&v62 + 1);
  *(double *)&long long v63 = depth + *(double *)&v63;
  v64  = vaddq_f64(v35, v64);
  *(double *)&v65  = depth + *(double *)&v65;
  *(double *)&long long v66 = v35.f64[0] + *(double *)&v66;
  v67[0]  = depth + v67[0];
  *(void *)&v29.f64[1]  = vextq_s8((int8x16_t)v29, (int8x16_t)v29, 8uLL).u64[0];
  *(void *)&v19.f64[1]  = vextq_s8((int8x16_t)v19, (int8x16_t)v19, 8uLL).u64[0];
  *(void *)&v33.f64[1]  = vextq_s8((int8x16_t)v33, (int8x16_t)v33, 8uLL).u64[0];
  *(void *)&v31.f64[1]  = vextq_s8((int8x16_t)v31, (int8x16_t)v31, 8uLL).u64[0];
  do
  {
    uint64_t v40 = (char *)&v52 + v39;
    float64x2_t v42 = *(float64x2_t *)((char *)&v52 + v39);
    float64x2_t v41 = *(float64x2_t *)((char *)&v52 + v39 + 16);
    *(float64x2_t *)uint64_t v40 = vaddq_f64(v31, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v29, v42.f64[0]), v19, v42, 1), v33, v41.f64[0]));
    *((void *)v40 + 2)  = *(_OWORD *)&vaddq_f64(v32, vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v28, v42), v30, v42, 1), v41, v18));
    v39 += 32;
  }
  while (v39 != 256);
  uint64_t v43 = 0;
  a5[2]  = 0u;
  a5[3]  = 0u;
  a5[1]  = 0u;
  float64x2_t v44 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v45.f64[0]  = INFINITY;
  float64x2_t v46 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
  v47.f64[0]  = -INFINITY;
  do
  {
    float64x2_t v48 = *(float64x2_t *)((char *)&v52 + v43);
    long long v49 = *(long long *)((char *)&v52 + v43 + 16);
    float64x2_t v44 = vminnmq_f64(v44, v48);
    float64x2_t v45 = vminnmq_f64((float64x2_t)*(unint64_t *)&v45.f64[0], (float64x2_t)(unint64_t)v49);
    float64x2_t v46 = vmaxnmq_f64(v46, v48);
    float64x2_t v47 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v47.f64[0], (float64x2_t)(unint64_t)v49);
    v43 += 32;
  }
  while (v43 != 256);
  *a5  = v44;
  a5[1].f64[0]  = v45.f64[0];
  float64x2_t result = vsubq_f64(v46, v44);
  a5[2]  = result;
  *(void *)&a5[3].f64[0]  = *(_OWORD *)&vsubq_f64(v47, v45);
  return result;
}

__n128 protocol witness for Shearable3D.sheared(_:) in conformance SPRect3D@<Q0>(uint64_t a1@<X0>, __n128 *a2@<X8>, float64x2_t a3@<Q0>, float64x2_t a4@<Q1>)
{
  double v7 = v4[2];
  double v6 = v4[3];
  double v8 = v4[1];
  double v10 = v4[6];
  double v9 = v4[7];
  double v12 = v4[4];
  double v11 = v4[5];
  unint64_t v14 = *(void *)a1;
  unint64_t v13 = *(void *)(a1 + 8);
  int v15 = *(unsigned __int8 *)(a1 + 16);
  if (v15)
  {
    *(void *)&a3.f64[0]  = v14;
    *(void *)&a4.f64[0]  = v13;
    v31.origin.x  = *v4;
    v31.origin.y  = v8;
    v31.origin.double z = v7;
    v31.origin.vector.f64[3]  = v6;
    v31.size.width  = v12;
    v31.size.height  = v11;
    v31.size.depth  = v10;
    v31.size.vector.f64[3]  = v9;
    if (v15 == 1) {
      SPAxis v16 = SPAxisY;
    }
    else {
      SPAxis v16 = SPAxisZ;
    }
  }
  else
  {
    *(void *)&a3.f64[0]  = v14;
    *(void *)&a4.f64[0]  = v13;
    v31.origin.x  = *v4;
    v31.origin.y  = v8;
    v31.origin.double z = v7;
    v31.origin.vector.f64[3]  = v6;
    v31.size.width  = v12;
    v31.size.height  = v11;
    v31.size.depth  = v10;
    v31.size.vector.f64[3]  = v9;
    SPAxis v16 = SPAxisX;
  }
  SPRect3DShear(&v31, v16, a3, a4, (float64x2_t *)&v24);
  unint64_t v17 = v30;
  unint64_t v18 = v29;
  unint64_t v19 = v26;
  unint64_t v20 = v27;
  unint64_t v21 = v25;
  __n128 result = v24;
  unint64_t v23 = v28;
  *a2  = v24;
  a2[1].n128_u64[0]  = v21;
  a2[1].n128_u64[1]  = v19;
  a2[2].n128_u64[0]  = v20;
  a2[2].n128_u64[1]  = v23;
  a2[3].n128_u64[0]  = v18;
  a2[3].n128_u64[1]  = v17;
  return result;
}

Swift::Bool __swiftcall SPRect3D.contains(anyOf:)(Swift::OpaquePointer anyOf)
{
  unint64_t v2 = *((void *)anyOf._rawValue + 2);
  if (v2 >> 31)
  {
    __break(1u);
  }
  else
  {
    long long v4 = *(_OWORD *)(v1 + 16);
    long long v3 = *(_OWORD *)(v1 + 32);
    double v5 = *(double *)(v1 + 48);
    double v6 = *(double *)(v1 + 56);
    *(_OWORD *)&v8.origin.x  = *(_OWORD *)v1;
    *(_OWORD *)&v8.origin.vector.f64[2]  = v4;
    v8.size.depth  = v5;
    v8.size.vector.f64[3]  = v6;
    *(_OWORD *)&v8.size.width  = v3;
    LOBYTE(anyOf._rawValue)  = SPRect3DContainsAnyPoint(&v8, (const SPPoint3D *)anyOf._rawValue + 1, v2);
  }
  return (Swift::Bool)anyOf._rawValue;
}

Swift::Void __swiftcall SPRect3D.formInset(by:)(SPSize3D *by)
{
  long long v5 = *(_OWORD *)(v1 + 16);
  long long v6 = *(_OWORD *)(v1 + 32);
  double v7 = *(double *)(v1 + 48);
  double v8 = *(double *)(v1 + 56);
  *(_OWORD *)&v17.origin.x  = *(_OWORD *)v1;
  *(_OWORD *)&v17.origin.vector.f64[2]  = v5;
  v17.size.depth  = v7;
  v17.size.vector.f64[3]  = v8;
  *(_OWORD *)&v17.size.width  = v6;
  v16.width  = v2;
  v16.height  = v3;
  v16.depth  = v4;
  SPRect3DInset(&v17, &v16, (uint64_t)v13);
  long long v9 = v13[1];
  long long v10 = v13[2];
  uint64_t v11 = v14;
  uint64_t v12 = v15;
  *(_OWORD *)uint64_t v1 = v13[0];
  *(_OWORD *)(v1 + 16)  = v9;
  *(void *)(v1 + 48)  = v11;
  *(void *)(v1 + 56)  = v12;
  *(_OWORD *)(v1 + 32)  = v10;
}

float64x2_t SPRect3DInset@<Q0>(SPRect3D *a1@<X0>, SPSize3D *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v3 = *(float64x2_t *)&a1->origin.x;
  float64x2_t v4 = *(float64x2_t *)&a1->size.vector.f64[2];
  *(void *)&double v5 = *(_OWORD *)&vabsq_f64(v4);
  *(void *)&a1->origin.double z = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a1->origin.vector.f64[2], vminnmq_f64((float64x2_t)*(unint64_t *)&v4.f64[0], (float64x2_t)0));
  float64x2_t v7 = *(float64x2_t *)&a1->origin.vector.f64[2];
  float64x2_t v6 = *(float64x2_t *)&a1->size.width;
  float64x2_t v8 = vaddq_f64(v3, vminnmq_f64(v6, (float64x2_t)0));
  *(float64x2_t *)&a1->origin.x  = v8;
  float64x2_t v9 = vabsq_f64(v6);
  *(float64x2_t *)&a1->size.width  = v9;
  a1->size.depth  = v5;
  float64x2_t v10 = *(float64x2_t *)&a2->vector.f64[2];
  float64x2_t result = vaddq_f64(*(float64x2_t *)&a2->width, v8);
  __asm { FMOV            V6.2D, #-2.0 }
  uint64_t v17 = *(_OWORD *)&vmlaq_f64(*(float64x2_t *)&a1->size.vector.f64[2], _Q6, v10);
  float64x2_t v18 = vmlaq_f64(v9, _Q6, *(float64x2_t *)&a2->width);
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = *(_OWORD *)&vaddq_f64(v10, v7);
  *(float64x2_t *)(a3 + 32)  = v18;
  *(void *)(a3 + 48)  = v17;
  return result;
}

void __swiftcall SPRect3D.intersection(_:)(SPRect3D_optional *__return_ptr retstr, SPRect3D *a2)
{
  long long v4 = *(_OWORD *)&a2->origin.x;
  long long v5 = *(_OWORD *)&a2->origin.vector.f64[2];
  long long v6 = *(_OWORD *)&a2->size.width;
  depth  = a2->size.depth;
  double v8 = a2->size.vector.f64[3];
  long long v9 = *(_OWORD *)(v2 + 16);
  long long v10 = *(_OWORD *)(v2 + 32);
  uint64_t v11 = *(void *)(v2 + 48);
  uint64_t v12 = *(void *)(v2 + 56);
  *(_OWORD *)&v34.x  = *(_OWORD *)v2;
  *(_OWORD *)&v34.vector.f64[2]  = v9;
  uint64_t v36 = v11;
  uint64_t v37 = v12;
  long long v35 = v10;
  *(_OWORD *)&v33.origin.x  = v4;
  *(_OWORD *)&v33.origin.vector.f64[2]  = v5;
  v33.size.depth  = depth;
  v33.size.vector.f64[3]  = v8;
  *(_OWORD *)&v33.size.width  = v6;
  SPRect3DIntersection((float64x2_t *)&v34, &v33, (float64x2_t *)&v28);
  double v13 = v28.vector.f64[3];
  double z = v28.z;
  y  = v28.y;
  x  = v28.x;
  double v18 = v31;
  double v17 = v32;
  double v20 = v29;
  double v19 = v30;
  *(SPPoint3D *)&v34.x  = *(SPPoint3D *)&v28.x;
  int v27 = SPPoint3DIsFinite(&v34, v21, v22, v23, v24, v25, v26);
  if (!v27)
  {
    x  = 0.0;
    y  = 0.0;
    double z = 0.0;
    double v13 = 0.0;
    double v20 = 0.0;
    double v19 = 0.0;
    double v18 = 0.0;
    double v17 = 0.0;
  }
  retstr->value.origin.x  = x;
  retstr->value.origin.y  = y;
  retstr->value.origin.double z = z;
  retstr->value.origin.vector.f64[3]  = v13;
  retstr->value.size.width  = v20;
  retstr->value.size.height  = v19;
  retstr->value.size.depth  = v18;
  retstr->value.size.vector.f64[3]  = v17;
  retstr->is_nil  = v27 ^ 1;
}

float64x2_t *SPRect3DIntersection@<X0>(float64x2_t *result@<X0>, SPRect3D *a2@<X1>, float64x2_t *a3@<X8>)
{
  float64x2_t v3 = result[2];
  float64x2_t v4 = result[3];
  *(void *)&float64_t v5 = *(_OWORD *)&vaddq_f64(result[1], vminnmq_f64((float64x2_t)*(unint64_t *)&v4.f64[0], (float64x2_t)0));
  *float64x2_t result = vaddq_f64(*result, vminnmq_f64(v3, (float64x2_t)0));
  result[1].f64[0]  = v5;
  result[2]  = vabsq_f64(v3);
  *(void *)&result[3].f64[0]  = *(_OWORD *)&vabsq_f64(v4);
  float64x2_t v6 = *(float64x2_t *)&a2->origin.x;
  float64x2_t v7 = *(float64x2_t *)&a2->size.vector.f64[2];
  *(void *)&v4.f64[0]  = *(_OWORD *)&vabsq_f64(v7);
  *(void *)&a2->origin.double z = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a2->origin.vector.f64[2], vminnmq_f64((float64x2_t)*(unint64_t *)&v7.f64[0], (float64x2_t)0));
  float64x2_t v9 = *(float64x2_t *)&a2->origin.vector.f64[2];
  float64x2_t v8 = *(float64x2_t *)&a2->size.width;
  float64x2_t v10 = vaddq_f64(v6, vminnmq_f64(v8, (float64x2_t)0));
  *(float64x2_t *)&a2->origin.x  = v10;
  float64x2_t v11 = vabsq_f64(v8);
  *(float64x2_t *)&a2->size.width  = v11;
  a2->size.depth  = v4.f64[0];
  unint64_t v12 = *(_OWORD *)&vaddq_f64(v9, vmaxnmq_f64((float64x2_t)*(unint64_t *)&a2->size.depth, (float64x2_t)0));
  float64x2_t v13 = vmaxnmq_f64(*result, v10);
  float64x2_t v14 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&result[1].f64[0], (float64x2_t)*(unint64_t *)&v9.f64[0]);
  float64x2_t v15 = vsubq_f64(vminnmq_f64(vaddq_f64(*result, vmaxnmq_f64(result[2], (float64x2_t)0)), vaddq_f64(v10, vmaxnmq_f64(v11, (float64x2_t)0))), v13);
  *(void *)&double v16 = *(_OWORD *)&vsubq_f64(vminnmq_f64((float64x2_t)(unint64_t)*(_OWORD *)&vaddq_f64(result[1], vmaxnmq_f64((float64x2_t)*(unint64_t *)&result[3].f64[0], (float64x2_t)0)), (float64x2_t)v12), v14);
  if (fmin(fmin(v15.f64[0], v16), v15.f64[1]) >= 0.0)
  {
    *a3  = v13;
    a3[1].f64[0]  = v14.f64[0];
    a3[2]  = v15;
    a3[3].f64[0]  = v16;
  }
  else
  {
    *a3  = (float64x2_t)SPRect3DNull;
    a3[1]  = (float64x2_t)unk_228C20FF0;
    a3[2]  = (float64x2_t)xmmword_228C21000;
    a3[3]  = (float64x2_t)unk_228C21010;
  }
  return result;
}

{
  float64x2_t v3;
  float64x2_t v4;
  float64_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  unint64_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  double v16;

  float64x2_t v3 = result[2];
  float64x2_t v4 = result[3];
  *(void *)&float64_t v5 = *(_OWORD *)&vaddq_f64(result[1], vminnmq_f64((float64x2_t)*(unint64_t *)&v4.f64[0], (float64x2_t)0));
  *float64x2_t result = vaddq_f64(*result, vminnmq_f64(v3, (float64x2_t)0));
  result[1].f64[0]  = v5;
  result[2]  = vabsq_f64(v3);
  *(void *)&result[3].f64[0]  = *(_OWORD *)&vabsq_f64(v4);
  float64x2_t v6 = *(float64x2_t *)&a2->origin.x;
  float64x2_t v7 = *(float64x2_t *)&a2->size.vector.f64[2];
  *(void *)&v4.f64[0]  = *(_OWORD *)&vabsq_f64(v7);
  *(void *)&a2->origin.double z = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a2->origin.vector.f64[2], vminnmq_f64((float64x2_t)*(unint64_t *)&v7.f64[0], (float64x2_t)0));
  float64x2_t v9 = *(float64x2_t *)&a2->origin.vector.f64[2];
  float64x2_t v8 = *(float64x2_t *)&a2->size.width;
  float64x2_t v10 = vaddq_f64(v6, vminnmq_f64(v8, (float64x2_t)0));
  *(float64x2_t *)&a2->origin.x  = v10;
  float64x2_t v11 = vabsq_f64(v8);
  *(float64x2_t *)&a2->size.width  = v11;
  a2->size.depth  = v4.f64[0];
  unint64_t v12 = *(_OWORD *)&vaddq_f64(v9, vmaxnmq_f64((float64x2_t)*(unint64_t *)&a2->size.depth, (float64x2_t)0));
  float64x2_t v13 = vmaxnmq_f64(*result, v10);
  float64x2_t v14 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&result[1].f64[0], (float64x2_t)*(unint64_t *)&v9.f64[0]);
  float64x2_t v15 = vsubq_f64(vminnmq_f64(vaddq_f64(*result, vmaxnmq_f64(result[2], (float64x2_t)0)), vaddq_f64(v10, vmaxnmq_f64(v11, (float64x2_t)0))), v13);
  *(void *)&double v16 = *(_OWORD *)&vsubq_f64(vminnmq_f64((float64x2_t)(unint64_t)*(_OWORD *)&vaddq_f64(result[1], vmaxnmq_f64((float64x2_t)*(unint64_t *)&result[3].f64[0], (float64x2_t)0)), (float64x2_t)v12), v14);
  if (fmin(fmin(v15.f64[0], v16), v15.f64[1]) >= 0.0)
  {
    *a3  = v13;
    a3[1].f64[0]  = v14.f64[0];
    a3[2]  = v15;
    a3[3].f64[0]  = v16;
  }
  else
  {
    *a3  = (float64x2_t)SPRect3DNull_0;
    a3[1]  = (float64x2_t)unk_228C21830;
    a3[2]  = (float64x2_t)xmmword_228C21840;
    a3[3]  = (float64x2_t)unk_228C21850;
  }
  return result;
}

double protocol witness for Volumetric.size.getter in conformance SPRect3D()
{
  return *(double *)(v0 + 32);
}

unint64_t protocol witness for Volumetric.contains(_:) in conformance SPRect3D(double *a1)
{
  long long v2 = *(_OWORD *)a1;
  long long v3 = *((_OWORD *)a1 + 1);
  long long v4 = *((_OWORD *)a1 + 2);
  double v5 = a1[6];
  double v6 = a1[7];
  long long v7 = *(_OWORD *)(v1 + 16);
  long long v8 = *(_OWORD *)(v1 + 32);
  double v9 = *(double *)(v1 + 48);
  double v10 = *(double *)(v1 + 56);
  *(_OWORD *)&v13.origin.x  = *(_OWORD *)v1;
  *(_OWORD *)&v13.origin.vector.f64[2]  = v7;
  v13.size.depth  = v9;
  v13.size.vector.f64[3]  = v10;
  *(_OWORD *)&v13.size.width  = v8;
  *(_OWORD *)&v12.origin.x  = v2;
  *(_OWORD *)&v12.origin.vector.f64[2]  = v3;
  v12.size.depth  = v5;
  v12.size.vector.f64[3]  = v6;
  *(_OWORD *)&v12.size.width  = v4;
  return SPRect3DContainsRect(&v13, &v12);
}

unint64_t protocol witness for Volumetric.contains(point:) in conformance SPRect3D(double a1, double a2, double a3)
{
  long long v4 = *(_OWORD *)(v3 + 16);
  long long v5 = *(_OWORD *)(v3 + 32);
  double v6 = *(double *)(v3 + 48);
  double v7 = *(double *)(v3 + 56);
  *(_OWORD *)&v10.origin.x  = *(_OWORD *)v3;
  *(_OWORD *)&v10.origin.vector.f64[2]  = v4;
  v10.size.depth  = v6;
  v10.size.vector.f64[3]  = v7;
  *(_OWORD *)&v10.size.width  = v5;
  v9.x  = a1;
  v9.y  = a2;
  v9.double z = a3;
  return SPRect3DContainsPoint(&v10, &v9);
}

float64_t protocol witness for Volumetric.union(_:) in conformance SPRect3D@<D0>(double *a1@<X0>, uint64_t a2@<X8>)
{
  long long v4 = *(_OWORD *)a1;
  long long v5 = *((_OWORD *)a1 + 1);
  long long v6 = *((_OWORD *)a1 + 2);
  double v7 = a1[6];
  double v8 = a1[7];
  long long v9 = *(_OWORD *)(v2 + 16);
  long long v10 = *(_OWORD *)(v2 + 32);
  double v11 = *(double *)(v2 + 48);
  double v12 = *(double *)(v2 + 56);
  *(_OWORD *)&v22.origin.x  = *(_OWORD *)v2;
  *(_OWORD *)&v22.origin.vector.f64[2]  = v9;
  v22.size.depth  = v11;
  v22.size.vector.f64[3]  = v12;
  *(_OWORD *)&v22.size.width  = v10;
  *(_OWORD *)&v21.origin.x  = v4;
  *(_OWORD *)&v21.origin.vector.f64[2]  = v5;
  v21.size.depth  = v7;
  v21.size.vector.f64[3]  = v8;
  *(_OWORD *)&v21.size.width  = v6;
  SPRect3DUnion(&v22, &v21, v18);
  float64_t result = v18[0].f64[0];
  float64x2_t v14 = v18[1];
  float64x2_t v15 = v18[2];
  uint64_t v16 = v19;
  uint64_t v17 = v20;
  *(float64x2_t *)a2  = v18[0];
  *(float64x2_t *)(a2 + 16)  = v14;
  *(void *)(a2 + 48)  = v16;
  *(void *)(a2 + 56)  = v17;
  *(float64x2_t *)(a2 + 32)  = v15;
  return result;
}

unint64_t protocol witness for Volumetric.intersection(_:) in conformance SPRect3D@<X0>(double *a1@<X0>, uint64_t a2@<X8>)
{
  long long v4 = *(_OWORD *)a1;
  long long v5 = *((_OWORD *)a1 + 1);
  long long v6 = *((_OWORD *)a1 + 2);
  double v7 = a1[6];
  double v8 = a1[7];
  long long v9 = *(_OWORD *)(v2 + 16);
  long long v10 = *(_OWORD *)(v2 + 32);
  uint64_t v11 = *(void *)(v2 + 48);
  uint64_t v12 = *(void *)(v2 + 56);
  *(_OWORD *)&v34.x  = *(_OWORD *)v2;
  *(_OWORD *)&v34.vector.f64[2]  = v9;
  uint64_t v36 = v11;
  uint64_t v37 = v12;
  long long v35 = v10;
  *(_OWORD *)&v33.origin.x  = v4;
  *(_OWORD *)&v33.origin.vector.f64[2]  = v5;
  v33.size.depth  = v7;
  v33.size.vector.f64[3]  = v8;
  *(_OWORD *)&v33.size.width  = v6;
  SPRect3DIntersection((float64x2_t *)&v34, &v33, (float64x2_t *)&v28);
  double v13 = v28.vector.f64[3];
  double z = v28.z;
  y  = v28.y;
  x  = v28.x;
  uint64_t v18 = v31;
  uint64_t v17 = v32;
  uint64_t v20 = v29;
  uint64_t v19 = v30;
  *(SPPoint3D *)&v34.x  = *(SPPoint3D *)&v28.x;
  unint64_t result = SPPoint3DIsFinite(&v34, v21, v22, v23, v24, v25, v26);
  if (!result)
  {
    x  = 0.0;
    y  = 0.0;
    double z = 0.0;
    double v13 = 0.0;
    uint64_t v20 = 0;
    uint64_t v19 = 0;
    uint64_t v18 = 0;
    uint64_t v17 = 0;
  }
  *(double *)a2  = x;
  *(double *)(a2 + 8)  = y;
  *(double *)(a2 + 16)  = z;
  *(double *)(a2 + 24)  = v13;
  *(void *)(a2 + 32)  = v20;
  *(void *)(a2 + 40)  = v19;
  *(void *)(a2 + 48)  = v18;
  *(void *)(a2 + 56)  = v17;
  *(unsigned char *)(a2 + 64)  = result ^ 1;
  return result;
}

BOOL static SPRect3D.== infix(_:_:)(double *a1, double *a2)
{
  if (*a1 != *a2 || a1[1] != a2[1] || a1[2] != a2[2]) {
    return 0;
  }
  return a1[4] == a2[4] && a1[5] == a2[5] && a1[6] == a2[6];
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance SPRect3D(double *a1, double *a2)
{
  if (*a1 != *a2 || a1[1] != a2[1] || a1[2] != a2[2]) {
    return 0;
  }
  return a1[4] == a2[4] && a1[5] == a2[5] && a1[6] == a2[6];
}

void SPRect3D.hash(into:)()
{
  specialized SIMD.hash(into:)(*(__n128 *)v0, *(double *)(v0 + 16));
  __n128 v1 = *(__n128 *)(v0 + 32);
  long long v2 = *(_OWORD *)(v0 + 48);

  specialized SIMD.hash(into:)(v1, *(double *)&v2);
}

Swift::Int SPRect3D.hashValue.getter()
{
  Hasher.init(_seed:)();
  specialized SIMD.hash(into:)(*v0, v0[1].n128_f64[0]);
  specialized SIMD.hash(into:)(v0[2], v0[3].n128_f64[0]);
  return Hasher._finalize()();
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPRect3D()
{
  __n128 v2 = *v0;
  __n128 v3 = v0[1];
  __n128 v4 = v0[2];
  __n128 v5 = v0[3];
  Hasher.init(_seed:)();
  specialized SIMD.hash(into:)(v2, v3.n128_f64[0]);
  specialized SIMD.hash(into:)(v4, v5.n128_f64[0]);
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance SPRect3D()
{
  __n128 v1 = *(__n128 *)(v0 + 32);
  long long v2 = *(_OWORD *)(v0 + 48);
  specialized SIMD.hash(into:)(*(__n128 *)v0, *(double *)(v0 + 16));

  specialized SIMD.hash(into:)(v1, *(double *)&v2);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPRect3D()
{
  __n128 v2 = *v0;
  __n128 v3 = v0[1];
  __n128 v4 = v0[2];
  __n128 v5 = v0[3];
  Hasher.init(_seed:)();
  specialized SIMD.hash(into:)(v2, v3.n128_f64[0]);
  specialized SIMD.hash(into:)(v4, v5.n128_f64[0]);
  return Hasher._finalize()();
}

uint64_t protocol witness for static Equatable.== infix(_:_:) in conformance SPRect3D.CodingKeys(char *a1, char *a2)
{
  return specialized == infix<A>(_:_:)(*a1, *a2);
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPRect3D.CodingKeys()
{
  return Hasher._finalize()();
}

uint64_t protocol witness for Hashable.hash(into:) in conformance SPRect3D.CodingKeys()
{
  String.hash(into:)();

  return swift_bridgeObjectRelease();
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPRect3D.CodingKeys()
{
  return Hasher._finalize()();
}

uint64_t protocol witness for RawRepresentable.init(rawValue:) in conformance SPRect3D.CodingKeys@<X0>(Swift::String *a1@<X0>, char *a2@<X8>)
{
  Swift::Int v3 = _findStringSwitchCase(cases:string:)((Swift::OpaquePointer)&outlined read-only object #0 of SPRect3D.CodingKeys.init(rawValue:), *a1);
  uint64_t result = swift_bridgeObjectRelease();
  if (v3 == 1) {
    char v5 = 1;
  }
  else {
    char v5 = 2;
  }
  if (!v3) {
    char v5 = 0;
  }
  *a2  = v5;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance SPRect3D.CodingKeys(uint64_t *a1@<X8>)
{
  uint64_t v2 = 1702521203;
  if (!*v1) {
    uint64_t v2 = 0x6E696769726FLL;
  }
  unint64_t v3 = 0xE600000000000000;
  if (*v1) {
    unint64_t v3 = 0xE400000000000000;
  }
  *a1  = v2;
  a1[1]  = v3;
}

uint64_t protocol witness for CodingKey.stringValue.getter in conformance SPRect3D.CodingKeys()
{
  if (*v0) {
    return 1702521203;
  }
  else {
    return 0x6E696769726FLL;
  }
}

uint64_t protocol witness for CodingKey.init(stringValue:) in conformance SPRect3D.CodingKeys@<X0>(Swift::String string@<0:X0, 8:X1>, char *a2@<X8>)
{
  object  = string._object;
  v3._countAndFlagsBits  = string._countAndFlagsBits;
  v3._object  = object;
  Swift::Int v5 = _findStringSwitchCase(cases:string:)((Swift::OpaquePointer)&outlined read-only object #0 of SPRect3D.CodingKeys.init(rawValue:), v3);
  uint64_t result = swift_bridgeObjectRelease();
  if (v5 == 1) {
    char v7 = 1;
  }
  else {
    char v7 = 2;
  }
  if (!v5) {
    char v7 = 0;
  }
  *a2  = v7;
  return result;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPRect3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys();

  return MEMORY[0x270FA00B0](a1, v2);
}

uint64_t protocol witness for CustomDebugStringConvertible.debugDescription.getter in conformance SPRect3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys();

  return MEMORY[0x270FA00B8](a1, v2);
}

uint64_t SPRect3D.encode(to:)(void *a1)
{
  Swift::String v3 = v1;
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedEncodingContainer<SPRect3D.CodingKeys>);
  uint64_t v6 = *(void *)(v5 - 8);
  MEMORY[0x270FA5388](v5);
  double v8 = (char *)&v12 - ((v7 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys();
  dispatch thunk of Encoder.container<A>(keyedBy:)();
  long long v9 = v3[1];
  long long v14 = *v3;
  long long v15 = v9;
  char v13 = 0;
  type metadata accessor for SPPoint3D(0);
  _sSo9SPPoint3DaABSE7SpatialWlTm_1(&lazy protocol witness table cache variable for type SPPoint3D and conformance SPPoint3D, type metadata accessor for SPPoint3D);
  KeyedEncodingContainer.encode<A>(_:forKey:)();
  if (!v2)
  {
    long long v10 = v3[3];
    long long v14 = v3[2];
    long long v15 = v10;
    char v13 = 1;
    type metadata accessor for SPSize3D(0);
    _sSo9SPPoint3DaABSE7SpatialWlTm_1(&lazy protocol witness table cache variable for type SPSize3D and conformance SPSize3D, type metadata accessor for SPSize3D);
    KeyedEncodingContainer.encode<A>(_:forKey:)();
  }
  return (*(uint64_t (**)(char *, uint64_t))(v6 + 8))(v8, v5);
}

double SPRect3D.init(from:)@<D0>(void *a1@<X0>, _OWORD *a2@<X8>)
{
  specialized SPRect3D.init(from:)(a1, (uint64_t)v7);
  if (!v2)
  {
    long long v5 = v7[1];
    *a2  = v7[0];
    a2[1]  = v5;
    double result = *(double *)&v8;
    long long v6 = v9;
    a2[2]  = v8;
    a2[3]  = v6;
  }
  return result;
}

double protocol witness for Decodable.init(from:) in conformance SPRect3D@<D0>(void *a1@<X0>, _OWORD *a2@<X8>)
{
  specialized SPRect3D.init(from:)(a1, (uint64_t)v7);
  if (!v2)
  {
    long long v5 = v7[1];
    *a2  = v7[0];
    a2[1]  = v5;
    double result = *(double *)&v8;
    long long v6 = v9;
    a2[2]  = v8;
    a2[3]  = v6;
  }
  return result;
}

uint64_t protocol witness for Encodable.encode(to:) in conformance SPRect3D(void *a1)
{
  return SPRect3D.encode(to:)(a1);
}

double SPRect3D.init(origin:size:)@<D0>(uint64_t a1@<X8>, __n128 a2@<Q0>, __n128 a3@<Q1>, __n128 a4@<Q2>, __n128 a5@<Q3>)
{
  return SPRect3D.init(origin:size:)((void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRect3DMakeAtOriginWithVector, a1, a2, a3, a4, a5);
}

void __swiftcall SPRect3D.init(origin:size:)(SPRect3D *__return_ptr retstr, SPVector3D *origin, SPVector3D *size)
{
  SPRect3D.init(origin:size:)((void (*)(_OWORD *__return_ptr, void *, void *))SPRect3DMakeAtOriginWithVector, (uint64_t)retstr, v3, v4, v5, v6, v7, v8);
}

double SPRect3D.init(center:size:)@<D0>(uint64_t a1@<X8>, __n128 a2@<Q0>, __n128 a3@<Q1>, __n128 a4@<Q2>, __n128 a5@<Q3>)
{
  return SPRect3D.init(origin:size:)((void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRect3DMakeAtCenterWithVector, a1, a2, a3, a4, a5);
}

double SPRect3D.init(origin:size:)@<D0>(void (*a1)(_OWORD *__return_ptr, _OWORD *, _OWORD *)@<X0>, uint64_t a2@<X8>, __n128 a3@<Q0>, __n128 a4@<Q1>, __n128 a5@<Q2>, __n128 a6@<Q3>)
{
  v16[0]  = a3;
  v16[1]  = a4;
  v15[0]  = a5;
  v15[1]  = a6;
  a1(v12, v16, v15);
  double result = *(double *)v12;
  long long v8 = v12[1];
  long long v9 = v12[2];
  uint64_t v10 = v13;
  uint64_t v11 = v14;
  *(_OWORD *)a2  = v12[0];
  *(_OWORD *)(a2 + 16)  = v8;
  *(void *)(a2 + 48)  = v10;
  *(void *)(a2 + 56)  = v11;
  *(_OWORD *)(a2 + 32)  = v9;
  return result;
}

void __swiftcall SPRect3D.init(center:size:)(SPRect3D *__return_ptr retstr, SPVector3D *center, SPVector3D *size)
{
  SPRect3D.init(origin:size:)((void (*)(_OWORD *__return_ptr, void *, void *))SPRect3DMakeAtCenterWithVector, (uint64_t)retstr, v3, v4, v5, v6, v7, v8);
}

double SPRect3D.init(origin:size:)@<D0>(void (*a1)(_OWORD *__return_ptr, void *, void *)@<X0>, uint64_t a2@<X8>, double a3@<D0>, double a4@<D1>, double a5@<D2>, double a6@<D3>, double a7@<D4>, double a8@<D5>)
{
  *(double *)uint64_t v18 = a3;
  *(double *)&v18[1]  = a4;
  *(double *)&v18[2]  = a5;
  *(double *)uint64_t v17 = a6;
  *(double *)&v17[1]  = a7;
  *(double *)&v17[2]  = a8;
  a1(v14, v18, v17);
  double result = *(double *)v14;
  long long v10 = v14[1];
  long long v11 = v14[2];
  uint64_t v12 = v15;
  uint64_t v13 = v16;
  *(_OWORD *)a2  = v14[0];
  *(_OWORD *)(a2 + 16)  = v10;
  *(void *)(a2 + 48)  = v12;
  *(void *)(a2 + 56)  = v13;
  *(_OWORD *)(a2 + 32)  = v11;
  return result;
}

void __swiftcall SPRect3D.init(points:)(SPRect3D *__return_ptr retstr, Swift::OpaquePointer points)
{
  unint64_t v2 = *((void *)points._rawValue + 2);
  if (!v2)
  {
    __break(1u);
    goto LABEL_7;
  }
  if (v2 >> 31)
  {
LABEL_7:
    __break(1u);
    return;
  }
  rawValue  = points._rawValue;
  if ((swift_isUniquelyReferenced_nonNull_native() & 1) == 0) {
    rawValue  = specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(0, v2, 0, (char *)rawValue);
  }
  SPRect3DMakeBoundingFromPoints((SPPoint3D *)rawValue + 1, v2, (uint64_t)v12, v5, v6);
  long long v9 = v12[1];
  long long v10 = v12[0];
  long long v11 = v12[2];
  double v7 = v13;
  double v8 = v14;
  swift_bridgeObjectRelease();
  *(_OWORD *)&retstr->origin.x  = v10;
  *(_OWORD *)&retstr->origin.vector.f64[2]  = v9;
  retstr->size.depth  = v7;
  retstr->size.vector.f64[3]  = v8;
  *(_OWORD *)&retstr->size.width  = v11;
}

uint64_t SPRect3D.cornerPoints.getter()
{
  long long v5 = *((_OWORD *)v0 + 1);
  long long v6 = *(_OWORD *)v0;
  long long v7 = *((_OWORD *)v0 + 2);
  double v2 = v0[6];
  double v1 = v0[7];
  type metadata accessor for SPPoint3D(0);
  uint64_t v3 = static Array._allocateBufferUninitialized(minimumCapacity:)();
  *(void *)(v3 + 16)  = 8;
  *(_OWORD *)&v8.origin.x  = v6;
  *(_OWORD *)&v8.origin.vector.f64[2]  = v5;
  v8.size.depth  = v2;
  v8.size.vector.f64[3]  = v1;
  *(_OWORD *)&v8.size.width  = v7;
  SPRect3DGetCornerPoints(&v8, (SPPoint3D *)(v3 + 32));
  *(void *)(v3 + 16)  = 8;
  return v3;
}

uint64_t SPRect3D.description.getter()
{
  _StringGuts.grow(_:)(22);
  swift_bridgeObjectRelease();
  _StringGuts.grow(_:)(21);
  v0._countAndFlagsBits  = 540702760;
  v0._object  = (void *)0xE400000000000000;
  String.append(_:)(v0);
  Double.write<A>(to:)();
  v1._countAndFlagsBits  = 0x203A79202CLL;
  v1._object  = (void *)0xE500000000000000;
  String.append(_:)(v1);
  Double.write<A>(to:)();
  v2._countAndFlagsBits  = 0x203A7A202CLL;
  v2._object  = (void *)0xE500000000000000;
  String.append(_:)(v2);
  Double.write<A>(to:)();
  v3._countAndFlagsBits  = 41;
  v3._object  = (void *)0xE100000000000000;
  String.append(_:)(v3);
  v4._countAndFlagsBits  = 0;
  v4._object  = (void *)0xE000000000000000;
  String.append(_:)(v4);
  swift_bridgeObjectRelease();
  v5._countAndFlagsBits  = 0x203A657A6973202CLL;
  v5._object  = (void *)0xE800000000000000;
  String.append(_:)(v5);
  v6._countAndFlagsBits  = SPSize3D.description.getter();
  String.append(_:)(v6);
  swift_bridgeObjectRelease();
  v7._countAndFlagsBits  = 41;
  v7._object  = (void *)0xE100000000000000;
  String.append(_:)(v7);
  return 0x3A6E696769726F28;
}

uint64_t SPRect3D.customMirror.getter()
{
  uint64_t v1 = type metadata accessor for Mirror.AncestorRepresentation();
  uint64_t v2 = *(void *)(v1 - 8);
  MEMORY[0x270FA5388](v1);
  Swift::String v4 = (char *)v21 - ((v3 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName((uint64_t *)&demangling cache variable for type metadata for Mirror.DisplayStyle?);
  MEMORY[0x270FA5388](v5 - 8);
  Swift::String v7 = (char *)v21 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  long long v8 = v0[1];
  v21[0]  = *v0;
  v21[1]  = v8;
  long long v9 = v0[3];
  v21[2]  = v0[2];
  v21[3]  = v9;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<(String, Any)>);
  uint64_t v10 = swift_allocObject();
  *(_OWORD *)(v10 + 16)  = xmmword_228C202E0;
  *(void *)(v10 + 32)  = 0x6E696769726FLL;
  *(void *)(v10 + 40)  = 0xE600000000000000;
  type metadata accessor for SPPoint3D(0);
  *(void *)(v10 + 72)  = v11;
  uint64_t v12 = swift_allocObject();
  *(void *)(v10 + 48)  = v12;
  long long v13 = v0[1];
  *(_OWORD *)(v12 + 16)  = *v0;
  *(_OWORD *)(v12 + 32)  = v13;
  *(void *)(v10 + 80)  = 1702521203;
  *(void *)(v10 + 88)  = 0xE400000000000000;
  type metadata accessor for SPSize3D(0);
  *(void *)(v10 + 120)  = v14;
  uint64_t v15 = swift_allocObject();
  *(void *)(v10 + 96)  = v15;
  long long v16 = v0[3];
  *(_OWORD *)(v15 + 16)  = v0[2];
  *(_OWORD *)(v15 + 32)  = v16;
  uint64_t v17 = *MEMORY[0x263F8E808];
  uint64_t v18 = type metadata accessor for Mirror.DisplayStyle();
  uint64_t v19 = *(void *)(v18 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v19 + 104))(v7, v17, v18);
  (*(void (**)(char *, void, uint64_t, uint64_t))(v19 + 56))(v7, 0, 1, v18);
  (*(void (**)(char *, void, uint64_t))(v2 + 104))(v4, *MEMORY[0x263F8E830], v1);
  type metadata accessor for SPRect3D(0);
  return Mirror.init<A>(_:children:displayStyle:ancestorRepresentation:)();
}

char *specialized _ArrayBuffer._consumeAndCreateNew(bufferIsUnique:minimumCapacity:growForAppend:)(char *result, int64_t a2, char a3, char *a4)
{
  char v5 = (char)result;
  if (a3)
  {
    unint64_t v6 = *((void *)a4 + 3);
    int64_t v7 = v6 >> 1;
    if ((uint64_t)(v6 >> 1) < a2)
    {
      if (v7 + 0x4000000000000000 < 0)
      {
        __break(1u);
        return result;
      }
      int64_t v7 = v6 & 0xFFFFFFFFFFFFFFFELL;
      if ((uint64_t)(v6 & 0xFFFFFFFFFFFFFFFELL) <= a2) {
        int64_t v7 = a2;
      }
    }
  }
  else
  {
    int64_t v7 = a2;
  }
  uint64_t v8 = *((void *)a4 + 2);
  if (v7 <= v8) {
    uint64_t v9 = *((void *)a4 + 2);
  }
  else {
    uint64_t v9 = v7;
  }
  if (v9)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<SPPoint3D>);
    uint64_t v10 = (char *)swift_allocObject();
    int64_t v11 = _swift_stdlib_malloc_size(v10);
    uint64_t v12 = v11 - 32;
    if (v11 < 32) {
      uint64_t v12 = v11 - 1;
    }
    *((void *)v10 + 2)  = v8;
    *((void *)v10 + 3)  = 2 * (v12 >> 5);
  }
  else
  {
    uint64_t v10 = (char *)MEMORY[0x263F8EE78];
  }
  long long v13 = v10 + 32;
  uint64_t v14 = a4 + 32;
  if (v5)
  {
    if (v10 != a4 || v13 >= &v14[32 * v8]) {
      memmove(v13, v14, 32 * v8);
    }
    *((void *)a4 + 2)  = 0;
  }
  else
  {
    memcpy(v13, v14, 32 * v8);
  }
  swift_bridgeObjectRelease();
  return v10;
}

unint64_t lazy protocol witness table accessor for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys()
{
  unint64_t result = lazy protocol witness table cache variable for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys);
  }
  return result;
}

uint64_t specialized SPRect3D.init(from:)@<X0>(void *a1@<X0>, uint64_t a2@<X8>)
{
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedDecodingContainer<SPRect3D.CodingKeys>);
  uint64_t v6 = *(void *)(v5 - 8);
  MEMORY[0x270FA5388](v5);
  uint64_t v8 = (char *)&v15 - ((v7 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPRect3D.CodingKeys and conformance SPRect3D.CodingKeys();
  dispatch thunk of Decoder.container<A>(keyedBy:)();
  if (v2) {
    return __swift_destroy_boxed_opaque_existential_1(a1);
  }
  type metadata accessor for SPPoint3D(0);
  LOBYTE(v18)  = 0;
  _sSo9SPPoint3DaABSE7SpatialWlTm_1(&lazy protocol witness table cache variable for type SPPoint3D and conformance SPPoint3D, type metadata accessor for SPPoint3D);
  KeyedDecodingContainer.decode<A>(_:forKey:)();
  uint64_t v10 = v23;
  uint64_t v9 = v24;
  uint64_t v16 = v21;
  uint64_t v17 = v22;
  type metadata accessor for SPSize3D(0);
  char v25 = 1;
  _sSo9SPPoint3DaABSE7SpatialWlTm_1((unint64_t *)&lazy protocol witness table cache variable for type SPSize3D and conformance SPSize3D, type metadata accessor for SPSize3D);
  KeyedDecodingContainer.decode<A>(_:forKey:)();
  (*(void (**)(char *, uint64_t))(v6 + 8))(v8, v5);
  long long v15 = v18;
  uint64_t v11 = v19;
  uint64_t v12 = v20;
  uint64_t result = __swift_destroy_boxed_opaque_existential_1(a1);
  *(void *)(a2 + 16)  = v10;
  *(void *)(a2 + 24)  = v9;
  uint64_t v14 = v17;
  *(void *)a2  = v16;
  *(void *)(a2 + 8)  = v14;
  *(void *)(a2 + 48)  = v11;
  *(void *)(a2 + 56)  = v12;
  *(_OWORD *)(a2 + 32)  = v15;
  return result;
}

__n128 SPRect3DMakeAtOriginWithVector@<Q0>(__n128 *a1@<X0>, long long *a2@<X1>, uint64_t a3@<X8>)
{
  __n128 result = *a1;
  long long v4 = *a2;
  unint64_t v5 = a1[1].n128_u64[0];
  uint64_t v6 = *((void *)a2 + 2);
  *(__n128 *)a3  = *a1;
  *(void *)(a3 + 16)  = v5;
  *(_OWORD *)(a3 + 32)  = v4;
  *(void *)(a3 + 48)  = v6;
  return result;
}

{
  __n128 result;
  long long v4;
  unint64_t v5;
  uint64_t v6;

  __n128 result = *a1;
  long long v4 = *a2;
  unint64_t v5 = a1[1].n128_u64[0];
  uint64_t v6 = *((void *)a2 + 2);
  *(__n128 *)a3  = *a1;
  *(void *)(a3 + 16)  = v5;
  *(_OWORD *)(a3 + 32)  = v4;
  *(void *)(a3 + 48)  = v6;
  return result;
}

__n128 SPRect3DMakeAtOriginWithVector@<Q0>(SPVector3D *a1@<X0>, SPVector3D *a2@<X1>, uint64_t a3@<X8>)
{
  __n128 result = *(__n128 *)&a1->x;
  double z = a1->z;
  long long v5 = *(_OWORD *)&a2->x;
  double v6 = a2->z;
  *(_OWORD *)a3  = *(_OWORD *)&a1->x;
  *(double *)(a3 + 16)  = z;
  *(_OWORD *)(a3 + 32)  = v5;
  *(double *)(a3 + 48)  = v6;
  return result;
}

float64x2_t SPRect3DMakeAtCenterWithVector@<Q0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v4 = *a2;
  float64x2_t v3 = a2[1];
  __asm { FMOV            V4.2D, #-0.5 }
  uint64_t v10 = *(_OWORD *)&vaddq_f64(a1[1], vmulq_f64(v3, _Q4));
  float64x2_t result = vaddq_f64(*a1, vmulq_f64(*a2, _Q4));
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = v10;
  *(float64x2_t *)(a3 + 32)  = v4;
  *(float64_t *)(a3 + 48)  = v3.f64[0];
  return result;
}

{
  float64x2_t v3;
  float64x2_t v4;
  uint64_t v10;
  float64x2_t result;

  float64x2_t v4 = *a2;
  float64x2_t v3 = a2[1];
  __asm { FMOV            V4.2D, #-0.5 }
  uint64_t v10 = *(_OWORD *)&vaddq_f64(a1[1], vmulq_f64(v3, _Q4));
  float64x2_t result = vaddq_f64(*a1, vmulq_f64(*a2, _Q4));
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = v10;
  *(float64x2_t *)(a3 + 32)  = v4;
  *(float64_t *)(a3 + 48)  = v3.f64[0];
  return result;
}

float64x2_t SPRect3DMakeAtCenterWithVector@<Q0>(SPVector3D *a1@<X0>, SPVector3D *a2@<X1>, uint64_t a3@<X8>)
{
  long long v4 = *(_OWORD *)&a2->x;
  float64x2_t v3 = *(float64x2_t *)&a2->vector.f64[2];
  __asm { FMOV            V4.2D, #-0.5 }
  uint64_t v10 = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a1->vector.f64[2], vmulq_f64(v3, _Q4));
  float64x2_t result = vaddq_f64(*(float64x2_t *)&a1->x, vmulq_f64(*(float64x2_t *)&a2->x, _Q4));
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = v10;
  *(_OWORD *)(a3 + 32)  = v4;
  *(float64_t *)(a3 + 48)  = v3.f64[0];
  return result;
}

float64x2_t SPRect3DMakeBoundingFromPoints@<Q0>(SPPoint3D *a1@<X0>, int a2@<W1>, uint64_t a3@<X8>, float64x2_t result@<Q0>, float64x2_t a5@<Q1>)
{
  if (a2)
  {
    if (a2 < 1)
    {
      a5.f64[0]  = -INFINITY;
      result.f64[0]  = INFINITY;
      float64x2_t v6 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
      float64x2_t v5 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    }
    else
    {
      result.f64[0]  = INFINITY;
      a5.f64[0]  = -INFINITY;
      float64x2_t v5 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
      float64x2_t v6 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
      uint64_t v7 = a2;
      do
      {
        float64x2_t v8 = *(float64x2_t *)&a1->x;
        long long v9 = *(_OWORD *)&a1->vector.f64[2];
        ++a1;
        float64x2_t v5 = vminnmq_f64(v5, v8);
        float64x2_t result = vminnmq_f64((float64x2_t)*(unint64_t *)&result.f64[0], (float64x2_t)(unint64_t)v9);
        float64x2_t v6 = vmaxnmq_f64(v6, v8);
        a5  = vmaxnmq_f64((float64x2_t)*(unint64_t *)&a5.f64[0], (float64x2_t)(unint64_t)v9);
        --v7;
      }
      while (v7);
    }
    uint64_t v10 = *(_OWORD *)&vsubq_f64(a5, result);
    *(float64x2_t *)a3  = v5;
    *(float64_t *)(a3 + 16)  = result.f64[0];
    float64x2_t result = vsubq_f64(v6, v5);
    *(float64x2_t *)(a3 + 32)  = result;
    *(void *)(a3 + 48)  = v10;
  }
  else
  {
    result.f64[0]  = 0.0;
    *(_OWORD *)(a3 + 32)  = 0u;
    *(_OWORD *)(a3 + 48)  = 0u;
    *(_OWORD *)a3  = 0u;
    *(_OWORD *)(a3 + 16)  = 0u;
  }
  return result;
}

double SPRect3DGetCornerPoints(SPRect3D *a1, SPPoint3D *a2)
{
  uint64_t v2 = 0;
  float64x2_t v3 = *(float64x2_t *)&a1->size.width;
  depth  = a1->size.depth;
  do
  {
    float64x2_t v5 = &a2[v2];
    *(_OWORD *)&v5->x  = v8;
    *(_OWORD *)&v5->vector.f64[2]  = v9;
    ++v2;
  }
  while (v2 != 8);
  a2[1].y  = v3.f64[1] + a2[1].y;
  *(float64x2_t *)&a2[2].x  = vaddq_f64(v3, *(float64x2_t *)&a2[2].x);
  a2[3].x  = v3.f64[0] + a2[3].x;
  a2[4].double z = depth + a2[4].z;
  double v6 = depth + a2[5].z;
  a2[5].y  = v3.f64[1] + a2[5].y;
  a2[5].double z = v6;
  *(float64x2_t *)&a2[6].x  = vaddq_f64(v3, *(float64x2_t *)&a2[6].x);
  a2[6].double z = depth + a2[6].z;
  a2[7].x  = v3.f64[0] + a2[7].x;
  double result = depth + a2[7].z;
  a2[7].double z = result;
  return result;
}

uint64_t sub_228C053E4()
{
  return MEMORY[0x270FA0238](v0, 48, 15);
}

uint64_t instantiation function for generic protocol witness table for SPRect3D(void *a1)
{
  a1[1]  = _sSo9SPPoint3DaABSE7SpatialWlTm_1(&lazy protocol witness table cache variable for type SPRect3D and conformance SPRect3D, type metadata accessor for SPRect3D);
  a1[2]  = _sSo9SPPoint3DaABSE7SpatialWlTm_1(&lazy protocol witness table cache variable for type SPRect3D and conformance SPRect3D, type metadata accessor for SPRect3D);
  uint64_t result = _sSo9SPPoint3DaABSE7SpatialWlTm_1(&lazy protocol witness table cache variable for type SPRect3D and conformance SPRect3D, type metadata accessor for SPRect3D);
  a1[3]  = result;
  return result;
}

uint64_t base witness table accessor for Equatable in SPRect3D()
{
  return _sSo9SPPoint3DaABSE7SpatialWlTm_1(&lazy protocol witness table cache variable for type SPRect3D and conformance SPRect3D, type metadata accessor for SPRect3D);
}

unsigned char *storeEnumTagSinglePayload for SPRect3D.CodingKeys(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 1 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFF) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFE)
  {
    unsigned int v6 = ((a2 - 255) >> 8) + 1;
    *uint64_t result = a2 + 1;
    switch(v5)
    {
      case 1:
        result[1]  = v6;
        break;
      case 2:
        *(_WORD *)(result + 1)  = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x228C055B4);
      case 4:
        *(_DWORD *)(result + 1)  = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1]  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1)  = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1)  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 1;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for SPRect3D.CodingKeys()
{
  return &type metadata for SPRect3D.CodingKeys;
}

float64x2_t SPRect3DUnion@<Q0>(SPRect3D *a1@<X0>, SPRect3D *a2@<X1>, float64x2_t *a3@<X8>)
{
  float64x2_t v3 = *(float64x2_t *)&a1->size.width;
  float64x2_t v4 = *(float64x2_t *)&a1->size.vector.f64[2];
  *(void *)&double v5 = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a1->origin.vector.f64[2], vminnmq_f64((float64x2_t)*(unint64_t *)&v4.f64[0], (float64x2_t)0));
  *(float64x2_t *)&a1->origin.x  = vaddq_f64(*(float64x2_t *)&a1->origin.x, vminnmq_f64(v3, (float64x2_t)0));
  a1->origin.double z = v5;
  *(float64x2_t *)&a1->size.width  = vabsq_f64(v3);
  *(void *)&a1->size.depth  = *(_OWORD *)&vabsq_f64(v4);
  float64x2_t v6 = *(float64x2_t *)&a2->origin.x;
  float64x2_t v7 = *(float64x2_t *)&a2->size.vector.f64[2];
  *(void *)&v4.f64[0]  = *(_OWORD *)&vabsq_f64(v7);
  *(void *)&a2->origin.double z = *(_OWORD *)&vaddq_f64(*(float64x2_t *)&a2->origin.vector.f64[2], vminnmq_f64((float64x2_t)*(unint64_t *)&v7.f64[0], (float64x2_t)0));
  float64x2_t v9 = *(float64x2_t *)&a2->origin.vector.f64[2];
  float64x2_t v8 = *(float64x2_t *)&a2->size.width;
  float64x2_t v10 = vaddq_f64(v6, vminnmq_f64(v8, (float64x2_t)0));
  *(float64x2_t *)&a2->origin.x  = v10;
  float64x2_t v11 = vabsq_f64(v8);
  *(float64x2_t *)&a2->size.width  = v11;
  a2->size.depth  = v4.f64[0];
  unint64_t v12 = *(_OWORD *)&vaddq_f64(v9, vmaxnmq_f64((float64x2_t)*(unint64_t *)&a2->size.depth, (float64x2_t)0));
  float64x2_t v13 = vminnmq_f64(*(float64x2_t *)&a1->origin.x, v10);
  float64x2_t v14 = vminnmq_f64((float64x2_t)*(unint64_t *)&a1->origin.z, (float64x2_t)*(unint64_t *)&v9.f64[0]);
  float64x2_t result = vsubq_f64(vmaxnmq_f64(vaddq_f64(*(float64x2_t *)&a1->origin.x, vmaxnmq_f64(*(float64x2_t *)&a1->size.width, (float64x2_t)0)), vaddq_f64(v10, vmaxnmq_f64(v11, (float64x2_t)0))), v13);
  *(void *)&v11.f64[0]  = *(_OWORD *)&vsubq_f64(vmaxnmq_f64((float64x2_t)(unint64_t)*(_OWORD *)&vaddq_f64(*(float64x2_t *)&a1->origin.vector.f64[2], vmaxnmq_f64((float64x2_t)*(unint64_t *)&a1->size.depth, (float64x2_t)0)), (float64x2_t)v12), v14);
  *a3  = v13;
  a3[1].f64[0]  = v14.f64[0];
  a3[2]  = result;
  a3[3].f64[0]  = v11.f64[0];
  return result;
}

unint64_t SPRect3DContainsPoint(SPRect3D *a1, SPPoint3D *a2)
{
  int64x2_t v2 = (int64x2_t)vandq_s8((int8x16_t)vcgeq_f64(vaddq_f64(*(float64x2_t *)&a1->origin.x, vmaxnmq_f64(*(float64x2_t *)&a1->size.width, (float64x2_t)0)), *(float64x2_t *)&a2->x), (int8x16_t)vcgeq_f64(*(float64x2_t *)&a2->x, vaddq_f64(*(float64x2_t *)&a1->origin.x, vminnmq_f64(*(float64x2_t *)&a1->size.width, (float64x2_t)0))));
  return vandq_s8(vandq_s8((int8x16_t)vdupq_laneq_s64(v2, 1), (int8x16_t)v2), (int8x16_t)vshlq_n_s64((int64x2_t)vandq_s8((int8x16_t)vcgeq_f64(vaddq_f64(*(float64x2_t *)&a1->origin.vector.f64[2], vmaxnmq_f64((float64x2_t)*(unint64_t *)&a1->size.depth, (float64x2_t)0)), *(float64x2_t *)&a2->vector.f64[2]), (int8x16_t)vcgeq_f64(*(float64x2_t *)&a2->vector.f64[2], vaddq_f64(*(float64x2_t *)&a1->origin.vector.f64[2], vminnmq_f64((float64x2_t)*(unint64_t *)&a1->size.depth, (float64x2_t)0)))), 0x3FuLL)).u64[0] >> 63;
}

unint64_t SPRect3DContainsRect(SPRect3D *a1, SPRect3D *a2)
{
  int64x2_t v2 = (int64x2_t)vandq_s8((int8x16_t)vcgeq_f64(vaddq_f64(*(float64x2_t *)&a1->origin.x, vmaxnmq_f64(*(float64x2_t *)&a1->size.width, (float64x2_t)0)), vaddq_f64(*(float64x2_t *)&a2->origin.x, vmaxnmq_f64(*(float64x2_t *)&a2->size.width, (float64x2_t)0))), (int8x16_t)vcgeq_f64(vaddq_f64(*(float64x2_t *)&a2->origin.x, vminnmq_f64(*(float64x2_t *)&a2->size.width, (float64x2_t)0)), vaddq_f64(*(float64x2_t *)&a1->origin.x, vminnmq_f64(*(float64x2_t *)&a1->size.width, (float64x2_t)0))));
  return vandq_s8(vandq_s8((int8x16_t)vdupq_laneq_s64(v2, 1), (int8x16_t)v2), (int8x16_t)vshlq_n_s64((int64x2_t)vandq_s8((int8x16_t)vcgeq_f64(vaddq_f64(*(float64x2_t *)&a1->origin.vector.f64[2], vmaxnmq_f64((float64x2_t)*(unint64_t *)&a1->size.depth, (float64x2_t)0)), vaddq_f64(*(float64x2_t *)&a2->origin.vector.f64[2], vmaxnmq_f64((float64x2_t)*(unint64_t *)&a2->size.depth, (float64x2_t)0))), (int8x16_t)vcgeq_f64(vaddq_f64(*(float64x2_t *)&a2->origin.vector.f64[2], vminnmq_f64((float64x2_t)*(unint64_t *)&a2->size.depth,
                                                      (float64x2_t)0)),
                                                  vaddq_f64(*(float64x2_t *)&a1->origin.vector.f64[2], vminnmq_f64((float64x2_t)*(unint64_t *)&a1->size.depth, (float64x2_t)0)))), 0x3FuLL)).u64[0] >> 63;
}

double SPRect3DRotateByQuaternion@<D0>(SPRect3D *a1@<X0>, simd_quatd a2@<0:Q0, 16:Q1>, SPRotation3D *a3@<X1>, float64x2_t *a4@<X8>, float64x2_t a5@<Q5>, float64x2_t a6@<Q7>)
{
  SPRotation3D v11 = *a3;
  long long v6 = *(_OWORD *)&a1->origin.vector.f64[2];
  *(_OWORD *)&v10.origin.x  = *(_OWORD *)&a1->origin.x;
  *(_OWORD *)&v10.origin.vector.f64[2]  = v6;
  long long v7 = *(_OWORD *)&a1->size.vector.f64[2];
  *(_OWORD *)&v10.size.width  = *(_OWORD *)&a1->size.width;
  *(_OWORD *)&v10.size.vector.f64[2]  = v7;
  v9[0]  = *(float64x2_t *)v11.vector.f64;
  v9[1]  = *(float64x2_t *)&v11.quaternion.vector.f64[2];
  *(void *)&double result = *(_OWORD *)&SPRect3DRotate(&v10, v11, v9, a4, a5, a6);
  return result;
}

float64x2_t SPRect3DRotate@<Q0>(SPRect3D *a1@<X0>, SPRotation3D a2@<0:Q0, 16:Q1>, float64x2_t *a3@<X1>, float64x2_t *a4@<X8>, float64x2_t a5@<Q5>, float64x2_t a6@<Q7>)
{
  uint64_t v6 = 0;
  uint64_t v63 = *MEMORY[0x263EF8340];
  _Q4  = *a3;
  _Q1  = a3[1];
  _D5  = a3->f64[1];
  __asm { FMLS            D0, D1, V1.D[0] }
  _D17  = a3[1].f64[1];
  __asm { FMLA            D0, D17, V1.D[1] }
  double v16 = vmlad_n_f64(vmuld_lane_f64(_Q1.f64[0], _Q1, 1), _D5, a3->f64[0]);
  double v17 = v16 + v16;
  v18.f64[0]  = vmuld_lane_f64(_D5, _Q1, 1);
  double v19 = vmlad_n_f64(-(_D5 * _D17), _Q1.f64[0], a3->f64[0]);
  double v20 = vmlad_n_f64(-(_Q1.f64[0] * _D17), _D5, a3->f64[0]);
  __asm
  {
    FMLA            D3, D5, V4.D[1]
    FMLA            D3, D17, V1.D[1]
    FMLS            D3, D4, V4.D[0]
    FMLA            D16, D1, V4.D[1]
  }
  float64x2_t v25 = (float64x2_t)vzip1q_s64(*(int64x2_t *)a3, (int64x2_t)_Q1);
  __asm { FMLS            D20, D4, V4.D[0] }
  _Q4.f64[0]  = a3[1].f64[0];
  v18.f64[1]  = -(a3->f64[0] * _D17);
  _Q1.f64[0]  = v20 + v20;
  float64x2_t v28 = *(float64x2_t *)&a1->size.width;
  depth  = a1->size.depth;
  long long v61 = 0u;
  *(_OWORD *)long long v62 = 0u;
  float64x2_t v30 = vmlaq_f64(v18, v25, _Q4);
  float64x2_t v59 = 0u;
  long long v60 = 0u;
  long long v57 = 0u;
  long long v58 = 0u;
  long long v55 = 0u;
  long long v56 = 0u;
  long long v53 = 0u;
  long long v54 = 0u;
  float64x2_t v51 = 0u;
  long long v52 = 0u;
  float64x2_t v31 = vaddq_f64(v30, v30);
  long long v49 = 0u;
  long long v50 = 0u;
  unint64_t v32 = vextq_s8((int8x16_t)v31, (int8x16_t)v31, 8uLL).u64[0];
  memset(v48, 0, sizeof(v48));
  do
  {
    SPRect3D v33 = &v48[v6];
    long long v34 = *(_OWORD *)&a1->origin.vector.f64[2];
    *SPRect3D v33 = *(_OWORD *)&a1->origin.x;
    v33[1]  = v34;
    v6 += 2;
  }
  while (v6 != 16);
  uint64_t v35 = 0;
  *((double *)&v49 + 1)  = v28.f64[1] + *((double *)&v49 + 1);
  a5.f64[0]  = _D20 + -_D5 * _D5;
  float64x2_t v36 = vaddq_f64(v28, v51);
  float64x2_t v51 = v36;
  *(double *)&long long v53 = v28.f64[0] + *(double *)&v53;
  *(double *)&long long v56 = depth + *(double *)&v56;
  v36.f64[0]  = v19 + v19;
  a6.f64[0]  = _D16 + _D16;
  *((double *)&v57 + 1)  = v28.f64[1] + *((double *)&v57 + 1);
  *(double *)&long long v58 = depth + *(double *)&v58;
  float64x2_t v59 = vaddq_f64(v28, v59);
  *(double *)&long long v60 = depth + *(double *)&v60;
  *(double *)&long long v61 = v28.f64[0] + *(double *)&v61;
  v62[0]  = depth + v62[0];
  _Q0.f64[1]  = v17;
  _Q1.f64[1]  = _D3;
  *(void *)&v31.f64[1]  = v32;
  do
  {
    uint64_t v37 = &v48[v35];
    float64x2_t v39 = (float64x2_t)v48[v35];
    float64x2_t v38 = (float64x2_t)v48[v35 + 1];
    *uint64_t v37 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q0, v39.f64[0]), _Q1, v39, 1), v31, v38.f64[0]), (float64x2_t)0);
    *((void *)v37 + 2)  = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v36, v39), a6, v39, 1), v38, a5), (float64x2_t)0);
    v35 += 2;
  }
  while (v35 != 16);
  uint64_t v40 = 0;
  a4[2]  = 0u;
  a4[3]  = 0u;
  a4[1]  = 0u;
  float64x2_t v41 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v42.f64[0]  = INFINITY;
  float64x2_t v43 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
  v44.f64[0]  = -INFINITY;
  do
  {
    float64x2_t v45 = (float64x2_t)v48[v40];
    long long v46 = v48[v40 + 1];
    float64x2_t v41 = vminnmq_f64(v41, v45);
    float64x2_t v42 = vminnmq_f64((float64x2_t)*(unint64_t *)&v42.f64[0], (float64x2_t)(unint64_t)v46);
    float64x2_t v43 = vmaxnmq_f64(v43, v45);
    float64x2_t v44 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v44.f64[0], (float64x2_t)(unint64_t)v46);
    v40 += 2;
  }
  while (v40 != 16);
  *a4  = v41;
  a4[1].f64[0]  = v42.f64[0];
  float64x2_t result = vsubq_f64(v43, v41);
  a4[2]  = result;
  *(void *)&a4[3].f64[0]  = *(_OWORD *)&vsubq_f64(v44, v42);
  return result;
}

float64x2_t SPRect3DScaleUniform@<Q0>(SPRect3D *a1@<X0>, float64x2_t a2@<Q0>, uint64_t a3@<X8>)
{
  uint64_t v3 = *(_OWORD *)&vmulq_f64(a2, *(float64x2_t *)&a1->origin.vector.f64[2]);
  float64x2_t v4 = vmulq_n_f64(*(float64x2_t *)&a1->origin.x, a2.f64[0]);
  uint64_t v5 = *(_OWORD *)&vmulq_f64(a2, *(float64x2_t *)&a1->size.vector.f64[2]);
  float64x2_t result = vmulq_n_f64(*(float64x2_t *)&a1->size.width, a2.f64[0]);
  *(float64x2_t *)a3  = v4;
  *(void *)(a3 + 16)  = v3;
  *(float64x2_t *)(a3 + 32)  = result;
  *(void *)(a3 + 48)  = v5;
  return result;
}

float64x2_t SPRect3DScaleBySize@<Q0>(SPRect3D *a1@<X0>, SPSize3D *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v3 = *(float64x2_t *)&a2->vector.f64[2];
  uint64_t v4 = *(_OWORD *)&vmulq_f64(v3, *(float64x2_t *)&a1->origin.vector.f64[2]);
  *(void *)&v3.f64[0]  = *(_OWORD *)&vmulq_f64(v3, *(float64x2_t *)&a1->size.vector.f64[2]);
  float64x2_t result = vmulq_f64(*(float64x2_t *)&a2->width, *(float64x2_t *)&a1->size.width);
  *(float64x2_t *)a3  = vmulq_f64(*(float64x2_t *)&a2->width, *(float64x2_t *)&a1->origin.x);
  *(void *)(a3 + 16)  = v4;
  *(float64x2_t *)(a3 + 32)  = result;
  *(float64_t *)(a3 + 48)  = v3.f64[0];
  return result;
}

{
  float64x2_t v3;
  uint64_t v4;
  float64x2_t result;

  float64x2_t v3 = *(float64x2_t *)&a2->vector.f64[2];
  uint64_t v4 = *(_OWORD *)&vmulq_f64(v3, *(float64x2_t *)&a1->origin.vector.f64[2]);
  *(void *)&v3.f64[0]  = *(_OWORD *)&vmulq_f64(v3, *(float64x2_t *)&a1->size.vector.f64[2]);
  float64x2_t result = vmulq_f64(*(float64x2_t *)&a2->width, *(float64x2_t *)&a1->size.width);
  *(float64x2_t *)a3  = vmulq_f64(*(float64x2_t *)&a2->width, *(float64x2_t *)&a1->origin.x);
  *(void *)(a3 + 16)  = v4;
  *(float64x2_t *)(a3 + 32)  = result;
  *(float64_t *)(a3 + 48)  = v3.f64[0];
  return result;
}

unint64_t SPRect3DIsNaN(float64x2_t *a1, double a2, float64x2_t a3)
{
  a3.f64[0]  = a1[1].f64[0];
  int8x16_t v3 = vorrq_s8((int8x16_t)vcltzq_f64(*a1), (int8x16_t)vcgezq_f64(*a1));
  float64x2_t v4 = (float64x2_t)vornq_s8((int8x16_t)vdupq_laneq_s64((int64x2_t)vmvnq_s8(v3), 1), vorrq_s8((int8x16_t)vcltzq_f64(a3), (int8x16_t)vcgezq_f64(a3)));
  if ((vornq_s8((int8x16_t)v4, v3).u64[0] & 0x8000000000000000) != 0) {
    return 1;
  }
  v4.f64[0]  = a1[3].f64[0];
  int8x16_t v5 = vorrq_s8((int8x16_t)vcltzq_f64(a1[2]), (int8x16_t)vcgezq_f64(a1[2]));
  return vornq_s8(vornq_s8((int8x16_t)vdupq_laneq_s64((int64x2_t)vmvnq_s8(v5), 1), vorrq_s8((int8x16_t)vcltzq_f64(v4), (int8x16_t)vcgezq_f64(v4))), v5).u64[0] >> 63;
}

unint64_t SPRect3DIsFinite(float64x2_t *a1, double a2, float64x2_t a3)
{
  float64x2_t v3 = *a1;
  a3.f64[0]  = a1[1].f64[0];
  int8x16_t v4 = vorrq_s8((int8x16_t)vcltzq_f64(a3), (int8x16_t)vcgezq_f64(a3));
  v3.f64[0]  = INFINITY;
  int8x16_t v5 = (int8x16_t)vceqq_f64(vabsq_f64(a3), v3);
  float64x2_t v6 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  int8x16_t v7 = vbicq_s8(v4, v5);
  float64x2_t v8 = (float64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(*a1), (int8x16_t)vcgezq_f64(*a1)), (int8x16_t)vceqq_f64(vabsq_f64(*a1), v6));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64((int64x2_t)v8, 1), vandq_s8(v7, (int8x16_t)v8)).u64[0] & 0x8000000000000000) == 0) {
    return 0;
  }
  float64x2_t v10 = a1[2];
  v8.f64[0]  = a1[3].f64[0];
  float64x2_t v11 = (float64x2_t)vcltzq_f64(v10);
  int8x16_t v12 = vorrq_s8((int8x16_t)v11, (int8x16_t)vcgezq_f64(v10));
  v11.f64[0]  = INFINITY;
  int64x2_t v13 = (int64x2_t)vbicq_s8(v12, (int8x16_t)vceqq_f64(vabsq_f64(v10), v6));
  return vandq_s8((int8x16_t)vdupq_laneq_s64(v13, 1), vandq_s8((int8x16_t)vshlq_n_s64((int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v8), (int8x16_t)vcgezq_f64(v8)), (int8x16_t)vceqq_f64(vabsq_f64(v8), v11)), 0x3FuLL), (int8x16_t)v13)).u64[0] >> 63;
}

unint64_t SPRect3DIsZero(float64x2_t *a1, double a2, float64x2_t a3)
{
  a3.f64[0]  = a1[1].f64[0];
  float64x2_t v3 = (float64x2_t)vceqzq_f64(a3);
  int64x2_t v4 = vceqzq_f64(*a1);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v4, 1), vandq_s8((int8x16_t)v3, (int8x16_t)v4)).u64[0] & 0x8000000000000000) == 0) {
    return 0;
  }
  v3.f64[0]  = a1[3].f64[0];
  int64x2_t v6 = vceqzq_f64(a1[2]);
  return vandq_s8((int8x16_t)vdupq_laneq_s64(v6, 1), vandq_s8((int8x16_t)vceqzq_f64(v3), (int8x16_t)v6)).u64[0] >> 63;
}

uint64_t _sSo9SPPoint3DaABSE7SpatialWlTm_1(unint64_t *a1, void (*a2)(uint64_t))
{
  uint64_t result = *a1;
  if (!result)
  {
    a2(255);
    uint64_t result = swift_getWitnessTable();
    atomic_store(result, a1);
  }
  return result;
}

void __swiftcall SPRotation3D.init(_:)(SPRotation3D *__return_ptr retstr, simd_quatf *a2)
{
  *(float64x2_t *)&v5.vector.f64[2]  = vcvtq_f64_f32(*(float32x2_t *)v2.f32);
  *(float64x2_t *)v5.vector.f64  = vcvt_hight_f64_f32(v2);
  v4[0]  = *(_OWORD *)&v5.vector.f64[2];
  v4[1]  = *(_OWORD *)v5.vector.f64;
  SPRotation3DMakeWithQuaternion(v5, (uint64_t)v4, &v3);
}

void __swiftcall SPAffineTransform3D.init(_:)(SPAffineTransform3D *__return_ptr retstr, simd_float4x3 *a2)
{
  specialized SPAffineTransform3D.init(_:)((float32x2_t *)a2, v7);
  long long v3 = v7[5];
  *(_OWORD *)retstr->matrix.columns[2].f64  = v7[4];
  *(_OWORD *)&retstr->matrix.columns[2].f64[2]  = v3;
  long long v4 = v7[7];
  *(_OWORD *)retstr->matrix.columns[3].f64  = v7[6];
  *(_OWORD *)&retstr->matrix.columns[3].f64[2]  = v4;
  long long v5 = v7[1];
  *(_OWORD *)retstr->matrix.columns[0].f64  = v7[0];
  *(_OWORD *)&retstr->matrix.columns[0].f64[2]  = v5;
  long long v6 = v7[3];
  *(_OWORD *)retstr->matrix.columns[1].f64  = v7[2];
  *(_OWORD *)&retstr->matrix.columns[1].f64[2]  = v6;
}

void __swiftcall SPProjectiveTransform3D.init(_:)(SPProjectiveTransform3D *__return_ptr retstr, simd_float4x4 *a2)
{
  specialized SPProjectiveTransform3D.init(_:)(v11, v2, v3, v4, v5);
  long long v7 = v11[5];
  *(_OWORD *)retstr->matrix.columns[2].f64  = v11[4];
  *(_OWORD *)&retstr->matrix.columns[2].f64[2]  = v7;
  long long v8 = v11[7];
  *(_OWORD *)retstr->matrix.columns[3].f64  = v11[6];
  *(_OWORD *)&retstr->matrix.columns[3].f64[2]  = v8;
  long long v9 = v11[1];
  *(_OWORD *)retstr->matrix.columns[0].f64  = v11[0];
  *(_OWORD *)&retstr->matrix.columns[0].f64[2]  = v9;
  long long v10 = v11[3];
  *(_OWORD *)retstr->matrix.columns[1].f64  = v11[2];
  *(_OWORD *)&retstr->matrix.columns[1].f64[2]  = v10;
}

void __swiftcall simd_float4x3.init(_:)(simd_float4x3 *__return_ptr retstr, SPAffineTransform3D *a2)
{
  specialized simd_float4x3.init(_:)((float64x2_t *)a2, v5);
  simd_float3 v3 = (simd_float3)v5[1];
  retstr->columns[0]  = (simd_float3)v5[0];
  retstr->columns[1]  = v3;
  simd_float3 v4 = (simd_float3)v5[3];
  retstr->columns[2]  = (simd_float3)v5[2];
  retstr->columns[3]  = v4;
}

void __swiftcall simd_float4x4.init(_:)(simd_float4x4 *__return_ptr retstr, SPAffineTransform3D *a2)
{
  __n128 v2 = *(__n128 *)&a2->matrix.columns[0].f64[2];
  long long v3 = *(_OWORD *)a2->matrix.columns[1].f64;
  __n128 v4 = *(__n128 *)&a2->matrix.columns[1].f64[2];
  long long v5 = *(_OWORD *)a2->matrix.columns[2].f64;
  __n128 v6 = *(__n128 *)&a2->matrix.columns[2].f64[2];
  long long v7 = *(_OWORD *)a2->matrix.columns[3].f64;
  long long v8 = *(_OWORD *)&a2->matrix.columns[3].f64[2];
  v10[0]  = *(_OWORD *)a2->matrix.columns[0].f64;
  v10[1]  = v2;
  v10[2]  = v3;
  v10[3]  = v4;
  v10[4]  = v5;
  v10[5]  = v6;
  v10[6]  = v7;
  v10[7]  = v8;
  SPAffineTransform3DGet4x4Matrix((uint64_t)v10, (uint64_t)&v9, v2, v4, v6);
}

void __swiftcall simd_float4x4.init(_:)(simd_float4x4 *__return_ptr retstr, SPPose3D *a2)
{
  long long v2 = *(_OWORD *)&a2->position.vector.f64[2];
  long long v3 = *(_OWORD *)a2->rotation.vector.f64;
  double v4 = a2->rotation.vector.f64[2];
  double v5 = a2->rotation.vector.f64[3];
  *(_OWORD *)&v7.position.x  = *(_OWORD *)&a2->position.x;
  *(_OWORD *)&v7.position.vector.f64[2]  = v2;
  v7.rotation.vector.f64[2]  = v4;
  v7.rotation.vector.f64[3]  = v5;
  *(_OWORD *)v7.rotation.vector.f64  = v3;
  SPPose3DGet4x4Matrix(&v7, (uint64_t)&v6);
}

double SPAngle.init<A>(radians:)(uint64_t a1, uint64_t a2)
{
  uint64_t v4 = *(void *)(a2 - 8);
  MEMORY[0x270FA5388](a1);
  (*(void (**)(char *, uint64_t, uint64_t))(v4 + 16))((char *)v7 - ((v5 + 15) & 0xFFFFFFFFFFFFFFF0), a1, a2);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  (*(void (**)(uint64_t, uint64_t))(v4 + 8))(a1, a2);
  return *(double *)&v7[1];
}

double SPAngle.init<A>(degrees:)(uint64_t a1, uint64_t a2)
{
  uint64_t v4 = *(void *)(a2 - 8);
  MEMORY[0x270FA5388](a1);
  (*(void (**)(char *, uint64_t, uint64_t))(v4 + 16))((char *)v8 - ((v5 + 15) & 0xFFFFFFFFFFFFFFF0), a1, a2);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  double v6 = SPAngleMakeWithDegrees(v8[1]);
  (*(void (**)(uint64_t, uint64_t))(v4 + 8))(a1, a2);
  return v6;
}

double SPRotationAxis3D.init(_:)(__n128 a1)
{
  return SPRotationAxis3D.init(_:)((void (*)(double *__return_ptr, _OWORD *))SPRotationAxis3DMakeWithVector, a1);
}

double SPRotationAxis3D.init<A>(x:y:z:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return SPRotationAxis3D.init<A>(x:y:z:)(a1, a2, a3, a4, a5, (void (*)(void *__return_ptr, uint64_t, double, double, double))SPSize3DMake);
}

double SPPoint3D.init(_:)(__n128 a1)
{
  return SPRotationAxis3D.init(_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1);
}

double SPPoint3D.init<A>(x:y:z:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return SPRotationAxis3D.init<A>(x:y:z:)(a1, a2, a3, a4, a5, (void (*)(void *__return_ptr, uint64_t, double, double, double))SPSize3DMake);
}

double SPVector3D.init(_:)(__n128 a1)
{
  return SPRotationAxis3D.init(_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1);
}

double SPVector3D.init<A>(x:y:z:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return SPRotationAxis3D.init<A>(x:y:z:)(a1, a2, a3, a4, a5, (void (*)(void *__return_ptr, uint64_t, double, double, double))SPSize3DMake);
}

double SIMD3<>.init(_:)(double a1, double a2)
{
  *(float *)&unsigned int v2 = a1;
  *(float *)&unsigned int v3 = a2;
  return COERCE_DOUBLE(__PAIR64__(v3, v2));
}

double SPSize3D.init(_:)(__n128 a1)
{
  return SPRotationAxis3D.init(_:)((void (*)(double *__return_ptr, _OWORD *))SPSize3DMakeWithVector, a1);
}

double SPRotationAxis3D.init(_:)(void (*a1)(double *__return_ptr, _OWORD *), __n128 a2)
{
  v4[0]  = vcvtq_f64_f32((float32x2_t)a2.n128_u64[0]);
  v4[1]  = COERCE_UNSIGNED_INT64(a2.n128_f32[2]);
  a1(&v3, v4);
  return v3;
}

double SPSize3D.init<A>(width:height:depth:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return SPRotationAxis3D.init<A>(x:y:z:)(a1, a2, a3, a4, a5, (void (*)(void *__return_ptr, uint64_t, double, double, double))SPSize3DMake);
}

double SPRotationAxis3D.init<A>(x:y:z:)(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5, void (*a6)(void *__return_ptr, uint64_t, double, double, double))
{
  void (*v20)(void *__return_ptr, uint64_t, double, double, double);
  void v21[8];

  uint64_t v19 = a1;
  double v20 = a6;
  uint64_t v9 = *(void *)(a4 - 8);
  MEMORY[0x270FA5388](a1);
  float64x2_t v11 = (char *)&v19 - ((v10 + 15) & 0xFFFFFFFFFFFFFFF0);
  int8x16_t v12 = *(void (**)(char *))(v9 + 16);
  v12(v11);
  lazy protocol witness table accessor for type Double and conformance Double();
  BinaryFloatingPoint.init<A>(_:)();
  double v13 = *(double *)&v21[7];
  ((void (*)(char *, uint64_t, uint64_t))v12)(v11, a2, a4);
  BinaryFloatingPoint.init<A>(_:)();
  double v14 = *(double *)&v21[6];
  ((void (*)(char *, uint64_t, uint64_t))v12)(v11, a3, a4);
  uint64_t v15 = BinaryFloatingPoint.init<A>(_:)();
  v20(v21, v15, v13, v14, *(double *)&v21[5]);
  double v16 = *(double *)v21;
  double v17 = *(void (**)(uint64_t, uint64_t))(v9 + 8);
  v17(a3, a4);
  v17(a2, a4);
  v17(v19, a4);
  return v16;
}

double SPRect3D.init(origin:size:)@<D0>(uint64_t a1@<X8>, __n128 a2@<Q0>, __n128 a3@<Q1>)
{
  return SPRect3D.init(origin:size:)((void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRect3DMakeAtOriginWithVector, a1, a2, a3);
}

double SPRect3D.init(center:size:)@<D0>(uint64_t a1@<X8>, __n128 a2@<Q0>, __n128 a3@<Q1>)
{
  return SPRect3D.init(origin:size:)((void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRect3DMakeAtCenterWithVector, a1, a2, a3);
}

double SPRect3D.init(origin:size:)@<D0>(void (*a1)(_OWORD *__return_ptr, _OWORD *, _OWORD *)@<X0>, uint64_t a2@<X8>, __n128 a3@<Q0>, __n128 a4@<Q1>)
{
  v14[0]  = vcvtq_f64_f32((float32x2_t)a3.n128_u64[0]);
  v14[1]  = COERCE_UNSIGNED_INT64(a3.n128_f32[2]);
  v13[0]  = vcvtq_f64_f32((float32x2_t)a4.n128_u64[0]);
  v13[1]  = COERCE_UNSIGNED_INT64(a4.n128_f32[2]);
  a1(v10, v14, v13);
  double result = *(double *)v10;
  long long v6 = v10[1];
  long long v7 = v10[2];
  uint64_t v8 = v11;
  uint64_t v9 = v12;
  *(_OWORD *)a2  = v10[0];
  *(_OWORD *)(a2 + 16)  = v6;
  *(void *)(a2 + 48)  = v8;
  *(void *)(a2 + 56)  = v9;
  *(_OWORD *)(a2 + 32)  = v7;
  return result;
}

void __swiftcall SPAffineTransform3D.init(truncating:)(SPAffineTransform3D *__return_ptr retstr, simd_float4x4 *truncating)
{
  v15[0]  = vcvtq_f64_f32(*(float32x2_t *)v2.f32);
  v15[1]  = vcvt_hight_f64_f32(v2);
  v15[2]  = vcvtq_f64_f32(*(float32x2_t *)v3.f32);
  v15[3]  = vcvt_hight_f64_f32(v3);
  v15[4]  = vcvtq_f64_f32(*(float32x2_t *)v4.f32);
  v15[5]  = vcvt_hight_f64_f32(v4);
  v15[6]  = vcvtq_f64_f32(*(float32x2_t *)v5.f32);
  v15[7]  = vcvt_hight_f64_f32(v5);
  SPAffineTransform3DMakeWithTruncated4x4Matrix((uint64_t)v15, v14);
  long long v7 = v14[1];
  long long v8 = v14[2];
  long long v9 = v14[3];
  long long v10 = v14[4];
  long long v11 = v14[5];
  long long v12 = v14[6];
  long long v13 = v14[7];
  *(_OWORD *)retstr->matrix.columns[0].f64  = v14[0];
  *(_OWORD *)&retstr->matrix.columns[0].f64[2]  = v7;
  *(_OWORD *)retstr->matrix.columns[1].f64  = v8;
  *(_OWORD *)&retstr->matrix.columns[1].f64[2]  = v9;
  *(_OWORD *)retstr->matrix.columns[2].f64  = v10;
  *(_OWORD *)&retstr->matrix.columns[2].f64[2]  = v11;
  *(_OWORD *)retstr->matrix.columns[3].f64  = v12;
  *(_OWORD *)&retstr->matrix.columns[3].f64[2]  = v13;
}

double specialized SPAffineTransform3D.init(_:)@<D0>(float32x2_t *a1@<X0>, _OWORD *a2@<X8>)
{
  *(double *)&unint64_t v3 = a1[1].f32[0];
  *(double *)&unint64_t v4 = a1[3].f32[0];
  *(double *)&unint64_t v5 = COERCE_FLOAT(*(void *)&a1[5]);
  *(double *)&unint64_t v6 = COERCE_FLOAT(*(void *)&a1[7]);
  float64x2_t v7 = vcvtq_f64_f32(a1[2]);
  float64x2_t v8 = vcvtq_f64_f32(a1[4]);
  float64x2_t v9 = vcvtq_f64_f32(a1[6]);
  v19[0]  = vcvtq_f64_f32(*a1);
  v19[1]  = v3;
  v19[2]  = v7;
  v19[3]  = v4;
  v19[4]  = v8;
  v19[5]  = v5;
  v19[6]  = v9;
  v19[7]  = v6;
  SPProjectiveTransform3DMakeWith4x4Matrix((uint64_t)v19, (uint64_t)v18);
  double result = *(double *)v18;
  long long v11 = v18[1];
  long long v12 = v18[2];
  long long v13 = v18[3];
  long long v14 = v18[4];
  long long v15 = v18[5];
  long long v16 = v18[6];
  long long v17 = v18[7];
  *a2  = v18[0];
  a2[1]  = v11;
  a2[2]  = v12;
  a2[3]  = v13;
  a2[4]  = v14;
  a2[5]  = v15;
  a2[6]  = v16;
  a2[7]  = v17;
  return result;
}

double specialized SPProjectiveTransform3D.init(_:)@<D0>(_OWORD *a1@<X8>, float32x4_t a2@<Q0>, float32x4_t a3@<Q1>, float32x4_t a4@<Q2>, float32x4_t a5@<Q3>)
{
  v15[0]  = vcvtq_f64_f32(*(float32x2_t *)a2.f32);
  v15[1]  = vcvt_hight_f64_f32(a2);
  v15[2]  = vcvtq_f64_f32(*(float32x2_t *)a3.f32);
  v15[3]  = vcvt_hight_f64_f32(a3);
  v15[4]  = vcvtq_f64_f32(*(float32x2_t *)a4.f32);
  v15[5]  = vcvt_hight_f64_f32(a4);
  v15[6]  = vcvtq_f64_f32(*(float32x2_t *)a5.f32);
  v15[7]  = vcvt_hight_f64_f32(a5);
  SPProjectiveTransform3DMakeWith4x4Matrix((uint64_t)v15, (uint64_t)v14);
  double result = *(double *)v14;
  long long v7 = v14[1];
  long long v8 = v14[2];
  long long v9 = v14[3];
  long long v10 = v14[4];
  long long v11 = v14[5];
  long long v12 = v14[6];
  long long v13 = v14[7];
  *a1  = v14[0];
  a1[1]  = v7;
  a1[2]  = v8;
  a1[3]  = v9;
  a1[4]  = v10;
  a1[5]  = v11;
  a1[6]  = v12;
  a1[7]  = v13;
  return result;
}

float specialized simd_float4x3.init(_:)@<S0>(float64x2_t *a1@<X0>, _OWORD *a2@<X8>)
{
  *(float32x2_t *)&long long v3 = vcvt_f32_f64(*a1);
  *(float *)&unsigned int v2 = a1[1].f64[0];
  *((void *)&v3 + 1)  = v2;
  *(float32x2_t *)&long long v4 = vcvt_f32_f64(a1[2]);
  *(float *)&unsigned int v5 = a1[3].f64[0];
  *((void *)&v4 + 1)  = v5;
  *(float32x2_t *)&long long v6 = vcvt_f32_f64(a1[4]);
  *(float *)&unsigned int v7 = a1[5].f64[0];
  *((void *)&v6 + 1)  = v7;
  *(float32x2_t *)&long long v8 = vcvt_f32_f64(a1[6]);
  *(float *)&unsigned int v9 = a1[7].f64[0];
  *((void *)&v8 + 1)  = v9;
  *a2  = v3;
  a2[1]  = v4;
  a2[2]  = v6;
  a2[3]  = v8;
  return *(float *)&v3;
}

unint64_t lazy protocol witness table accessor for type Double and conformance Double()
{
  unint64_t result = lazy protocol witness table cache variable for type Double and conformance Double;
  if (!lazy protocol witness table cache variable for type Double and conformance Double)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Double and conformance Double);
  }
  return result;
}

__n128 SPRotationAxis3DMakeWithVector@<Q0>(uint64_t a1@<X0>, _OWORD *a2@<X8>)
{
  __n128 result = *(__n128 *)a1;
  long long v3 = *(_OWORD *)(a1 + 16);
  *a2  = *(_OWORD *)a1;
  a2[1]  = v3;
  return result;
}

void SPSize3DMake(double a1@<D0>, double a2@<D1>, double a3@<D2>, double *a4@<X8>)
{
  *a4  = a1;
  a4[1]  = a2;
  a4[2]  = a3;
}

__n128 SPAffineTransform3DMakeWithTruncated4x4Matrix@<Q0>(uint64_t a1@<X0>, _OWORD *a2@<X8>)
{
  __n128 result = *(__n128 *)a1;
  long long v3 = *(_OWORD *)(a1 + 16);
  long long v4 = *(_OWORD *)(a1 + 32);
  long long v5 = *(_OWORD *)(a1 + 48);
  long long v6 = *(_OWORD *)(a1 + 64);
  long long v7 = *(_OWORD *)(a1 + 80);
  long long v8 = *(_OWORD *)(a1 + 96);
  long long v9 = *(_OWORD *)(a1 + 112);
  *a2  = *(_OWORD *)a1;
  a2[1]  = v3;
  a2[2]  = v4;
  a2[3]  = v5;
  a2[4]  = v6;
  a2[5]  = v7;
  a2[6]  = v8;
  a2[7]  = v9;
  return result;
}

__n128 SPProjectiveTransform3DMakeWith4x4Matrix@<Q0>(uint64_t a1@<X0>, uint64_t a2@<X8>)
{
  long long v2 = *(_OWORD *)(a1 + 80);
  *(_OWORD *)(a2 + 64)  = *(_OWORD *)(a1 + 64);
  *(_OWORD *)(a2 + 80)  = v2;
  long long v3 = *(_OWORD *)(a1 + 112);
  *(_OWORD *)(a2 + 96)  = *(_OWORD *)(a1 + 96);
  *(_OWORD *)(a2 + 112)  = v3;
  long long v4 = *(_OWORD *)(a1 + 16);
  *(_OWORD *)a2  = *(_OWORD *)a1;
  *(_OWORD *)(a2 + 16)  = v4;
  __n128 result = *(__n128 *)(a1 + 32);
  long long v6 = *(_OWORD *)(a1 + 48);
  *(__n128 *)(a2 + 32)  = result;
  *(_OWORD *)(a2 + 48)  = v6;
  return result;
}

void __swiftcall SPRay3D.applying(_:)(SPRay3D *__return_ptr retstr, SPPose3D *a2)
{
  SPRay3D.applying(_:)((long long *)a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DApplyPose, (uint64_t)retstr);
}

void __swiftcall SPRay3D.init(origin:direction:)(SPRay3D *__return_ptr retstr, SPPoint3D *origin, SPVector3D *direction)
{
  v18.x  = v3;
  v18.y  = v4.f64[0];
  v18.double z = v5;
  v17.x  = v6;
  v17.y  = v7;
  v17.double z = v8;
  SPRay3DMake(&v18, &v17, v14, v4);
  long long v10 = v14[1];
  long long v11 = v14[2];
  double v12 = v15;
  double v13 = v16;
  *(_OWORD *)&retstr->origin.x  = v14[0];
  *(_OWORD *)&retstr->origin.vector.f64[2]  = v10;
  retstr->direction.double z = v12;
  retstr->direction.vector.f64[3]  = v13;
  *(_OWORD *)&retstr->direction.x  = v11;
}

float64x2_t SPRay3DMake@<Q0>(SPPoint3D *a1@<X0>, SPVector3D *a2@<X1>, _OWORD *a3@<X8>, float64x2_t a4@<Q1>)
{
  a4.f64[0]  = a2->z;
  int64x2_t v4 = vceqzq_f64(*(float64x2_t *)&a2->x);
  unint64_t v5 = vandq_s8((int8x16_t)vdupq_laneq_s64(v4, 1), vandq_s8((int8x16_t)vceqzq_f64(a4), (int8x16_t)v4)).u64[0];
  long long v6 = *(_OWORD *)&a1->vector.f64[2];
  *a3  = *(_OWORD *)&a1->x;
  a3[1]  = v6;
  if ((v5 & 0x8000000000000000) != 0)
  {
    float64x2_t result = *(float64x2_t *)&a2->x;
    long long v10 = *(_OWORD *)&a2->vector.f64[2];
    a3[2]  = *(_OWORD *)&a2->x;
    a3[3]  = v10;
  }
  else
  {
    float64x2_t v7 = *(float64x2_t *)&a2->vector.f64[2];
    float64x2_t v8 = vmulq_f64(v7, v7);
    v8.f64[0]  = 1.0 / sqrt(v8.f64[0] + vaddvq_f64(vmulq_f64(*(float64x2_t *)&a2->x, *(float64x2_t *)&a2->x)));
    float64x2_t result = vmulq_n_f64(*(float64x2_t *)&a2->x, v8.f64[0]);
    a3[2]  = result;
    *((void *)a3 + 6)  = *(_OWORD *)&vmulq_f64(v7, v8);
  }
  return result;
}

double SPRay3D.init(origin:direction:)@<D0>(uint64_t a1@<X8>, __n128 a2@<Q0>, __n128 a3@<Q1>, float64x2_t a4@<Q2>, float64x2_t a5@<Q3>)
{
  v15[0]  = a2;
  v15[1]  = a3;
  v14[0]  = a4;
  v14[1]  = a5;
  SPRay3DMakeWithVector(v15, v14, (uint64_t)v11);
  double result = *(double *)v11;
  long long v7 = v11[1];
  long long v8 = v11[2];
  uint64_t v9 = v12;
  uint64_t v10 = v13;
  *(_OWORD *)a1  = v11[0];
  *(_OWORD *)(a1 + 16)  = v7;
  *(void *)(a1 + 48)  = v9;
  *(void *)(a1 + 56)  = v10;
  *(_OWORD *)(a1 + 32)  = v8;
  return result;
}

float64x2_t SPRay3DMakeWithVector@<Q0>(_OWORD *a1@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>)
{
  long long v3 = a1[1];
  float64x2_t v4 = *a2;
  float64x2_t v5 = a2[1];
  *(_OWORD *)a3  = *a1;
  *(_OWORD *)(a3 + 16)  = v3;
  float64x2_t v6 = vmulq_f64(v5, v5);
  v6.f64[0]  = 1.0 / sqrt(v6.f64[0] + vaddvq_f64(vmulq_f64(v4, v4)));
  float64x2_t v7 = vmulq_n_f64(v4, v6.f64[0]);
  float64x2_t result = vmulq_f64(v5, v6);
  *(float64x2_t *)(a3 + 32)  = v7;
  *(float64x2_t *)(a3 + 48)  = result;
  return result;
}

double SPRay3D.init(origin:direction:)@<D0>(uint64_t a1@<X8>, float32x4_t a2@<Q0>, float32x4_t a3@<Q1>)
{
  v13[0]  = vcvtq_f64_f32(*(float32x2_t *)a2.f32);
  v13[1]  = vcvt_hight_f64_f32(a2);
  v12[0]  = vcvtq_f64_f32(*(float32x2_t *)a3.f32);
  v12[1]  = vcvt_hight_f64_f32(a3);
  SPRay3DMakeWithVector(v13, v12, (uint64_t)v9);
  double result = *(double *)v9;
  long long v5 = v9[1];
  long long v6 = v9[2];
  uint64_t v7 = v10;
  uint64_t v8 = v11;
  *(_OWORD *)a1  = v9[0];
  *(_OWORD *)(a1 + 16)  = v5;
  *(void *)(a1 + 48)  = v7;
  *(void *)(a1 + 56)  = v8;
  *(_OWORD *)(a1 + 32)  = v6;
  return result;
}

Swift::Bool __swiftcall SPRay3D.intersects(_:)(SPRect3D *a1)
{
  long long v2 = *(_OWORD *)&a1->origin.x;
  long long v3 = *(_OWORD *)&a1->origin.vector.f64[2];
  long long v4 = *(_OWORD *)&a1->size.width;
  depth  = a1->size.depth;
  double v6 = a1->size.vector.f64[3];
  long long v7 = *(_OWORD *)(v1 + 16);
  long long v8 = *(_OWORD *)(v1 + 32);
  double v9 = *(double *)(v1 + 48);
  double v10 = *(double *)(v1 + 56);
  *(_OWORD *)&v13.origin.x  = *(_OWORD *)v1;
  *(_OWORD *)&v13.origin.vector.f64[2]  = v7;
  v13.direction.double z = v9;
  v13.direction.vector.f64[3]  = v10;
  *(_OWORD *)&v13.direction.x  = v8;
  *(_OWORD *)&v12.origin.x  = v2;
  *(_OWORD *)&v12.origin.vector.f64[2]  = v3;
  v12.size.depth  = depth;
  v12.size.vector.f64[3]  = v6;
  *(_OWORD *)&v12.size.width  = v4;
  return SPRay3DIntersectsRect(&v13, &v12);
}

BOOL SPRay3DIntersectsRect(SPRay3D *a1, SPRect3D *a2)
{
  uint64_t v18 = *MEMORY[0x263EF8340];
  long long v2 = *(_OWORD *)&a1->origin.x;
  __asm { FMOV            V3.2D, #1.0 }
  double v8 = vdivq_f64(_Q3, *(float64x2_t *)&a1->direction.vector.f64[2]).f64[0];
  float64x2_t v9 = vdivq_f64(_Q3, *(float64x2_t *)&a1->direction.x);
  double z = a1->origin.z;
  float64x2_t v11 = *(float64x2_t *)&a2->origin.x;
  float64x2_t v12 = *(float64x2_t *)&a2->origin.vector.f64[2];
  float64x2_t v13 = *(float64x2_t *)&a2->size.width;
  long long v14 = *(_OWORD *)&a2->size.vector.f64[2];
  v17[0]  = vaddq_f64(*(float64x2_t *)&a2->origin.x, vminnmq_f64(v13, (float64x2_t)0));
  v17[1]  = vaddq_f64(v12, vminnmq_f64((float64x2_t)(unint64_t)v14, (float64x2_t)0));
  v17[2]  = vaddq_f64(v11, vmaxnmq_f64(v13, (float64x2_t)0));
  v17[3]  = vaddq_f64(v12, vmaxnmq_f64((float64x2_t)(unint64_t)v14, (float64x2_t)0));
  v11.f64[0]  = (*(double *)&v17[2 * (v9.f64[0] < 0.0)] - *(double *)&v2) * v9.f64[0];
  v12.f64[0]  = (*(double *)&v17[2 * (v9.f64[0] >= 0.0)] - *(double *)&v2) * v9.f64[0];
  v13.f64[0]  = vmuld_lane_f64(*((double *)&v17[2 * (v9.f64[1] < 0.0)] + 1) - *((double *)&v2 + 1), v9, 1);
  *(double *)&long long v2 = vmuld_lane_f64(*((double *)&v17[2 * (v9.f64[1] >= 0.0)] + 1) - *((double *)&v2 + 1), v9, 1);
  v9.f64[0]  = (*(double *)&v17[2 * (v8 < 0.0) + 1] - z) * v8;
  double v15 = (*(double *)&v17[2 * (v8 >= 0.0) + 1] - z) * v8;
  return fmin(fmin(fmax(v11.f64[0], v12.f64[0]), fmax(v13.f64[0], *(double *)&v2)), fmax(v9.f64[0], v15)) >= fmax(fmax(fmin(v11.f64[0], v12.f64[0]), fmin(v13.f64[0], *(double *)&v2)), fmin(v9.f64[0], v15));
}

Swift::Void __swiftcall SPRay3D.apply(_:)(SPPose3D *a1)
{
  long long v2 = *(_OWORD *)&a1->position.x;
  long long v3 = *(_OWORD *)&a1->position.vector.f64[2];
  long long v4 = *(_OWORD *)a1->rotation.vector.f64;
  double v5 = a1->rotation.vector.f64[2];
  double v6 = a1->rotation.vector.f64[3];
  long long v7 = *(_OWORD *)(v1 + 16);
  long long v8 = *(_OWORD *)(v1 + 32);
  double v9 = *(double *)(v1 + 48);
  double v10 = *(double *)(v1 + 56);
  *(_OWORD *)&v19.origin.x  = *(_OWORD *)v1;
  *(_OWORD *)&v19.origin.vector.f64[2]  = v7;
  v19.direction.double z = v9;
  v19.direction.vector.f64[3]  = v10;
  *(_OWORD *)&v19.direction.x  = v8;
  *(_OWORD *)&v18.position.x  = v2;
  *(_OWORD *)&v18.position.vector.f64[2]  = v3;
  v18.rotation.vector.f64[2]  = v5;
  v18.rotation.vector.f64[3]  = v6;
  *(_OWORD *)v18.rotation.vector.f64  = v4;
  SPRay3DApplyPose(&v19, &v18, v15);
  long long v11 = v15[1];
  long long v12 = v15[2];
  uint64_t v13 = v16;
  uint64_t v14 = v17;
  *(_OWORD *)uint64_t v1 = v15[0];
  *(_OWORD *)(v1 + 16)  = v11;
  *(void *)(v1 + 48)  = v13;
  *(void *)(v1 + 56)  = v14;
  *(_OWORD *)(v1 + 32)  = v12;
}

double SPRay3D.unapplying(_:)@<D0>(long long *a1@<X0>, uint64_t a2@<X8>)
{
  return SPRay3D.unapplying(_:)(a1, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DUnapplyScaledPose, a2);
}

uint64_t SPRay3DUnapplyScaledPose@<X0>(uint64_t result@<X0>, uint64_t a2@<X1>, float64x2_t *a3@<X8>, float64x2_t a4@<Q1>)
{
  long long v4 = *(_OWORD *)(result + 16);
  long long v5 = *(_OWORD *)(a2 + 16);
  float64x2_t v6 = *(float64x2_t *)(a2 + 32);
  float64x2_t v7 = *(float64x2_t *)(a2 + 48);
  a4.f64[0]  = *(float64_t *)(a2 + 64);
  float64x2_t v8 = vnegq_f64(v6);
  float64x2_t v9 = vaddq_f64(vmulq_f64(v6, v6), vmulq_f64(v7, v7));
  double v10 = 1.0 / vaddvq_f64(v9);
  float64x2_t v11 = vmulq_n_f64(vmulq_f64(v7, (float64x2_t)xmmword_228C1FC40), v10);
  float64x2_t v12 = vmulq_n_f64(v8, v10);
  float64x2_t v13 = vmulq_f64(v11, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v14 = (int8x16_t)vnegq_f64(v12);
  float64x2_t v15 = vnegq_f64(*(float64x2_t *)a2);
  float64x2_t v16 = (float64x2_t)vextq_s8((int8x16_t)v13, (int8x16_t)vnegq_f64(v13), 8uLL);
  float64x2_t v17 = (float64x2_t)vextq_s8(v14, (int8x16_t)v12, 8uLL);
  float64x2_t v18 = (float64x2_t)vextq_s8((int8x16_t)v12, v14, 8uLL);
  float64x2_t v19 = vmlsq_lane_f64(vmlsq_lane_f64(vmulq_laneq_f64(v13, v15, 1), v16, *(double *)a2, 0), v18, *(double *)&v5, 0);
  float64x2_t v20 = vmlsq_lane_f64(vmlsq_lane_f64(vmulq_laneq_f64(v12, v15, 1), v17, *(double *)a2, 0), v16, *(double *)&v5, 0);
  float64x2_t v21 = vnegq_f64(v19);
  float64x2_t v22 = (float64x2_t)vextq_s8((int8x16_t)v20, (int8x16_t)vnegq_f64(v20), 8uLL);
  float64x2_t v23 = vmlaq_n_f64(vmulq_laneq_f64(v20, v12, 1), v22, v12.f64[0]);
  float64x2_t v24 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v20, v11, 1), v22, v11.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v21, v12, 1), (float64x2_t)vextq_s8((int8x16_t)v19, (int8x16_t)v21, 8uLL), v12.f64[0]));
  float64x2_t v25 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v19, v11, 1), (float64x2_t)vextq_s8((int8x16_t)v21, (int8x16_t)v19, 8uLL), v11.f64[0]), v23);
  float64x2_t v26 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v13, *(float64x2_t *)result, 1), v16, *(double *)result), v18, *(double *)&v4);
  float64x2_t v27 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v12, *(float64x2_t *)result, 1), v17, *(double *)result), v16, *(double *)&v4);
  float64x2_t v28 = vnegq_f64(v26);
  float64x2_t v29 = (float64x2_t)vextq_s8((int8x16_t)v27, (int8x16_t)vnegq_f64(v27), 8uLL);
  float64x2_t v30 = vmlaq_n_f64(vmulq_laneq_f64(v27, v12, 1), v29, v12.f64[0]);
  float64x2_t v31 = vaddq_f64(vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v27, v11, 1), v29, v11.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v28, v12, 1), (float64x2_t)vextq_s8((int8x16_t)v26, (int8x16_t)v28, 8uLL), v12.f64[0])), v24);
  float64x2_t v32 = (float64x2_t)vdupq_lane_s64(*(uint64_t *)&a4.f64[0], 0);
  *(void *)&v31.f64[0]  = *(_OWORD *)&vdivq_f64(v31, a4);
  float64x2_t v33 = vdivq_f64(vaddq_f64(vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v26, v11, 1), (float64x2_t)vextq_s8((int8x16_t)v28, (int8x16_t)v26, 8uLL), v11.f64[0]), v30), v25), v32);
  float64x2_t v35 = *(float64x2_t *)(result + 32);
  long long v34 = *(_OWORD *)(result + 48);
  float64x2_t v36 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v12, v35, 1), v17, v35.f64[0]), v16, *(double *)&v34);
  float64x2_t v37 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v13, v35, 1), v16, v35.f64[0]), v18, *(double *)&v34);
  float64x2_t v38 = vnegq_f64(v37);
  float64x2_t v39 = (float64x2_t)vextq_s8((int8x16_t)v36, (int8x16_t)vnegq_f64(v36), 8uLL);
  float64x2_t v40 = vdivq_f64(vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v36, v11, 1), v39, v11.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v38, v12, 1), (float64x2_t)vextq_s8((int8x16_t)v37, (int8x16_t)v38, 8uLL), v12.f64[0])), a4);
  float64x2_t v42 = vdivq_f64(vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v37, v11, 1), (float64x2_t)vextq_s8((int8x16_t)v38, (int8x16_t)v37, 8uLL), v11.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v36, v12, 1), v39, v12.f64[0])), v32);
  int64x2_t v41 = vceqzq_f64(v42);
  *a3  = v33;
  a3[1].f64[0]  = v31.f64[0];
  *(void *)&v42.f64[1]  = vextq_s8((int8x16_t)v42, (int8x16_t)v42, 8uLL).u64[0];
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v41, 1), vandq_s8((int8x16_t)vceqzq_f64(v40), (int8x16_t)v41)).u64[0] & 0x8000000000000000) != 0)
  {
    a3[2]  = v42;
    a3[3]  = v40;
  }
  else
  {
    v9.f64[0]  = 1.0 / sqrt(vmulq_f64(v40, v40).f64[0] + vaddvq_f64(vmulq_f64(v42, v42)));
    a3[2]  = vmulq_n_f64(v42, v9.f64[0]);
    *(void *)&a3[3].f64[0]  = *(_OWORD *)&vmulq_f64(v40, v9);
  }
  return result;
}

double SPRay3D.applying(_:)@<D0>(long long *a1@<X0>, uint64_t a2@<X8>)
{
  return SPRay3D.unapplying(_:)(a1, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DApplyScaledPose, a2);
}

double SPRay3D.unapplying(_:)@<D0>(long long *a1@<X0>, void (*a2)(_OWORD *__return_ptr, _OWORD *, _OWORD *)@<X1>, uint64_t a3@<X8>)
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  uint64_t v8 = *((void *)a1 + 6);
  uint64_t v9 = *((void *)a1 + 7);
  uint64_t v10 = *((void *)a1 + 8);
  long long v11 = *(_OWORD *)(v3 + 16);
  long long v12 = *(_OWORD *)(v3 + 32);
  uint64_t v13 = *(void *)(v3 + 48);
  uint64_t v14 = *(void *)(v3 + 56);
  v27[0]  = *(_OWORD *)v3;
  v27[1]  = v11;
  uint64_t v28 = v13;
  uint64_t v29 = v14;
  v27[2]  = v12;
  v23[0]  = v5;
  v23[1]  = v6;
  uint64_t v24 = v8;
  uint64_t v25 = v9;
  v23[2]  = v7;
  uint64_t v26 = v10;
  a2(v20, v27, v23);
  double result = *(double *)v20;
  long long v16 = v20[1];
  long long v17 = v20[2];
  uint64_t v18 = v21;
  uint64_t v19 = v22;
  *(_OWORD *)a3  = v20[0];
  *(_OWORD *)(a3 + 16)  = v16;
  *(void *)(a3 + 48)  = v18;
  *(void *)(a3 + 56)  = v19;
  *(_OWORD *)(a3 + 32)  = v17;
  return result;
}

{
  uint64_t v3;
  long long v5;
  long long v6;
  long long v7;
  long long v8;
  long long v9;
  long long v10;
  long long v11;
  long long v12;
  long long v13;
  long long v14;
  uint64_t v15;
  uint64_t v16;
  double result;
  long long v18;
  long long v19;
  uint64_t v20;
  uint64_t v21;
  _OWORD v22[3];
  uint64_t v23;
  uint64_t v24;
  _OWORD v25[8];
  _OWORD v26[3];
  uint64_t v27;
  uint64_t v28;

  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  uint64_t v8 = a1[3];
  uint64_t v9 = a1[4];
  uint64_t v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  uint64_t v13 = *(_OWORD *)(v3 + 16);
  uint64_t v14 = *(_OWORD *)(v3 + 32);
  float64x2_t v15 = *(void *)(v3 + 48);
  long long v16 = *(void *)(v3 + 56);
  v26[0]  = *(_OWORD *)v3;
  v26[1]  = v13;
  float64x2_t v27 = v15;
  uint64_t v28 = v16;
  v26[2]  = v14;
  v25[0]  = v5;
  v25[1]  = v6;
  v25[2]  = v7;
  v25[3]  = v8;
  double v25[4] = v9;
  v25[5]  = v10;
  v25[6]  = v11;
  v25[7]  = v12;
  a2(v22, v26, v25);
  double result = *(double *)v22;
  uint64_t v18 = v22[1];
  uint64_t v19 = v22[2];
  float64x2_t v20 = v23;
  uint64_t v21 = v24;
  *(_OWORD *)a3  = v22[0];
  *(_OWORD *)(a3 + 16)  = v18;
  *(void *)(a3 + 48)  = v20;
  *(void *)(a3 + 56)  = v21;
  *(_OWORD *)(a3 + 32)  = v19;
  return result;
}

__n128 SPRay3DApplyScaledPose@<Q0>(uint64_t a1@<X0>, float64x2_t *a2@<X1>, _OWORD *a3@<X8>)
{
  float64x2_t v6 = *(float64x2_t *)(a1 + 16);
  float64x2_t v8 = *(float64x2_t *)(a1 + 32);
  float64x2_t v7 = *(float64x2_t *)(a1 + 48);
  _Q6  = a2[2];
  _Q5  = a2[3];
  _D7  = a2[2].f64[1];
  __asm { FMLS            D3, D5, V5.D[0] }
  _Q17.f64[0]  = a2[3].f64[1];
  __asm { FMLA            D3, D17, V5.D[1] }
  v5.f64[0]  = vmlad_n_f64(vmuld_lane_f64(_Q5.f64[0], _Q5, 1), _D7, _Q6.f64[0]);
  v18.f64[0]  = vmuld_lane_f64(_D7, _Q5, 1);
  double v19 = vmlad_n_f64(-(_D7 * _Q17.f64[0]), _Q5.f64[0], _Q6.f64[0]);
  _Q3.f64[1]  = v5.f64[0] + v5.f64[0];
  v5.f64[0]  = v19 + v19;
  double v20 = vmlad_n_f64(-(_Q5.f64[0] * _Q17.f64[0]), _D7, _Q6.f64[0]);
  v21.f64[0]  = v20 + v20;
  __asm
  {
    FMLA            D16, D7, V6.D[1]
    FMLA            D16, D17, V5.D[1]
    FMLS            D16, D6, V6.D[0]
    FMLA            D21, D5, V6.D[1]
  }
  v21.f64[1]  = _Q16.f64[0];
  _Q16.f64[0]  = _D21 + _D21;
  float64_t v23 = -(_Q6.f64[0] * _Q17.f64[0]);
  float64x2_t v24 = (float64x2_t)vzip1q_s64((int64x2_t)_Q6, (int64x2_t)_Q5);
  __asm
  {
    FMLS            D17, D6, V6.D[0]
    FMLS            D17, D7, V6.D[1]
  }
  _Q6.f64[0]  = a2[3].f64[0];
  v18.f64[1]  = v23;
  float64x2_t v25 = vmlaq_f64(v18, v24, _Q6);
  float64x2_t v26 = vaddq_f64(v25, v25);
  float64x2_t v27 = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)a1, v5), _Q16, *(float64x2_t *)a1, 1), v6, _Q17);
  float64x2_t v28 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q3, *(double *)a1), v21, *(float64x2_t *)a1, 1), v26, v6.f64[0]), (float64x2_t)0);
  unint64_t v29 = vextq_s8((int8x16_t)v28, (int8x16_t)v28, 8uLL).u64[0];
  float64x2_t v30 = vaddq_f64(v27, (float64x2_t)0);
  float64x2_t v31 = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v8, v5), _Q16, v8, 1), v7, _Q17);
  float64x2_t v32 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q3, v8.f64[0]), v21, v8, 1), v26, v7.f64[0]), (float64x2_t)0);
  float64x2_t v33 = vaddq_f64(v31, (float64x2_t)0);
  float64x2_t v34 = (float64x2_t)*(unint64_t *)&v33.f64[0];
  int64x2_t v35 = vceqzq_f64(v32);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v35, 1), vandq_s8((int8x16_t)vceqzq_f64(v33), (int8x16_t)v35)).u64[0] & 0x8000000000000000) == 0)
  {
    float64x2_t v36 = vmulq_f64((float64x2_t)*(unint64_t *)&v33.f64[0], (float64x2_t)*(unint64_t *)&v33.f64[0]);
    v36.f64[0]  = 1.0 / sqrt(v36.f64[0] + vaddvq_f64(vmulq_f64(v32, v32)));
    *(void *)&v32.f64[1]  = vextq_s8((int8x16_t)v32, (int8x16_t)v32, 8uLL).u64[0];
    float64x2_t v32 = vmulq_n_f64(v32, v36.f64[0]);
    float64x2_t v34 = (float64x2_t)(unint64_t)*(_OWORD *)&vmulq_f64(v33, v36);
  }
  *(void *)&v28.f64[1]  = v29;
  *(float64x2_t *)a1  = v28;
  *(float64_t *)(a1 + 16)  = v30.f64[0];
  *(void *)(a1 + 24)  = 0;
  *(float64x2_t *)(a1 + 32)  = v32;
  *(float64x2_t *)(a1 + 48)  = v34;
  v30.f64[0]  = a2[4].f64[0];
  float64x2_t v37 = vmulq_n_f64(v28, v30.f64[0]);
  uint64_t v38 = *(_OWORD *)&vmulq_f64(*(float64x2_t *)(a1 + 16), v30);
  *(float64x2_t *)a1  = v37;
  *(void *)(a1 + 16)  = v38;
  float64x2_t v39 = a2[1];
  float64x2_t v40 = *(float64x2_t *)(a1 + 16);
  int64x2_t v41 = vceqzq_f64(v32);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v41, 1), vandq_s8((int8x16_t)vceqzq_f64(v34), (int8x16_t)v41)).u64[0] & 0x8000000000000000) == 0)
  {
    float64x2_t v42 = vmulq_f64(v34, v34);
    v42.f64[0]  = 1.0 / sqrt(v42.f64[0] + vaddvq_f64(vmulq_f64(v32, v32)));
    float64x2_t v32 = vmulq_n_f64(v32, v42.f64[0]);
    float64x2_t v34 = (float64x2_t)(unint64_t)*(_OWORD *)&vmulq_f64(v34, v42);
  }
  *(float64x2_t *)a1  = vaddq_f64(*a2, v37);
  *(_OWORD *)(a1 + 16)  = (unint64_t)*(_OWORD *)&vaddq_f64(v39, v40);
  *(float64x2_t *)(a1 + 32)  = v32;
  *(float64x2_t *)(a1 + 48)  = v34;
  long long v43 = *(_OWORD *)(a1 + 48);
  a3[2]  = *(_OWORD *)(a1 + 32);
  a3[3]  = v43;
  __n128 result = *(__n128 *)a1;
  long long v45 = *(_OWORD *)(a1 + 16);
  *a3  = *(_OWORD *)a1;
  a3[1]  = v45;
  return result;
}

void __swiftcall SPRay3D.applying(_:)(SPRay3D *__return_ptr retstr, SPAffineTransform3D *a2)
{
  SPRay3D.applying(_:)((long long *)a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DApplyAffineTransform, (uint64_t)retstr);
}

float64x2_t *SPRay3DApplyAffineTransform@<X0>(float64x2_t *result@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v3 = result[1];
  float64x2_t v4 = a2[1];
  float64x2_t v5 = a2[2];
  float64x2_t v6 = a2[3];
  float64x2_t v8 = a2[4];
  float64x2_t v7 = a2[5];
  float64x2_t v9 = a2[6];
  float64x2_t v10 = a2[7];
  uint64_t v11 = *(_OWORD *)&vaddq_f64(v10, vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*result, v4), v6, *result, 1), v3, v7));
  float64x2_t v12 = vaddq_f64(v9, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, result->f64[0]), v5, *result, 1), v8, v3.f64[0]));
  float64x2_t v14 = result[2];
  float64x2_t v13 = result[3];
  float64x2_t v15 = vmlaq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v4, v14), v6, v14, 1), v13, v7), (float64x2_t)0, v10);
  float64x2_t v17 = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, v14.f64[0]), v5, v14, 1), v8, v13.f64[0]), (float64x2_t)0, v9);
  int64x2_t v16 = vceqzq_f64(v17);
  *(float64x2_t *)a3  = v12;
  *(void *)(a3 + 16)  = v11;
  *(void *)&v17.f64[1]  = vextq_s8((int8x16_t)v17, (int8x16_t)v17, 8uLL).u64[0];
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v16, 1), vandq_s8((int8x16_t)vceqzq_f64(v15), (int8x16_t)v16)).u64[0] & 0x8000000000000000) != 0)
  {
    *(float64x2_t *)(a3 + 32)  = v17;
    *(float64x2_t *)(a3 + 48)  = v15;
  }
  else
  {
    float64x2_t v18 = vmulq_f64(v17, v17);
    v18.f64[0]  = 1.0 / sqrt(vmulq_f64(v15, v15).f64[0] + vaddvq_f64(v18));
    *(float64x2_t *)(a3 + 32)  = vmulq_n_f64(v17, v18.f64[0]);
    *(void *)(a3 + 48)  = *(_OWORD *)&vmulq_f64(v15, v18);
  }
  return result;
}

{
  float64x2_t v3;
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  uint64_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  int64x2_t v16;
  float64x2_t v17;
  float64x2_t v18;

  float64x2_t v3 = result[1];
  float64x2_t v4 = a2[1];
  float64x2_t v5 = a2[2];
  float64x2_t v6 = a2[3];
  float64x2_t v8 = a2[4];
  float64x2_t v7 = a2[5];
  float64x2_t v9 = a2[6];
  float64x2_t v10 = a2[7];
  uint64_t v11 = *(_OWORD *)&vaddq_f64(v10, vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*result, v4), v6, *result, 1), v3, v7));
  float64x2_t v12 = vaddq_f64(v9, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, result->f64[0]), v5, *result, 1), v8, v3.f64[0]));
  float64x2_t v14 = result[2];
  float64x2_t v13 = result[3];
  float64x2_t v15 = vmlaq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v4, v14), v6, v14, 1), v13, v7), (float64x2_t)0, v10);
  float64x2_t v17 = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, v14.f64[0]), v5, v14, 1), v8, v13.f64[0]), (float64x2_t)0, v9);
  int64x2_t v16 = vceqzq_f64(v17);
  *(float64x2_t *)a3  = v12;
  *(void *)(a3 + 16)  = v11;
  *(void *)&v17.f64[1]  = vextq_s8((int8x16_t)v17, (int8x16_t)v17, 8uLL).u64[0];
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v16, 1), vandq_s8((int8x16_t)vceqzq_f64(v15), (int8x16_t)v16)).u64[0] & 0x8000000000000000) != 0)
  {
    *(float64x2_t *)(a3 + 32)  = v17;
    *(float64x2_t *)(a3 + 48)  = v15;
  }
  else
  {
    float64x2_t v18 = vmulq_f64(v17, v17);
    v18.f64[0]  = 1.0 / sqrt(vmulq_f64(v15, v15).f64[0] + vaddvq_f64(v18));
    *(float64x2_t *)(a3 + 32)  = vmulq_n_f64(v17, v18.f64[0]);
    *(void *)(a3 + 48)  = *(_OWORD *)&vmulq_f64(v15, v18);
  }
  return result;
}

void __swiftcall SPRay3D.unapplying(_:)(SPRay3D *__return_ptr retstr, SPAffineTransform3D *a2)
{
  SPRay3D.unapplying(_:)((long long *)a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DUnapplyAffineTransform, (uint64_t)retstr);
}

float64x2_t *SPRay3DUnapplyAffineTransform@<X0>(float64x2_t *result@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v4 = result;
  float64x2_t v7 = *result;
  float64x2_t v6 = result[1];
  v8.f64[0]  = *(float64_t *)(a2 + 80);
  v8.f64[1]  = *(float64_t *)(a2 + 64);
  v9.f64[0]  = *(float64_t *)(a2 + 48);
  v9.f64[1]  = *(float64_t *)(a2 + 32);
  double v10 = vmulq_f64(*(float64x2_t *)(a2 + 16), vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)(a2 + 64), vnegq_f64((float64x2_t)vdupq_laneq_s64(*(int64x2_t *)(a2 + 32), 1))), *(float64x2_t *)(a2 + 32), *(float64x2_t *)(a2 + 64), 1)).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)a2, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 64), *(int8x16_t *)(a2 + 80), 8uLL), vnegq_f64(v9)), v8, (float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 32), *(int8x16_t *)(a2 + 48), 8uLL))));
  float64x2_t v11 = (float64x2_t)xmmword_228C1FC60;
  float64x2_t v12 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if (v10 == 0.0)
  {
    float64x2_t v13 = v12;
    float64x2_t v14 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v15 = v12;
    float64x2_t v16 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v17 = v12;
    float64x2_t v18 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v20 = v12;
    float64x2_t v19 = (float64x2_t)xmmword_228C1FC60;
  }
  else
  {
    float64x2_t v39 = *(float64x2_t *)(a2 + 96);
    float64x2_t v41 = *(float64x2_t *)(a2 + 112);
    float64x2_t v47 = v12;
    float64x2_t v43 = *result;
    float64x2_t v45 = result[1];
    __n128 result = (float64x2_t *)__invert_d3();
    float64x2_t v13 = 0u;
    float64x2_t v14 = 0u;
    float64x2_t v15 = 0u;
    float64x2_t v16 = 0u;
    float64x2_t v17 = 0u;
    float64x2_t v18 = 0u;
    float64x2_t v19 = vnegq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v39, (float64x2_t)0), (float64x2_t)0, v39, 1), v41, (float64x2_t)0));
    float64x2_t v20 = vnegq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v39.f64[0]), (float64x2_t)0, v39, 1), (float64x2_t)0, v41.f64[0]));
    v21.f64[0]  = *(float64_t *)(a2 + 80);
    v21.f64[1]  = *(float64_t *)(a2 + 64);
    v22.f64[0]  = *(float64_t *)(a2 + 48);
    v22.f64[1]  = *(float64_t *)(a2 + 32);
    float64x2_t v7 = v43;
    float64x2_t v6 = v45;
    double v10 = vmulq_f64(*(float64x2_t *)(a2 + 16), vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)(a2 + 64), vnegq_f64((float64x2_t)vdupq_laneq_s64(*(int64x2_t *)(a2 + 32), 1))), *(float64x2_t *)(a2 + 32), *(float64x2_t *)(a2 + 64), 1)).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)a2, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 64), *(int8x16_t *)(a2 + 80), 8uLL), vnegq_f64(v22)), v21, (float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 32), *(int8x16_t *)(a2 + 48), 8uLL))));
    float64x2_t v12 = v47;
    float64x2_t v11 = (float64x2_t)xmmword_228C1FC60;
  }
  int8x16_t v23 = (int8x16_t)vaddq_f64(v20, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v13, v7.f64[0]), v15, v7, 1), v17, v6.f64[0]));
  uint64_t v24 = *(_OWORD *)&vaddq_f64(v19, vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v7, v14), v16, v7, 1), v6, v18));
  unint64_t v25 = vextq_s8(v23, v23, 8uLL).u64[0];
  float64x2_t v27 = v4[2];
  float64x2_t v26 = v4[3];
  float64x2_t v28 = v12;
  float64x2_t v29 = v11;
  float64x2_t v30 = v12;
  float64x2_t v31 = v11;
  float64x2_t v32 = v12;
  float64x2_t v33 = v11;
  if (v10 != 0.0)
  {
    float64x2_t v38 = *(float64x2_t *)(a2 + 96);
    float64x2_t v40 = *(float64x2_t *)(a2 + 112);
    uint64_t v48 = v24;
    uint64_t v49 = v23.i64[0];
    float64x2_t v44 = v4[3];
    unint64_t v46 = v25;
    float64x2_t v42 = v4[2];
    __n128 result = (float64x2_t *)__invert_d3();
    float64x2_t v27 = v42;
    float64x2_t v26 = v44;
    unint64_t v25 = v46;
    uint64_t v24 = v48;
    v23.i64[0]  = v49;
    float64x2_t v12 = 0u;
    float64x2_t v11 = 0u;
    float64x2_t v28 = 0u;
    float64x2_t v29 = 0u;
    float64x2_t v30 = 0u;
    float64x2_t v31 = 0u;
    float64x2_t v33 = vnegq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v38, (float64x2_t)0), (float64x2_t)0, v38, 1), v40, (float64x2_t)0));
    float64x2_t v32 = vnegq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v38.f64[0]), (float64x2_t)0, v38, 1), (float64x2_t)0, v40.f64[0]));
  }
  float64x2_t v36 = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v12, v27.f64[0]), v28, v27, 1), v30, v26.f64[0]), (float64x2_t)0, v32);
  float64x2_t v34 = vmlaq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v27, v11), v29, v27, 1), v26, v31), (float64x2_t)0, v33);
  int64x2_t v35 = vceqzq_f64(v36);
  v23.i64[1]  = v25;
  *(int8x16_t *)a3  = v23;
  *(void *)(a3 + 16)  = v24;
  *(void *)&v36.f64[1]  = vextq_s8((int8x16_t)v36, (int8x16_t)v36, 8uLL).u64[0];
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v35, 1), vandq_s8((int8x16_t)vceqzq_f64(v34), (int8x16_t)v35)).u64[0] & 0x8000000000000000) != 0)
  {
    *(float64x2_t *)(a3 + 32)  = v36;
    *(float64x2_t *)(a3 + 48)  = v34;
  }
  else
  {
    float64x2_t v37 = vmulq_f64(v36, v36);
    v37.f64[0]  = 1.0 / sqrt(vmulq_f64(v34, v34).f64[0] + vaddvq_f64(v37));
    *(float64x2_t *)(a3 + 32)  = vmulq_n_f64(v36, v37.f64[0]);
    *(void *)(a3 + 48)  = *(_OWORD *)&vmulq_f64(v34, v37);
  }
  return result;
}

{
  float64x2_t *v4;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  double v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  float64x2_t v17;
  float64x2_t v18;
  float64x2_t v19;
  float64x2_t v20;
  float64x2_t v21;
  float64x2_t v22;
  int8x16_t v23;
  uint64_t v24;
  unint64_t v25;
  float64x2_t v26;
  float64x2_t v27;
  float64x2_t v28;
  float64x2_t v29;
  float64x2_t v30;
  float64x2_t v31;
  float64x2_t v32;
  float64x2_t v33;
  float64x2_t v34;
  int64x2_t v35;
  float64x2_t v36;
  float64x2_t v37;
  float64x2_t v38;
  float64x2_t v39;
  float64x2_t v40;
  float64x2_t v41;
  float64x2_t v42;
  float64x2_t v43;
  float64x2_t v44;
  float64x2_t v45;
  unint64_t v46;
  float64x2_t v47;
  uint64_t v48;
  uint64_t v49;

  float64x2_t v4 = result;
  float64x2_t v7 = *result;
  float64x2_t v6 = result[1];
  v8.f64[0]  = *(float64_t *)(a2 + 80);
  v8.f64[1]  = *(float64_t *)(a2 + 64);
  v9.f64[0]  = *(float64_t *)(a2 + 48);
  v9.f64[1]  = *(float64_t *)(a2 + 32);
  double v10 = vmulq_f64(*(float64x2_t *)(a2 + 16), vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)(a2 + 64), vnegq_f64((float64x2_t)vdupq_laneq_s64(*(int64x2_t *)(a2 + 32), 1))), *(float64x2_t *)(a2 + 32), *(float64x2_t *)(a2 + 64), 1)).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)a2, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 64), *(int8x16_t *)(a2 + 80), 8uLL), vnegq_f64(v9)), v8, (float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 32), *(int8x16_t *)(a2 + 48), 8uLL))));
  float64x2_t v11 = (float64x2_t)xmmword_228C1FC60;
  float64x2_t v12 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if (v10 == 0.0)
  {
    float64x2_t v13 = v12;
    float64x2_t v14 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v15 = v12;
    float64x2_t v16 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v17 = v12;
    float64x2_t v18 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v20 = v12;
    float64x2_t v19 = (float64x2_t)xmmword_228C1FC60;
  }
  else
  {
    float64x2_t v39 = *(float64x2_t *)(a2 + 96);
    float64x2_t v41 = *(float64x2_t *)(a2 + 112);
    float64x2_t v47 = v12;
    float64x2_t v43 = *result;
    float64x2_t v45 = result[1];
    __n128 result = (float64x2_t *)__invert_d3();
    float64x2_t v13 = 0u;
    float64x2_t v14 = 0u;
    float64x2_t v15 = 0u;
    float64x2_t v16 = 0u;
    float64x2_t v17 = 0u;
    float64x2_t v18 = 0u;
    float64x2_t v19 = vnegq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v39, (float64x2_t)0), (float64x2_t)0, v39, 1), v41, (float64x2_t)0));
    float64x2_t v20 = vnegq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v39.f64[0]), (float64x2_t)0, v39, 1), (float64x2_t)0, v41.f64[0]));
    v21.f64[0]  = *(float64_t *)(a2 + 80);
    v21.f64[1]  = *(float64_t *)(a2 + 64);
    v22.f64[0]  = *(float64_t *)(a2 + 48);
    v22.f64[1]  = *(float64_t *)(a2 + 32);
    float64x2_t v7 = v43;
    float64x2_t v6 = v45;
    double v10 = vmulq_f64(*(float64x2_t *)(a2 + 16), vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)(a2 + 64), vnegq_f64((float64x2_t)vdupq_laneq_s64(*(int64x2_t *)(a2 + 32), 1))), *(float64x2_t *)(a2 + 32), *(float64x2_t *)(a2 + 64), 1)).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)a2, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 64), *(int8x16_t *)(a2 + 80), 8uLL), vnegq_f64(v22)), v21, (float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 32), *(int8x16_t *)(a2 + 48), 8uLL))));
    float64x2_t v12 = v47;
    float64x2_t v11 = (float64x2_t)xmmword_228C1FC60;
  }
  int8x16_t v23 = (int8x16_t)vaddq_f64(v20, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v13, v7.f64[0]), v15, v7, 1), v17, v6.f64[0]));
  uint64_t v24 = *(_OWORD *)&vaddq_f64(v19, vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v7, v14), v16, v7, 1), v6, v18));
  unint64_t v25 = vextq_s8(v23, v23, 8uLL).u64[0];
  float64x2_t v27 = v4[2];
  float64x2_t v26 = v4[3];
  float64x2_t v28 = v12;
  float64x2_t v29 = v11;
  float64x2_t v30 = v12;
  float64x2_t v31 = v11;
  float64x2_t v32 = v12;
  float64x2_t v33 = v11;
  if (v10 != 0.0)
  {
    float64x2_t v38 = *(float64x2_t *)(a2 + 96);
    float64x2_t v40 = *(float64x2_t *)(a2 + 112);
    uint64_t v48 = v24;
    uint64_t v49 = v23.i64[0];
    float64x2_t v44 = v4[3];
    unint64_t v46 = v25;
    float64x2_t v42 = v4[2];
    __n128 result = (float64x2_t *)__invert_d3();
    float64x2_t v27 = v42;
    float64x2_t v26 = v44;
    unint64_t v25 = v46;
    uint64_t v24 = v48;
    v23.i64[0]  = v49;
    float64x2_t v12 = 0u;
    float64x2_t v11 = 0u;
    float64x2_t v28 = 0u;
    float64x2_t v29 = 0u;
    float64x2_t v30 = 0u;
    float64x2_t v31 = 0u;
    float64x2_t v33 = vnegq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v38, (float64x2_t)0), (float64x2_t)0, v38, 1), v40, (float64x2_t)0));
    float64x2_t v32 = vnegq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v38.f64[0]), (float64x2_t)0, v38, 1), (float64x2_t)0, v40.f64[0]));
  }
  float64x2_t v36 = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v12, v27.f64[0]), v28, v27, 1), v30, v26.f64[0]), (float64x2_t)0, v32);
  float64x2_t v34 = vmlaq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v27, v11), v29, v27, 1), v26, v31), (float64x2_t)0, v33);
  int64x2_t v35 = vceqzq_f64(v36);
  v23.i64[1]  = v25;
  *(int8x16_t *)a3  = v23;
  *(void *)(a3 + 16)  = v24;
  *(void *)&v36.f64[1]  = vextq_s8((int8x16_t)v36, (int8x16_t)v36, 8uLL).u64[0];
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v35, 1), vandq_s8((int8x16_t)vceqzq_f64(v34), (int8x16_t)v35)).u64[0] & 0x8000000000000000) != 0)
  {
    *(float64x2_t *)(a3 + 32)  = v36;
    *(float64x2_t *)(a3 + 48)  = v34;
  }
  else
  {
    float64x2_t v37 = vmulq_f64(v36, v36);
    v37.f64[0]  = 1.0 / sqrt(vmulq_f64(v34, v34).f64[0] + vaddvq_f64(v37));
    *(float64x2_t *)(a3 + 32)  = vmulq_n_f64(v36, v37.f64[0]);
    *(void *)(a3 + 48)  = *(_OWORD *)&vmulq_f64(v34, v37);
  }
  return result;
}

void __swiftcall SPRay3D.applying(_:)(SPRay3D *__return_ptr retstr, SPProjectiveTransform3D *a2)
{
  SPRay3D.applying(_:)((long long *)a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DApplyProjectiveTransform, (uint64_t)retstr);
}

double SPRay3D.applying(_:)@<D0>(long long *a1@<X0>, void (*a2)(_OWORD *__return_ptr, _OWORD *, _OWORD *)@<X1>, uint64_t a3@<X8>)
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  long long v13 = *(_OWORD *)(v3 + 16);
  long long v14 = *(_OWORD *)(v3 + 32);
  uint64_t v15 = *(void *)(v3 + 48);
  uint64_t v16 = *(void *)(v3 + 56);
  v26[0]  = *(_OWORD *)v3;
  v26[1]  = v13;
  uint64_t v27 = v15;
  uint64_t v28 = v16;
  v26[2]  = v14;
  v25[0]  = v5;
  v25[1]  = v6;
  v25[2]  = v7;
  v25[3]  = v8;
  double v25[4] = v9;
  v25[5]  = v10;
  v25[6]  = v11;
  v25[7]  = v12;
  a2(v22, v26, v25);
  double result = *(double *)v22;
  long long v18 = v22[1];
  long long v19 = v22[2];
  uint64_t v20 = v23;
  uint64_t v21 = v24;
  *(_OWORD *)a3  = v22[0];
  *(_OWORD *)(a3 + 16)  = v18;
  *(void *)(a3 + 48)  = v20;
  *(void *)(a3 + 56)  = v21;
  *(_OWORD *)(a3 + 32)  = v19;
  return result;
}

{
  uint64_t v3;
  long long v5;
  long long v6;
  long long v7;
  uint64_t v8;
  uint64_t v9;
  long long v10;
  long long v11;
  uint64_t v12;
  uint64_t v13;
  double result;
  long long v15;
  long long v16;
  uint64_t v17;
  uint64_t v18;
  _OWORD v19[3];
  uint64_t v20;
  uint64_t v21;
  _OWORD v22[3];
  uint64_t v23;
  uint64_t v24;
  _OWORD v25[3];
  uint64_t v26;
  uint64_t v27;

  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = *((void *)a1 + 6);
  long long v9 = *((void *)a1 + 7);
  long long v10 = *(_OWORD *)(v3 + 16);
  long long v11 = *(_OWORD *)(v3 + 32);
  long long v12 = *(void *)(v3 + 48);
  long long v13 = *(void *)(v3 + 56);
  v25[0]  = *(_OWORD *)v3;
  v25[1]  = v10;
  float64x2_t v26 = v12;
  uint64_t v27 = v13;
  v25[2]  = v11;
  v22[0]  = v5;
  v22[1]  = v6;
  uint64_t v23 = v8;
  uint64_t v24 = v9;
  v22[2]  = v7;
  a2(v19, v25, v22);
  double result = *(double *)v19;
  uint64_t v15 = v19[1];
  uint64_t v16 = v19[2];
  float64x2_t v17 = v20;
  long long v18 = v21;
  *(_OWORD *)a3  = v19[0];
  *(_OWORD *)(a3 + 16)  = v15;
  *(void *)(a3 + 48)  = v17;
  *(void *)(a3 + 56)  = v18;
  *(_OWORD *)(a3 + 32)  = v16;
  return result;
}

uint64_t SPRay3DApplyProjectiveTransform@<X0>(uint64_t result@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>)
{
  long long v3 = *(_OWORD *)(result + 16);
  float64x2_t v4 = a2[1];
  float64x2_t v5 = a2[2];
  float64x2_t v6 = a2[3];
  float64x2_t v8 = a2[4];
  float64x2_t v7 = a2[5];
  float64x2_t v9 = a2[6];
  float64x2_t v10 = a2[7];
  uint64_t v11 = *(_OWORD *)&vaddq_f64(v10, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v4, *(double *)result), v6, *(float64x2_t *)result, 1), v7, *(double *)&v3));
  float64x2_t v12 = vaddq_f64(v9, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, *(double *)result), v5, *(float64x2_t *)result, 1), v8, *(double *)&v3));
  float64x2_t v14 = *(float64x2_t *)(result + 32);
  long long v13 = *(_OWORD *)(result + 48);
  float64x2_t v15 = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, v14.f64[0]), v5, v14, 1), v8, *(double *)&v13), (float64x2_t)0, v9);
  float64x2_t v16 = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v4, v14.f64[0]), v6, v14, 1), v7, *(double *)&v13), (float64x2_t)0, v10);
  int64x2_t v17 = vceqzq_f64(v15);
  *(float64x2_t *)a3  = v12;
  *(void *)(a3 + 16)  = v11;
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v17, 1), vandq_s8((int8x16_t)vceqzq_f64(v16), (int8x16_t)v17)).u64[0] & 0x8000000000000000) != 0)
  {
    *(float64x2_t *)(a3 + 32)  = v15;
    *(float64x2_t *)(a3 + 48)  = v16;
  }
  else
  {
    float64x2_t v18 = vmulq_f64(v16, v16);
    v18.f64[0]  = 1.0 / sqrt(v18.f64[0] + vaddvq_f64(vmulq_f64(v15, v15)));
    *(void *)&v15.f64[1]  = vextq_s8((int8x16_t)v15, (int8x16_t)v15, 8uLL).u64[0];
    *(float64x2_t *)(a3 + 32)  = vmulq_n_f64(v15, v18.f64[0]);
    *(void *)(a3 + 48)  = *(_OWORD *)&vmulq_f64(v16, v18);
  }
  return result;
}

{
  long long v3;
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  uint64_t v11;
  float64x2_t v12;
  long long v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  int64x2_t v17;
  float64x2_t v18;

  long long v3 = *(_OWORD *)(result + 16);
  float64x2_t v4 = a2[1];
  float64x2_t v5 = a2[2];
  float64x2_t v6 = a2[3];
  float64x2_t v8 = a2[4];
  float64x2_t v7 = a2[5];
  float64x2_t v9 = a2[6];
  float64x2_t v10 = a2[7];
  uint64_t v11 = *(_OWORD *)&vaddq_f64(v10, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v4, *(double *)result), v6, *(float64x2_t *)result, 1), v7, *(double *)&v3));
  float64x2_t v12 = vaddq_f64(v9, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, *(double *)result), v5, *(float64x2_t *)result, 1), v8, *(double *)&v3));
  float64x2_t v14 = *(float64x2_t *)(result + 32);
  long long v13 = *(_OWORD *)(result + 48);
  float64x2_t v15 = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, v14.f64[0]), v5, v14, 1), v8, *(double *)&v13), (float64x2_t)0, v9);
  float64x2_t v16 = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v4, v14.f64[0]), v6, v14, 1), v7, *(double *)&v13), (float64x2_t)0, v10);
  int64x2_t v17 = vceqzq_f64(v15);
  *(float64x2_t *)a3  = v12;
  *(void *)(a3 + 16)  = v11;
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v17, 1), vandq_s8((int8x16_t)vceqzq_f64(v16), (int8x16_t)v17)).u64[0] & 0x8000000000000000) != 0)
  {
    *(float64x2_t *)(a3 + 32)  = v15;
    *(float64x2_t *)(a3 + 48)  = v16;
  }
  else
  {
    float64x2_t v18 = vmulq_f64(v16, v16);
    v18.f64[0]  = 1.0 / sqrt(v18.f64[0] + vaddvq_f64(vmulq_f64(v15, v15)));
    *(void *)&v15.f64[1]  = vextq_s8((int8x16_t)v15, (int8x16_t)v15, 8uLL).u64[0];
    *(float64x2_t *)(a3 + 32)  = vmulq_n_f64(v15, v18.f64[0]);
    *(void *)(a3 + 48)  = *(_OWORD *)&vmulq_f64(v16, v18);
  }
  return result;
}

void __swiftcall SPRay3D.unapplying(_:)(SPRay3D *__return_ptr retstr, SPProjectiveTransform3D *a2)
{
  SPRay3D.unapplying(_:)((long long *)a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DUnapplyProjectiveTransform, (uint64_t)retstr);
}

float64x2_t *SPRay3DUnapplyProjectiveTransform@<X0>(float64x2_t *result@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>)
{
  long long v3 = result;
  float64x2_t v6 = *result;
  float64x2_t v5 = result[1];
  float64x2_t v7 = a2[2];
  float64x2_t v8 = a2[3];
  float64x2_t v9 = a2[4];
  float64x2_t v10 = a2[5];
  float64x2_t v11 = a2[6];
  float64x2_t v12 = a2[7];
  float64x2_t v13 = (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v8, 8uLL);
  float64x2_t v14 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)v7, 8uLL);
  float64x2_t v15 = (float64x2_t)vextq_s8((int8x16_t)v11, (int8x16_t)v12, 8uLL);
  float64x2_t v16 = (float64x2_t)vextq_s8((int8x16_t)v12, (int8x16_t)v11, 8uLL);
  float64x2_t v17 = (float64x2_t)vextq_s8((int8x16_t)v9, (int8x16_t)v10, 8uLL);
  float64x2_t v18 = (float64x2_t)vextq_s8((int8x16_t)v10, (int8x16_t)v9, 8uLL);
  float64x2_t v19 = vnegq_f64(v18);
  float64x2_t v20 = vnegq_f64(v17);
  float64x2_t v21 = vmlaq_f64(vmulq_f64(v11, v20), v15, v9);
  float64x2_t v22 = vmlaq_f64(vmulq_f64(v8, vmlaq_f64(vmulq_f64(v16, v20), v15, v18)), vmlaq_f64(vmulq_f64(v12, v19), v16, v10), v13);
  int64x2_t v23 = (int64x2_t)vmulq_f64(a2[1], vmlaq_f64(vmlaq_f64(vmulq_f64(v7, vmlaq_f64(vmulq_f64(v15, v19), v16, v17)), v21, v14), vmlaq_f64(vmulq_f64(v16, vnegq_f64(v9)), v11, v18), v13));
  int64x2_t v24 = (int64x2_t)vmulq_f64(*a2, vmlaq_f64(v22, vmlaq_f64(vmulq_f64(v15, vnegq_f64(v10)), v12, v17), v14));
  double v25 = vaddvq_f64(vsubq_f64((float64x2_t)vzip1q_s64(v24, v23), (float64x2_t)vzip2q_s64(v24, v23)));
  float64x2_t v26 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if (v25 == 0.0)
  {
    float64x2_t v28 = v26;
    float64x2_t v29 = v26;
    float64x2_t v31 = v26;
    float64x2_t v30 = v26;
    float64x2_t v32 = v26;
    float64x2_t v33 = v26;
    float64x2_t v35 = v26;
    float64x2_t v34 = v26;
  }
  else
  {
    v75  = *result;
    vars0  = result[1];
    v73  = v26;
    double result = (float64x2_t *)__invert_d4();
    float64x2_t v28 = 0u;
    float64x2_t v29 = 0u;
    float64x2_t v31 = 0u;
    float64x2_t v30 = 0u;
    float64x2_t v32 = 0u;
    float64x2_t v33 = 0u;
    float64x2_t v35 = 0u;
    float64x2_t v34 = 0u;
    float64x2_t v36 = a2[2];
    float64x2_t v37 = a2[3];
    float64x2_t v38 = a2[4];
    float64x2_t v39 = a2[5];
    float64x2_t v40 = a2[6];
    float64x2_t v41 = a2[7];
    float64x2_t v42 = (float64x2_t)vextq_s8((int8x16_t)v36, (int8x16_t)v37, 8uLL);
    float64x2_t v43 = (float64x2_t)vextq_s8((int8x16_t)v37, (int8x16_t)v36, 8uLL);
    float64x2_t v44 = (float64x2_t)vextq_s8((int8x16_t)v40, (int8x16_t)v41, 8uLL);
    float64x2_t v45 = (float64x2_t)vextq_s8((int8x16_t)v41, (int8x16_t)v40, 8uLL);
    float64x2_t v46 = (float64x2_t)vextq_s8((int8x16_t)v38, (int8x16_t)v39, 8uLL);
    float64x2_t v47 = (float64x2_t)vextq_s8((int8x16_t)v39, (int8x16_t)v38, 8uLL);
    float64x2_t v48 = vnegq_f64(v47);
    float64x2_t v49 = vnegq_f64(v46);
    float64x2_t v50 = vmlaq_f64(vmulq_f64(v41, v48), v45, v39);
    float64x2_t v51 = vmlaq_f64(vmulq_f64(v36, vmlaq_f64(vmulq_f64(v44, v48), v45, v46)), vmlaq_f64(vmulq_f64(v40, v49), v44, v38), v43);
    float64x2_t v52 = vmlaq_f64(vmulq_f64(v37, vmlaq_f64(vmulq_f64(v45, v49), v44, v47)), v50, v42);
    int64x2_t v53 = (int64x2_t)vmulq_f64(a2[1], vmlaq_f64(v51, vmlaq_f64(vmulq_f64(v45, vnegq_f64(v38)), v40, v47), v42));
    int64x2_t v54 = (int64x2_t)vmulq_f64(*a2, vmlaq_f64(v52, vmlaq_f64(vmulq_f64(v44, vnegq_f64(v39)), v41, v46), v43));
    float64x2_t v5 = (float64x2_t)vzip1q_s64(v54, v53);
    float64x2_t v55 = (float64x2_t)vzip2q_s64(v54, v53);
    float64x2_t v26 = v73;
    float64x2_t v6 = v75;
    float64x2_t v56 = vsubq_f64(v5, v55);
    v5.f64[0]  = vars0.f64[0];
    double v25 = vaddvq_f64(v56);
  }
  float64x2_t v57 = vaddq_f64(v35, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v28, v6.f64[0]), v31, v6, 1), v32, v5.f64[0]));
  uint64_t v58 = *(_OWORD *)&vaddq_f64(v34, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v29, v6.f64[0]), v30, v6, 1), v33, v5.f64[0]));
  float64x2_t v60 = v3[2];
  float64x2_t v59 = v3[3];
  if (v25 == 0.0)
  {
    float64x2_t v61 = v26;
    float64x2_t v62 = v26;
    float64x2_t v63 = v26;
    v65  = v26;
    v64  = v26;
    float64x2_t v66 = v26;
    float64x2_t v67 = v26;
  }
  else
  {
    v76  = v58;
    vars0a  = v57;
    float64x2_t v72 = v3[2];
    v74  = v3[3];
    double result = (float64x2_t *)__invert_d4();
    float64x2_t v60 = v72;
    v59.f64[0]  = v74.f64[0];
    uint64_t v58 = v76;
    float64x2_t v57 = vars0a;
    float64x2_t v26 = 0u;
    float64x2_t v61 = 0u;
    float64x2_t v62 = 0u;
    float64x2_t v63 = 0u;
    v65  = 0u;
    v64  = 0u;
    float64x2_t v66 = 0u;
    float64x2_t v67 = 0u;
  }
  float64x2_t v68 = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v61, v60.f64[0]), v63, v60, 1), v64, v59.f64[0]), (float64x2_t)0, v67);
  float64x2_t v69 = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v26, v60.f64[0]), v62, v60, 1), v65, v59.f64[0]), (float64x2_t)0, v66);
  int64x2_t v70 = vceqzq_f64(v69);
  *(float64x2_t *)a3  = v57;
  *(void *)(a3 + 16)  = v58;
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v70, 1), vandq_s8((int8x16_t)vceqzq_f64(v68), (int8x16_t)v70)).u64[0] & 0x8000000000000000) != 0)
  {
    *(float64x2_t *)(a3 + 32)  = v69;
    *(float64x2_t *)(a3 + 48)  = v68;
  }
  else
  {
    float64x2_t v71 = vmulq_f64(v68, v68);
    v71.f64[0]  = 1.0 / sqrt(v71.f64[0] + vaddvq_f64(vmulq_f64(v69, v69)));
    *(void *)&v69.f64[1]  = vextq_s8((int8x16_t)v69, (int8x16_t)v69, 8uLL).u64[0];
    *(float64x2_t *)(a3 + 32)  = vmulq_n_f64(v69, v71.f64[0]);
    *(void *)(a3 + 48)  = *(_OWORD *)&vmulq_f64(v68, v71);
  }
  return result;
}

{
  float64x2_t *v3;
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  float64x2_t v17;
  float64x2_t v18;
  float64x2_t v19;
  float64x2_t v20;
  float64x2_t v21;
  float64x2_t v22;
  int64x2_t v23;
  int64x2_t v24;
  double v25;
  float64x2_t v26;
  float64x2_t v28;
  float64x2_t v29;
  float64x2_t v30;
  float64x2_t v31;
  float64x2_t v32;
  float64x2_t v33;
  float64x2_t v34;
  float64x2_t v35;
  float64x2_t v36;
  float64x2_t v37;
  float64x2_t v38;
  float64x2_t v39;
  float64x2_t v40;
  float64x2_t v41;
  float64x2_t v42;
  float64x2_t v43;
  float64x2_t v44;
  float64x2_t v45;
  float64x2_t v46;
  float64x2_t v47;
  float64x2_t v48;
  float64x2_t v49;
  float64x2_t v50;
  float64x2_t v51;
  float64x2_t v52;
  int64x2_t v53;
  int64x2_t v54;
  float64x2_t v55;
  float64x2_t v56;
  float64x2_t v57;
  uint64_t v58;
  float64x2_t v59;
  float64x2_t v60;
  float64x2_t v61;
  float64x2_t v62;
  float64x2_t v63;
  float64x2_t v64;
  float64x2_t v65;
  float64x2_t v66;
  float64x2_t v67;
  float64x2_t v68;
  float64x2_t v69;
  int64x2_t v70;
  float64x2_t v71;
  float64x2_t v72;
  float64x2_t v73;
  float64x2_t v74;
  float64x2_t v75;
  uint64_t v76;
  float64x2_t vars0;
  float64x2_t vars0a;

  long long v3 = result;
  float64x2_t v6 = *result;
  float64x2_t v5 = result[1];
  float64x2_t v7 = a2[2];
  float64x2_t v8 = a2[3];
  float64x2_t v9 = a2[4];
  float64x2_t v10 = a2[5];
  float64x2_t v11 = a2[6];
  float64x2_t v12 = a2[7];
  float64x2_t v13 = (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v8, 8uLL);
  float64x2_t v14 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)v7, 8uLL);
  float64x2_t v15 = (float64x2_t)vextq_s8((int8x16_t)v11, (int8x16_t)v12, 8uLL);
  float64x2_t v16 = (float64x2_t)vextq_s8((int8x16_t)v12, (int8x16_t)v11, 8uLL);
  float64x2_t v17 = (float64x2_t)vextq_s8((int8x16_t)v9, (int8x16_t)v10, 8uLL);
  float64x2_t v18 = (float64x2_t)vextq_s8((int8x16_t)v10, (int8x16_t)v9, 8uLL);
  float64x2_t v19 = vnegq_f64(v18);
  float64x2_t v20 = vnegq_f64(v17);
  float64x2_t v21 = vmlaq_f64(vmulq_f64(v11, v20), v15, v9);
  float64x2_t v22 = vmlaq_f64(vmulq_f64(v8, vmlaq_f64(vmulq_f64(v16, v20), v15, v18)), vmlaq_f64(vmulq_f64(v12, v19), v16, v10), v13);
  int64x2_t v23 = (int64x2_t)vmulq_f64(a2[1], vmlaq_f64(vmlaq_f64(vmulq_f64(v7, vmlaq_f64(vmulq_f64(v15, v19), v16, v17)), v21, v14), vmlaq_f64(vmulq_f64(v16, vnegq_f64(v9)), v11, v18), v13));
  int64x2_t v24 = (int64x2_t)vmulq_f64(*a2, vmlaq_f64(v22, vmlaq_f64(vmulq_f64(v15, vnegq_f64(v10)), v12, v17), v14));
  double v25 = vaddvq_f64(vsubq_f64((float64x2_t)vzip1q_s64(v24, v23), (float64x2_t)vzip2q_s64(v24, v23)));
  float64x2_t v26 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if (v25 == 0.0)
  {
    float64x2_t v28 = v26;
    float64x2_t v29 = v26;
    float64x2_t v31 = v26;
    float64x2_t v30 = v26;
    float64x2_t v32 = v26;
    float64x2_t v33 = v26;
    float64x2_t v35 = v26;
    float64x2_t v34 = v26;
  }
  else
  {
    v75  = *result;
    vars0  = result[1];
    v73  = v26;
    double result = (float64x2_t *)__invert_d4();
    float64x2_t v28 = 0u;
    float64x2_t v29 = 0u;
    float64x2_t v31 = 0u;
    float64x2_t v30 = 0u;
    float64x2_t v32 = 0u;
    float64x2_t v33 = 0u;
    float64x2_t v35 = 0u;
    float64x2_t v34 = 0u;
    float64x2_t v36 = a2[2];
    float64x2_t v37 = a2[3];
    float64x2_t v38 = a2[4];
    float64x2_t v39 = a2[5];
    float64x2_t v40 = a2[6];
    float64x2_t v41 = a2[7];
    float64x2_t v42 = (float64x2_t)vextq_s8((int8x16_t)v36, (int8x16_t)v37, 8uLL);
    float64x2_t v43 = (float64x2_t)vextq_s8((int8x16_t)v37, (int8x16_t)v36, 8uLL);
    float64x2_t v44 = (float64x2_t)vextq_s8((int8x16_t)v40, (int8x16_t)v41, 8uLL);
    float64x2_t v45 = (float64x2_t)vextq_s8((int8x16_t)v41, (int8x16_t)v40, 8uLL);
    float64x2_t v46 = (float64x2_t)vextq_s8((int8x16_t)v38, (int8x16_t)v39, 8uLL);
    float64x2_t v47 = (float64x2_t)vextq_s8((int8x16_t)v39, (int8x16_t)v38, 8uLL);
    float64x2_t v48 = vnegq_f64(v47);
    float64x2_t v49 = vnegq_f64(v46);
    float64x2_t v50 = vmlaq_f64(vmulq_f64(v41, v48), v45, v39);
    float64x2_t v51 = vmlaq_f64(vmulq_f64(v36, vmlaq_f64(vmulq_f64(v44, v48), v45, v46)), vmlaq_f64(vmulq_f64(v40, v49), v44, v38), v43);
    float64x2_t v52 = vmlaq_f64(vmulq_f64(v37, vmlaq_f64(vmulq_f64(v45, v49), v44, v47)), v50, v42);
    int64x2_t v53 = (int64x2_t)vmulq_f64(a2[1], vmlaq_f64(v51, vmlaq_f64(vmulq_f64(v45, vnegq_f64(v38)), v40, v47), v42));
    int64x2_t v54 = (int64x2_t)vmulq_f64(*a2, vmlaq_f64(v52, vmlaq_f64(vmulq_f64(v44, vnegq_f64(v39)), v41, v46), v43));
    float64x2_t v5 = (float64x2_t)vzip1q_s64(v54, v53);
    float64x2_t v55 = (float64x2_t)vzip2q_s64(v54, v53);
    float64x2_t v26 = v73;
    float64x2_t v6 = v75;
    float64x2_t v56 = vsubq_f64(v5, v55);
    v5.f64[0]  = vars0.f64[0];
    double v25 = vaddvq_f64(v56);
  }
  float64x2_t v57 = vaddq_f64(v35, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v28, v6.f64[0]), v31, v6, 1), v32, v5.f64[0]));
  uint64_t v58 = *(_OWORD *)&vaddq_f64(v34, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v29, v6.f64[0]), v30, v6, 1), v33, v5.f64[0]));
  float64x2_t v60 = v3[2];
  float64x2_t v59 = v3[3];
  if (v25 == 0.0)
  {
    float64x2_t v61 = v26;
    float64x2_t v62 = v26;
    float64x2_t v63 = v26;
    v65  = v26;
    v64  = v26;
    float64x2_t v66 = v26;
    float64x2_t v67 = v26;
  }
  else
  {
    v76  = v58;
    vars0a  = v57;
    float64x2_t v72 = v3[2];
    v74  = v3[3];
    double result = (float64x2_t *)__invert_d4();
    float64x2_t v60 = v72;
    v59.f64[0]  = v74.f64[0];
    uint64_t v58 = v76;
    float64x2_t v57 = vars0a;
    float64x2_t v26 = 0u;
    float64x2_t v61 = 0u;
    float64x2_t v62 = 0u;
    float64x2_t v63 = 0u;
    v65  = 0u;
    v64  = 0u;
    float64x2_t v66 = 0u;
    float64x2_t v67 = 0u;
  }
  float64x2_t v68 = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v61, v60.f64[0]), v63, v60, 1), v64, v59.f64[0]), (float64x2_t)0, v67);
  float64x2_t v69 = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v26, v60.f64[0]), v62, v60, 1), v65, v59.f64[0]), (float64x2_t)0, v66);
  int64x2_t v70 = vceqzq_f64(v69);
  *(float64x2_t *)a3  = v57;
  *(void *)(a3 + 16)  = v58;
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v70, 1), vandq_s8((int8x16_t)vceqzq_f64(v68), (int8x16_t)v70)).u64[0] & 0x8000000000000000) != 0)
  {
    *(float64x2_t *)(a3 + 32)  = v69;
    *(float64x2_t *)(a3 + 48)  = v68;
  }
  else
  {
    float64x2_t v71 = vmulq_f64(v68, v68);
    v71.f64[0]  = 1.0 / sqrt(v71.f64[0] + vaddvq_f64(vmulq_f64(v69, v69)));
    *(void *)&v69.f64[1]  = vextq_s8((int8x16_t)v69, (int8x16_t)v69, 8uLL).u64[0];
    *(float64x2_t *)(a3 + 32)  = vmulq_n_f64(v69, v71.f64[0]);
    *(void *)(a3 + 48)  = *(_OWORD *)&vmulq_f64(v68, v71);
  }
  return result;
}

void __swiftcall SPRay3D.unapplying(_:)(SPRay3D *__return_ptr retstr, SPPose3D *a2)
{
  SPRay3D.applying(_:)((long long *)a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DUnapplyPose, (uint64_t)retstr);
}

uint64_t SPRay3DUnapplyPose@<X0>(uint64_t result@<X0>, SPPose3D *a2@<X1>, float64x2_t *a3@<X8>)
{
  long long v3 = *(_OWORD *)(result + 16);
  long long v4 = *(_OWORD *)&a2->position.vector.f64[2];
  float64x2_t v5 = *(float64x2_t *)a2->rotation.vector.f64;
  float64x2_t v6 = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  float64x2_t v7 = vnegq_f64(v5);
  float64x2_t v8 = vaddq_f64(vmulq_f64(v5, v5), vmulq_f64(v6, v6));
  double v9 = 1.0 / vaddvq_f64(v8);
  _Q1  = vmulq_n_f64(vmulq_f64(v6, (float64x2_t)xmmword_228C1FC40), v9);
  _Q2  = vmulq_n_f64(v7, v9);
  float64x2_t v12 = vmulq_f64(_Q1, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v13 = (int8x16_t)vnegq_f64(_Q2);
  float64x2_t v14 = vnegq_f64(*(float64x2_t *)&a2->position.x);
  float64x2_t v15 = (float64x2_t)vextq_s8((int8x16_t)v12, (int8x16_t)vnegq_f64(v12), 8uLL);
  float64x2_t v16 = (float64x2_t)vextq_s8(v13, (int8x16_t)_Q2, 8uLL);
  float64x2_t v17 = vmulq_laneq_f64(v12, v14, 1);
  float64x2_t v18 = (float64x2_t)vextq_s8((int8x16_t)_Q2, v13, 8uLL);
  float64x2_t v19 = vmlsq_lane_f64(vmlsq_lane_f64(vmulq_laneq_f64(_Q2, v14, 1), v16, a2->position.x, 0), v15, *(double *)&v4, 0);
  float64x2_t v20 = vmlsq_lane_f64(vmlsq_lane_f64(v17, v15, a2->position.x, 0), v18, *(double *)&v4, 0);
  float64x2_t v21 = vnegq_f64(v20);
  float64x2_t v22 = (float64x2_t)vextq_s8((int8x16_t)v19, (int8x16_t)vnegq_f64(v19), 8uLL);
  float64x2_t v23 = (float64x2_t)vextq_s8((int8x16_t)v20, (int8x16_t)v21, 8uLL);
  _Q22  = vmlaq_n_f64(vmulq_laneq_f64(v19, _Q2, 1), v22, _Q2.f64[0]);
  _Q16  = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v19, _Q1, 1), v22, _Q1.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v21, _Q2, 1), v23, _Q2.f64[0]));
  _Q17  = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(_Q2, *(float64x2_t *)result, 1), v16, *(double *)result), v15, *(double *)&v3);
  float64x2_t v27 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v12, *(float64x2_t *)result, 1), v15, *(double *)result), v18, *(double *)&v3);
  float64x2_t v28 = vnegq_f64(v27);
  float64x2_t v29 = (float64x2_t)vextq_s8((int8x16_t)_Q17, (int8x16_t)vnegq_f64(_Q17), 8uLL);
  float64x2_t v30 = vmlaq_n_f64(vmulq_laneq_f64(v28, _Q2, 1), (float64x2_t)vextq_s8((int8x16_t)v27, (int8x16_t)v28, 8uLL), _Q2.f64[0]);
  _Q18  = vmlaq_n_f64(vmulq_laneq_f64(_Q17, _Q2, 1), v29, _Q2.f64[0]);
  float64x2_t v32 = (float64x2_t)vextq_s8((int8x16_t)v28, (int8x16_t)v27, 8uLL);
  float64x2_t v33 = vmulq_laneq_f64(v27, _Q1, 1);
  _Q7  = vmlaq_n_f64(vmulq_laneq_f64(_Q17, _Q1, 1), v29, _Q1.f64[0]);
  float64x2_t v35 = vaddq_f64(vmlaq_n_f64(v33, v32, _Q1.f64[0]), _Q18);
  *(void *)&v32.f64[0]  = *(_OWORD *)&vaddq_f64(vaddq_f64(_Q7, v30), _Q16);
  float64x2_t v36 = vaddq_f64(v35, vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v20, _Q1, 1), (float64x2_t)vextq_s8((int8x16_t)v21, (int8x16_t)v20, 8uLL), _Q1.f64[0]), _Q22));
  float64x2_t v38 = *(float64x2_t *)(result + 32);
  float64x2_t v37 = *(float64x2_t *)(result + 48);
  _Q16.f64[0]  = _Q2.f64[1];
  __asm { FMLS            D7, D1, V1.D[0] }
  _Q18.f64[0]  = _Q1.f64[1];
  __asm { FMLA            D7, D18, V1.D[1] }
  v30.f64[0]  = vmlad_n_f64(vmuld_lane_f64(_Q1.f64[0], _Q1, 1), _Q2.f64[1], _Q2.f64[0]);
  v20.f64[0]  = vmuld_lane_f64(_Q2.f64[1], _Q1, 1);
  v23.f64[0]  = vmlad_n_f64(-(_Q2.f64[1] * _Q1.f64[1]), _Q1.f64[0], _Q2.f64[0]);
  v23.f64[0]  = v23.f64[0] + v23.f64[0];
  _Q7.f64[1]  = v30.f64[0] + v30.f64[0];
  v30.f64[0]  = vmlad_n_f64(-(_Q1.f64[0] * _Q1.f64[1]), _Q2.f64[1], _Q2.f64[0]);
  v30.f64[0]  = v30.f64[0] + v30.f64[0];
  __asm
  {
    FMLA            D17, D16, V2.D[1]
    FMLA            D17, D18, V1.D[1]
    FMLS            D17, D2, V2.D[0]
    FMLA            D22, D1, V2.D[1]
  }
  v30.f64[1]  = _Q17.f64[0];
  _Q17.f64[0]  = -(_Q2.f64[0] * _Q1.f64[1]);
  float64x2_t v43 = (float64x2_t)vzip1q_s64((int64x2_t)_Q2, (int64x2_t)_Q1);
  __asm
  {
    FMLS            D18, D2, V2.D[0]
    FMLS            D18, D16, V2.D[1]
  }
  _Q2.f64[0]  = _Q1.f64[0];
  _Q1.f64[0]  = _Q22.f64[0] + _Q22.f64[0];
  v20.f64[1]  = _Q17.f64[0];
  float64x2_t v44 = vmlaq_f64(v20, v43, _Q2);
  float64x2_t v45 = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v38, v23), _Q1, v38, 1), v37, _Q18);
  float64x2_t v48 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q7, v38.f64[0]), v30, v38, 1), vaddq_f64(v44, v44), v37.f64[0]), (float64x2_t)0);
  float64x2_t v46 = vaddq_f64(v45, (float64x2_t)0);
  int64x2_t v47 = vceqzq_f64(v48);
  *a3  = v36;
  a3[1].f64[0]  = v32.f64[0];
  *(void *)&v48.f64[1]  = vextq_s8((int8x16_t)v48, (int8x16_t)v48, 8uLL).u64[0];
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v47, 1), vandq_s8((int8x16_t)vceqzq_f64(v46), (int8x16_t)v47)).u64[0] & 0x8000000000000000) != 0)
  {
    a3[2]  = v48;
    a3[3]  = v46;
  }
  else
  {
    v8.f64[0]  = 1.0 / sqrt(vmulq_f64(v46, v46).f64[0] + vaddvq_f64(vmulq_f64(v48, v48)));
    a3[2]  = vmulq_n_f64(v48, v8.f64[0]);
    *(void *)&a3[3].f64[0]  = *(_OWORD *)&vmulq_f64(v46, v8);
  }
  return result;
}

void __swiftcall SPRay3D.rotated(by:around:)(SPRay3D *__return_ptr retstr, SPRotation3D *by, SPPoint3D *around)
{
  long long v9 = *(_OWORD *)(v3 + 16);
  float64x2_t v10 = *(float64x2_t *)(v3 + 32);
  double v11 = *(double *)(v3 + 48);
  double v12 = *(double *)(v3 + 56);
  *(_OWORD *)&v23.origin.x  = *(_OWORD *)v3;
  *(_OWORD *)&v23.origin.vector.f64[2]  = v9;
  v23.direction.double z = v11;
  v23.direction.vector.f64[3]  = v12;
  *(float64x2_t *)&v23.direction.x  = v10;
  *(SPRotation3D *)&v22.x  = v4;
  v20.f64[0]  = v5;
  v20.f64[1]  = v6;
  uint64_t v21 = v7;
  SPRay3DRotateAroundPoint((float64x2_t *)&v23, v4, &v22, &v20, (uint64_t)v17, v10);
  long long v13 = v17[1];
  long long v14 = v17[2];
  double v15 = v18;
  double v16 = v19;
  *(_OWORD *)&retstr->origin.x  = v17[0];
  *(_OWORD *)&retstr->origin.vector.f64[2]  = v13;
  retstr->direction.double z = v15;
  retstr->direction.vector.f64[3]  = v16;
  *(_OWORD *)&retstr->direction.x  = v14;
}

float64x2_t *SPRay3DRotateAroundPoint@<X0>(float64x2_t *result@<X0>, SPRotation3D a2@<0:Q0, 16:Q1>, SPPoint3D *a3@<X1>, float64x2_t *a4@<X2>, uint64_t a5@<X8>, float64x2_t a6@<Q7>)
{
  _Q6  = *(float64x2_t *)&a3->x;
  _Q5  = *(float64x2_t *)&a3->vector.f64[2];
  float64x2_t v9 = a4[1];
  _D16  = a3->y;
  __asm { FMLS            D2, D5, V5.D[0] }
  _Q18.f64[0]  = a3->vector.f64[3];
  __asm { FMLA            D2, D18, V5.D[1] }
  a6.f64[0]  = vmlad_n_f64(vmuld_lane_f64(_Q5.f64[0], _Q5, 1), _D16, a3->x);
  float64_t v17 = a6.f64[0] + a6.f64[0];
  v18.f64[0]  = vmuld_lane_f64(_D16, _Q5, 1);
  a6.f64[0]  = vmlad_n_f64(-(_D16 * _Q18.f64[0]), _Q5.f64[0], a3->x);
  a6.f64[0]  = a6.f64[0] + a6.f64[0];
  _Q2.f64[1]  = v17;
  double v19 = vmlad_n_f64(-(_Q5.f64[0] * _Q18.f64[0]), _D16, a3->x);
  v20.f64[0]  = v19 + v19;
  __asm
  {
    FMLA            D17, D16, V6.D[1]
    FMLA            D17, D18, V5.D[1]
    FMLS            D17, D6, V6.D[0]
    FMLA            D21, D5, V6.D[1]
  }
  v20.f64[1]  = _D17;
  double v25 = -(a3->x * _Q18.f64[0]);
  float64x2_t v26 = (float64x2_t)vzip1q_s64(*(int64x2_t *)&a3->x, (int64x2_t)_Q5);
  __asm
  {
    FMLS            D18, D6, V6.D[0]
    FMLS            D18, D16, V6.D[1]
  }
  _Q6.f64[0]  = a3->z;
  _Q5.f64[0]  = _D21 + _D21;
  v18.f64[1]  = v25;
  float64x2_t v27 = vmlaq_f64(v18, v26, _Q6);
  float64x2_t v28 = vaddq_f64(v27, v27);
  float64x2_t v29 = vsubq_f64(result[1], v9);
  float64x2_t v30 = vsubq_f64(*result, *a4);
  float64x2_t v31 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q2, v30.f64[0]), v20, v30, 1), v28, v29.f64[0]);
  float64x2_t v32 = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v30, a6), _Q5, v30, 1), v29, _Q18);
  float64x2_t v33 = vaddq_f64(*a4, vaddq_f64(v31, (float64x2_t)0));
  *(void *)&v32.f64[0]  = *(_OWORD *)&vaddq_f64(v9, vaddq_f64(v32, (float64x2_t)0));
  v31.f64[0]  = result[3].f64[0];
  float64x2_t v34 = vsubq_f64(v31, v9);
  float64x2_t v35 = vsubq_f64(result[2], *a4);
  float64x2_t v36 = vaddq_f64(*a4, vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q2, v35.f64[0]), v20, v35, 1), v28, v34.f64[0]), (float64x2_t)0));
  float64x2_t v37 = vaddq_f64(v9, vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v35, a6), _Q5, v35, 1), v34, _Q18), (float64x2_t)0));
  int64x2_t v38 = vceqzq_f64(v36);
  *(float64x2_t *)a5  = v33;
  *(float64_t *)(a5 + 16)  = v32.f64[0];
  *(void *)(a5 + 24)  = 0;
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v38, 1), vandq_s8((int8x16_t)vceqzq_f64(v37), (int8x16_t)v38)).u64[0] & 0x8000000000000000) != 0)
  {
    *(float64x2_t *)(a5 + 32)  = v36;
    *(float64x2_t *)(a5 + 48)  = v37;
  }
  else
  {
    float64x2_t v39 = vmulq_f64(v37, v37);
    v39.f64[0]  = 1.0 / sqrt(v39.f64[0] + vaddvq_f64(vmulq_f64(v36, v36)));
    *(void *)&v36.f64[1]  = vextq_s8((int8x16_t)v36, (int8x16_t)v36, 8uLL).u64[0];
    *(float64x2_t *)(a5 + 32)  = vmulq_n_f64(v36, v39.f64[0]);
    *(void *)(a5 + 48)  = *(_OWORD *)&vmulq_f64(v37, v39);
  }
  return result;
}

void __swiftcall SPRay3D.rotated(by:around:)(SPRay3D *__return_ptr retstr, simd_quatd *by, SPPoint3D *around)
{
  long long v9 = *(_OWORD *)(v3 + 16);
  float64x2_t v10 = *(float64x2_t *)(v3 + 32);
  double v11 = *(double *)(v3 + 48);
  double v12 = *(double *)(v3 + 56);
  *(_OWORD *)&v23.origin.x  = *(_OWORD *)v3;
  *(_OWORD *)&v23.origin.vector.f64[2]  = v9;
  v23.direction.double z = v11;
  v23.direction.vector.f64[3]  = v12;
  *(float64x2_t *)&v23.direction.x  = v10;
  *(SPRotation3D *)&v22.x  = v4;
  v20.f64[0]  = v5;
  v20.f64[1]  = v6;
  uint64_t v21 = v7;
  SPRay3DRotateAroundPoint((float64x2_t *)&v23, v4, &v22, &v20, (uint64_t)v17, v10);
  long long v13 = v17[1];
  long long v14 = v17[2];
  double v15 = v18;
  double v16 = v19;
  *(_OWORD *)&retstr->origin.x  = v17[0];
  *(_OWORD *)&retstr->origin.vector.f64[2]  = v13;
  retstr->direction.double z = v15;
  retstr->direction.vector.f64[3]  = v16;
  *(_OWORD *)&retstr->direction.x  = v14;
}

uint64_t SPRay3D.isNaN.getter()
{
  return SPRay3D.isNaN.getter((uint64_t (*)(_OWORD *))SPRay3DIsNaN);
}

unint64_t SPRay3DIsNaN(SPRay3D *a1, double a2, float64x2_t a3)
{
  a3.f64[0]  = a1->origin.z;
  int8x16_t v3 = vorrq_s8((int8x16_t)vcltzq_f64(*(float64x2_t *)&a1->origin.x), (int8x16_t)vcgezq_f64(*(float64x2_t *)&a1->origin.x));
  float64x2_t v4 = (float64x2_t)vornq_s8(vornq_s8((int8x16_t)vdupq_laneq_s64((int64x2_t)vmvnq_s8(v3), 1), vorrq_s8((int8x16_t)vcltzq_f64(a3), (int8x16_t)vcgezq_f64(a3))), v3);
  if ((*(void *)&v4.f64[0] & 0x8000000000000000) != 0) {
    return 1;
  }
  v4.f64[0]  = a1->direction.z;
  int8x16_t v5 = vorrq_s8((int8x16_t)vcltzq_f64(*(float64x2_t *)&a1->direction.x), (int8x16_t)vcgezq_f64(*(float64x2_t *)&a1->direction.x));
  return vornq_s8(vornq_s8((int8x16_t)vdupq_laneq_s64((int64x2_t)vmvnq_s8(v5), 1), vorrq_s8((int8x16_t)vcltzq_f64(v4), (int8x16_t)vcgezq_f64(v4))), v5).u64[0] >> 63;
}

uint64_t SPRay3D.isFinite.getter()
{
  return SPRay3D.isNaN.getter((uint64_t (*)(_OWORD *))SPRay3DIsFinite);
}

unint64_t SPRay3DIsFinite(SPRay3D *a1, double a2, float64x2_t a3)
{
  float64x2_t v3 = *(float64x2_t *)&a1->origin.x;
  a3.f64[0]  = a1->origin.z;
  int8x16_t v4 = vorrq_s8((int8x16_t)vcltzq_f64(a3), (int8x16_t)vcgezq_f64(a3));
  v3.f64[0]  = INFINITY;
  int8x16_t v5 = (int8x16_t)vceqq_f64(vabsq_f64(a3), v3);
  float64x2_t v6 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  int8x16_t v7 = vbicq_s8(v4, v5);
  int64x2_t v8 = (int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(*(float64x2_t *)&a1->origin.x), (int8x16_t)vcgezq_f64(*(float64x2_t *)&a1->origin.x)), (int8x16_t)vceqq_f64(vabsq_f64(*(float64x2_t *)&a1->origin.x), v6));
  float64x2_t v9 = (float64x2_t)vandq_s8((int8x16_t)vdupq_laneq_s64(v8, 1), vandq_s8(v7, (int8x16_t)v8));
  if ((*(void *)&v9.f64[0] & 0x8000000000000000) == 0) {
    return 0;
  }
  v9.f64[0]  = a1->direction.z;
  float64x2_t v11 = *(float64x2_t *)&a1->direction.x;
  float64x2_t v12 = (float64x2_t)vcltzq_f64(v11);
  int8x16_t v13 = vorrq_s8((int8x16_t)v12, (int8x16_t)vcgezq_f64(v11));
  v12.f64[0]  = INFINITY;
  int64x2_t v14 = (int64x2_t)vbicq_s8(v13, (int8x16_t)vceqq_f64(vabsq_f64(v11), v6));
  return vandq_s8((int8x16_t)vdupq_laneq_s64(v14, 1), vandq_s8((int8x16_t)vshlq_n_s64((int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v9), (int8x16_t)vcgezq_f64(v9)), (int8x16_t)vceqq_f64(vabsq_f64(v9), v12)), 0x3FuLL), (int8x16_t)v14)).u64[0] >> 63;
}

uint64_t SPRay3D.isZero.getter()
{
  return SPRay3D.isNaN.getter((uint64_t (*)(_OWORD *))SPRay3DIsZero);
}

uint64_t SPRay3D.isNaN.getter(uint64_t (*a1)(_OWORD *))
{
  long long v2 = *(_OWORD *)(v1 + 16);
  long long v3 = *(_OWORD *)(v1 + 32);
  uint64_t v4 = *(void *)(v1 + 48);
  uint64_t v5 = *(void *)(v1 + 56);
  v7[0]  = *(_OWORD *)v1;
  v7[1]  = v2;
  uint64_t v8 = v4;
  uint64_t v9 = v5;
  v7[2]  = v3;
  return a1(v7);
}

unint64_t SPRay3DIsZero(SPRay3D *a1, double a2, float64x2_t a3)
{
  a3.f64[0]  = a1->origin.z;
  float64x2_t v3 = (float64x2_t)vceqzq_f64(a3);
  int64x2_t v4 = vceqzq_f64(*(float64x2_t *)&a1->origin.x);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v4, 1), vandq_s8((int8x16_t)v3, (int8x16_t)v4)).u64[0] & 0x8000000000000000) == 0) {
    return 0;
  }
  v3.f64[0]  = a1->direction.z;
  int64x2_t v6 = vceqzq_f64(*(float64x2_t *)&a1->direction.x);
  return vandq_s8((int8x16_t)vdupq_laneq_s64(v6, 1), vandq_s8((int8x16_t)vceqzq_f64(v3), (int8x16_t)v6)).u64[0] >> 63;
}

uint64_t protocol witness for Primitive3D.isZero.getter in conformance SPRay3D(uint64_t a1, uint64_t a2)
{
  return protocol witness for Primitive3D.isZero.getter in conformance SPRay3D(a1, a2, (uint64_t (*)(_OWORD *))SPRay3DIsZero);
}

uint64_t protocol witness for Primitive3D.isFinite.getter in conformance SPRay3D(uint64_t a1, uint64_t a2)
{
  return protocol witness for Primitive3D.isZero.getter in conformance SPRay3D(a1, a2, (uint64_t (*)(_OWORD *))SPRay3DIsFinite);
}

uint64_t protocol witness for Primitive3D.isNaN.getter in conformance SPRay3D(uint64_t a1, uint64_t a2)
{
  return protocol witness for Primitive3D.isZero.getter in conformance SPRay3D(a1, a2, (uint64_t (*)(_OWORD *))SPRay3DIsNaN);
}

uint64_t protocol witness for Primitive3D.isZero.getter in conformance SPRay3D(uint64_t a1, uint64_t a2, uint64_t (*a3)(_OWORD *))
{
  long long v4 = *(_OWORD *)(v3 + 16);
  long long v5 = *(_OWORD *)(v3 + 32);
  uint64_t v6 = *(void *)(v3 + 48);
  uint64_t v7 = *(void *)(v3 + 56);
  v9[0]  = *(_OWORD *)v3;
  v9[1]  = v4;
  uint64_t v10 = v6;
  uint64_t v11 = v7;
  v9[2]  = v5;
  return a3(v9);
}

double protocol witness for Primitive3D.applying(_:) in conformance SPRay3D@<D0>(long long *a1@<X0>, uint64_t a2@<X8>)
{
  return protocol witness for Primitive3D.applying(_:) in conformance SPRay3D(a1, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DApplyAffineTransform, a2);
}

{
  return protocol witness for Primitive3D.applying(_:) in conformance SPRay3D(a1, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DApplyProjectiveTransform, a2);
}

{
  return protocol witness for Primitive3D.applying(_:) in conformance SPRay3D(a1, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DApplyPose, a2);
}

double protocol witness for Primitive3D.apply(_:) in conformance SPRay3D(double *a1)
{
  long long v2 = *(_OWORD *)a1;
  long long v3 = *((_OWORD *)a1 + 1);
  long long v4 = *((_OWORD *)a1 + 2);
  double v5 = a1[6];
  double v6 = a1[7];
  long long v7 = *(_OWORD *)(v1 + 16);
  long long v8 = *(_OWORD *)(v1 + 32);
  double v9 = *(double *)(v1 + 48);
  double v10 = *(double *)(v1 + 56);
  *(_OWORD *)&v20.origin.x  = *(_OWORD *)v1;
  *(_OWORD *)&v20.origin.vector.f64[2]  = v7;
  v20.direction.double z = v9;
  v20.direction.vector.f64[3]  = v10;
  *(_OWORD *)&v20.direction.x  = v8;
  *(_OWORD *)&v19.position.x  = v2;
  *(_OWORD *)&v19.position.vector.f64[2]  = v3;
  v19.rotation.vector.f64[2]  = v5;
  v19.rotation.vector.f64[3]  = v6;
  *(_OWORD *)v19.rotation.vector.f64  = v4;
  SPRay3DApplyPose(&v20, &v19, v16);
  double result = *(double *)v16;
  long long v12 = v16[1];
  long long v13 = v16[2];
  uint64_t v14 = v17;
  uint64_t v15 = v18;
  *(_OWORD *)uint64_t v1 = v16[0];
  *(_OWORD *)(v1 + 16)  = v12;
  *(void *)(v1 + 48)  = v14;
  *(void *)(v1 + 56)  = v15;
  *(_OWORD *)(v1 + 32)  = v13;
  return result;
}

double protocol witness for Primitive3D.unapplying(_:) in conformance SPRay3D@<D0>(long long *a1@<X0>, uint64_t a2@<X8>)
{
  return protocol witness for Primitive3D.applying(_:) in conformance SPRay3D(a1, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DUnapplyAffineTransform, a2);
}

{
  return protocol witness for Primitive3D.applying(_:) in conformance SPRay3D(a1, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DUnapplyProjectiveTransform, a2);
}

{
  return protocol witness for Primitive3D.applying(_:) in conformance SPRay3D(a1, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DUnapplyPose, a2);
}

double protocol witness for Primitive3D.applying(_:) in conformance SPRay3D@<D0>(long long *a1@<X0>, void (*a2)(_OWORD *__return_ptr, _OWORD *, _OWORD *)@<X3>, uint64_t a3@<X8>)
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  long long v13 = *(_OWORD *)(v3 + 16);
  long long v14 = *(_OWORD *)(v3 + 32);
  uint64_t v15 = *(void *)(v3 + 48);
  uint64_t v16 = *(void *)(v3 + 56);
  v26[0]  = *(_OWORD *)v3;
  v26[1]  = v13;
  uint64_t v27 = v15;
  uint64_t v28 = v16;
  v26[2]  = v14;
  v25[0]  = v5;
  v25[1]  = v6;
  v25[2]  = v7;
  v25[3]  = v8;
  double v25[4] = v9;
  v25[5]  = v10;
  v25[6]  = v11;
  v25[7]  = v12;
  a2(v22, v26, v25);
  double result = *(double *)v22;
  long long v18 = v22[1];
  long long v19 = v22[2];
  uint64_t v20 = v23;
  uint64_t v21 = v24;
  *(_OWORD *)a3  = v22[0];
  *(_OWORD *)(a3 + 16)  = v18;
  *(void *)(a3 + 48)  = v20;
  *(void *)(a3 + 56)  = v21;
  *(_OWORD *)(a3 + 32)  = v19;
  return result;
}

{
  uint64_t v3;
  long long v5;
  long long v6;
  long long v7;
  uint64_t v8;
  uint64_t v9;
  long long v10;
  long long v11;
  uint64_t v12;
  uint64_t v13;
  double result;
  long long v15;
  long long v16;
  uint64_t v17;
  uint64_t v18;
  _OWORD v19[3];
  uint64_t v20;
  uint64_t v21;
  _OWORD v22[3];
  uint64_t v23;
  uint64_t v24;
  _OWORD v25[3];
  uint64_t v26;
  uint64_t v27;

  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = *((void *)a1 + 6);
  long long v9 = *((void *)a1 + 7);
  long long v10 = *(_OWORD *)(v3 + 16);
  long long v11 = *(_OWORD *)(v3 + 32);
  long long v12 = *(void *)(v3 + 48);
  long long v13 = *(void *)(v3 + 56);
  v25[0]  = *(_OWORD *)v3;
  v25[1]  = v10;
  float64x2_t v26 = v12;
  uint64_t v27 = v13;
  v25[2]  = v11;
  v22[0]  = v5;
  v22[1]  = v6;
  uint64_t v23 = v8;
  uint64_t v24 = v9;
  v22[2]  = v7;
  a2(v19, v25, v22);
  double result = *(double *)v19;
  uint64_t v15 = v19[1];
  uint64_t v16 = v19[2];
  uint64_t v17 = v20;
  long long v18 = v21;
  *(_OWORD *)a3  = v19[0];
  *(_OWORD *)(a3 + 16)  = v15;
  *(void *)(a3 + 48)  = v17;
  *(void *)(a3 + 56)  = v18;
  *(_OWORD *)(a3 + 32)  = v16;
  return result;
}

float64_t protocol witness for Translatable3D.translated(by:) in conformance SPRay3D@<D0>(uint64_t a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>)
{
  long long v6 = *(_OWORD *)(v4 + 16);
  long long v7 = *(_OWORD *)(v4 + 32);
  double v8 = *(double *)(v4 + 48);
  double v9 = *(double *)(v4 + 56);
  *(_OWORD *)&v19.origin.x  = *(_OWORD *)v4;
  *(_OWORD *)&v19.origin.vector.f64[2]  = v6;
  v19.direction.double z = v8;
  v19.direction.vector.f64[3]  = v9;
  *(_OWORD *)&v19.direction.x  = v7;
  v18.x  = a2;
  v18.y  = a3;
  v18.double z = a4;
  SPRay3DTranslate((float64x2_t *)&v19, &v18, v15);
  float64_t result = v15[0].f64[0];
  float64x2_t v11 = v15[1];
  float64x2_t v12 = v15[2];
  uint64_t v13 = v16;
  uint64_t v14 = v17;
  *(float64x2_t *)a1  = v15[0];
  *(float64x2_t *)(a1 + 16)  = v11;
  *(void *)(a1 + 48)  = v13;
  *(void *)(a1 + 56)  = v14;
  *(float64x2_t *)(a1 + 32)  = v12;
  return result;
}

void __swiftcall SPRay3D.rotated(by:)(SPRay3D *__return_ptr retstr, SPRotation3D *by)
{
  long long v7 = *(_OWORD *)(v2 + 16);
  long long v8 = *(_OWORD *)(v2 + 32);
  double v9 = *(double *)(v2 + 48);
  double v10 = *(double *)(v2 + 56);
  *(_OWORD *)&v20.origin.x  = *(_OWORD *)v2;
  *(_OWORD *)&v20.origin.vector.f64[2]  = v7;
  v20.direction.double z = v9;
  v20.direction.vector.f64[3]  = v10;
  *(_OWORD *)&v20.direction.x  = v8;
  float64x2_t v18 = *(float64x2_t *)v3.vector.f64;
  long long v19 = *(_OWORD *)&v3.quaternion.vector.f64[2];
  SPRay3DRotate((float64x2_t *)&v20, v3, &v18, (uint64_t)v15, v4, v5);
  long long v11 = v15[1];
  long long v12 = v15[2];
  double v13 = v16;
  double v14 = v17;
  *(_OWORD *)&retstr->origin.x  = v15[0];
  *(_OWORD *)&retstr->origin.vector.f64[2]  = v11;
  retstr->direction.double z = v13;
  retstr->direction.vector.f64[3]  = v14;
  *(_OWORD *)&retstr->direction.x  = v12;
}

float64x2_t *SPRay3DRotate@<X0>(float64x2_t *result@<X0>, SPRotation3D a2@<0:Q0, 16:Q1>, float64x2_t *a3@<X1>, uint64_t a4@<X8>, float64x2_t _Q6@<Q6>, float64x2_t _Q7@<Q7>)
{
  float64x2_t v7 = result[1];
  _Q4  = *a3;
  _Q3  = a3[1];
  _D5  = a3->f64[1];
  __asm { FMLS            D1, D3, V3.D[0] }
  _Q7.f64[0]  = a3[1].f64[1];
  __asm { FMLA            D1, D7, V3.D[1] }
  v6.f64[0]  = vmlad_n_f64(vmuld_lane_f64(_Q3.f64[0], _Q3, 1), _D5, a3->f64[0]);
  v17.f64[0]  = vmuld_lane_f64(_D5, _Q3, 1);
  double v18 = vmlad_n_f64(-(_D5 * _Q7.f64[0]), _Q3.f64[0], a3->f64[0]);
  _Q1.f64[1]  = v6.f64[0] + v6.f64[0];
  v6.f64[0]  = v18 + v18;
  double v19 = vmlad_n_f64(-(_Q3.f64[0] * _Q7.f64[0]), _D5, a3->f64[0]);
  v20.f64[0]  = v19 + v19;
  __asm
  {
    FMLA            D6, D5, V4.D[1]
    FMLA            D6, D7, V3.D[1]
    FMLS            D6, D4, V4.D[0]
    FMLA            D19, D3, V4.D[1]
  }
  v20.f64[1]  = _Q6.f64[0];
  _Q6.f64[0]  = _D19 + _D19;
  float64_t v22 = -(a3->f64[0] * _Q7.f64[0]);
  __asm
  {
    FMLS            D7, D4, V4.D[0]
    FMLS            D7, D5, V4.D[1]
  }
  _Q4.f64[0]  = a3[1].f64[0];
  v17.f64[1]  = v22;
  float64x2_t v23 = vmlaq_f64(v17, (float64x2_t)vzip1q_s64(*(int64x2_t *)a3, (int64x2_t)_Q3), _Q4);
  float64x2_t v24 = vaddq_f64(v23, v23);
  float64x2_t v25 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q1, result->f64[0]), v20, *result, 1), v24, v7.f64[0]), (float64x2_t)0);
  uint64_t v26 = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*result, v6), _Q6, *result, 1), v7, _Q7), (float64x2_t)0);
  float64x2_t v28 = result[2];
  float64x2_t v27 = result[3];
  float64x2_t v29 = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v28, v6), _Q6, v28, 1), v27, _Q7);
  float64x2_t v30 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q1, v28.f64[0]), v20, v28, 1), v24, v27.f64[0]), (float64x2_t)0);
  float64x2_t v31 = vaddq_f64(v29, (float64x2_t)0);
  *(_OWORD *)(a4 + 32)  = 0u;
  *(_OWORD *)(a4 + 48)  = 0u;
  int64x2_t v32 = vceqzq_f64(v30);
  *(float64x2_t *)a4  = v25;
  *(void *)(a4 + 16)  = v26;
  *(void *)(a4 + 24)  = 0;
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v32, 1), vandq_s8((int8x16_t)vceqzq_f64(v31), (int8x16_t)v32)).u64[0] & 0x8000000000000000) != 0)
  {
    *(float64x2_t *)(a4 + 32)  = v30;
    *(_OWORD *)(a4 + 48)  = *(unint64_t *)&v31.f64[0];
  }
  else
  {
    float64x2_t v33 = vmulq_f64((float64x2_t)*(unint64_t *)&v31.f64[0], (float64x2_t)*(unint64_t *)&v31.f64[0]);
    v33.f64[0]  = 1.0 / sqrt(v33.f64[0] + vaddvq_f64(vmulq_f64(v30, v30)));
    *(void *)&v30.f64[1]  = vextq_s8((int8x16_t)v30, (int8x16_t)v30, 8uLL).u64[0];
    *(float64x2_t *)(a4 + 32)  = vmulq_n_f64(v30, v33.f64[0]);
    *(void *)(a4 + 48)  = *(_OWORD *)&vmulq_f64(v31, v33);
  }
  return result;
}

void __swiftcall SPRay3D.rotated(by:)(SPRay3D *__return_ptr retstr, simd_quatd *by)
{
  long long v5 = *(_OWORD *)(v2 + 16);
  long long v6 = *(_OWORD *)(v2 + 32);
  double v7 = *(double *)(v2 + 48);
  double v8 = *(double *)(v2 + 56);
  *(_OWORD *)&v18.origin.x  = *(_OWORD *)v2;
  *(_OWORD *)&v18.origin.vector.f64[2]  = v5;
  v18.direction.double z = v7;
  v18.direction.vector.f64[3]  = v8;
  *(_OWORD *)&v18.direction.x  = v6;
  float64x2_t v16 = *(float64x2_t *)v3.vector.f64;
  long long v17 = *(_OWORD *)&v3.vector.f64[2];
  SPRay3DRotateByQuaternion((float64x2_t *)&v18, v3, &v16, (uint64_t)v13);
  long long v9 = v13[1];
  long long v10 = v13[2];
  double v11 = v14;
  double v12 = v15;
  *(_OWORD *)&retstr->origin.x  = v13[0];
  *(_OWORD *)&retstr->origin.vector.f64[2]  = v9;
  retstr->direction.double z = v11;
  retstr->direction.vector.f64[3]  = v12;
  *(_OWORD *)&retstr->direction.x  = v10;
}

float64x2_t *SPRay3DRotateByQuaternion@<X0>(float64x2_t *result@<X0>, simd_quatd a2@<0:Q0, 16:Q1>, float64x2_t *a3@<X1>, uint64_t a4@<X8>)
{
  _Q6  = *a3;
  _Q5  = a3[1];
  float64x2_t v9 = result[1];
  float64x2_t v11 = result[2];
  float64x2_t v10 = result[3];
  _D7  = a3->f64[1];
  __asm { FMLS            D3, D5, V5.D[0] }
  _Q17.f64[0]  = a3[1].f64[1];
  __asm { FMLA            D3, D17, V5.D[1] }
  double v19 = vmlad_n_f64(vmuld_lane_f64(_Q5.f64[0], _Q5, 1), _D7, a3->f64[0]);
  v20.f64[0]  = vmuld_lane_f64(_D7, _Q5, 1);
  v5.f64[0]  = vmlad_n_f64(-(_D7 * _Q17.f64[0]), _Q5.f64[0], a3->f64[0]);
  v5.f64[0]  = v5.f64[0] + v5.f64[0];
  _Q3.f64[1]  = v19 + v19;
  double v21 = vmlad_n_f64(-(_Q5.f64[0] * _Q17.f64[0]), _D7, a3->f64[0]);
  v22.f64[0]  = v21 + v21;
  __asm
  {
    FMLA            D16, D7, V6.D[1]
    FMLA            D16, D17, V5.D[1]
    FMLS            D16, D6, V6.D[0]
    FMLA            D21, D5, V6.D[1]
  }
  _Q21.f64[0]  = _Q21.f64[0] + _Q21.f64[0];
  v22.f64[1]  = _D16;
  float64_t v26 = -(a3->f64[0] * _Q17.f64[0]);
  __asm
  {
    FMLS            D17, D6, V6.D[0]
    FMLS            D17, D7, V6.D[1]
  }
  _Q6.f64[0]  = a3[1].f64[0];
  v20.f64[1]  = v26;
  float64x2_t v27 = vmlaq_f64(v20, (float64x2_t)vzip1q_s64(*(int64x2_t *)a3, (int64x2_t)_Q5), _Q6);
  float64x2_t v28 = vaddq_f64(v27, v27);
  float64x2_t v29 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q3, result->f64[0]), v22, *result, 1), v28, v9.f64[0]), (float64x2_t)0);
  *(void *)&_Q6.f64[0]  = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*result, v5), _Q21, *result, 1), v9, _Q17), (float64x2_t)0);
  float64x2_t v30 = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v11, v5), _Q21, v11, 1), v10, _Q17);
  float64x2_t v31 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q3, v11.f64[0]), v22, v11, 1), v28, v10.f64[0]), (float64x2_t)0);
  float64x2_t v32 = vaddq_f64(v30, (float64x2_t)0);
  *(void *)(a4 + 48)  = 0;
  *(void *)(a4 + 56)  = 0;
  int64x2_t v33 = vceqzq_f64(v31);
  *(float64x2_t *)a4  = v29;
  *(float64_t *)(a4 + 16)  = _Q6.f64[0];
  *(void *)(a4 + 24)  = 0;
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v33, 1), vandq_s8((int8x16_t)vceqzq_f64(v32), (int8x16_t)v33)).u64[0] & 0x8000000000000000) != 0)
  {
    *(float64x2_t *)(a4 + 32)  = v31;
    *(_OWORD *)(a4 + 48)  = *(unint64_t *)&v32.f64[0];
  }
  else
  {
    float64x2_t v34 = vmulq_f64((float64x2_t)*(unint64_t *)&v32.f64[0], (float64x2_t)*(unint64_t *)&v32.f64[0]);
    v34.f64[0]  = 1.0 / sqrt(v34.f64[0] + vaddvq_f64(vmulq_f64(v31, v31)));
    *(void *)&v31.f64[1]  = vextq_s8((int8x16_t)v31, (int8x16_t)v31, 8uLL).u64[0];
    *(float64x2_t *)(a4 + 32)  = vmulq_n_f64(v31, v34.f64[0]);
    *(void *)(a4 + 48)  = *(_OWORD *)&vmulq_f64(v32, v34);
  }
  return result;
}

double protocol witness for Rotatable3D.rotated(by:) in conformance SPRay3D@<D0>(uint64_t a1@<X8>, SPRotation3D a2@<Q1:Q0>, float64x2_t a3@<Q6>, float64x2_t a4@<Q7>)
{
  long long v6 = *(_OWORD *)(v4 + 16);
  long long v7 = *(_OWORD *)(v4 + 32);
  double v8 = *(double *)(v4 + 48);
  double v9 = *(double *)(v4 + 56);
  *(_OWORD *)&v19.origin.x  = *(_OWORD *)v4;
  *(_OWORD *)&v19.origin.vector.f64[2]  = v6;
  v19.direction.double z = v8;
  v19.direction.vector.f64[3]  = v9;
  *(_OWORD *)&v19.direction.x  = v7;
  v18[0]  = *(float64x2_t *)a2.vector.f64;
  v18[1]  = *(float64x2_t *)&a2.quaternion.vector.f64[2];
  SPRay3DRotate((float64x2_t *)&v19, a2, v18, (uint64_t)v15, a3, a4);
  double result = *(double *)v15;
  long long v11 = v15[1];
  long long v12 = v15[2];
  uint64_t v13 = v16;
  uint64_t v14 = v17;
  *(_OWORD *)a1  = v15[0];
  *(_OWORD *)(a1 + 16)  = v11;
  *(void *)(a1 + 48)  = v13;
  *(void *)(a1 + 56)  = v14;
  *(_OWORD *)(a1 + 32)  = v12;
  return result;
}

double protocol witness for Rotatable3D.rotated(by:) in conformance SPRay3D@<D0>(uint64_t a1@<X8>, simd_quatd a2@<Q1:Q0>)
{
  long long v4 = *(_OWORD *)(v2 + 16);
  long long v5 = *(_OWORD *)(v2 + 32);
  double v6 = *(double *)(v2 + 48);
  double v7 = *(double *)(v2 + 56);
  *(_OWORD *)&v17.origin.x  = *(_OWORD *)v2;
  *(_OWORD *)&v17.origin.vector.f64[2]  = v4;
  v17.direction.double z = v6;
  v17.direction.vector.f64[3]  = v7;
  *(_OWORD *)&v17.direction.x  = v5;
  v16[0]  = *(float64x2_t *)a2.vector.f64;
  v16[1]  = *(float64x2_t *)&a2.vector.f64[2];
  SPRay3DRotateByQuaternion((float64x2_t *)&v17, a2, v16, (uint64_t)v13);
  double result = *(double *)v13;
  long long v9 = v13[1];
  long long v10 = v13[2];
  uint64_t v11 = v14;
  uint64_t v12 = v15;
  *(_OWORD *)a1  = v13[0];
  *(_OWORD *)(a1 + 16)  = v9;
  *(void *)(a1 + 48)  = v11;
  *(void *)(a1 + 56)  = v12;
  *(_OWORD *)(a1 + 32)  = v10;
  return result;
}

BOOL static SPRay3D.== infix(_:_:)(double *a1, double *a2)
{
  if (*a1 != *a2 || a1[1] != a2[1] || a1[2] != a2[2]) {
    return 0;
  }
  return a1[4] == a2[4] && a1[5] == a2[5] && a1[6] == a2[6];
}

void SPRay3D.hash(into:)()
{
  specialized SIMD.hash(into:)(*(__n128 *)v0, *(double *)(v0 + 16));
  __n128 v1 = *(__n128 *)(v0 + 32);
  long long v2 = *(_OWORD *)(v0 + 48);

  specialized SIMD.hash(into:)(v1, *(double *)&v2);
}

Swift::Int SPRay3D.hashValue.getter()
{
  Hasher.init(_seed:)();
  specialized SIMD.hash(into:)(*v0, v0[1].n128_f64[0]);
  specialized SIMD.hash(into:)(v0[2], v0[3].n128_f64[0]);
  return Hasher._finalize()();
}

uint64_t protocol witness for static Equatable.== infix(_:_:) in conformance SPRay3D.CodingKeys(char *a1, char *a2)
{
  return specialized == infix<A>(_:_:)(*a1, *a2);
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPRay3D.CodingKeys()
{
  return Hasher._finalize()();
}

uint64_t protocol witness for Hashable.hash(into:) in conformance SPRay3D.CodingKeys()
{
  String.hash(into:)();

  return swift_bridgeObjectRelease();
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPRay3D.CodingKeys()
{
  return Hasher._finalize()();
}

uint64_t protocol witness for RawRepresentable.init(rawValue:) in conformance SPRay3D.CodingKeys@<X0>(Swift::String *a1@<X0>, char *a2@<X8>)
{
  Swift::Int v3 = _findStringSwitchCase(cases:string:)((Swift::OpaquePointer)&outlined read-only object #0 of SPRay3D.CodingKeys.init(rawValue:), *a1);
  uint64_t result = swift_bridgeObjectRelease();
  if (v3 == 1) {
    char v5 = 1;
  }
  else {
    char v5 = 2;
  }
  if (!v3) {
    char v5 = 0;
  }
  *a2  = v5;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance SPRay3D.CodingKeys(uint64_t *a1@<X8>)
{
  uint64_t v2 = 0x6E696769726FLL;
  if (*v1) {
    uint64_t v2 = 0x6F69746365726964;
  }
  unint64_t v3 = 0xE600000000000000;
  if (*v1) {
    unint64_t v3 = 0xE90000000000006ELL;
  }
  *a1  = v2;
  a1[1]  = v3;
}

uint64_t protocol witness for CodingKey.stringValue.getter in conformance SPRay3D.CodingKeys()
{
  if (*v0) {
    return 0x6F69746365726964;
  }
  else {
    return 0x6E696769726FLL;
  }
}

uint64_t protocol witness for CodingKey.init(stringValue:) in conformance SPRay3D.CodingKeys@<X0>(Swift::String string@<0:X0, 8:X1>, char *a2@<X8>)
{
  object  = string._object;
  v3._countAndFlagsBits  = string._countAndFlagsBits;
  v3._object  = object;
  Swift::Int v5 = _findStringSwitchCase(cases:string:)((Swift::OpaquePointer)&outlined read-only object #0 of SPRay3D.CodingKeys.init(rawValue:), v3);
  uint64_t result = swift_bridgeObjectRelease();
  if (v5 == 1) {
    char v7 = 1;
  }
  else {
    char v7 = 2;
  }
  if (!v5) {
    char v7 = 0;
  }
  *a2  = v7;
  return result;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPRay3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys();

  return MEMORY[0x270FA00B0](a1, v2);
}

uint64_t protocol witness for CustomDebugStringConvertible.debugDescription.getter in conformance SPRay3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys();

  return MEMORY[0x270FA00B8](a1, v2);
}

uint64_t SPRay3D.encode(to:)(void *a1)
{
  Swift::String v3 = v1;
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedEncodingContainer<SPRay3D.CodingKeys>);
  uint64_t v6 = *(void *)(v5 - 8);
  MEMORY[0x270FA5388](v5);
  double v8 = (char *)&v12 - ((v7 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys();
  dispatch thunk of Encoder.container<A>(keyedBy:)();
  long long v9 = v3[1];
  long long v14 = *v3;
  long long v15 = v9;
  char v13 = 0;
  type metadata accessor for SPPoint3D(0);
  _sSo9SPPoint3DaABSE7SpatialWlTm_2(&lazy protocol witness table cache variable for type SPPoint3D and conformance SPPoint3D, type metadata accessor for SPPoint3D);
  KeyedEncodingContainer.encode<A>(_:forKey:)();
  if (!v2)
  {
    long long v10 = v3[3];
    long long v14 = v3[2];
    long long v15 = v10;
    char v13 = 1;
    type metadata accessor for SPVector3D(0);
    _sSo9SPPoint3DaABSE7SpatialWlTm_2(&lazy protocol witness table cache variable for type SPVector3D and conformance SPVector3D, type metadata accessor for SPVector3D);
    KeyedEncodingContainer.encode<A>(_:forKey:)();
  }
  return (*(uint64_t (**)(char *, uint64_t))(v6 + 8))(v8, v5);
}

double SPRay3D.init(from:)@<D0>(void *a1@<X0>, _OWORD *a2@<X8>)
{
  specialized SPRay3D.init(from:)(a1, (uint64_t)v7);
  if (!v2)
  {
    long long v5 = v7[1];
    *a2  = v7[0];
    a2[1]  = v5;
    double result = *(double *)&v8;
    long long v6 = v9;
    a2[2]  = v8;
    a2[3]  = v6;
  }
  return result;
}

double protocol witness for Decodable.init(from:) in conformance SPRay3D@<D0>(void *a1@<X0>, _OWORD *a2@<X8>)
{
  specialized SPRay3D.init(from:)(a1, (uint64_t)v7);
  if (!v2)
  {
    long long v5 = v7[1];
    *a2  = v7[0];
    a2[1]  = v5;
    double result = *(double *)&v8;
    long long v6 = v9;
    a2[2]  = v8;
    a2[3]  = v6;
  }
  return result;
}

uint64_t protocol witness for Encodable.encode(to:) in conformance SPRay3D(void *a1)
{
  return SPRay3D.encode(to:)(a1);
}

uint64_t SPRay3D.description.getter()
{
  _StringGuts.grow(_:)(27);
  swift_bridgeObjectRelease();
  _StringGuts.grow(_:)(21);
  v0._countAndFlagsBits  = 540702760;
  v0._object  = (void *)0xE400000000000000;
  String.append(_:)(v0);
  Double.write<A>(to:)();
  v1._countAndFlagsBits  = 0x203A79202CLL;
  v1._object  = (void *)0xE500000000000000;
  String.append(_:)(v1);
  Double.write<A>(to:)();
  v2._countAndFlagsBits  = 0x203A7A202CLL;
  v2._object  = (void *)0xE500000000000000;
  String.append(_:)(v2);
  Double.write<A>(to:)();
  v3._countAndFlagsBits  = 41;
  v3._object  = (void *)0xE100000000000000;
  String.append(_:)(v3);
  v4._countAndFlagsBits  = 0;
  v4._object  = (void *)0xE000000000000000;
  String.append(_:)(v4);
  swift_bridgeObjectRelease();
  v5._countAndFlagsBits  = 0x746365726964202CLL;
  v5._object  = (void *)0xED0000203A6E6F69;
  String.append(_:)(v5);
  _StringGuts.grow(_:)(21);
  v6._countAndFlagsBits  = 540702760;
  v6._object  = (void *)0xE400000000000000;
  String.append(_:)(v6);
  Double.write<A>(to:)();
  v7._countAndFlagsBits  = 0x203A79202CLL;
  v7._object  = (void *)0xE500000000000000;
  String.append(_:)(v7);
  Double.write<A>(to:)();
  v8._countAndFlagsBits  = 0x203A7A202CLL;
  v8._object  = (void *)0xE500000000000000;
  String.append(_:)(v8);
  Double.write<A>(to:)();
  v9._countAndFlagsBits  = 41;
  v9._object  = (void *)0xE100000000000000;
  String.append(_:)(v9);
  v10._countAndFlagsBits  = 0;
  v10._object  = (void *)0xE000000000000000;
  String.append(_:)(v10);
  swift_bridgeObjectRelease();
  v11._countAndFlagsBits  = 41;
  v11._object  = (void *)0xE100000000000000;
  String.append(_:)(v11);
  return 0x3A6E696769726F28;
}

uint64_t SPRay3D.customMirror.getter()
{
  uint64_t v1 = type metadata accessor for Mirror.AncestorRepresentation();
  uint64_t v2 = *(void *)(v1 - 8);
  MEMORY[0x270FA5388](v1);
  Swift::String v4 = (char *)v21 - ((v3 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName((uint64_t *)&demangling cache variable for type metadata for Mirror.DisplayStyle?);
  MEMORY[0x270FA5388](v5 - 8);
  Swift::String v7 = (char *)v21 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  long long v8 = v0[1];
  v21[0]  = *v0;
  v21[1]  = v8;
  long long v9 = v0[3];
  v21[2]  = v0[2];
  v21[3]  = v9;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<(String, Any)>);
  uint64_t v10 = swift_allocObject();
  *(_OWORD *)(v10 + 16)  = xmmword_228C202E0;
  *(void *)(v10 + 32)  = 0x6E696769726FLL;
  *(void *)(v10 + 40)  = 0xE600000000000000;
  type metadata accessor for SPPoint3D(0);
  *(void *)(v10 + 72)  = v11;
  uint64_t v12 = swift_allocObject();
  *(void *)(v10 + 48)  = v12;
  long long v13 = v0[1];
  *(_OWORD *)(v12 + 16)  = *v0;
  *(_OWORD *)(v12 + 32)  = v13;
  *(void *)(v10 + 80)  = 0x6F69746365726964;
  *(void *)(v10 + 88)  = 0xE90000000000006ELL;
  type metadata accessor for SPVector3D(0);
  *(void *)(v10 + 120)  = v14;
  uint64_t v15 = swift_allocObject();
  *(void *)(v10 + 96)  = v15;
  long long v16 = v0[3];
  *(_OWORD *)(v15 + 16)  = v0[2];
  *(_OWORD *)(v15 + 32)  = v16;
  uint64_t v17 = *MEMORY[0x263F8E808];
  uint64_t v18 = type metadata accessor for Mirror.DisplayStyle();
  uint64_t v19 = *(void *)(v18 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v19 + 104))(v7, v17, v18);
  (*(void (**)(char *, void, uint64_t, uint64_t))(v19 + 56))(v7, 0, 1, v18);
  (*(void (**)(char *, void, uint64_t))(v2 + 104))(v4, *MEMORY[0x263F8E830], v1);
  type metadata accessor for SPRay3D(0);
  return Mirror.init<A>(_:children:displayStyle:ancestorRepresentation:)();
}

unint64_t lazy protocol witness table accessor for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys()
{
  unint64_t result = lazy protocol witness table cache variable for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys);
  }
  return result;
}

uint64_t specialized SPRay3D.init(from:)@<X0>(void *a1@<X0>, uint64_t a2@<X8>)
{
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName((uint64_t *)&demangling cache variable for type metadata for KeyedDecodingContainer<SPRay3D.CodingKeys>);
  uint64_t v6 = *(void *)(v5 - 8);
  MEMORY[0x270FA5388](v5);
  long long v8 = (char *)&v16 - ((v7 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPRay3D.CodingKeys and conformance SPRay3D.CodingKeys();
  dispatch thunk of Decoder.container<A>(keyedBy:)();
  if (v2) {
    return __swift_destroy_boxed_opaque_existential_1(a1);
  }
  type metadata accessor for SPPoint3D(0);
  LOBYTE(v19[0])  = 0;
  _sSo9SPPoint3DaABSE7SpatialWlTm_2(&lazy protocol witness table cache variable for type SPPoint3D and conformance SPPoint3D, type metadata accessor for SPPoint3D);
  KeyedDecodingContainer.decode<A>(_:forKey:)();
  *(void *)&long long v18 = v28;
  double v9 = v26;
  *(void *)&long long v17 = v27;
  double v10 = v25;
  type metadata accessor for SPVector3D(0);
  LOBYTE(v19[0])  = 1;
  _sSo9SPPoint3DaABSE7SpatialWlTm_2((unint64_t *)&lazy protocol witness table cache variable for type SPVector3D and conformance SPVector3D, type metadata accessor for SPVector3D);
  KeyedDecodingContainer.decode<A>(_:forKey:)();
  v23.x  = v10;
  v23.y  = v9;
  *(void *)&v23.double z = v17;
  *(void *)&v23.vector.f64[3]  = v18;
  *(SPVector3D *)&v22.x  = *(SPVector3D *)&v24.x;
  SPRay3DMake(&v23, &v22, v19, v11);
  long long v17 = v19[0];
  long long v16 = v19[1];
  long long v18 = v19[2];
  uint64_t v12 = v20;
  uint64_t v13 = v21;
  (*(void (**)(char *, uint64_t))(v6 + 8))(v8, v5);
  uint64_t result = __swift_destroy_boxed_opaque_existential_1(a1);
  long long v15 = v16;
  *(_OWORD *)a2  = v17;
  *(_OWORD *)(a2 + 16)  = v15;
  *(void *)(a2 + 48)  = v12;
  *(void *)(a2 + 56)  = v13;
  *(_OWORD *)(a2 + 32)  = v18;
  return result;
}

uint64_t sub_228C09E54()
{
  return MEMORY[0x270FA0238](v0, 48, 15);
}

uint64_t instantiation function for generic protocol witness table for SPRay3D(void *a1)
{
  a1[1]  = _sSo9SPPoint3DaABSE7SpatialWlTm_2(&lazy protocol witness table cache variable for type SPRay3D and conformance SPRay3D, type metadata accessor for SPRay3D);
  a1[2]  = _sSo9SPPoint3DaABSE7SpatialWlTm_2(&lazy protocol witness table cache variable for type SPRay3D and conformance SPRay3D, type metadata accessor for SPRay3D);
  uint64_t result = _sSo9SPPoint3DaABSE7SpatialWlTm_2(&lazy protocol witness table cache variable for type SPRay3D and conformance SPRay3D, type metadata accessor for SPRay3D);
  a1[3]  = result;
  return result;
}

uint64_t base witness table accessor for Equatable in SPRay3D()
{
  return _sSo9SPPoint3DaABSE7SpatialWlTm_2(&lazy protocol witness table cache variable for type SPRay3D and conformance SPRay3D, type metadata accessor for SPRay3D);
}

unsigned char *storeEnumTagSinglePayload for SPRay3D.CodingKeys(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 1 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFF) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFE)
  {
    unsigned int v6 = ((a2 - 255) >> 8) + 1;
    *uint64_t result = a2 + 1;
    switch(v5)
    {
      case 1:
        result[1]  = v6;
        break;
      case 2:
        *(_WORD *)(result + 1)  = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x228C0A024);
      case 4:
        *(_DWORD *)(result + 1)  = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1]  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1)  = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1)  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 1;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for SPRay3D.CodingKeys()
{
  return &type metadata for SPRay3D.CodingKeys;
}

uint64_t _sSo9SPPoint3DaABSE7SpatialWlTm_2(unint64_t *a1, void (*a2)(uint64_t))
{
  uint64_t result = *a1;
  if (!result)
  {
    a2(255);
    uint64_t result = swift_getWitnessTable();
    atomic_store(result, a1);
  }
  return result;
}

void __swiftcall SPVector3D.init(x:y:z:)(SPVector3D *__return_ptr retstr, Swift::Double x, Swift::Double y, Swift::Double z)
{
  SPPoint3DMake(x, y, z, (double *)&v4);
}

void __swiftcall SPVector3D.init(_:)(SPVector3D *__return_ptr retstr, SPSize3D *a2)
{
  SPVector3D.init(_:)((void (*)(double *__return_ptr, void *))SPVector3DMakeWithSize, v2, v3, v4);
}

double SPVector3D.init(_:)(__n128 a1, __n128 a2)
{
  v4[0]  = a1;
  v4[1]  = a2;
  SPPoint3DMakeWithVector(v4, &v3);
  return v3.n128_f64[0];
}

void __swiftcall SPVector3D.applying(_:)(SPVector3D *__return_ptr retstr, SPAffineTransform3D *a2)
{
  SPVector3D.applying(_:)((long long *)a2, (void (*)(double *__return_ptr, void *, _OWORD *))SPVector3DApplyAffineTransform, v2, v3, v4);
}

double SPVector3DApplyAffineTransform@<D0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>)
{
  float64x2_t v3 = a1[1];
  float64x2_t v4 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, a1->f64[0]), a2[2], *a1, 1), a2[4], v3.f64[0]);
  float64x2_t v5 = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*a1, a2[1]), a2[3], *a1, 1), v3, a2[5]);
  double result = 0.0;
  *(void *)&v5.f64[0]  = *(_OWORD *)&vmlaq_f64(v5, (float64x2_t)0, a2[7]);
  *a3  = vmlaq_f64(v4, (float64x2_t)0, a2[6]);
  a3[1].f64[0]  = v5.f64[0];
  return result;
}

void __swiftcall SPVector3D.applying(_:)(SPVector3D *__return_ptr retstr, SPProjectiveTransform3D *a2)
{
  SPVector3D.applying(_:)((long long *)a2, (void (*)(double *__return_ptr, void *, _OWORD *))SPVector3DApplyProjectiveTransform, v2, v3, v4);
}

double SPVector3D.applying(_:)(long long *a1, void (*a2)(double *__return_ptr, void *, _OWORD *), double a3, double a4, double a5)
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  *(double *)long long v16 = a3;
  *(double *)&v16[1]  = a4;
  *(double *)&v16[2]  = a5;
  v15[0]  = v5;
  v15[1]  = v6;
  v15[2]  = v7;
  v15[3]  = v8;
  v15[4]  = v9;
  v15[5]  = v10;
  v15[6]  = v11;
  v15[7]  = v12;
  a2(&v14, v16, v15);
  return v14;
}

{
  long long v5;
  long long v6;
  long long v7;
  uint64_t v8;
  uint64_t v9;
  double v11;
  _OWORD v12[3];
  uint64_t v13;
  uint64_t v14;
  void v15[4];

  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = *((void *)a1 + 6);
  long long v9 = *((void *)a1 + 7);
  *(double *)long long v15 = a3;
  *(double *)&v15[1]  = a4;
  *(double *)&v15[2]  = a5;
  v12[0]  = v5;
  v12[1]  = v6;
  uint64_t v13 = v8;
  double v14 = v9;
  v12[2]  = v7;
  a2(&v11, v15, v12);
  return v11;
}

{
  long long v5;
  long long v6;
  long long v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  double v12;
  _OWORD v13[3];
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void v17[4];

  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = *((void *)a1 + 6);
  long long v9 = *((void *)a1 + 7);
  long long v10 = *((void *)a1 + 8);
  *(double *)long long v17 = a3;
  *(double *)&v17[1]  = a4;
  *(double *)&v17[2]  = a5;
  v13[0]  = v5;
  v13[1]  = v6;
  double v14 = v8;
  long long v15 = v9;
  v13[2]  = v7;
  long long v16 = v10;
  a2(&v12, v17, v13);
  return v12;
}

double SPVector3DApplyProjectiveTransform@<D0>(uint64_t a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>)
{
  long long v3 = *(_OWORD *)(a1 + 16);
  float64x2_t v4 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(*a2, *(double *)a1), a2[2], *(float64x2_t *)a1, 1), a2[4], *(double *)&v3);
  float64x2_t v5 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(a2[1], *(double *)a1), a2[3], *(float64x2_t *)a1, 1), a2[5], *(double *)&v3);
  double result = 0.0;
  *(void *)&v5.f64[0]  = *(_OWORD *)&vmlaq_f64(v5, (float64x2_t)0, a2[7]);
  *a3  = vmlaq_f64(v4, (float64x2_t)0, a2[6]);
  a3[1].f64[0]  = v5.f64[0];
  return result;
}

void __swiftcall SPVector3D.applying(_:)(SPVector3D *__return_ptr retstr, SPPose3D *a2)
{
  SPVector3D.applying(_:)((long long *)a2, (void (*)(double *__return_ptr, void *, _OWORD *))SPVector3DApplyPose, v2, v3, v4);
}

float64_t SPVector3DApplyPose@<D0>(SPVector3D *a1@<X0>, SPPose3D *a2@<X1>, float64x2_t *a3@<X8>, float64x2_t _Q7@<Q7>)
{
  float64x2_t v6 = *(float64x2_t *)&a1->vector.f64[2];
  _Q3  = *(float64x2_t *)a2->rotation.vector.f64;
  _Q2  = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  _D5  = a2->rotation.vector.f64[1];
  __asm { FMLS            D4, D2, V2.D[0] }
  _Q7.f64[0]  = a2->rotation.vector.f64[3];
  __asm { FMLA            D4, D7, V2.D[1] }
  double v16 = vmlad_n_f64(vmuld_lane_f64(_Q2.f64[0], _Q2, 1), _D5, _Q3.f64[0]);
  v17.f64[0]  = vmuld_lane_f64(_D5, _Q2, 1);
  v4.f64[0]  = vmlad_n_f64(-(_D5 * _Q7.f64[0]), _Q2.f64[0], _Q3.f64[0]);
  v4.f64[0]  = v4.f64[0] + v4.f64[0];
  _Q4.f64[1]  = v16 + v16;
  double v18 = vmlad_n_f64(-(_Q2.f64[0] * _Q7.f64[0]), _D5, _Q3.f64[0]);
  v19.f64[0]  = v18 + v18;
  __asm
  {
    FMLA            D6, D5, V3.D[1]
    FMLA            D6, D7, V2.D[1]
    FMLS            D6, D3, V3.D[0]
    FMLA            D19, D2, V3.D[1]
  }
  _Q19.f64[0]  = _Q19.f64[0] + _Q19.f64[0];
  v19.f64[1]  = _D6;
  float64_t v23 = -(_Q3.f64[0] * _Q7.f64[0]);
  float64x2_t v24 = (float64x2_t)vzip1q_s64((int64x2_t)_Q3, (int64x2_t)_Q2);
  __asm
  {
    FMLS            D7, D3, V3.D[0]
    FMLS            D7, D5, V3.D[1]
  }
  _Q3.f64[0]  = a2->rotation.vector.f64[2];
  v17.f64[1]  = v23;
  float64x2_t v25 = vmlaq_f64(v17, v24, _Q3);
  float64x2_t v26 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q4, a1->x), v19, *(float64x2_t *)&a1->x, 1), vaddq_f64(v25, v25), v6.f64[0]), (float64x2_t)0);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)&a1->x, v4), _Q19, *(float64x2_t *)&a1->x, 1), v6, _Q7), (float64x2_t)0);
  *a3  = v26;
  a3[1].f64[0]  = result;
  return result;
}

BOOL static SPVector3D.== infix(_:_:)(double a1, double a2, double a3, double a4, double a5, double a6)
{
  BOOL v6 = a1 == a4;
  if (a2 != a5) {
    BOOL v6 = 0;
  }
  return a3 == a6 && v6;
}

uint64_t SPVector3D.description.getter()
{
  _StringGuts.grow(_:)(21);
  v0._countAndFlagsBits  = 540702760;
  v0._object  = (void *)0xE400000000000000;
  String.append(_:)(v0);
  Double.write<A>(to:)();
  v1._countAndFlagsBits  = 0x203A79202CLL;
  v1._object  = (void *)0xE500000000000000;
  String.append(_:)(v1);
  Double.write<A>(to:)();
  v2._countAndFlagsBits  = 0x203A7A202CLL;
  v2._object  = (void *)0xE500000000000000;
  String.append(_:)(v2);
  Double.write<A>(to:)();
  v3._countAndFlagsBits  = 41;
  v3._object  = (void *)0xE100000000000000;
  String.append(_:)(v3);
  return 0;
}

void one-time initialization function for right()
{
}

double static SPVector3D.right.getter()
{
  if (one-time initialization token for right != -1) {
    swift_once();
  }
  return *(double *)&static SPVector3D.right;
}

void one-time initialization function for up()
{
}

double static SPVector3D.up.getter()
{
  if (one-time initialization token for up != -1) {
    swift_once();
  }
  return *(double *)&static SPVector3D.up;
}

void one-time initialization function for forward()
{
}

double static SPVector3D.forward.getter()
{
  if (one-time initialization token for forward != -1) {
    swift_once();
  }
  return *(double *)&static SPVector3D.forward;
}

void __swiftcall SPVector3D.init(_:)(SPVector3D *__return_ptr retstr, SPPoint3D *a2)
{
  SPVector3D.init(_:)((void (*)(double *__return_ptr, void *))SPVector3DMakeWithSize, v2, v3, v4);
}

void __swiftcall SPVector3D.init(_:)(SPVector3D *__return_ptr retstr, SPRotationAxis3D *a2)
{
  SPVector3D.init(_:)((void (*)(double *__return_ptr, void *))SPVector3DMakeWithSize, v2, v3, v4);
}

Swift::Double __swiftcall SPVector3D.dot(_:)(SPVector3D *a1)
{
  v9.x  = v4;
  v9.y  = v5;
  v9.double z = v6;
  v8.x  = v1;
  v8.y  = v2;
  v8.double z = v3;
  return SPVector3DDotProduct(&v9, &v8);
}

float64_t SPVector3DDotProduct(SPVector3D *a1, SPVector3D *a2)
{
  return vmulq_f64(*(float64x2_t *)&a1->vector.f64[2], *(float64x2_t *)&a2->vector.f64[2]).f64[0]
       + vaddvq_f64(vmulq_f64(*(float64x2_t *)&a1->x, *(float64x2_t *)&a2->x));
}

void __swiftcall SPVector3D.cross(_:)(SPVector3D *__return_ptr retstr, SPVector3D *a2)
{
  SPVector3D.cross(_:)((void (*)(double *__return_ptr, void *, void *))SPVector3DCrossProduct, v2, v3, v4, v5, v6, v7);
}

float64x2_t SPVector3DCrossProduct@<Q0>(SPVector3D *a1@<X0>, SPVector3D *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v3 = *(float64x2_t *)&a1->vector.f64[2];
  float64x2_t v4 = *(float64x2_t *)&a2->vector.f64[2];
  uint64_t v5 = *(_OWORD *)&vmlaq_laneq_f64(vmulq_laneq_f64(vnegq_f64(v3), *(float64x2_t *)&a2->x, 1), v4, *(float64x2_t *)&a1->x, 1);
  v4.f64[1]  = a2->x;
  v3.f64[1]  = a1->x;
  float64x2_t result = vmlaq_f64(vmulq_f64(v4, vnegq_f64(*(float64x2_t *)&a1->x)), *(float64x2_t *)&a2->x, v3);
  *(void *)a3  = v5;
  *(float64x2_t *)(a3 + 8)  = result;
  return result;
}

double SPVector3D.normalized.getter(double a1, double a2, double a3)
{
  return SPVector3D.init(_:)((void (*)(double *__return_ptr, void *))SPVector3DNormalize, a1, a2, a3);
}

double SPVector3D.init(_:)(void (*a1)(double *__return_ptr, void *), double a2, double a3, double a4)
{
  *(double *)double v6 = a2;
  *(double *)&v6[1]  = a3;
  *(double *)&v6[2]  = a4;
  a1(&v5, v6);
  return v5;
}

float64x2_t SPVector3DNormalize@<Q0>(SPVector3D *a1@<X0>, uint64_t a2@<X8>)
{
  float64x2_t v2 = *(float64x2_t *)&a1->vector.f64[2];
  float64x2_t v3 = vmulq_f64(v2, v2);
  v3.f64[0]  = 1.0 / sqrt(v3.f64[0] + vaddvq_f64(vmulq_f64(*(float64x2_t *)&a1->x, *(float64x2_t *)&a1->x)));
  float64x2_t result = vmulq_n_f64(*(float64x2_t *)&a1->x, v3.f64[0]);
  *(float64x2_t *)a2  = result;
  *(void *)(a2 + 16)  = *(_OWORD *)&vmulq_f64(v2, v3);
  return result;
}

Swift::Void __swiftcall SPVector3D.normalize()()
{
  long long v1 = *(_OWORD *)v0;
  double v2 = *(double *)(v0 + 24);
  v3.double z = *(double *)(v0 + 16);
  v3.vector.f64[3]  = v2;
  *(_OWORD *)&v3.x  = v1;
  SPVector3DNormalize(&v3, v0);
}

void __swiftcall SPVector3D.projected(_:)(SPVector3D *__return_ptr retstr, SPVector3D *a2)
{
  SPVector3D.cross(_:)((void (*)(double *__return_ptr, void *, void *))SPVector3DProject, v2, v3, v4, v5, v6, v7);
}

float64x2_t SPVector3DProject@<Q0>(SPVector3D *a1@<X0>, SPVector3D *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v3 = *(float64x2_t *)&a2->vector.f64[2];
  float64x2_t v4 = vmulq_f64(*(float64x2_t *)&a1->x, *(float64x2_t *)&a2->x);
  v4.f64[0]  = (vmulq_f64(*(float64x2_t *)&a1->vector.f64[2], v3).f64[0] + vaddvq_f64(v4))
            / (vmulq_f64(v3, v3).f64[0] + vaddvq_f64(vmulq_f64(*(float64x2_t *)&a2->x, *(float64x2_t *)&a2->x)));
  uint64_t v5 = *(_OWORD *)&vmulq_f64(v3, v4);
  float64x2_t result = vmulq_n_f64(*(float64x2_t *)&a2->x, v4.f64[0]);
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = v5;
  return result;
}

void __swiftcall SPVector3D.reflected(_:)(SPVector3D *__return_ptr retstr, SPVector3D *a2)
{
  SPVector3D.cross(_:)((void (*)(double *__return_ptr, void *, void *))SPVector3DReflect, v2, v3, v4, v5, v6, v7);
}

float64x2_t SPVector3DReflect@<Q0>(SPVector3D *a1@<X0>, SPVector3D *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v3 = *(float64x2_t *)&a1->vector.f64[2];
  float64x2_t v4 = *(float64x2_t *)&a2->vector.f64[2];
  float64x2_t v5 = vmulq_f64(v3, v4);
  double v6 = vaddvq_f64(vmulq_f64(*(float64x2_t *)&a1->x, *(float64x2_t *)&a2->x));
  v5.f64[0]  = v5.f64[0] + v6 + v5.f64[0] + v6;
  float64x2_t result = vmlsq_lane_f64(*(float64x2_t *)&a1->x, *(float64x2_t *)&a2->x, v5.f64[0], 0);
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = *(_OWORD *)&vmlsq_f64(v3, v4, v5);
  return result;
}

uint64_t SPVector3D.length.getter(double a1, double a2, double a3)
{
  return SPVector3D.length.getter((uint64_t (*)(void *))SPVector3DLength, a1, a2, a3);
}

double SPVector3DLength(SPVector3D *a1)
{
  return sqrt(vmulq_f64(*(float64x2_t *)&a1->vector.f64[2], *(float64x2_t *)&a1->vector.f64[2]).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)&a1->x, *(float64x2_t *)&a1->x)));
}

uint64_t SPVector3D.lengthSquared.getter(double a1, double a2, double a3)
{
  return SPVector3D.length.getter((uint64_t (*)(void *))SPVector3DLengthSquared, a1, a2, a3);
}

uint64_t SPVector3D.length.getter(uint64_t (*a1)(void *), double a2, double a3, double a4)
{
  *(double *)float64x2_t v5 = a2;
  *(double *)&v5[1]  = a3;
  *(double *)&v5[2]  = a4;
  return a1(v5);
}

float64_t SPVector3DLengthSquared(SPVector3D *a1)
{
  return vmulq_f64(*(float64x2_t *)&a1->vector.f64[2], *(float64x2_t *)&a1->vector.f64[2]).f64[0]
       + vaddvq_f64(vmulq_f64(*(float64x2_t *)&a1->x, *(float64x2_t *)&a1->x));
}

void __swiftcall SPVector3D.unapplying(_:)(SPVector3D *__return_ptr retstr, SPAffineTransform3D *a2)
{
  SPVector3D.unapplying(_:)((long long *)a2, (void (*)(double *__return_ptr, void *, _OWORD *))SPVector3DUnapplyAffineTransform, v2, v3, v4);
}

double SPVector3DUnapplyAffineTransform@<D0>(float64x2_t *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  v5.f64[0]  = *(float64_t *)(a2 + 80);
  v5.f64[1]  = *(float64_t *)(a2 + 64);
  v6.f64[0]  = *(float64_t *)(a2 + 48);
  v6.f64[1]  = *(float64_t *)(a2 + 32);
  if (vmulq_f64(*(float64x2_t *)(a2 + 16), vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)(a2 + 64), vnegq_f64((float64x2_t)vdupq_laneq_s64(*(int64x2_t *)(a2 + 32), 1))), *(float64x2_t *)(a2 + 32), *(float64x2_t *)(a2 + 64), 1)).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)a2, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 64), *(int8x16_t *)(a2 + 80), 8uLL), vnegq_f64(v6)), v5, (float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 32), *(int8x16_t *)(a2 + 48), 8uLL)))) == 0.0)
  {
    float64x2_t v7 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    float64x2_t v9 = v7;
    float64x2_t v8 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v10 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v12 = v7;
    float64x2_t v11 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v14 = v7;
    float64x2_t v13 = (float64x2_t)xmmword_228C1FC60;
  }
  else
  {
    float64x2_t v19 = *(float64x2_t *)(a2 + 96);
    float64x2_t v20 = *(float64x2_t *)(a2 + 112);
    __invert_d3();
    float64x2_t v7 = 0u;
    float64x2_t v8 = 0u;
    float64x2_t v9 = 0u;
    float64x2_t v10 = 0u;
    float64x2_t v12 = 0u;
    float64x2_t v11 = 0u;
    float64x2_t v13 = vnegq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v19, (float64x2_t)0), (float64x2_t)0, v19, 1), v20, (float64x2_t)0));
    float64x2_t v14 = vnegq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v19.f64[0]), (float64x2_t)0, v19, 1), (float64x2_t)0, v20.f64[0]));
  }
  float64x2_t v15 = a1[1];
  float64x2_t v16 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v7, a1->f64[0]), v9, *a1, 1), v12, v15.f64[0]);
  double result = 0.0;
  uint64_t v18 = *(_OWORD *)&vmlaq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v8, *a1), v10, *a1, 1), v15, v11), (float64x2_t)0, v13);
  *(float64x2_t *)a3  = vmlaq_f64(v16, (float64x2_t)0, v14);
  *(void *)(a3 + 16)  = v18;
  return result;
}

void __swiftcall SPVector3D.unapplying(_:)(SPVector3D *__return_ptr retstr, SPProjectiveTransform3D *a2)
{
  SPVector3D.unapplying(_:)((long long *)a2, (void (*)(double *__return_ptr, void *, _OWORD *))SPVector3DUnapplyProjectiveTransform, v2, v3, v4);
}

double SPVector3D.unapplying(_:)(long long *a1, void (*a2)(double *__return_ptr, void *, _OWORD *), double a3, double a4, double a5)
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  *(double *)float64x2_t v16 = a3;
  *(double *)&v16[1]  = a4;
  *(double *)&v16[2]  = a5;
  v15[0]  = v5;
  v15[1]  = v6;
  v15[2]  = v7;
  v15[3]  = v8;
  v15[4]  = v9;
  v15[5]  = v10;
  v15[6]  = v11;
  v15[7]  = v12;
  a2(&v14, v16, v15);
  return v14;
}

float64x2_t SPVector3DUnapplyProjectiveTransform@<Q0>(uint64_t a1@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v5 = a2[2];
  float64x2_t v6 = a2[3];
  float64x2_t v7 = a2[4];
  float64x2_t v8 = a2[5];
  float64x2_t v9 = a2[6];
  float64x2_t v10 = a2[7];
  float64x2_t v11 = (float64x2_t)vextq_s8((int8x16_t)v5, (int8x16_t)v6, 8uLL);
  float64x2_t v12 = (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v5, 8uLL);
  float64x2_t v13 = (float64x2_t)vextq_s8((int8x16_t)v9, (int8x16_t)v10, 8uLL);
  float64x2_t v14 = (float64x2_t)vextq_s8((int8x16_t)v10, (int8x16_t)v9, 8uLL);
  float64x2_t v15 = (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v8, 8uLL);
  float64x2_t v16 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)v7, 8uLL);
  float64x2_t v17 = vnegq_f64(v16);
  float64x2_t v18 = vnegq_f64(v15);
  float64x2_t v19 = vmlaq_f64(vmulq_f64(v9, v18), v13, v7);
  float64x2_t v20 = vmlaq_f64(vmulq_f64(v6, vmlaq_f64(vmulq_f64(v14, v18), v13, v16)), vmlaq_f64(vmulq_f64(v10, v17), v14, v8), v11);
  int64x2_t v21 = (int64x2_t)vmulq_f64(a2[1], vmlaq_f64(vmlaq_f64(vmulq_f64(v5, vmlaq_f64(vmulq_f64(v13, v17), v14, v15)), v19, v12), vmlaq_f64(vmulq_f64(v14, vnegq_f64(v7)), v9, v16), v11));
  int64x2_t v22 = (int64x2_t)vmulq_f64(*a2, vmlaq_f64(v20, vmlaq_f64(vmulq_f64(v13, vnegq_f64(v8)), v10, v15), v12));
  if (vaddvq_f64(vsubq_f64((float64x2_t)vzip1q_s64(v22, v21), (float64x2_t)vzip2q_s64(v22, v21))) == 0.0)
  {
    float64x2_t v23 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    float64x2_t v24 = v23;
    float64x2_t v25 = v23;
    float64x2_t v26 = v23;
    float64x2_t v28 = v23;
    float64x2_t v27 = v23;
    float64x2_t v29 = v23;
    float64x2_t v30 = v23;
  }
  else
  {
    __invert_d4();
    float64x2_t v23 = 0u;
    float64x2_t v24 = 0u;
    float64x2_t v25 = 0u;
    float64x2_t v26 = 0u;
    float64x2_t v28 = 0u;
    float64x2_t v27 = 0u;
    float64x2_t v29 = 0u;
    float64x2_t v30 = 0u;
  }
  long long v31 = *(_OWORD *)(a1 + 16);
  uint64_t v32 = *(_OWORD *)&vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v24, *(double *)a1), v26, *(float64x2_t *)a1, 1), v27, *(double *)&v31), (float64x2_t)0, v30);
  float64x2_t result = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v23, *(double *)a1), v25, *(float64x2_t *)a1, 1), v28, *(double *)&v31), (float64x2_t)0, v29);
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = v32;
  return result;
}

void __swiftcall SPVector3D.unapplying(_:)(SPVector3D *__return_ptr retstr, SPPose3D *a2)
{
  SPVector3D.applying(_:)((long long *)a2, (void (*)(double *__return_ptr, void *, _OWORD *))SPVector3DUnapplyPose, v2, v3, v4);
}

float64_t SPVector3DUnapplyPose@<D0>(SPVector3D *a1@<X0>, SPPose3D *a2@<X1>, float64x2_t *a3@<X8>, float64x2_t _Q7@<Q7>)
{
  float64x2_t v6 = *(float64x2_t *)a2->rotation.vector.f64;
  float64x2_t v7 = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  float64x2_t v8 = vnegq_f64(v6);
  v6.f64[0]  = 1.0 / vaddvq_f64(vaddq_f64(vmulq_f64(v6, v6), vmulq_f64(v7, v7)));
  _Q2  = vmulq_n_f64(vmulq_f64(v7, (float64x2_t)xmmword_228C1FC40), v6.f64[0]);
  _Q3  = vmulq_n_f64(v8, v6.f64[0]);
  float64x2_t v11 = *(float64x2_t *)&a1->vector.f64[2];
  _D5  = _Q3.f64[1];
  __asm { FMLS            D4, D2, V2.D[0] }
  _Q7.f64[0]  = _Q2.f64[1];
  __asm { FMLA            D4, D7, V2.D[1] }
  double v19 = vmlad_n_f64(vmuld_lane_f64(_Q2.f64[0], _Q2, 1), _Q3.f64[1], _Q3.f64[0]);
  v20.f64[0]  = vmuld_lane_f64(_Q3.f64[1], _Q2, 1);
  v4.f64[0]  = vmlad_n_f64(-(_Q3.f64[1] * _Q2.f64[1]), _Q2.f64[0], _Q3.f64[0]);
  v4.f64[0]  = v4.f64[0] + v4.f64[0];
  _Q4.f64[1]  = v19 + v19;
  double v21 = vmlad_n_f64(-(_Q2.f64[0] * _Q2.f64[1]), _Q3.f64[1], _Q3.f64[0]);
  v22.f64[0]  = v21 + v21;
  __asm
  {
    FMLA            D6, D5, V3.D[1]
    FMLA            D6, D7, V2.D[1]
    FMLS            D6, D3, V3.D[0]
    FMLA            D19, D2, V3.D[1]
  }
  _Q19.f64[0]  = _Q19.f64[0] + _Q19.f64[0];
  v22.f64[1]  = _D6;
  float64_t v26 = -(_Q3.f64[0] * _Q2.f64[1]);
  float64x2_t v27 = (float64x2_t)vzip1q_s64((int64x2_t)_Q3, (int64x2_t)_Q2);
  __asm
  {
    FMLS            D7, D3, V3.D[0]
    FMLS            D7, D5, V3.D[1]
  }
  _Q3.f64[0]  = _Q2.f64[0];
  v20.f64[1]  = v26;
  float64x2_t v28 = vmlaq_f64(v20, v27, _Q3);
  float64x2_t v29 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q4, a1->x), v22, *(float64x2_t *)&a1->x, 1), vaddq_f64(v28, v28), v11.f64[0]), (float64x2_t)0);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)&a1->x, v4), _Q19, *(float64x2_t *)&a1->x, 1), v11, _Q7), (float64x2_t)0);
  *a3  = v29;
  a3[1].f64[0]  = result;
  return result;
}

uint64_t protocol witness for Primitive3D.applying(_:) in conformance SPVector3D(long long *a1, uint64_t a2, uint64_t a3)
{
  return protocol witness for Primitive3D.applying(_:) in conformance SPVector3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPVector3DApplyAffineTransform);
}

{
  return protocol witness for Primitive3D.applying(_:) in conformance SPVector3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPVector3DApplyProjectiveTransform);
}

{
  return protocol witness for Primitive3D.applying(_:) in conformance SPVector3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPVector3DApplyPose);
}

uint64_t protocol witness for Primitive3D.unapplying(_:) in conformance SPVector3D(long long *a1, uint64_t a2, uint64_t a3)
{
  return protocol witness for Primitive3D.applying(_:) in conformance SPVector3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPVector3DUnapplyAffineTransform);
}

{
  return protocol witness for Primitive3D.applying(_:) in conformance SPVector3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPVector3DUnapplyProjectiveTransform);
}

{
  return protocol witness for Primitive3D.applying(_:) in conformance SPVector3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPVector3DUnapplyPose);
}

uint64_t protocol witness for Primitive3D.applying(_:) in conformance SPVector3D(long long *a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(long long *, _OWORD *))
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  long long v13 = *v4;
  uint64_t v14 = *((void *)v4 + 3);
  uint64_t v18 = *((void *)v4 + 2);
  uint64_t v19 = v14;
  long long v17 = v13;
  v16[0]  = v5;
  v16[1]  = v6;
  v16[2]  = v7;
  _OWORD v16[3] = v8;
  v16[4]  = v9;
  v16[5]  = v10;
  v16[6]  = v11;
  v16[7]  = v12;
  return a4(&v17, v16);
}

{
  long long *v4;
  long long v5;
  long long v6;
  long long v7;
  uint64_t v8;
  uint64_t v9;
  long long v10;
  uint64_t v11;
  _OWORD v13[3];
  uint64_t v14;
  uint64_t v15;
  long long v16;
  uint64_t v17;
  uint64_t v18;

  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = *((void *)a1 + 6);
  long long v9 = *((void *)a1 + 7);
  long long v10 = *v4;
  long long v11 = *((void *)v4 + 3);
  long long v17 = *((void *)v4 + 2);
  uint64_t v18 = v11;
  float64x2_t v16 = v10;
  v13[0]  = v5;
  v13[1]  = v6;
  uint64_t v14 = v8;
  float64x2_t v15 = v9;
  v13[2]  = v7;
  return a4(&v16, v13);
}

double SPVector3D.applying(_:)(long long *a1, double a2, double a3, double a4)
{
  return SPVector3D.applying(_:)(a1, (void (*)(double *__return_ptr, void *, _OWORD *))SPVector3DApplyScaledPose, a2, a3, a4);
}

float64_t SPVector3DApplyScaledPose@<D0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>, float64x2_t a4@<Q2>)
{
  a4.f64[0]  = a2[4].f64[0];
  *(void *)&double v4 = *(_OWORD *)&vmulq_f64(a1[1], a4);
  float64x2_t v5 = vmulq_n_f64(*a1, a4.f64[0]);
  float64x2_t v6 = a2[2];
  float64x2_t v7 = a2[3];
  float64x2_t v8 = vmulq_f64(v7, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v9 = (int8x16_t)vnegq_f64(v6);
  float64x2_t v10 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)vnegq_f64(v8), 8uLL);
  float64x2_t v11 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v8, v5, 1), v10, v5.f64[0]), (float64x2_t)vextq_s8((int8x16_t)v6, v9, 8uLL), v4);
  float64x2_t v12 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v6, v5, 1), (float64x2_t)vextq_s8(v9, (int8x16_t)v6, 8uLL), v5.f64[0]), v10, v4);
  float64x2_t v13 = vnegq_f64(v11);
  float64x2_t v14 = (float64x2_t)vextq_s8((int8x16_t)v12, (int8x16_t)vnegq_f64(v12), 8uLL);
  float64x2_t v15 = vmlaq_n_f64(vmulq_laneq_f64(v12, v6, 1), v14, v6.f64[0]);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v12, v7, 1), v14, v7.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v13, v6, 1), (float64x2_t)vextq_s8((int8x16_t)v11, (int8x16_t)v13, 8uLL), v6.f64[0]));
  *a3  = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v11, v7, 1), (float64x2_t)vextq_s8((int8x16_t)v13, (int8x16_t)v11, 8uLL), v7.f64[0]), v15);
  a3[1].f64[0]  = result;
  return result;
}

double SPVector3D.unapplying(_:)(long long *a1, double a2, double a3, double a4)
{
  return SPVector3D.applying(_:)(a1, (void (*)(double *__return_ptr, void *, _OWORD *))SPVector3DUnapplyScaledPose, a2, a3, a4);
}

float64x2_t SPVector3DUnapplyScaledPose@<Q0>(uint64_t a1@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v3 = a2[2];
  float64x2_t v4 = a2[3];
  float64x2_t v5 = vnegq_f64(v3);
  v3.f64[0]  = 1.0 / vaddvq_f64(vaddq_f64(vmulq_f64(v3, v3), vmulq_f64(v4, v4)));
  float64x2_t v6 = vmulq_n_f64(vmulq_f64(v4, (float64x2_t)xmmword_228C1FC40), v3.f64[0]);
  float64x2_t v7 = vmulq_n_f64(v5, v3.f64[0]);
  float64x2_t v8 = vmulq_f64(v6, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v9 = (int8x16_t)vnegq_f64(v7);
  float64x2_t v10 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)vnegq_f64(v8), 8uLL);
  long long v11 = *(_OWORD *)(a1 + 16);
  float64x2_t v12 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v7, *(float64x2_t *)a1, 1), (float64x2_t)vextq_s8(v9, (int8x16_t)v7, 8uLL), *(double *)a1), v10, *(double *)&v11);
  float64x2_t v13 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v8, *(float64x2_t *)a1, 1), v10, *(double *)a1), (float64x2_t)vextq_s8((int8x16_t)v7, v9, 8uLL), *(double *)&v11);
  float64x2_t v14 = vnegq_f64(v13);
  float64x2_t v15 = (float64x2_t)vextq_s8((int8x16_t)v12, (int8x16_t)vnegq_f64(v12), 8uLL);
  float64x2_t v16 = vmlaq_n_f64(vmulq_laneq_f64(v14, v7, 1), (float64x2_t)vextq_s8((int8x16_t)v13, (int8x16_t)v14, 8uLL), v7.f64[0]);
  float64x2_t v17 = vmlaq_n_f64(vmulq_laneq_f64(v13, v6, 1), (float64x2_t)vextq_s8((int8x16_t)v14, (int8x16_t)v13, 8uLL), v6.f64[0]);
  float64x2_t v18 = vaddq_f64(v17, vmlaq_n_f64(vmulq_laneq_f64(v12, v7, 1), v15, v7.f64[0]));
  v17.f64[0]  = a2[4].f64[0];
  float64x2_t result = vdivq_f64(v18, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v17.f64[0], 0));
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = *(_OWORD *)&vdivq_f64(vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v12, v6, 1), v15, v6.f64[0]), v16), v17);
  return result;
}

void __swiftcall SPVector3D.rotated(by:)(SPVector3D *__return_ptr retstr, SPRotation3D *by)
{
  v10.x  = v3;
  v10.y  = v4;
  v10.double z = v5;
  float64x2_t v8 = *(float64x2_t *)v2.vector.f64;
  long long v9 = *(_OWORD *)&v2.quaternion.vector.f64[2];
  SPPoint3DRotate(&v10, v2, &v8, &v7, v6);
}

void __swiftcall SPVector3D.rotated(by:)(SPVector3D *__return_ptr retstr, simd_quatd *by)
{
  v10.x  = v3;
  v10.y  = v4;
  v10.double z = v5;
  float64x2_t v8 = *(float64x2_t *)v2.vector.f64;
  long long v9 = *(_OWORD *)&v2.vector.f64[2];
  SPPoint3DRotateByQuaternion(&v10, v2, &v8, &v7, v6);
}

void __swiftcall SPVector3D.rotation(to:)(SPRotation3D *__return_ptr retstr, SPVector3D *to)
{
  v10.x  = v5;
  v10.y  = v6;
  v10.double z = v7.f64[0];
  v9.x  = v2;
  v9.y  = v3;
  v9.double z = v4;
  SPPoint3DRotationToPoint((float64x2_t *)&v10, &v9, &v8, v7);
}

void __swiftcall SPVector3D.scaledBy(x:y:z:)(SPVector3D *__return_ptr retstr, Swift::Double x, Swift::Double y, Swift::Double z)
{
  v8.x  = v4;
  v8.y  = v5;
  v8.double z = v6;
  SPVector3DScaleBy(&v8, *(float64x2_t *)&x, y, *(float64x2_t *)&z, (uint64_t)&v7);
}

float64x2_t SPVector3DScaleBy@<Q0>(SPVector3D *a1@<X0>, float64x2_t a2@<Q0>, float64_t a3@<D1>, float64x2_t a4@<Q2>, uint64_t a5@<X8>)
{
  a2.f64[1]  = a3;
  float64x2_t result = vmulq_f64(a2, *(float64x2_t *)&a1->x);
  uint64_t v6 = *(_OWORD *)&vmulq_f64(a4, *(float64x2_t *)&a1->vector.f64[2]);
  *(float64x2_t *)a5  = result;
  *(void *)(a5 + 16)  = v6;
  return result;
}

void __swiftcall SPVector3D.scaled(by:)(SPVector3D *__return_ptr retstr, SPSize3D *by)
{
  SPVector3D.cross(_:)((void (*)(double *__return_ptr, void *, void *))SPVector3DScaleBySize, v2, v3, v4, v5, v6, v7);
}

double SPVector3D.cross(_:)(void (*a1)(double *__return_ptr, void *, void *), double a2, double a3, double a4, double a5, double a6, double a7)
{
  *(double *)SPPoint3D v10 = a5;
  *(double *)&v10[1]  = a6;
  *(double *)&v10[2]  = a7;
  *(double *)SPPoint3D v9 = a2;
  *(double *)&v9[1]  = a3;
  *(double *)&v9[2]  = a4;
  a1(&v8, v10, v9);
  return v8;
}

float64x2_t SPVector3DScaleBySize@<Q0>(SPVector3D *a1@<X0>, SPSize3D *a2@<X1>, uint64_t a3@<X8>)
{
  uint64_t v3 = *(_OWORD *)&vmulq_f64(*(float64x2_t *)&a1->vector.f64[2], *(float64x2_t *)&a2->vector.f64[2]);
  float64x2_t result = vmulq_f64(*(float64x2_t *)&a1->x, *(float64x2_t *)&a2->width);
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = v3;
  return result;
}

void __swiftcall SPVector3D.uniformlyScaled(by:)(SPVector3D *__return_ptr retstr, Swift::Double by)
{
  v6.x  = v2;
  v6.y  = v3;
  v6.double z = v4;
  SPVector3DScaleUniform(&v6, *(float64x2_t *)&by, (uint64_t)&v5);
}

float64x2_t SPVector3DScaleUniform@<Q0>(SPVector3D *a1@<X0>, float64x2_t a2@<Q0>, uint64_t a3@<X8>)
{
  uint64_t v3 = *(_OWORD *)&vmulq_f64(a2, *(float64x2_t *)&a1->vector.f64[2]);
  float64x2_t result = vmulq_n_f64(*(float64x2_t *)&a1->x, a2.f64[0]);
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = v3;
  return result;
}

double protocol witness for Scalable3D.scaledBy(x:y:z:) in conformance SPVector3D@<D0>(uint64_t a1@<X8>, float64x2_t a2@<Q0>, float64_t a3@<D1>, float64x2_t a4@<Q2>)
{
  long long v5 = *(_OWORD *)v4;
  double v6 = v4[3];
  v8.double z = v4[2];
  v8.vector.f64[3]  = v6;
  *(_OWORD *)&v8.x  = v5;
  *(void *)&double result = *(_OWORD *)&SPVector3DScaleBy(&v8, a2, a3, a4, a1);
  return result;
}

double protocol witness for Scalable3D.scaled(by:) in conformance SPVector3D@<D0>(uint64_t a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>)
{
  long long v5 = *(_OWORD *)v4;
  double v6 = v4[3];
  v9.double z = v4[2];
  v9.vector.f64[3]  = v6;
  *(_OWORD *)&v9.x  = v5;
  v8.width  = a2;
  v8.height  = a3;
  v8.depth  = a4;
  *(void *)&double result = *(_OWORD *)&SPVector3DScaleBySize(&v9, &v8, a1);
  return result;
}

double protocol witness for Scalable3D.uniformlyScaled(by:) in conformance SPVector3D@<D0>(uint64_t a1@<X8>, float64x2_t a2@<Q0>)
{
  long long v3 = *(_OWORD *)v2;
  double v4 = v2[3];
  v6.double z = v2[2];
  v6.vector.f64[3]  = v4;
  *(_OWORD *)&v6.x  = v3;
  *(void *)&double result = *(_OWORD *)&SPVector3DScaleUniform(&v6, a2, a1);
  return result;
}

float64_t SPVector3D.sheared(_:)(uint64_t a1, float64x2_t a2, float64x2_t a3, double a4)
{
  if (*(unsigned char *)(a1 + 16))
  {
    unint64_t v4 = *(void *)a1;
    unint64_t v5 = *(void *)(a1 + 8);
    if (*(unsigned char *)(a1 + 16) == 1)
    {
      v9.x  = a2.f64[0];
      v9.y  = a3.f64[0];
      v9.double z = a4;
      SPAxis v6 = SPAxisY;
    }
    else
    {
      v9.x  = a2.f64[0];
      v9.y  = a3.f64[0];
      v9.double z = a4;
      SPAxis v6 = SPAxisZ;
    }
  }
  else
  {
    unint64_t v4 = *(void *)a1;
    unint64_t v5 = *(void *)(a1 + 8);
    v9.x  = a2.f64[0];
    v9.y  = a3.f64[0];
    v9.double z = a4;
    SPAxis v6 = SPAxisX;
  }
  *(void *)&a2.f64[0]  = v4;
  *(void *)&a3.f64[0]  = v5;
  SPVector3DShear(&v9, v6, a2, a3, &v8);
  return v8.f64[0];
}

float64_t SPVector3DShear@<D0>(SPVector3D *a1@<X0>, SPAxis a2@<W1>, float64x2_t a3@<Q0>, float64x2_t a4@<Q1>, float64x2_t *a5@<X8>)
{
  __asm { FMOV            V7.2D, #1.0 }
  switch(a2)
  {
    case SPAxisZ:
      a3.f64[1]  = a4.f64[0];
      float64x2_t v10 = (float64x2_t)xmmword_228C1F7D0;
      float64x2_t v13 = 0uLL;
      _Q5  = _Q7;
      a4  = 0uLL;
      float64x2_t v11 = (float64x2_t)xmmword_228C1F7A0;
      break;
    case SPAxisY:
      __asm { FMOV            V5.2D, #1.0 }
      v11.f64[1]  = _Q5.f64[1];
      v11.f64[0]  = a3.f64[0];
      float64x2_t v10 = (float64x2_t)xmmword_228C1F7D0;
      a3  = 0uLL;
      float64x2_t v13 = a4;
      a4  = 0uLL;
      break;
    case SPAxisX:
      v10.f64[0]  = _Q7.f64[0];
      v10.f64[1]  = a3.f64[0];
      float64x2_t v11 = (float64x2_t)xmmword_228C1F7A0;
      a3  = 0uLL;
      _Q5  = _Q7;
      float64x2_t v13 = 0uLL;
      break;
    default:
      float64x2_t v29 = (float64x2_t)xmmword_228C1F7D0;
      float64x2_t v20 = (float64x2_t)xmmword_228C1F7A0;
      float64x2_t v28 = 0uLL;
      float64x2_t v30 = 0uLL;
      float64x2_t v33 = 0uLL;
      float64x2_t v19 = (float64x2_t)xmmword_228C1F7D0;
LABEL_14:
      float64x2_t v31 = 0uLL;
      float64x2_t v32 = 0uLL;
      goto LABEL_15;
  }
  float64x2_t v14 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v17 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v18 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  float64x2_t v19 = (float64x2_t)xmmword_228C1F7D0;
  float64x2_t v20 = (float64x2_t)xmmword_228C1F7A0;
  int64x2_t v21 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v16, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], (float64x2_t)xmmword_228C1F7D0)), (int8x16_t)vceqzq_f64(v17));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v21, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqzq_f64(v15), (int8x16_t)vceqzq_f64(v14)), (int8x16_t)vceqq_f64(v18, _Q7)), (int8x16_t)v21)).u64[0] & 0x8000000000000000) != 0)
  {
    int64x2_t v22 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v16, v11), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v10)), (int8x16_t)vceqq_f64(v17, a3));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v22, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, v13), (int8x16_t)vceqq_f64(v14, a4)), (int8x16_t)vceqq_f64(v18, _Q5)), (int8x16_t)v22)).u64[0] & 0x8000000000000000) != 0)
    {
      float64x2_t v28 = 0uLL;
      float64x2_t v29 = (float64x2_t)xmmword_228C1F7D0;
      float64x2_t v30 = 0uLL;
      float64x2_t v33 = 0uLL;
      goto LABEL_14;
    }
  }
  uint64_t v23 = 0;
  v36[0]  = v10;
  v36[1]  = a4;
  v36[2]  = v11;
  v36[3]  = v13;
  v36[4]  = a3;
  v36[5]  = _Q5;
  float64x2_t v37 = 0u;
  float64x2_t v38 = 0u;
  float64x2_t v39 = 0u;
  float64x2_t v40 = 0u;
  __asm { FMOV            V1.2D, #1.0 }
  float64x2_t v41 = 0u;
  float64x2_t v42 = 0u;
  do
  {
    float64x2_t v26 = (float64x2_t)v36[v23];
    float64x2_t v25 = (float64x2_t)v36[v23 + 1];
    float64x2_t v27 = (float64x2_t *)((char *)&v37 + v23 * 16);
    *float64x2_t v27 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)xmmword_228C1F7D0, v26.f64[0]), (float64x2_t)xmmword_228C1F7A0, v26, 1), (float64x2_t)0, v25.f64[0]);
    v27[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v26, (float64x2_t)0), (float64x2_t)0, v26, 1), _Q1, v25);
    v23 += 2;
  }
  while (v23 != 6);
  float64x2_t v29 = v37;
  float64x2_t v28 = v38;
  float64x2_t v20 = v39;
  float64x2_t v30 = v40;
  float64x2_t v31 = 0uLL;
  float64x2_t v32 = 0uLL;
  float64x2_t v33 = v41;
  float64x2_t v19 = v42;
LABEL_15:
  float64x2_t v34 = *(float64x2_t *)&a1->vector.f64[2];
  *(void *)&float64_t result = *(_OWORD *)&vmlaq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v28, *(float64x2_t *)&a1->x), v30, *(float64x2_t *)&a1->x, 1), v34, v19), (float64x2_t)0, v32);
  *a5  = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v29, a1->x), v20, *(float64x2_t *)&a1->x, 1), v33, v34.f64[0]), (float64x2_t)0, v31);
  a5[1].f64[0]  = result;
  return result;
}

{
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  float64x2_t v17;
  float64x2_t v18;
  float64x2_t v19;
  float64x2_t v20;
  int64x2_t v21;
  int64x2_t v22;
  uint64_t v23;
  float64x2_t v25;
  float64x2_t v26;
  float64x2_t *v27;
  float64x2_t v28;
  float64x2_t v29;
  float64x2_t v30;
  float64x2_t v31;
  float64x2_t v32;
  float64x2_t v33;
  float64x2_t v34;
  float64_t result;
  _OWORD v36[6];
  float64x2_t v37;
  float64x2_t v38;
  float64x2_t v39;
  float64x2_t v40;
  float64x2_t v41;
  float64x2_t v42;

  __asm { FMOV            V7.2D, #1.0 }
  switch(a2)
  {
    case SPAxisZ:
      a3.f64[1]  = a4.f64[0];
      float64x2_t v10 = (float64x2_t)xmmword_228C1F7D0;
      float64x2_t v13 = 0uLL;
      _Q5  = _Q7;
      a4  = 0uLL;
      float64x2_t v11 = (float64x2_t)xmmword_228C1F7A0;
      break;
    case SPAxisY:
      __asm { FMOV            V5.2D, #1.0 }
      v11.f64[1]  = _Q5.f64[1];
      v11.f64[0]  = a3.f64[0];
      float64x2_t v10 = (float64x2_t)xmmword_228C1F7D0;
      a3  = 0uLL;
      float64x2_t v13 = a4;
      a4  = 0uLL;
      break;
    case SPAxisX:
      v10.f64[0]  = _Q7.f64[0];
      v10.f64[1]  = a3.f64[0];
      float64x2_t v11 = (float64x2_t)xmmword_228C1F7A0;
      a3  = 0uLL;
      _Q5  = _Q7;
      float64x2_t v13 = 0uLL;
      break;
    default:
      float64x2_t v29 = (float64x2_t)xmmword_228C1F7D0;
      float64x2_t v20 = (float64x2_t)xmmword_228C1F7A0;
      float64x2_t v28 = 0uLL;
      float64x2_t v30 = 0uLL;
      float64x2_t v33 = 0uLL;
      float64x2_t v19 = (float64x2_t)xmmword_228C1F7D0;
LABEL_14:
      float64x2_t v31 = 0uLL;
      float64x2_t v32 = 0uLL;
      goto LABEL_15;
  }
  float64x2_t v14 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v17 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v18 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  float64x2_t v19 = (float64x2_t)xmmword_228C1F7D0;
  float64x2_t v20 = (float64x2_t)xmmword_228C1F7A0;
  int64x2_t v21 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v16, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], (float64x2_t)xmmword_228C1F7D0)), (int8x16_t)vceqzq_f64(v17));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v21, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqzq_f64(v15), (int8x16_t)vceqzq_f64(v14)), (int8x16_t)vceqq_f64(v18, _Q7)), (int8x16_t)v21)).u64[0] & 0x8000000000000000) != 0)
  {
    int64x2_t v22 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v16, v11), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v10)), (int8x16_t)vceqq_f64(v17, a3));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v22, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, v13), (int8x16_t)vceqq_f64(v14, a4)), (int8x16_t)vceqq_f64(v18, _Q5)), (int8x16_t)v22)).u64[0] & 0x8000000000000000) != 0)
    {
      float64x2_t v28 = 0uLL;
      float64x2_t v29 = (float64x2_t)xmmword_228C1F7D0;
      float64x2_t v30 = 0uLL;
      float64x2_t v33 = 0uLL;
      goto LABEL_14;
    }
  }
  uint64_t v23 = 0;
  v36[0]  = v10;
  v36[1]  = a4;
  v36[2]  = v11;
  v36[3]  = v13;
  v36[4]  = a3;
  v36[5]  = _Q5;
  float64x2_t v37 = 0u;
  float64x2_t v38 = 0u;
  float64x2_t v39 = 0u;
  float64x2_t v40 = 0u;
  __asm { FMOV            V1.2D, #1.0 }
  float64x2_t v41 = 0u;
  float64x2_t v42 = 0u;
  do
  {
    float64x2_t v26 = (float64x2_t)v36[v23];
    float64x2_t v25 = (float64x2_t)v36[v23 + 1];
    float64x2_t v27 = (float64x2_t *)((char *)&v37 + v23 * 16);
    *float64x2_t v27 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)xmmword_228C1F7D0, v26.f64[0]), (float64x2_t)xmmword_228C1F7A0, v26, 1), (float64x2_t)0, v25.f64[0]);
    v27[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v26, (float64x2_t)0), (float64x2_t)0, v26, 1), _Q1, v25);
    v23 += 2;
  }
  while (v23 != 6);
  float64x2_t v29 = v37;
  float64x2_t v28 = v38;
  float64x2_t v20 = v39;
  float64x2_t v30 = v40;
  float64x2_t v31 = 0uLL;
  float64x2_t v32 = 0uLL;
  float64x2_t v33 = v41;
  float64x2_t v19 = v42;
LABEL_15:
  float64x2_t v34 = *(float64x2_t *)&a1->vector.f64[2];
  *(void *)&float64_t result = *(_OWORD *)&vmlaq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v28, *(float64x2_t *)&a1->x), v30, *(float64x2_t *)&a1->x, 1), v34, v19), (float64x2_t)0, v32);
  *a5  = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v29, a1->x), v20, *(float64x2_t *)&a1->x, 1), v33, v34.f64[0]), (float64x2_t)0, v31);
  a5[1].f64[0]  = result;
  return result;
}

void protocol witness for Shearable3D.sheared(_:) in conformance SPVector3D(uint64_t a1@<X0>, uint64_t a2@<X8>, float64x2_t a3@<Q0>, float64x2_t a4@<Q1>)
{
  double v7 = v4[2];
  double v6 = v4[3];
  double v8 = v4[1];
  if (*(unsigned char *)(a1 + 16))
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    if (*(unsigned char *)(a1 + 16) == 1)
    {
      v16.x  = *v4;
      v16.y  = v8;
      v16.double z = v7;
      v16.vector.f64[3]  = v6;
      SPAxis v9 = SPAxisY;
    }
    else
    {
      v16.x  = *v4;
      v16.y  = v8;
      v16.double z = v7;
      v16.vector.f64[3]  = v6;
      SPAxis v9 = SPAxisZ;
    }
  }
  else
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    v16.x  = *v4;
    v16.y  = v8;
    v16.double z = v7;
    v16.vector.f64[3]  = v6;
    SPAxis v9 = SPAxisX;
  }
  SPVector3DShear(&v16, v9, a3, a4, &v13);
  uint64_t v11 = v14;
  uint64_t v10 = v15;
  float64_t v12 = v13.f64[1];
  *(float64_t *)a2  = v13.f64[0];
  *(float64_t *)(a2 + 8)  = v12;
  *(void *)(a2 + 16)  = v11;
  *(void *)(a2 + 24)  = v10;
}

double protocol witness for static AdditiveArithmetic.+ infix(_:_:) in conformance SPVector3D@<D0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, __n128 *a3@<X8>)
{
  *(double *)&unint64_t v3 = a1[1].f64[0] + a2[1].f64[0];
  v5[0]  = (__n128)vaddq_f64(*a1, *a2);
  v5[1]  = (__n128)v3;
  *(void *)&double result = SPPoint3DMakeWithVector(v5, a3).n128_u64[0];
  return result;
}

double protocol witness for static AdditiveArithmetic.+= infix(_:_:) in conformance SPVector3D(float64x2_t *a1, float64x2_t *a2)
{
  *(double *)&unint64_t v2 = a2[1].f64[0] + a1[1].f64[0];
  v4[0]  = (__n128)vaddq_f64(*a2, *a1);
  v4[1]  = (__n128)v2;
  *(void *)&double result = SPPoint3DMakeWithVector(v4, (__n128 *)a1).n128_u64[0];
  return result;
}

double protocol witness for static AdditiveArithmetic.- infix(_:_:) in conformance SPVector3D@<D0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, __n128 *a3@<X8>)
{
  *(double *)&unint64_t v3 = a1[1].f64[0] - a2[1].f64[0];
  v5[0]  = (__n128)vsubq_f64(*a1, *a2);
  v5[1]  = (__n128)v3;
  *(void *)&double result = SPPoint3DMakeWithVector(v5, a3).n128_u64[0];
  return result;
}

double protocol witness for static AdditiveArithmetic.-= infix(_:_:) in conformance SPVector3D(float64x2_t *a1, float64x2_t *a2)
{
  *(double *)&unint64_t v2 = a1[1].f64[0] - a2[1].f64[0];
  v4[0]  = (__n128)vsubq_f64(*a1, *a2);
  v4[1]  = (__n128)v2;
  *(void *)&double result = SPPoint3DMakeWithVector(v4, (__n128 *)a1).n128_u64[0];
  return result;
}

void SPVector3D.hash(into:)(__n128 a1, double a2, double a3)
{
  a1.n128_f64[1]  = a2;
  specialized SIMD.hash(into:)(a1, a3);
}

Swift::Int SPVector3D.hashValue.getter(double a1, double a2, double a3)
{
  Hasher.init(_seed:)();
  v3.n128_f64[0]  = a1;
  v3.n128_f64[1]  = a2;
  specialized SIMD.hash(into:)(v3, a3);
  return Hasher._finalize()();
}

unint64_t protocol witness for RawRepresentable.init(rawValue:) in conformance SPVector3D.CodingKeys@<X0>(Swift::String *a1@<X0>, unsigned char *a2@<X8>)
{
  unint64_t result = specialized SPVector3D.CodingKeys.init(rawValue:)(*a1);
  *a2  = result;
  return result;
}

unint64_t protocol witness for CodingKey.init(stringValue:) in conformance SPVector3D.CodingKeys@<X0>(Swift::String a1@<X1:X0>, unsigned char *a2@<X8>)
{
  unint64_t result = specialized SPVector3D.CodingKeys.init(rawValue:)(a1);
  *a2  = result;
  return result;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPVector3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys();

  return MEMORY[0x270FA00B0](a1, v2);
}

uint64_t protocol witness for CustomDebugStringConvertible.debugDescription.getter in conformance SPVector3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys();

  return MEMORY[0x270FA00B8](a1, v2);
}

uint64_t SPVector3D.encode(to:)(void *a1)
{
  uint64_t v3 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedEncodingContainer<SPVector3D.CodingKeys>);
  uint64_t v4 = *(void *)(v3 - 8);
  MEMORY[0x270FA5388](v3);
  double v6 = &v8[-((v5 + 15) & 0xFFFFFFFFFFFFFFF0)];
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys();
  dispatch thunk of Encoder.container<A>(keyedBy:)();
  v8[15]  = 0;
  KeyedEncodingContainer.encode(_:forKey:)();
  if (!v1)
  {
    v8[14]  = 1;
    KeyedEncodingContainer.encode(_:forKey:)();
    v8[13]  = 2;
    KeyedEncodingContainer.encode(_:forKey:)();
  }
  return (*(uint64_t (**)(unsigned char *, uint64_t))(v4 + 8))(v6, v3);
}

double SPVector3D.init(from:)(void *a1)
{
  return specialized SPVector3D.init(from:)(a1);
}

void protocol witness for Decodable.init(from:) in conformance SPVector3D(void *a1@<X0>, uint64_t a2@<X8>)
{
  double v4 = specialized SPVector3D.init(from:)(a1);
  if (!v2)
  {
    *(void *)(a2 + 24)  = 0;
    *(double *)a2  = v4;
    *(void *)(a2 + 8)  = v5;
    *(void *)(a2 + 16)  = v6;
  }
}

uint64_t protocol witness for Encodable.encode(to:) in conformance SPVector3D(void *a1)
{
  return SPVector3D.encode(to:)(a1);
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPVector3D()
{
  return SPVector3D.description.getter();
}

uint64_t SPVector3D.customMirror.getter(double a1, double a2, double a3)
{
  uint64_t v6 = type metadata accessor for Mirror.AncestorRepresentation();
  uint64_t v7 = *(void *)(v6 - 8);
  MEMORY[0x270FA5388](v6);
  SPAxis v9 = (char *)v19 - ((v8 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v10 = __swift_instantiateConcreteTypeFromMangledName((uint64_t *)&demangling cache variable for type metadata for Mirror.DisplayStyle?);
  MEMORY[0x270FA5388](v10 - 8);
  float64_t v12 = (char *)v19 - ((v11 + 15) & 0xFFFFFFFFFFFFFFF0);
  *(double *)float64x2_t v19 = a1;
  *(double *)&v19[1]  = a2;
  *(double *)&v19[2]  = a3;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<(String, Any)>);
  uint64_t v13 = swift_allocObject();
  *(_OWORD *)(v13 + 16)  = xmmword_228C1FC50;
  *(void *)(v13 + 32)  = 120;
  *(void *)(v13 + 40)  = 0xE100000000000000;
  uint64_t v14 = MEMORY[0x263F8D538];
  *(double *)(v13 + 48)  = a1;
  *(void *)(v13 + 72)  = v14;
  *(void *)(v13 + 80)  = 121;
  *(void *)(v13 + 88)  = 0xE100000000000000;
  *(double *)(v13 + 96)  = a2;
  *(void *)(v13 + 120)  = v14;
  *(void *)(v13 + 128)  = 122;
  *(void *)(v13 + 136)  = 0xE100000000000000;
  *(void *)(v13 + 168)  = v14;
  *(double *)(v13 + 144)  = a3;
  uint64_t v15 = *MEMORY[0x263F8E808];
  uint64_t v16 = type metadata accessor for Mirror.DisplayStyle();
  uint64_t v17 = *(void *)(v16 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v17 + 104))(v12, v15, v16);
  (*(void (**)(char *, void, uint64_t, uint64_t))(v17 + 56))(v12, 0, 1, v16);
  (*(void (**)(char *, void, uint64_t))(v7 + 104))(v9, *MEMORY[0x263F8E830], v6);
  type metadata accessor for SPVector3D(0);
  return Mirror.init<A>(_:children:displayStyle:ancestorRepresentation:)();
}

uint64_t protocol witness for CustomReflectable.customMirror.getter in conformance SPVector3D()
{
  return SPVector3D.customMirror.getter(*v0, v0[1], v0[2]);
}

unint64_t lazy protocol witness table accessor for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys()
{
  unint64_t result = lazy protocol witness table cache variable for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys);
  }
  return result;
}

unint64_t specialized SPVector3D.CodingKeys.init(rawValue:)(Swift::String string)
{
  object  = string._object;
  v2._countAndFlagsBits  = string._countAndFlagsBits;
  v2._object  = object;
  unint64_t v3 = _findStringSwitchCase(cases:string:)((Swift::OpaquePointer)&outlined read-only object #0 of SPVector3D.CodingKeys.init(rawValue:), v2);
  swift_bridgeObjectRelease();
  if (v3 >= 3) {
    return 3;
  }
  else {
    return v3;
  }
}

double specialized SPVector3D.init(from:)(void *a1)
{
  uint64_t v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedDecodingContainer<SPVector3D.CodingKeys>);
  uint64_t v5 = *(void *)(v4 - 8);
  MEMORY[0x270FA5388](v4);
  uint64_t v7 = (char *)&v14 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPVector3D.CodingKeys and conformance SPVector3D.CodingKeys();
  dispatch thunk of Decoder.container<A>(keyedBy:)();
  if (!v1)
  {
    LOBYTE(v14)  = 0;
    KeyedDecodingContainer.decode(_:forKey:)();
    double v9 = v8;
    LOBYTE(v14)  = 1;
    KeyedDecodingContainer.decode(_:forKey:)();
    double v11 = v10;
    LOBYTE(v14)  = 2;
    KeyedDecodingContainer.decode(_:forKey:)();
    SPPoint3DMake(v9, v11, v13, &v14);
    double v2 = v14;
    (*(void (**)(char *, uint64_t))(v5 + 8))(v7, v4);
  }
  __swift_destroy_boxed_opaque_existential_1(a1);
  return v2;
}

uint64_t instantiation function for generic protocol witness table for SPVector3D(void *a1)
{
  a1[1]  = lazy protocol witness table accessor for type SPVector3D and conformance SPVector3D((unint64_t *)&lazy protocol witness table cache variable for type SPVector3D and conformance SPVector3D);
  a1[2]  = lazy protocol witness table accessor for type SPVector3D and conformance SPVector3D(&lazy protocol witness table cache variable for type SPVector3D and conformance SPVector3D);
  uint64_t result = lazy protocol witness table accessor for type SPVector3D and conformance SPVector3D(&lazy protocol witness table cache variable for type SPVector3D and conformance SPVector3D);
  a1[3]  = result;
  return result;
}

uint64_t lazy protocol witness table accessor for type SPVector3D and conformance SPVector3D(unint64_t *a1)
{
  uint64_t result = *a1;
  if (!result)
  {
    type metadata accessor for SPVector3D(255);
    uint64_t result = swift_getWitnessTable();
    atomic_store(result, a1);
  }
  return result;
}

uint64_t base witness table accessor for Equatable in SPVector3D()
{
  return lazy protocol witness table accessor for type SPVector3D and conformance SPVector3D(&lazy protocol witness table cache variable for type SPVector3D and conformance SPVector3D);
}

unsigned char *storeEnumTagSinglePayload for SPVector3D.CodingKeys(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 2 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 2) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFE) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFD)
  {
    unsigned int v6 = ((a2 - 254) >> 8) + 1;
    *uint64_t result = a2 + 2;
    switch(v5)
    {
      case 1:
        result[1]  = v6;
        break;
      case 2:
        *(_WORD *)(result + 1)  = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x228C0C2FCLL);
      case 4:
        *(_DWORD *)(result + 1)  = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1]  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1)  = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1)  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 2;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for SPVector3D.CodingKeys()
{
  return &type metadata for SPVector3D.CodingKeys;
}

uint64_t protocol witness for Primitive3D.apply(_:) in conformance SPPoint3D(long long *a1, uint64_t a2, uint64_t a3)
{
  return protocol witness for Primitive3D.apply(_:) in conformance SPPoint3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPPoint3DApplyAffineTransform);
}

{
  return protocol witness for Primitive3D.apply(_:) in conformance SPPoint3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPPoint3DApplyProjectiveTransform);
}

{
  return protocol witness for Primitive3D.apply(_:) in conformance SPPoint3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPPoint3DApplyPose);
}

Swift::Void __swiftcall Primitive3D.apply(_:)(SPAffineTransform3D *a1)
{
  uint64_t v3 = v1;
  uint64_t v4 = *(void *)(v1 - 8);
  uint64_t v5 = MEMORY[0x270FA5388](a1);
  uint64_t v7 = (char *)&v9 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(uint64_t))(v8 + 72))(v5);
  (*(void (**)(uint64_t, uint64_t))(v4 + 8))(v2, v3);
  (*(void (**)(uint64_t, char *, uint64_t))(v4 + 32))(v2, v7, v3);
}

uint64_t protocol witness for Primitive3D.apply(_:) in conformance SPPoint3D(long long *a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(long long *, _OWORD *))
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  long long v13 = *v4;
  uint64_t v14 = *((void *)v4 + 3);
  uint64_t v18 = *((void *)v4 + 2);
  uint64_t v19 = v14;
  long long v17 = v13;
  v16[0]  = v5;
  v16[1]  = v6;
  v16[2]  = v7;
  _OWORD v16[3] = v8;
  v16[4]  = v9;
  v16[5]  = v10;
  v16[6]  = v11;
  v16[7]  = v12;
  return a4(&v17, v16);
}

{
  long long *v4;
  long long v5;
  long long v6;
  long long v7;
  uint64_t v8;
  uint64_t v9;
  long long v10;
  uint64_t v11;
  _OWORD v13[3];
  uint64_t v14;
  uint64_t v15;
  long long v16;
  uint64_t v17;
  uint64_t v18;

  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = *((void *)a1 + 6);
  long long v9 = *((void *)a1 + 7);
  long long v10 = *v4;
  long long v11 = *((void *)v4 + 3);
  long long v17 = *((void *)v4 + 2);
  uint64_t v18 = v11;
  uint64_t v16 = v10;
  v13[0]  = v5;
  v13[1]  = v6;
  uint64_t v14 = v8;
  uint64_t v15 = v9;
  v13[2]  = v7;
  return a4(&v16, v13);
}

Swift::Void __swiftcall Primitive3D.apply(_:)(SPProjectiveTransform3D *a1)
{
  uint64_t v3 = v1;
  uint64_t v4 = *(void *)(v1 - 8);
  uint64_t v5 = MEMORY[0x270FA5388](a1);
  long long v7 = (char *)&v9 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(uint64_t))(v8 + 80))(v5);
  (*(void (**)(uint64_t, uint64_t))(v4 + 8))(v2, v3);
  (*(void (**)(uint64_t, char *, uint64_t))(v4 + 32))(v2, v7, v3);
}

Swift::Void __swiftcall Primitive3D.apply(_:)(SPPose3D *a1)
{
  uint64_t v3 = v1;
  uint64_t v4 = *(void *)(v1 - 8);
  uint64_t v5 = MEMORY[0x270FA5388](a1);
  long long v7 = (char *)&v9 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(uint64_t))(v8 + 88))(v5);
  (*(void (**)(uint64_t, uint64_t))(v4 + 8))(v2, v3);
  (*(void (**)(uint64_t, char *, uint64_t))(v4 + 32))(v2, v7, v3);
}

double protocol witness for Primitive3D.unapply(_:) in conformance SPPoint3D(long long *a1, uint64_t a2, uint64_t a3)
{
  *(void *)&double result = protocol witness for Primitive3D.unapply(_:) in conformance SPPoint3D(a1, a2, a3, (void (*)(__n128 *__return_ptr, __n128 *, _OWORD *))SPPoint3DUnapplyAffineTransform).n128_u64[0];
  return result;
}

{
  double result;

  *(void *)&double result = protocol witness for Primitive3D.unapply(_:) in conformance SPPoint3D(a1, a2, a3, (void (*)(__n128 *__return_ptr, __n128 *, _OWORD *))SPPoint3DUnapplyProjectiveTransform).n128_u64[0];
  return result;
}

Swift::Void __swiftcall Primitive3D.unapply(_:)(SPAffineTransform3D *a1)
{
  uint64_t v3 = v1;
  uint64_t v4 = *(void *)(v1 - 8);
  uint64_t v5 = MEMORY[0x270FA5388](a1);
  long long v7 = (char *)&v9 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(uint64_t))(v8 + 120))(v5);
  (*(void (**)(uint64_t, uint64_t))(v4 + 8))(v2, v3);
  (*(void (**)(uint64_t, char *, uint64_t))(v4 + 32))(v2, v7, v3);
}

__n128 protocol witness for Primitive3D.unapply(_:) in conformance SPPoint3D(long long *a1, uint64_t a2, uint64_t a3, void (*a4)(__n128 *__return_ptr, __n128 *, _OWORD *))
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  __n128 v13 = *v4;
  unint64_t v14 = v4[1].n128_u64[1];
  unint64_t v22 = v4[1].n128_u64[0];
  unint64_t v23 = v14;
  __n128 v21 = v13;
  v20[0]  = v5;
  v20[1]  = v6;
  v20[2]  = v7;
  v20[3]  = v8;
  v20[4]  = v9;
  v20[5]  = v10;
  v20[6]  = v11;
  v20[7]  = v12;
  a4(&v17, &v21, v20);
  __n128 result = v17;
  unint64_t v16 = v19;
  v4[1].n128_u64[0]  = v18;
  v4[1].n128_u64[1]  = v16;
  *uint64_t v4 = result;
  return result;
}

Swift::Void __swiftcall Primitive3D.unapply(_:)(SPProjectiveTransform3D *a1)
{
  uint64_t v3 = v1;
  uint64_t v4 = *(void *)(v1 - 8);
  uint64_t v5 = MEMORY[0x270FA5388](a1);
  long long v7 = (char *)&v9 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(uint64_t))(v8 + 136))(v5);
  (*(void (**)(uint64_t, uint64_t))(v4 + 8))(v2, v3);
  (*(void (**)(uint64_t, char *, uint64_t))(v4 + 32))(v2, v7, v3);
}

uint64_t protocol witness for Primitive3D.unapply(_:) in conformance SPPoint3D(long long *a1, uint64_t a2, uint64_t a3)
{
  return protocol witness for Primitive3D.apply(_:) in conformance SPPoint3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPPoint3DUnapplyPose);
}

Swift::Void __swiftcall Primitive3D.unapply(_:)(SPPose3D *a1)
{
  uint64_t v3 = v1;
  uint64_t v4 = *(void *)(v1 - 8);
  uint64_t v5 = MEMORY[0x270FA5388](a1);
  long long v7 = (char *)&v9 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(uint64_t))(v8 + 144))(v5);
  (*(void (**)(uint64_t, uint64_t))(v4 + 8))(v2, v3);
  (*(void (**)(uint64_t, char *, uint64_t))(v4 + 32))(v2, v7, v3);
}

Swift::Void __swiftcall Rotatable3D.rotate(by:)(SPRotation3D *by)
{
  double v3 = by[-1].vector.f64[3];
  uint64_t v4 = MEMORY[0x270FA5388](by);
  uint64_t v6 = (char *)&v8 - ((v5 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(uint64_t))(v7 + 8))(v4);
  (*(void (**)(uint64_t, SPRotation3D *))(*(void *)&v3 + 8))(v1, by);
  (*(void (**)(uint64_t, char *, SPRotation3D *))(*(void *)&v3 + 32))(v1, v6, by);
}

Swift::Void __swiftcall Rotatable3D.rotate(by:)(simd_quatd *by)
{
  double v3 = by[-1].vector.f64[3];
  uint64_t v4 = MEMORY[0x270FA5388](by);
  uint64_t v6 = (char *)&v8 - ((v5 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(uint64_t))(v7 + 24))(v4);
  (*(void (**)(uint64_t, simd_quatd *))(*(void *)&v3 + 8))(v1, by);
  (*(void (**)(uint64_t, char *, simd_quatd *))(*(void *)&v3 + 32))(v1, v6, by);
}

double protocol witness for Translatable3D.translate(by:) in conformance SPPoint3D(double a1, double a2, double a3)
{
  long long v7 = *(_OWORD *)v3;
  double v4 = *(double *)(v3 + 16);
  double v5 = *(double *)(v3 + 24);
  v9.width  = a1;
  v9.height  = a2;
  v9.depth  = a3;
  SPVector3DMakeWithSize(&v9, (uint64_t)&v8);
  v9.depth  = v4;
  v9.vector.f64[3]  = v5;
  *(_OWORD *)&v9.width  = v7;
  *(void *)&double result = *(_OWORD *)&SPPoint3DTranslate((SPPoint3D *)&v9, &v8, v3);
  return result;
}

{
  uint64_t v3;
  long long v4;
  double v5;
  double result;
  SPVector3D v7;
  SPPoint3D v8;

  double v4 = *(_OWORD *)v3;
  double v5 = *(double *)(v3 + 24);
  v8.double z = *(double *)(v3 + 16);
  v8.vector.f64[3]  = v5;
  *(_OWORD *)&v8.x  = v4;
  v7.x  = a1;
  v7.y  = a2;
  v7.double z = a3;
  *(void *)&double result = *(_OWORD *)&SPPoint3DTranslate(&v8, &v7, v3);
  return result;
}

Swift::Void __swiftcall Translatable3D.translate(by:)(SPSize3D *by)
{
  double v3 = by[-1].vector.f64[3];
  uint64_t v4 = MEMORY[0x270FA5388](by);
  uint64_t v6 = (char *)&v8 - ((v5 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(uint64_t))(v7 + 8))(v4);
  (*(void (**)(uint64_t, SPSize3D *))(*(void *)&v3 + 8))(v1, by);
  (*(void (**)(uint64_t, char *, SPSize3D *))(*(void *)&v3 + 32))(v1, v6, by);
}

Swift::Void __swiftcall Translatable3D.translate(by:)(SPVector3D *by)
{
  double v3 = by[-1].vector.f64[3];
  uint64_t v4 = MEMORY[0x270FA5388](by);
  uint64_t v6 = (char *)&v8 - ((v5 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(uint64_t))(v7 + 24))(v4);
  (*(void (**)(uint64_t, SPVector3D *))(*(void *)&v3 + 8))(v1, by);
  (*(void (**)(uint64_t, char *, SPVector3D *))(*(void *)&v3 + 32))(v1, v6, by);
}

double protocol witness for Translatable3D.translate(by:) in conformance SPScaledPose3D(double a1, double a2, double a3)
{
  long long v13 = v3[1];
  long long v14 = *v3;
  long long v15 = v3[2];
  uint64_t v4 = *((void *)v3 + 6);
  uint64_t v5 = *((void *)v3 + 7);
  uint64_t v6 = *((void *)v3 + 8);
  v21.width  = a1;
  v21.height  = a2;
  v21.depth  = a3;
  SPVector3DMakeWithSize(&v21, (uint64_t)v20);
  *(_OWORD *)&v21.width  = v14;
  *(_OWORD *)&v21.vector.f64[2]  = v13;
  uint64_t v23 = v4;
  uint64_t v24 = v5;
  long long v22 = v15;
  uint64_t v25 = v6;
  SPScaledPose3DTranslate((uint64_t)&v21, v20, (uint64_t)v16);
  double result = *(double *)v16;
  long long v8 = v16[1];
  long long v9 = v16[2];
  uint64_t v10 = v17;
  uint64_t v11 = v18;
  uint64_t v12 = v19;
  *double v3 = v16[0];
  v3[1]  = v8;
  *((void *)v3 + 6)  = v10;
  *((void *)v3 + 7)  = v11;
  v3[2]  = v9;
  *((void *)v3 + 8)  = v12;
  return result;
}

double protocol witness for Translatable3D.translate(by:) in conformance SPScaledPose3D(float64_t a1, float64_t a2, double a3)
{
  long long v4 = *(_OWORD *)(v3 + 16);
  long long v5 = *(_OWORD *)(v3 + 32);
  uint64_t v6 = *(void *)(v3 + 48);
  uint64_t v7 = *(void *)(v3 + 56);
  uint64_t v8 = *(void *)(v3 + 64);
  v21[0]  = *(_OWORD *)v3;
  v21[1]  = v4;
  uint64_t v22 = v6;
  uint64_t v23 = v7;
  v21[2]  = v5;
  uint64_t v24 = v8;
  v19.f64[0]  = a1;
  v19.f64[1]  = a2;
  double v20 = a3;
  SPScaledPose3DTranslate((uint64_t)v21, &v19, (uint64_t)v15);
  double result = *(double *)v15;
  long long v10 = v15[1];
  long long v11 = v15[2];
  uint64_t v12 = v16;
  uint64_t v13 = v17;
  uint64_t v14 = v18;
  *(_OWORD *)uint64_t v3 = v15[0];
  *(_OWORD *)(v3 + 16)  = v10;
  *(void *)(v3 + 48)  = v12;
  *(void *)(v3 + 56)  = v13;
  *(_OWORD *)(v3 + 32)  = v11;
  *(void *)(v3 + 64)  = v14;
  return result;
}

double protocol witness for Rotatable3D.rotate(by:) in conformance SPScaledPose3D(float64x2_t a1, float64x2_t a2)
{
  long long v3 = *(_OWORD *)(v2 + 16);
  long long v4 = *(_OWORD *)(v2 + 32);
  uint64_t v5 = *(void *)(v2 + 48);
  uint64_t v6 = *(void *)(v2 + 56);
  uint64_t v7 = *(void *)(v2 + 64);
  v19[0]  = *(_OWORD *)v2;
  v19[1]  = v3;
  uint64_t v20 = v5;
  uint64_t v21 = v6;
  v19[2]  = v4;
  uint64_t v22 = v7;
  v18[0]  = a1;
  v18[1]  = a2;
  SPScaledPose3DRotate((uint64_t)v19, v18, (uint64_t)v14);
  double result = *(double *)v14;
  long long v9 = v14[1];
  long long v10 = v14[2];
  uint64_t v11 = v15;
  uint64_t v12 = v16;
  uint64_t v13 = v17;
  *(_OWORD *)uint64_t v2 = v14[0];
  *(_OWORD *)(v2 + 16)  = v9;
  *(void *)(v2 + 48)  = v11;
  *(void *)(v2 + 56)  = v12;
  *(_OWORD *)(v2 + 32)  = v10;
  *(void *)(v2 + 64)  = v13;
  return result;
}

{
  uint64_t v2;
  long long v3;
  long long v4;
  uint64_t v5;
  uint64_t v6;
  uint64_t v7;
  double result;
  long long v9;
  long long v10;
  uint64_t v11;
  uint64_t v12;
  uint64_t v13;
  _OWORD v14[3];
  uint64_t v15;
  uint64_t v16;
  uint64_t v17;
  float64x2_t v18[2];
  _OWORD v19[3];
  uint64_t v20;
  uint64_t v21;
  uint64_t v22;

  long long v3 = *(_OWORD *)(v2 + 16);
  long long v4 = *(_OWORD *)(v2 + 32);
  uint64_t v5 = *(void *)(v2 + 48);
  uint64_t v6 = *(void *)(v2 + 56);
  uint64_t v7 = *(void *)(v2 + 64);
  v19[0]  = *(_OWORD *)v2;
  v19[1]  = v3;
  uint64_t v20 = v5;
  uint64_t v21 = v6;
  v19[2]  = v4;
  uint64_t v22 = v7;
  v18[0]  = a1;
  v18[1]  = a2;
  SPScaledPose3DRotateByQuaternion((uint64_t)v19, v18, (uint64_t)v14);
  double result = *(double *)v14;
  long long v9 = v14[1];
  long long v10 = v14[2];
  uint64_t v11 = v15;
  uint64_t v12 = v16;
  uint64_t v13 = v17;
  *(_OWORD *)uint64_t v2 = v14[0];
  *(_OWORD *)(v2 + 16)  = v9;
  *(void *)(v2 + 48)  = v11;
  *(void *)(v2 + 56)  = v12;
  *(_OWORD *)(v2 + 32)  = v10;
  *(void *)(v2 + 64)  = v13;
  return result;
}

double protocol witness for Translatable3D.translate(by:) in conformance SPPose3D(double a1, double a2, double a3)
{
  long long v11 = *((_OWORD *)v3 + 1);
  long long v12 = *(_OWORD *)v3;
  long long v13 = *((_OWORD *)v3 + 2);
  double v4 = v3[6];
  double v5 = v3[7];
  v18.position.x  = a1;
  v18.position.y  = a2;
  v18.position.double z = a3;
  SPVector3DMakeWithSize((SPSize3D *)&v18, (uint64_t)&v17);
  *(_OWORD *)&v18.position.x  = v12;
  *(_OWORD *)&v18.position.vector.f64[2]  = v11;
  v18.rotation.vector.f64[2]  = v4;
  v18.rotation.vector.f64[3]  = v5;
  *(_OWORD *)v18.rotation.vector.f64  = v13;
  SPPose3DTranslate(&v18, &v17, (uint64_t)v14);
  double result = *(double *)v14;
  long long v7 = v14[1];
  long long v8 = v14[2];
  uint64_t v9 = v15;
  uint64_t v10 = v16;
  *(_OWORD *)long long v3 = v14[0];
  *((_OWORD *)v3 + 1)  = v7;
  *((void *)v3 + 6)  = v9;
  *((void *)v3 + 7)  = v10;
  *((_OWORD *)v3 + 2)  = v8;
  return result;
}

{
  uint64_t v3;
  long long v4;
  long long v5;
  double v6;
  double v7;
  double result;
  long long v9;
  long long v10;
  uint64_t v11;
  uint64_t v12;
  _OWORD v13[3];
  uint64_t v14;
  uint64_t v15;
  SPVector3D v16;
  SPPose3D v17;

  double v4 = *(_OWORD *)(v3 + 16);
  double v5 = *(_OWORD *)(v3 + 32);
  uint64_t v6 = *(double *)(v3 + 48);
  long long v7 = *(double *)(v3 + 56);
  *(_OWORD *)&v17.position.x  = *(_OWORD *)v3;
  *(_OWORD *)&v17.position.vector.f64[2]  = v4;
  v17.rotation.vector.f64[2]  = v6;
  v17.rotation.vector.f64[3]  = v7;
  *(_OWORD *)v17.rotation.vector.f64  = v5;
  v16.x  = a1;
  v16.y  = a2;
  v16.double z = a3;
  SPPose3DTranslate(&v17, &v16, (uint64_t)v13);
  double result = *(double *)v13;
  uint64_t v9 = v13[1];
  uint64_t v10 = v13[2];
  long long v11 = v14;
  long long v12 = v15;
  *(_OWORD *)long long v3 = v13[0];
  *(_OWORD *)(v3 + 16)  = v9;
  *(void *)(v3 + 48)  = v11;
  *(void *)(v3 + 56)  = v12;
  *(_OWORD *)(v3 + 32)  = v10;
  return result;
}

double protocol witness for Rotatable3D.rotate(by:) in conformance SPPose3D(uint64_t a1, uint64_t a2, __n128 a3, __n128 a4)
{
  return protocol witness for Rotatable3D.rotate(by:) in conformance SPPose3D(a3, a4, a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPPose3DRotate);
}

{
  return protocol witness for Rotatable3D.rotate(by:) in conformance SPPose3D(a3, a4, a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPPose3DRotateByQuaternion);
}

uint64_t Dimension3DSet.rawValue.getter()
{
  return *(void *)v0;
}

double protocol witness for Translatable3D.translate(by:) in conformance SPProjectiveTransform3D(uint64_t a1, uint64_t a2, double a3, double a4, double a5)
{
  return protocol witness for Translatable3D.translate(by:) in conformance SPProjectiveTransform3D(a3, a4, a5, a1, a2, (void (*)(_OWORD *__return_ptr, SPSize3D *, unsigned char *))SPProjectiveTransform3DTranslate);
}

{
  return protocol witness for Translatable3D.translate(by:) in conformance SPProjectiveTransform3D(a3, a4, a5, a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, void *))SPProjectiveTransform3DTranslate);
}

double protocol witness for Rotatable3D.rotate(by:) in conformance SPProjectiveTransform3D(uint64_t a1, uint64_t a2, __n128 a3, __n128 a4)
{
  return protocol witness for Rotatable3D.rotate(by:) in conformance SPProjectiveTransform3D(a3, a4, a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPProjectiveTransform3DRotate);
}

{
  return protocol witness for Rotatable3D.rotate(by:) in conformance SPProjectiveTransform3D(a3, a4, a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPProjectiveTransform3DRotateByQuaternion);
}

double protocol witness for Scalable3D.scaleBy(x:y:z:) in conformance SPProjectiveTransform3D(uint64_t a1, uint64_t a2)
{
  return protocol witness for Scalable3D.scaleBy(x:y:z:) in conformance SPProjectiveTransform3D(a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *))SPProjectiveTransform3DScaleBy);
}

Swift::Void __swiftcall Scalable3D.scaleBy(x:y:z:)(Swift::Double x, Swift::Double y, Swift::Double z)
{
  uint64_t v5 = v3;
  uint64_t v6 = *(void *)(v3 - 8);
  uint64_t v7 = ((uint64_t (*)())MEMORY[0x270FA5388])();
  uint64_t v9 = (char *)&v11 - ((v8 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(uint64_t))(v10 + 8))(v7);
  (*(void (**)(uint64_t, uint64_t))(v6 + 8))(v4, v5);
  (*(void (**)(uint64_t, char *, uint64_t))(v6 + 32))(v4, v9, v5);
}

double protocol witness for Scalable3D.scale(by:) in conformance SPProjectiveTransform3D(uint64_t a1, uint64_t a2, double a3, double a4, double a5)
{
  return protocol witness for Translatable3D.translate(by:) in conformance SPProjectiveTransform3D(a3, a4, a5, a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, void *))SPProjectiveTransform3DScaleBySize);
}

Swift::Void __swiftcall Scalable3D.scale(by:)(SPSize3D *by)
{
  double v3 = by[-1].vector.f64[3];
  uint64_t v4 = MEMORY[0x270FA5388](by);
  uint64_t v6 = (char *)&v8 - ((v5 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(uint64_t))(v7 + 24))(v4);
  (*(void (**)(uint64_t, SPSize3D *))(*(void *)&v3 + 8))(v1, by);
  (*(void (**)(uint64_t, char *, SPSize3D *))(*(void *)&v3 + 32))(v1, v6, by);
}

double protocol witness for Scalable3D.uniformlyScale(by:) in conformance SPProjectiveTransform3D(uint64_t a1, uint64_t a2)
{
  return protocol witness for Scalable3D.uniformlyScale(by:) in conformance SPProjectiveTransform3D(a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *))SPProjectiveTransform3DScaleUniform);
}

Swift::Void __swiftcall Scalable3D.uniformlyScale(by:)(Swift::Double by)
{
  uint64_t v3 = v1;
  uint64_t v4 = *(void *)(v1 - 8);
  uint64_t v5 = ((uint64_t (*)())MEMORY[0x270FA5388])();
  uint64_t v7 = (char *)&v9 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(uint64_t))(v8 + 40))(v5);
  (*(void (**)(uint64_t, uint64_t))(v4 + 8))(v2, v3);
  (*(void (**)(uint64_t, char *, uint64_t))(v4 + 32))(v2, v7, v3);
}

double protocol witness for Shearable3D.shear(_:) in conformance SPProjectiveTransform3D(long long *a1)
{
  char v2 = *((unsigned char *)a1 + 16);
  long long v12 = *a1;
  char v13 = v2;
  long long v3 = *(_OWORD *)(v1 + 80);
  long long v25 = *(_OWORD *)(v1 + 64);
  long long v26 = v3;
  long long v4 = *(_OWORD *)(v1 + 112);
  long long v27 = *(_OWORD *)(v1 + 96);
  long long v28 = v4;
  long long v5 = *(_OWORD *)(v1 + 16);
  float64x2_t v21 = *(float64x2_t *)v1;
  long long v22 = v5;
  float64x2_t v6 = *(float64x2_t *)(v1 + 48);
  float64x2_t v23 = *(float64x2_t *)(v1 + 32);
  float64x2_t v24 = v6;
  SPProjectiveTransform3D.sheared(_:)((uint64_t)&v12, v14, v23, v6);
  long long v7 = v18;
  *(_OWORD *)(v1 + 64)  = v17;
  *(_OWORD *)(v1 + 80)  = v7;
  long long v8 = v20;
  *(_OWORD *)(v1 + 96)  = v19;
  *(_OWORD *)(v1 + 112)  = v8;
  float64x2_t v9 = v14[1];
  *(float64x2_t *)uint64_t v1 = v14[0];
  *(float64x2_t *)(v1 + 16)  = v9;
  double result = *(double *)&v15;
  long long v11 = v16;
  *(_OWORD *)(v1 + 32)  = v15;
  *(_OWORD *)(v1 + 48)  = v11;
  return result;
}

uint64_t Shearable3D.shear(_:)(uint64_t a1, uint64_t a2)
{
  uint64_t v4 = *(void *)(a2 - 8);
  uint64_t v5 = MEMORY[0x270FA5388](a1);
  long long v7 = (char *)&v10 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  LOBYTE(v6)  = *(unsigned char *)(v5 + 16);
  long long v10 = *(_OWORD *)v5;
  char v11 = v6;
  (*(void (**)(long long *))(v8 + 8))(&v10);
  (*(void (**)(uint64_t, uint64_t))(v4 + 8))(v2, a2);
  return (*(uint64_t (**)(uint64_t, char *, uint64_t))(v4 + 32))(v2, v7, a2);
}

double protocol witness for Primitive3D.apply(_:) in conformance SPRect3D(long long *a1, uint64_t a2, uint64_t a3)
{
  return protocol witness for Primitive3D.apply(_:) in conformance SPRect3D(a1, a2, a3, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRect3DApplyAffineTransform);
}

{
  return protocol witness for Primitive3D.apply(_:) in conformance SPRect3D(a1, a2, a3, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRect3DApplyProjectiveTransform);
}

{
  return protocol witness for Primitive3D.apply(_:) in conformance SPRect3D(a1, a2, a3, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRect3DApplyPose);
}

double protocol witness for Primitive3D.apply(_:) in conformance SPRect3D(long long *a1, uint64_t a2, uint64_t a3, void (*a4)(_OWORD *__return_ptr, _OWORD *, _OWORD *))
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  long long v13 = *(_OWORD *)(v4 + 16);
  long long v14 = *(_OWORD *)(v4 + 32);
  uint64_t v15 = *(void *)(v4 + 48);
  uint64_t v16 = *(void *)(v4 + 56);
  v26[0]  = *(_OWORD *)v4;
  v26[1]  = v13;
  uint64_t v27 = v15;
  uint64_t v28 = v16;
  v26[2]  = v14;
  v25[0]  = v5;
  v25[1]  = v6;
  v25[2]  = v7;
  v25[3]  = v8;
  double v25[4] = v9;
  v25[5]  = v10;
  v25[6]  = v11;
  v25[7]  = v12;
  a4(v22, v26, v25);
  double result = *(double *)v22;
  long long v18 = v22[1];
  long long v19 = v22[2];
  uint64_t v20 = v23;
  uint64_t v21 = v24;
  *(_OWORD *)uint64_t v4 = v22[0];
  *(_OWORD *)(v4 + 16)  = v18;
  *(void *)(v4 + 48)  = v20;
  *(void *)(v4 + 56)  = v21;
  *(_OWORD *)(v4 + 32)  = v19;
  return result;
}

{
  uint64_t v4;
  long long v5;
  long long v6;
  long long v7;
  uint64_t v8;
  uint64_t v9;
  long long v10;
  long long v11;
  uint64_t v12;
  uint64_t v13;
  double result;
  long long v15;
  long long v16;
  uint64_t v17;
  uint64_t v18;
  _OWORD v19[3];
  uint64_t v20;
  uint64_t v21;
  _OWORD v22[3];
  uint64_t v23;
  uint64_t v24;
  _OWORD v25[3];
  uint64_t v26;
  uint64_t v27;

  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = *((void *)a1 + 6);
  long long v9 = *((void *)a1 + 7);
  long long v10 = *(_OWORD *)(v4 + 16);
  long long v11 = *(_OWORD *)(v4 + 32);
  long long v12 = *(void *)(v4 + 48);
  long long v13 = *(void *)(v4 + 56);
  v25[0]  = *(_OWORD *)v4;
  v25[1]  = v10;
  long long v26 = v12;
  uint64_t v27 = v13;
  v25[2]  = v11;
  v22[0]  = v5;
  v22[1]  = v6;
  uint64_t v23 = v8;
  uint64_t v24 = v9;
  v22[2]  = v7;
  a4(v19, v25, v22);
  double result = *(double *)v19;
  uint64_t v15 = v19[1];
  uint64_t v16 = v19[2];
  long long v17 = v20;
  long long v18 = v21;
  *(_OWORD *)uint64_t v4 = v19[0];
  *(_OWORD *)(v4 + 16)  = v15;
  *(void *)(v4 + 48)  = v17;
  *(void *)(v4 + 56)  = v18;
  *(_OWORD *)(v4 + 32)  = v16;
  return result;
}

BOOL protocol witness for Primitive3D.unapply(_:) in conformance SPRect3D(float64x2_t *a1, uint64_t a2, uint64_t a3)
{
  return protocol witness for Primitive3D.unapply(_:) in conformance SPRect3D(a1, a2, a3, (void (*)(float64x2_t *__return_ptr, void *, float64x2_t *))SPRect3DUnapplyAffineTransform);
}

{
  return protocol witness for Primitive3D.unapply(_:) in conformance SPRect3D(a1, a2, a3, (void (*)(float64x2_t *__return_ptr, void *, float64x2_t *))SPRect3DUnapplyProjectiveTransform);
}

BOOL protocol witness for Primitive3D.unapply(_:) in conformance SPRect3D(float64x2_t *a1, uint64_t a2, uint64_t a3, void (*a4)(float64x2_t *__return_ptr, void *, float64x2_t *))
{
  float64x2_t v5 = *a1;
  float64x2_t v6 = a1[1];
  float64x2_t v7 = a1[2];
  float64x2_t v8 = a1[3];
  float64x2_t v9 = a1[4];
  float64x2_t v10 = a1[5];
  float64x2_t v11 = a1[6];
  float64x2_t v12 = a1[7];
  uint64_t v13 = *((void *)v4 + 3);
  double v34 = v4[4];
  double v35 = v4[5];
  double v36 = *v4;
  uint64_t v37 = *((void *)v4 + 6);
  double v38 = v4[1];
  uint64_t v39 = *((void *)v4 + 7);
  uint64_t v53 = *((void *)v4 + 2);
  uint64_t v33 = v53;
  uint64_t v54 = v13;
  *(double *)float64x2_t v52 = v36;
  *(double *)&v52[1]  = v38;
  uint64_t v57 = v37;
  uint64_t v58 = v39;
  double v55 = v34;
  double v56 = v35;
  float64x2_t v44 = v5;
  float64x2_t v45 = v6;
  float64x2_t v46 = v7;
  float64x2_t v47 = v8;
  float64x2_t v48 = v9;
  float64x2_t v49 = v10;
  float64x2_t v50 = v11;
  float64x2_t v51 = v12;
  a4(&v40, v52, &v44);
  float64x2_t v14 = v40;
  unsigned long long v15 = v41;
  float64x2_t v16 = v42;
  unsigned long long v17 = v43;
  float64x2_t v45 = (float64x2_t)v41;
  float64x2_t v44 = v40;
  float64x2_t v47 = (float64x2_t)v43;
  float64x2_t v46 = v42;
  BOOL result = SPRect3DIsNull(&v44, v18, v19, v20, v21, v22, v23);
  uint64_t v25 = v33;
  float64_t v26 = v34;
  if (result)
  {
    uint64_t v27 = v13;
  }
  else
  {
    uint64_t v27 = v15 >> 64;
    uint64_t v25 = v15;
  }
  *((void *)v4 + 2)  = v25;
  *((void *)v4 + 3)  = v27;
  float64_t v28 = v36;
  if (!result) {
    float64_t v28 = v14.f64[0];
  }
  float64_t v29 = v38;
  uint64_t v30 = v39;
  if (!result)
  {
    float64_t v29 = v14.f64[1];
    float64_t v26 = v16.f64[0];
  }
  *uint64_t v4 = v28;
  v4[1]  = v29;
  float64_t v31 = v35;
  if (!result) {
    float64_t v31 = v16.f64[1];
  }
  uint64_t v32 = v37;
  if (!result)
  {
    uint64_t v30 = v17 >> 64;
    uint64_t v32 = v17;
  }
  *((void *)v4 + 6)  = v32;
  *((void *)v4 + 7)  = v30;
  v4[4]  = v26;
  v4[5]  = v31;
  return result;
}

BOOL protocol witness for Primitive3D.unapply(_:) in conformance SPRect3D(double *a1)
{
  long long v2 = *(_OWORD *)a1;
  long long v3 = *((_OWORD *)a1 + 1);
  long long v4 = *((_OWORD *)a1 + 2);
  double v5 = a1[6];
  double v6 = a1[7];
  double v7 = *(double *)(v1 + 24);
  long long v30 = *(_OWORD *)(v1 + 32);
  double v31 = *(double *)v1;
  double v32 = *(double *)(v1 + 48);
  double v33 = *(double *)(v1 + 8);
  double v34 = *(double *)(v1 + 56);
  v37.origin.double z = *(double *)(v1 + 16);
  double z = v37.origin.z;
  v37.origin.vector.f64[3]  = v7;
  v37.origin.x  = v31;
  v37.origin.y  = v33;
  v37.size.depth  = v32;
  v37.size.vector.f64[3]  = v34;
  *(_OWORD *)&v37.size.width  = v30;
  *(_OWORD *)&v36.position.x  = v2;
  *(_OWORD *)&v36.position.vector.f64[2]  = v3;
  v36.rotation.vector.f64[2]  = v5;
  v36.rotation.vector.f64[3]  = v6;
  *(_OWORD *)v36.rotation.vector.f64  = v4;
  *(void *)&double v8 = *(_OWORD *)&SPRect3DUnapplyPose(&v37, &v36, (float64x2_t *)&v35);
  x  = v35.origin.x;
  long long v11 = *(_OWORD *)&v35.origin.vector.f64[1];
  double v10 = v35.origin.vector.f64[3];
  width  = v35.size.width;
  long long v14 = *(_OWORD *)&v35.size.vector.f64[1];
  double v13 = v35.size.vector.f64[3];
  SPRect3D v37 = v35;
  BOOL result = SPRect3DIsNull((float64x2_t *)&v37, v8, v15, v16, v17, v18, v19);
  double v21 = z;
  double v22 = *(double *)&v30;
  if (result)
  {
    double v23 = v7;
  }
  else
  {
    double v21 = *((double *)&v11 + 1);
    double v23 = v10;
  }
  *(double *)(v1 + 16)  = v21;
  *(double *)(v1 + 24)  = v23;
  double v24 = v31;
  if (!result) {
    double v24 = x;
  }
  double v25 = v33;
  double v26 = v34;
  if (!result)
  {
    double v25 = *(double *)&v11;
    double v22 = width;
  }
  *(double *)uint64_t v1 = v24;
  *(double *)(v1 + 8)  = v25;
  uint64_t v27 = *((void *)&v30 + 1);
  if (!result) {
    uint64_t v27 = v14;
  }
  double v28 = v32;
  if (!result)
  {
    double v28 = *((double *)&v14 + 1);
    double v26 = v13;
  }
  *(double *)(v1 + 48)  = v28;
  *(double *)(v1 + 56)  = v26;
  *(double *)(v1 + 32)  = v22;
  *(void *)(v1 + 40)  = v27;
  return result;
}

double protocol witness for Translatable3D.translate(by:) in conformance SPRect3D(uint64_t a1, uint64_t a2, double a3, double a4, double a5)
{
  return protocol witness for Translatable3D.translate(by:) in conformance SPRect3D(a3, a4, a5, a1, a2, (void (*)(_OWORD *__return_ptr, SPSize3D *, unsigned char *))SPRect3DTranslate);
}

{
  return protocol witness for Translatable3D.translate(by:) in conformance SPRect3D(a3, a4, a5, a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, void *))SPRect3DTranslate);
}

double protocol witness for Scalable3D.scaleBy(x:y:z:) in conformance SPRect3D(float64x2_t a1, float64_t a2, float64x2_t a3)
{
  long long v4 = *(_OWORD *)(v3 + 16);
  long long v5 = *(_OWORD *)(v3 + 32);
  double v6 = *(double *)(v3 + 48);
  double v7 = *(double *)(v3 + 56);
  *(_OWORD *)&v16.origin.x  = *(_OWORD *)v3;
  *(_OWORD *)&v16.origin.vector.f64[2]  = v4;
  v16.size.depth  = v6;
  v16.size.vector.f64[3]  = v7;
  *(_OWORD *)&v16.size.width  = v5;
  SPRect3DScaleBy(&v16, a1, a2, a3, (uint64_t)v13);
  double result = *(double *)v13;
  long long v9 = v13[1];
  long long v10 = v13[2];
  uint64_t v11 = v14;
  uint64_t v12 = v15;
  *(_OWORD *)uint64_t v3 = v13[0];
  *(_OWORD *)(v3 + 16)  = v9;
  *(void *)(v3 + 48)  = v11;
  *(void *)(v3 + 56)  = v12;
  *(_OWORD *)(v3 + 32)  = v10;
  return result;
}

double protocol witness for Scalable3D.scale(by:) in conformance SPRect3D(uint64_t a1, uint64_t a2, double a3, double a4, double a5)
{
  return protocol witness for Translatable3D.translate(by:) in conformance SPRect3D(a3, a4, a5, a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, void *))SPRect3DScaleBySize);
}

double protocol witness for Scalable3D.uniformlyScale(by:) in conformance SPRect3D(float64x2_t a1)
{
  long long v2 = *(_OWORD *)(v1 + 16);
  long long v3 = *(_OWORD *)(v1 + 32);
  double v4 = *(double *)(v1 + 48);
  double v5 = *(double *)(v1 + 56);
  *(_OWORD *)&v14.origin.x  = *(_OWORD *)v1;
  *(_OWORD *)&v14.origin.vector.f64[2]  = v2;
  v14.size.depth  = v4;
  v14.size.vector.f64[3]  = v5;
  *(_OWORD *)&v14.size.width  = v3;
  SPRect3DScaleUniform(&v14, a1, (uint64_t)v11);
  double result = *(double *)v11;
  long long v7 = v11[1];
  long long v8 = v11[2];
  uint64_t v9 = v12;
  uint64_t v10 = v13;
  *(_OWORD *)uint64_t v1 = v11[0];
  *(_OWORD *)(v1 + 16)  = v7;
  *(void *)(v1 + 48)  = v9;
  *(void *)(v1 + 56)  = v10;
  *(_OWORD *)(v1 + 32)  = v8;
  return result;
}

float64_t protocol witness for Rotatable3D.rotate(by:) in conformance SPRect3D(SPRotation3D a1)
{
  long long v2 = *(_OWORD *)(v1 + 16);
  long long v3 = *(_OWORD *)(v1 + 32);
  double v4 = *(double *)(v1 + 48);
  double v5 = *(double *)(v1 + 56);
  *(_OWORD *)&v15.origin.x  = *(_OWORD *)v1;
  *(_OWORD *)&v15.origin.vector.f64[2]  = v2;
  v15.size.depth  = v4;
  v15.size.vector.f64[3]  = v5;
  *(_OWORD *)&v15.size.width  = v3;
  SPRotation3D v14 = a1;
  SPRect3DRotate(&v15, a1, (simd_quatd *)&v14, v11);
  float64_t result = v11[0].f64[0];
  float64x2_t v7 = v11[1];
  float64x2_t v8 = v11[2];
  uint64_t v9 = v12;
  uint64_t v10 = v13;
  *(float64x2_t *)uint64_t v1 = v11[0];
  *(float64x2_t *)(v1 + 16)  = v7;
  *(void *)(v1 + 48)  = v9;
  *(void *)(v1 + 56)  = v10;
  *(float64x2_t *)(v1 + 32)  = v8;
  return result;
}

float64_t protocol witness for Rotatable3D.rotate(by:) in conformance SPRect3D(simd_quatd a1)
{
  long long v2 = *(_OWORD *)(v1 + 16);
  long long v3 = *(_OWORD *)(v1 + 32);
  double v4 = *(double *)(v1 + 48);
  double v5 = *(double *)(v1 + 56);
  *(_OWORD *)&v15.origin.x  = *(_OWORD *)v1;
  *(_OWORD *)&v15.origin.vector.f64[2]  = v2;
  v15.size.depth  = v4;
  v15.size.vector.f64[3]  = v5;
  *(_OWORD *)&v15.size.width  = v3;
  simd_quatd v14 = a1;
  SPRect3DRotateByQuaternion(&v15, a1, &v14, v11);
  float64_t result = v11[0].f64[0];
  float64x2_t v7 = v11[1];
  float64x2_t v8 = v11[2];
  uint64_t v9 = v12;
  uint64_t v10 = v13;
  *(float64x2_t *)uint64_t v1 = v11[0];
  *(float64x2_t *)(v1 + 16)  = v7;
  *(void *)(v1 + 48)  = v9;
  *(void *)(v1 + 56)  = v10;
  *(float64x2_t *)(v1 + 32)  = v8;
  return result;
}

uint64_t Shearable3D._shear(shearFunc:shear:)(uint64_t (*a1)(uint64_t, void, double, double), uint64_t a2, uint64_t a3)
{
  return a1(v3, dword_228C21860[*(char *)(a3 + 16)], *(double *)a3, *(double *)(a3 + 8));
}

__n128 protocol witness for Shearable3D.shear(_:) in conformance SPRect3D(uint64_t a1, float64x2_t a2, float64x2_t a3)
{
  unint64_t v5 = *(void *)a1;
  unint64_t v4 = *(void *)(a1 + 8);
  int v6 = *(unsigned __int8 *)(a1 + 16);
  double v8 = v3[2];
  double v7 = v3[3];
  double v9 = v3[1];
  double v11 = v3[6];
  double v10 = v3[7];
  double v13 = v3[4];
  double v12 = v3[5];
  *(void *)&a2.f64[0]  = v5;
  *(void *)&a3.f64[0]  = v4;
  if (v6)
  {
    v29.origin.x  = *v3;
    v29.origin.y  = v9;
    v29.origin.double z = v8;
    v29.origin.vector.f64[3]  = v7;
    v29.size.width  = v13;
    v29.size.height  = v12;
    v29.size.depth  = v11;
    v29.size.vector.f64[3]  = v10;
    if (v6 == 1) {
      SPAxis v14 = SPAxisY;
    }
    else {
      SPAxis v14 = SPAxisZ;
    }
  }
  else
  {
    v29.origin.x  = *v3;
    v29.origin.y  = v9;
    v29.origin.double z = v8;
    v29.origin.vector.f64[3]  = v7;
    v29.size.width  = v13;
    v29.size.height  = v12;
    v29.size.depth  = v11;
    v29.size.vector.f64[3]  = v10;
    SPAxis v14 = SPAxisX;
  }
  SPRect3DShear(&v29, v14, a2, a3, (float64x2_t *)&v22);
  uint64_t v15 = v28;
  uint64_t v16 = v27;
  uint64_t v17 = v24;
  uint64_t v18 = v25;
  uint64_t v19 = v23;
  __n128 result = v22;
  uint64_t v21 = v26;
  *(__n128 *)uint64_t v3 = v22;
  *((void *)v3 + 2)  = v19;
  *((void *)v3 + 3)  = v17;
  *((void *)v3 + 4)  = v18;
  *((void *)v3 + 5)  = v21;
  *((void *)v3 + 6)  = v16;
  *((void *)v3 + 7)  = v15;
  return result;
}

unint64_t protocol witness for Volumetric.formIntersection(_:) in conformance SPRect3D(double *a1)
{
  long long v2 = *(_OWORD *)a1;
  long long v3 = *((_OWORD *)a1 + 1);
  long long v4 = *((_OWORD *)a1 + 2);
  double v5 = a1[6];
  double v6 = a1[7];
  long long v7 = *(_OWORD *)(v1 + 16);
  long long v8 = *(_OWORD *)(v1 + 32);
  uint64_t v9 = *(void *)(v1 + 48);
  uint64_t v10 = *(void *)(v1 + 56);
  *(_OWORD *)&v30.x  = *(_OWORD *)v1;
  *(_OWORD *)&v30.vector.f64[2]  = v7;
  uint64_t v32 = v9;
  uint64_t v33 = v10;
  long long v31 = v8;
  *(_OWORD *)&v29.origin.x  = v2;
  *(_OWORD *)&v29.origin.vector.f64[2]  = v3;
  v29.size.depth  = v5;
  v29.size.vector.f64[3]  = v6;
  *(_OWORD *)&v29.size.width  = v4;
  SPRect3DIntersection((float64x2_t *)&v30, &v29, (float64x2_t *)&v24);
  long long v11 = *(_OWORD *)&v24.vector.f64[2];
  long long v12 = *(_OWORD *)&v24.x;
  uint64_t v14 = v27;
  uint64_t v13 = v28;
  uint64_t v16 = v25;
  uint64_t v15 = v26;
  *(SPPoint3D *)&v30.x  = *(SPPoint3D *)&v24.x;
  unint64_t result = SPPoint3DIsFinite(&v30, v17, v18, v19, v20, v21, v22);
  if (result)
  {
    *(_OWORD *)uint64_t v1 = v12;
    *(_OWORD *)(v1 + 16)  = v11;
    *(void *)(v1 + 32)  = v16;
    *(void *)(v1 + 40)  = v15;
    *(void *)(v1 + 48)  = v14;
    *(void *)(v1 + 56)  = v13;
  }
  return result;
}

uint64_t Volumetric.formIntersection(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  uint64_t v7 = type metadata accessor for Optional();
  uint64_t v8 = *(void *)(v7 - 8);
  MEMORY[0x270FA5388](v7);
  uint64_t v10 = (char *)&v13 - v9;
  (*(void (**)(uint64_t, uint64_t, uint64_t))(a3 + 40))(a1, a2, a3);
  uint64_t v11 = *(void *)(a2 - 8);
  if ((*(unsigned int (**)(char *, uint64_t, uint64_t))(v11 + 48))(v10, 1, a2) == 1) {
    return (*(uint64_t (**)(char *, uint64_t))(v8 + 8))(v10, v7);
  }
  (*(void (**)(uint64_t, uint64_t))(v11 + 8))(v3, a2);
  return (*(uint64_t (**)(uint64_t, char *, uint64_t))(v11 + 32))(v3, v10, a2);
}

float64_t protocol witness for Volumetric.formUnion(_:) in conformance SPRect3D(double *a1)
{
  long long v2 = *(_OWORD *)a1;
  long long v3 = *((_OWORD *)a1 + 1);
  long long v4 = *((_OWORD *)a1 + 2);
  double v5 = a1[6];
  double v6 = a1[7];
  long long v7 = *(_OWORD *)(v1 + 16);
  long long v8 = *(_OWORD *)(v1 + 32);
  double v9 = *(double *)(v1 + 48);
  double v10 = *(double *)(v1 + 56);
  *(_OWORD *)&v20.origin.x  = *(_OWORD *)v1;
  *(_OWORD *)&v20.origin.vector.f64[2]  = v7;
  v20.size.depth  = v9;
  v20.size.vector.f64[3]  = v10;
  *(_OWORD *)&v20.size.width  = v8;
  *(_OWORD *)&v19.origin.x  = v2;
  *(_OWORD *)&v19.origin.vector.f64[2]  = v3;
  v19.size.depth  = v5;
  v19.size.vector.f64[3]  = v6;
  *(_OWORD *)&v19.size.width  = v4;
  SPRect3DUnion(&v20, &v19, v16);
  float64_t result = v16[0].f64[0];
  float64x2_t v12 = v16[1];
  float64x2_t v13 = v16[2];
  uint64_t v14 = v17;
  uint64_t v15 = v18;
  *(float64x2_t *)uint64_t v1 = v16[0];
  *(float64x2_t *)(v1 + 16)  = v12;
  *(void *)(v1 + 48)  = v14;
  *(void *)(v1 + 56)  = v15;
  *(float64x2_t *)(v1 + 32)  = v13;
  return result;
}

uint64_t Volumetric.formUnion(_:)(uint64_t a1, uint64_t a2)
{
  uint64_t v4 = *(void *)(a2 - 8);
  uint64_t v5 = MEMORY[0x270FA5388](a1);
  long long v7 = (char *)&v10 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  (*(void (**)(uint64_t))(v8 + 32))(v5);
  (*(void (**)(uint64_t, uint64_t))(v4 + 8))(v2, a2);
  return (*(uint64_t (**)(uint64_t, char *, uint64_t))(v4 + 32))(v2, v7, a2);
}

double protocol witness for Primitive3D.apply(_:) in conformance SPRay3D(long long *a1, uint64_t a2, uint64_t a3)
{
  return protocol witness for Primitive3D.apply(_:) in conformance SPRay3D(a1, a2, a3, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DApplyAffineTransform);
}

{
  return protocol witness for Primitive3D.apply(_:) in conformance SPRay3D(a1, a2, a3, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DApplyProjectiveTransform);
}

double protocol witness for Primitive3D.unapply(_:) in conformance SPRay3D(long long *a1, uint64_t a2, uint64_t a3)
{
  return protocol witness for Primitive3D.apply(_:) in conformance SPRay3D(a1, a2, a3, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DUnapplyAffineTransform);
}

{
  return protocol witness for Primitive3D.apply(_:) in conformance SPRay3D(a1, a2, a3, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DUnapplyProjectiveTransform);
}

{
  return protocol witness for Primitive3D.apply(_:) in conformance SPRect3D(a1, a2, a3, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DUnapplyPose);
}

double protocol witness for Primitive3D.apply(_:) in conformance SPRay3D(long long *a1, uint64_t a2, uint64_t a3, void (*a4)(_OWORD *__return_ptr, _OWORD *, _OWORD *))
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  long long v13 = *(_OWORD *)(v4 + 16);
  long long v14 = *(_OWORD *)(v4 + 32);
  uint64_t v15 = *(void *)(v4 + 48);
  uint64_t v16 = *(void *)(v4 + 56);
  v26[0]  = *(_OWORD *)v4;
  v26[1]  = v13;
  uint64_t v27 = v15;
  uint64_t v28 = v16;
  v26[2]  = v14;
  v25[0]  = v5;
  v25[1]  = v6;
  v25[2]  = v7;
  v25[3]  = v8;
  double v25[4] = v9;
  v25[5]  = v10;
  v25[6]  = v11;
  v25[7]  = v12;
  a4(v22, v26, v25);
  double result = *(double *)v22;
  long long v18 = v22[1];
  long long v19 = v22[2];
  uint64_t v20 = v23;
  uint64_t v21 = v24;
  *(_OWORD *)uint64_t v4 = v22[0];
  *(_OWORD *)(v4 + 16)  = v18;
  *(void *)(v4 + 48)  = v20;
  *(void *)(v4 + 56)  = v21;
  *(_OWORD *)(v4 + 32)  = v19;
  return result;
}

double protocol witness for Translatable3D.translate(by:) in conformance SPRay3D(uint64_t a1, uint64_t a2, double a3, double a4, double a5)
{
  return protocol witness for Translatable3D.translate(by:) in conformance SPRect3D(a3, a4, a5, a1, a2, (void (*)(_OWORD *__return_ptr, SPSize3D *, unsigned char *))SPRay3DTranslate);
}

{
  return protocol witness for Translatable3D.translate(by:) in conformance SPRect3D(a3, a4, a5, a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, void *))SPRay3DTranslate);
}

double protocol witness for Translatable3D.translate(by:) in conformance SPRect3D(double a1, double a2, double a3, uint64_t a4, uint64_t a5, void (*a6)(_OWORD *__return_ptr, SPSize3D *, unsigned char *))
{
  long long v15 = v6[1];
  long long v16 = *v6;
  long long v17 = v6[2];
  uint64_t v8 = *((void *)v6 + 6);
  uint64_t v9 = *((void *)v6 + 7);
  v22.width  = a1;
  v22.height  = a2;
  v22.depth  = a3;
  SPVector3DMakeWithSize(&v22, (uint64_t)v21);
  *(_OWORD *)&v22.width  = v16;
  *(_OWORD *)&v22.vector.f64[2]  = v15;
  uint64_t v24 = v8;
  uint64_t v25 = v9;
  long long v23 = v17;
  a6(v18, &v22, v21);
  double result = *(double *)v18;
  long long v11 = v18[1];
  long long v12 = v18[2];
  uint64_t v13 = v19;
  uint64_t v14 = v20;
  *long long v6 = v18[0];
  v6[1]  = v11;
  *((void *)v6 + 6)  = v13;
  *((void *)v6 + 7)  = v14;
  v6[2]  = v12;
  return result;
}

double protocol witness for Translatable3D.translate(by:) in conformance SPRect3D(double a1, double a2, double a3, uint64_t a4, uint64_t a5, void (*a6)(_OWORD *__return_ptr, _OWORD *, void *))
{
  long long v7 = *(_OWORD *)(v6 + 16);
  long long v8 = *(_OWORD *)(v6 + 32);
  uint64_t v9 = *(void *)(v6 + 48);
  uint64_t v10 = *(void *)(v6 + 56);
  v20[0]  = *(_OWORD *)v6;
  v20[1]  = v7;
  uint64_t v21 = v9;
  uint64_t v22 = v10;
  v20[2]  = v8;
  *(double *)uint64_t v19 = a1;
  *(double *)&v19[1]  = a2;
  *(double *)&v19[2]  = a3;
  a6(v16, v20, v19);
  double result = *(double *)v16;
  long long v12 = v16[1];
  long long v13 = v16[2];
  uint64_t v14 = v17;
  uint64_t v15 = v18;
  *(_OWORD *)uint64_t v6 = v16[0];
  *(_OWORD *)(v6 + 16)  = v12;
  *(void *)(v6 + 48)  = v14;
  *(void *)(v6 + 56)  = v15;
  *(_OWORD *)(v6 + 32)  = v13;
  return result;
}

double protocol witness for Rotatable3D.rotate(by:) in conformance SPRay3D(uint64_t a1, uint64_t a2, __n128 a3, __n128 a4)
{
  return protocol witness for Rotatable3D.rotate(by:) in conformance SPPose3D(a3, a4, a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DRotate);
}

{
  return protocol witness for Rotatable3D.rotate(by:) in conformance SPPose3D(a3, a4, a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPRay3DRotateByQuaternion);
}

double protocol witness for Rotatable3D.rotate(by:) in conformance SPPose3D(__n128 a1, __n128 a2, uint64_t a3, uint64_t a4, void (*a5)(_OWORD *__return_ptr, _OWORD *, _OWORD *))
{
  long long v6 = *(_OWORD *)(v5 + 16);
  long long v7 = *(_OWORD *)(v5 + 32);
  uint64_t v8 = *(void *)(v5 + 48);
  uint64_t v9 = *(void *)(v5 + 56);
  v19[0]  = *(_OWORD *)v5;
  v19[1]  = v6;
  uint64_t v20 = v8;
  uint64_t v21 = v9;
  v19[2]  = v7;
  v18[0]  = a1;
  v18[1]  = a2;
  a5(v15, v19, v18);
  double result = *(double *)v15;
  long long v11 = v15[1];
  long long v12 = v15[2];
  uint64_t v13 = v16;
  uint64_t v14 = v17;
  *(_OWORD *)uint64_t v5 = v15[0];
  *(_OWORD *)(v5 + 16)  = v11;
  *(void *)(v5 + 48)  = v13;
  *(void *)(v5 + 56)  = v14;
  *(_OWORD *)(v5 + 32)  = v12;
  return result;
}

{
  uint64_t v5;
  long long v6;
  long long v7;
  uint64_t v8;
  uint64_t v9;
  double result;
  long long v11;
  long long v12;
  uint64_t v13;
  uint64_t v14;
  _OWORD v15[3];
  uint64_t v16;
  uint64_t v17;
  _OWORD v18[2];
  _OWORD v19[3];
  uint64_t v20;
  uint64_t v21;

  long long v6 = *(_OWORD *)(v5 + 16);
  long long v7 = *(_OWORD *)(v5 + 32);
  uint64_t v8 = *(void *)(v5 + 48);
  uint64_t v9 = *(void *)(v5 + 56);
  v19[0]  = *(_OWORD *)v5;
  v19[1]  = v6;
  uint64_t v20 = v8;
  uint64_t v21 = v9;
  v19[2]  = v7;
  v18[0]  = a1;
  v18[1]  = a2;
  a5(v15, v19, v18);
  double result = *(double *)v15;
  long long v11 = v15[1];
  long long v12 = v15[2];
  uint64_t v13 = v16;
  uint64_t v14 = v17;
  *(_OWORD *)uint64_t v5 = v15[0];
  *(_OWORD *)(v5 + 16)  = v11;
  *(void *)(v5 + 48)  = v13;
  *(void *)(v5 + 56)  = v14;
  *(_OWORD *)(v5 + 32)  = v12;
  return result;
}

uint64_t protocol witness for Primitive3D.apply(_:) in conformance SPVector3D(long long *a1, uint64_t a2, uint64_t a3)
{
  return protocol witness for Primitive3D.apply(_:) in conformance SPVector3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPSize3DApplyAffineTransform);
}

{
  return protocol witness for Primitive3D.apply(_:) in conformance SPVector3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPSize3DApplyProjectiveTransform);
}

{
  return protocol witness for Primitive3D.unapply(_:) in conformance SPSize3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPSize3DApplyPose);
}

uint64_t protocol witness for Primitive3D.apply(_:) in conformance SPVector3D(long long *a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(long long *, _OWORD *))
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  long long v13 = *v4;
  uint64_t v14 = *((void *)v4 + 3);
  uint64_t v18 = *((void *)v4 + 2);
  uint64_t v19 = v14;
  long long v17 = v13;
  v16[0]  = v5;
  v16[1]  = v6;
  v16[2]  = v7;
  _OWORD v16[3] = v8;
  v16[4]  = v9;
  v16[5]  = v10;
  v16[6]  = v11;
  v16[7]  = v12;
  return a4(&v17, v16);
}

uint64_t protocol witness for Primitive3D.unapply(_:) in conformance SPSize3D(long long *a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(long long *, _OWORD *))
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  uint64_t v8 = *((void *)a1 + 6);
  uint64_t v9 = *((void *)a1 + 7);
  long long v10 = *v4;
  uint64_t v11 = *((void *)v4 + 3);
  uint64_t v17 = *((void *)v4 + 2);
  uint64_t v18 = v11;
  long long v16 = v10;
  v13[0]  = v5;
  v13[1]  = v6;
  uint64_t v14 = v8;
  uint64_t v15 = v9;
  v13[2]  = v7;
  return a4(&v16, v13);
}

double protocol witness for Primitive3D.unapply(_:) in conformance SPVector3D(long long *a1, uint64_t a2, uint64_t a3)
{
  *(void *)&double result = protocol witness for Primitive3D.unapply(_:) in conformance SPVector3D(a1, a2, a3, (void (*)(__n128 *__return_ptr, __n128 *, _OWORD *))SPSize3DUnapplyAffineTransform).n128_u64[0];
  return result;
}

{
  double result;

  *(void *)&double result = protocol witness for Primitive3D.unapply(_:) in conformance SPVector3D(a1, a2, a3, (void (*)(__n128 *__return_ptr, __n128 *, _OWORD *))SPSize3DUnapplyProjectiveTransform).n128_u64[0];
  return result;
}

__n128 protocol witness for Primitive3D.unapply(_:) in conformance SPVector3D(long long *a1, uint64_t a2, uint64_t a3, void (*a4)(__n128 *__return_ptr, __n128 *, _OWORD *))
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  __n128 v13 = *v4;
  unint64_t v14 = v4[1].n128_u64[1];
  unint64_t v22 = v4[1].n128_u64[0];
  unint64_t v23 = v14;
  __n128 v21 = v13;
  v20[0]  = v5;
  v20[1]  = v6;
  v20[2]  = v7;
  v20[3]  = v8;
  v20[4]  = v9;
  v20[5]  = v10;
  v20[6]  = v11;
  v20[7]  = v12;
  a4(&v17, &v21, v20);
  __n128 result = v17;
  unint64_t v16 = v19;
  v4[1].n128_u64[0]  = v18;
  v4[1].n128_u64[1]  = v16;
  *uint64_t v4 = result;
  return result;
}

uint64_t protocol witness for Primitive3D.unapply(_:) in conformance SPVector3D(long long *a1, uint64_t a2, uint64_t a3)
{
  return protocol witness for Primitive3D.unapply(_:) in conformance SPSize3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPSize3DUnapplyPose);
}

float64_t protocol witness for Rotatable3D.rotate(by:) in conformance SPVector3D(__n128 a1, __n128 a2)
{
  float64x2_t v3 = *(float64x2_t *)v2;
  uint64_t v4 = *(void *)(v2 + 24);
  uint64_t v8 = *(void *)(v2 + 16);
  uint64_t v9 = v4;
  *(__n128 *)&v6.vector.f64[2]  = a2;
  float64x2_t v7 = v3;
  *(__n128 *)v6.vector.f64  = a1;
  return SPSize3DRotate(&v7, &v6, (float64x2_t *)v2);
}

{
  uint64_t v2;
  float64x2_t v3;
  uint64_t v4;
  simd_quatd v6;
  float64x2_t v7;
  uint64_t v8;
  uint64_t v9;

  float64x2_t v3 = *(float64x2_t *)v2;
  uint64_t v4 = *(void *)(v2 + 24);
  uint64_t v8 = *(void *)(v2 + 16);
  uint64_t v9 = v4;
  *(__n128 *)&v6.vector.f64[2]  = a2;
  float64x2_t v7 = v3;
  *(__n128 *)v6.vector.f64  = a1;
  return SPSize3DRotateByQuaternion(&v7, &v6, (float64x2_t *)v2);
}

double protocol witness for Scalable3D.scale(by:) in conformance SPVector3D(float64_t a1, float64_t a2, double a3)
{
  float64x2_t v4 = *(float64x2_t *)v3;
  uint64_t v5 = *(void *)(v3 + 24);
  uint64_t v10 = *(void *)(v3 + 16);
  uint64_t v11 = v5;
  float64x2_t v9 = v4;
  v7.f64[0]  = a1;
  v7.f64[1]  = a2;
  double v8 = a3;
  *(void *)&double result = *(_OWORD *)&SPSize3DScaleBySize(&v9, &v7, v3);
  return result;
}

double protocol witness for Scalable3D.uniformlyScale(by:) in conformance SPVector3D(float64x2_t a1)
{
  long long v2 = *(_OWORD *)v1;
  double v3 = *(double *)(v1 + 24);
  v5.double z = *(double *)(v1 + 16);
  v5.vector.f64[3]  = v3;
  *(_OWORD *)&v5.x  = v2;
  *(void *)&double result = *(_OWORD *)&SPVector3DScaleUniform(&v5, a1, v1);
  return result;
}

void *protocol witness for Shearable3D.shear(_:) in conformance SPVector3D(uint64_t a1, uint64_t a2, uint64_t a3, __n128 a4, __n128 a5)
{
  return protocol witness for Shearable3D.shear(_:) in conformance SPVector3D(a1, a4, a5, a2, a3, (void *(*)(void *__return_ptr, uint64_t *, uint64_t, __n128, __n128))SPVector3DShear);
}

uint64_t Translatable3D.translated(by:)(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 24))();
}

Spatial::Dimension3DSet __swiftcall Dimension3DSet.init(rawValue:)(Spatial::Dimension3DSet rawValue)
{
  if ((unint64_t)rawValue.rawValue > 7) {
    __break(1u);
  }
  else {
    v1->rawValue  = rawValue.rawValue;
  }
  return rawValue;
}

void static Dimension3DSet.x.getter(void *a1@<X8>)
{
  *a1  = 1;
}

void static Dimension3DSet.y.getter(void *a1@<X8>)
{
  *a1  = 2;
}

void static Dimension3DSet.z.getter(void *a1@<X8>)
{
  *a1  = 4;
}

void one-time initialization function for all()
{
  v0  = qword_26DD28EB0;
  if ((unint64_t)qword_26DD28EB0 < 8)
  {
    if ((qword_26DD28EB8 & ~qword_26DD28EB0) != 0
      && (v0  = qword_26DD28EB8 | qword_26DD28EB0, (qword_26DD28EB8 | (unint64_t)qword_26DD28EB0) > 7))
    {
      __break(1u);
    }
    else if ((qword_26DD28EC0 & ~v0) == 0 || (v0 |= qword_26DD28EC0, v0 <= 7))
    {
      static Dimension3DSet.all  = v0;
      return;
    }
    __break(1u);
  }
  __break(1u);
}

uint64_t specialized SetAlgebra<>.init(arrayLiteral:)@<X0>(uint64_t a1@<X0>, unint64_t *a2@<X8>)
{
  uint64_t v3 = *(void *)(a1 + 16);
  if (v3)
  {
    unint64_t v4 = 0;
    SPVector3D v5 = (uint64_t *)(a1 + 32);
    while (1)
    {
      uint64_t v7 = *v5++;
      uint64_t v6 = v7;
      if ((v7 & ~v4) != 0)
      {
        v4 |= v6;
        if (v4 > 7) {
          break;
        }
      }
      if (!--v3) {
        goto LABEL_8;
      }
    }
    __break(1u);
  }
  unint64_t v4 = 0;
LABEL_8:
  uint64_t result = swift_bridgeObjectRelease();
  *a2  = v4;
  return result;
}

uint64_t static Dimension3DSet.all.getter@<X0>(void *a1@<X8>)
{
  if (one-time initialization token for all != -1) {
    uint64_t result = swift_once();
  }
  *a1  = static Dimension3DSet.all;
  return result;
}

unint64_t lazy protocol witness table accessor for type Dimension3DSet and conformance Dimension3DSet()
{
  unint64_t result = lazy protocol witness table cache variable for type Dimension3DSet and conformance Dimension3DSet;
  if (!lazy protocol witness table cache variable for type Dimension3DSet and conformance Dimension3DSet)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Dimension3DSet and conformance Dimension3DSet);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type Dimension3DSet and conformance Dimension3DSet;
  if (!lazy protocol witness table cache variable for type Dimension3DSet and conformance Dimension3DSet)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Dimension3DSet and conformance Dimension3DSet);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type Dimension3DSet and conformance Dimension3DSet;
  if (!lazy protocol witness table cache variable for type Dimension3DSet and conformance Dimension3DSet)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Dimension3DSet and conformance Dimension3DSet);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type Dimension3DSet and conformance Dimension3DSet;
  if (!lazy protocol witness table cache variable for type Dimension3DSet and conformance Dimension3DSet)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type Dimension3DSet and conformance Dimension3DSet);
  }
  return result;
}

void *protocol witness for OptionSet.init(rawValue:) in conformance Dimension3DSet@<X0>(void *result@<X0>, void *a2@<X8>)
{
  if (*result > 7uLL) {
    __break(1u);
  }
  else {
    *a2  = *result;
  }
  return result;
}

void *protocol witness for SetAlgebra.union(_:) in conformance Dimension3DSet@<X0>(void *result@<X0>, unint64_t *a2@<X8>)
{
  unint64_t v3 = *v2 | *result;
  if (v3 > 7) {
    __break(1u);
  }
  else {
    *a2  = v3;
  }
  return result;
}

void *protocol witness for SetAlgebra.intersection(_:) in conformance Dimension3DSet@<X0>(void *result@<X0>, void *a2@<X8>)
{
  if (*v2 > 7uLL) {
    __break(1u);
  }
  else {
    *a2  = *result & *v2;
  }
  return result;
}

void *protocol witness for SetAlgebra.symmetricDifference(_:) in conformance Dimension3DSet@<X0>(void *result@<X0>, void *a2@<X8>)
{
  if ((*v2 | *result) > 7) {
    __break(1u);
  }
  else {
    *a2  = *v2 ^ *result;
  }
  return result;
}

void *protocol witness for SetAlgebra.insert(_:) in conformance Dimension3DSet(void *a1, uint64_t *a2)
{
  return specialized OptionSet<>.insert(_:)(a1, *a2);
}

void *specialized OptionSet<>.insert(_:)(void *result, uint64_t a2)
{
  unint64_t v3 = *v2;
  if (*v2 > 7)
  {
    __break(1u);
  }
  else
  {
    unint64_t v4 = v3 & a2;
    if ((v3 & a2) == a2)
    {
LABEL_5:
      *unint64_t result = a2;
      return (void *)(v4 != a2);
    }
    unint64_t v5 = v3 | a2;
    if (v5 <= 7)
    {
      *long long v2 = v5;
      goto LABEL_5;
    }
  }
  __break(1u);
  return result;
}

unint64_t protocol witness for SetAlgebra.remove(_:) in conformance Dimension3DSet@<X0>(unint64_t *a1@<X0>, uint64_t a2@<X8>)
{
  return specialized OptionSet<>.remove(_:)(*a1, a2);
}

unint64_t specialized OptionSet<>.remove(_:)@<X0>(unint64_t result@<X0>, uint64_t a2@<X8>)
{
  uint64_t v3 = *v2;
  if ((unint64_t)*v2 > 7)
  {
    __break(1u);
  }
  else
  {
    uint64_t v4 = v3 & result;
    if ((v3 & result) == 0)
    {
      uint64_t v5 = 0;
      goto LABEL_6;
    }
    if ((v3 ^ result) <= 7)
    {
      *long long v2 = (v3 ^ result) & v3;
      uint64_t v5 = v3 & result;
LABEL_6:
      *(void *)a2  = v5;
      *(unsigned char *)(a2 + 8)  = v4 == 0;
      return result;
    }
  }
  __break(1u);
  return result;
}

uint64_t protocol witness for SetAlgebra.update(with:) in conformance Dimension3DSet@<X0>(uint64_t *a1@<X0>, uint64_t a2@<X8>)
{
  return specialized OptionSet<>.update(with:)(*a1, a2);
}

uint64_t specialized OptionSet<>.update(with:)@<X0>(uint64_t result@<X0>, uint64_t a2@<X8>)
{
  unint64_t v3 = *v2;
  unint64_t v4 = *v2 | result;
  if (v4 > 7)
  {
    __break(1u);
  }
  else
  {
    *long long v2 = v4;
    unint64_t v5 = v3 & result;
    *(void *)a2  = v5;
    *(unsigned char *)(a2 + 8)  = v5 == 0;
  }
  return result;
}

void *protocol witness for SetAlgebra.formUnion(_:) in conformance Dimension3DSet(void *result)
{
  unint64_t v2 = *v1 | *result;
  if (v2 > 7) {
    __break(1u);
  }
  else {
    unint64_t *v1 = v2;
  }
  return result;
}

void *protocol witness for SetAlgebra.formIntersection(_:) in conformance Dimension3DSet(void *result)
{
  unint64_t v2 = *v1 & *result;
  if (v2 > 7) {
    __break(1u);
  }
  else {
    unint64_t *v1 = v2;
  }
  return result;
}

void *protocol witness for SetAlgebra.formSymmetricDifference(_:) in conformance Dimension3DSet(void *result)
{
  unint64_t v2 = *v1 ^ *result;
  if (v2 > 7) {
    __break(1u);
  }
  else {
    unint64_t *v1 = v2;
  }
  return result;
}

void *protocol witness for SetAlgebra.subtracting(_:) in conformance Dimension3DSet@<X0>(void *result@<X0>, void *a2@<X8>)
{
  if ((*v2 | *result) > 7) {
    __break(1u);
  }
  else {
    *a2  = *v2 & ~*result;
  }
  return result;
}

void *protocol witness for SetAlgebra.isSubset(of:) in conformance Dimension3DSet(void *result)
{
  if (*v1 <= 7uLL) {
    return (void *)((*v1 & ~*result) == 0);
  }
  __break(1u);
  return result;
}

void *protocol witness for SetAlgebra.isDisjoint(with:) in conformance Dimension3DSet(void *result)
{
  if (*v1 <= 7uLL) {
    return (void *)((*result & *v1) == 0);
  }
  __break(1u);
  return result;
}

void *protocol witness for SetAlgebra.contains(_:) in conformance Dimension3DSet(void *result)
{
  if (*result <= 7uLL) {
    return (void *)((*result & ~*v1) == 0);
  }
  __break(1u);
  return result;
}

BOOL protocol witness for SetAlgebra.isEmpty.getter in conformance Dimension3DSet()
{
  return *v0 == 0;
}

uint64_t protocol witness for SetAlgebra.init<A>(_:) in conformance Dimension3DSet(uint64_t a1, uint64_t a2, uint64_t a3, uint64_t a4, uint64_t a5)
{
  return MEMORY[0x270F9E7D8](a1, a4, a2, a5, a3);
}

void *protocol witness for SetAlgebra.subtract(_:) in conformance Dimension3DSet(void *result)
{
  if ((*v1 | *result) > 7) {
    __break(1u);
  }
  else {
    *v1 &= ~*result;
  }
  return result;
}

void *protocol witness for RawRepresentable.init(rawValue:) in conformance Dimension3DSet@<X0>(void *result@<X0>, uint64_t a2@<X8>)
{
  if (*result > 7uLL)
  {
    __break(1u);
  }
  else
  {
    *(void *)a2  = *result;
    *(unsigned char *)(a2 + 8)  = 0;
  }
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance Dimension3DSet(void *a1@<X8>)
{
  *a1  = *v1;
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance Dimension3DSet(void *a1, void *a2)
{
  return *a1 == *a2;
}

uint64_t dispatch thunk of ClampableWithinRect.clamped(to:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  long long v3 = *(_OWORD *)(a1 + 16);
  long long v4 = *(_OWORD *)(a1 + 32);
  unint64_t v5 = *(uint64_t (**)(_OWORD *))(a3 + 8);
  uint64_t v6 = *(void *)(a1 + 48);
  uint64_t v7 = *(void *)(a1 + 56);
  v9[0]  = *(_OWORD *)a1;
  v9[1]  = v3;
  uint64_t v10 = v6;
  uint64_t v11 = v7;
  v9[2]  = v4;
  return v5(v9);
}

uint64_t dispatch thunk of ClampableWithinRect.clamp(to:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  long long v3 = *(_OWORD *)(a1 + 16);
  long long v4 = *(_OWORD *)(a1 + 32);
  unint64_t v5 = *(uint64_t (**)(_OWORD *))(a3 + 16);
  uint64_t v6 = *(void *)(a1 + 48);
  uint64_t v7 = *(void *)(a1 + 56);
  v9[0]  = *(_OWORD *)a1;
  v9[1]  = v3;
  uint64_t v10 = v6;
  uint64_t v11 = v7;
  v9[2]  = v4;
  return v5(v9);
}

uint64_t dispatch thunk of static Primitive3D.zero.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 32))();
}

uint64_t dispatch thunk of static Primitive3D.infinity.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 40))();
}

uint64_t dispatch thunk of Primitive3D.isZero.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 48))();
}

uint64_t dispatch thunk of Primitive3D.isFinite.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 56))();
}

uint64_t dispatch thunk of Primitive3D.isNaN.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 64))();
}

uint64_t dispatch thunk of Primitive3D.applying(_:)(_OWORD *a1, uint64_t a2, uint64_t a3)
{
  long long v3 = a1[1];
  long long v4 = a1[2];
  long long v5 = a1[3];
  long long v6 = a1[4];
  long long v7 = a1[5];
  long long v8 = a1[6];
  long long v9 = a1[7];
  uint64_t v10 = *(uint64_t (**)(_OWORD *))(a3 + 72);
  v12[0]  = *a1;
  v12[1]  = v3;
  v12[2]  = v4;
  v12[3]  = v5;
  v12[4]  = v6;
  v12[5]  = v7;
  v12[6]  = v8;
  v12[7]  = v9;
  return v10(v12);
}

{
  long long v3;
  long long v4;
  long long v5;
  long long v6;
  long long v7;
  long long v8;
  long long v9;
  uint64_t (*v10)(_OWORD *);
  _OWORD v12[8];

  long long v3 = a1[1];
  long long v4 = a1[2];
  long long v5 = a1[3];
  long long v6 = a1[4];
  long long v7 = a1[5];
  long long v8 = a1[6];
  long long v9 = a1[7];
  uint64_t v10 = *(uint64_t (**)(_OWORD *))(a3 + 80);
  v12[0]  = *a1;
  v12[1]  = v3;
  v12[2]  = v4;
  v12[3]  = v5;
  v12[4]  = v6;
  v12[5]  = v7;
  v12[6]  = v8;
  v12[7]  = v9;
  return v10(v12);
}

uint64_t dispatch thunk of Primitive3D.applying(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  long long v3 = *(_OWORD *)(a1 + 16);
  long long v4 = *(_OWORD *)(a1 + 32);
  long long v5 = *(uint64_t (**)(_OWORD *))(a3 + 88);
  uint64_t v6 = *(void *)(a1 + 48);
  uint64_t v7 = *(void *)(a1 + 56);
  v9[0]  = *(_OWORD *)a1;
  v9[1]  = v3;
  uint64_t v10 = v6;
  uint64_t v11 = v7;
  v9[2]  = v4;
  return v5(v9);
}

uint64_t dispatch thunk of Primitive3D.apply(_:)(_OWORD *a1, uint64_t a2, uint64_t a3)
{
  long long v3 = a1[1];
  long long v4 = a1[2];
  long long v5 = a1[3];
  long long v6 = a1[4];
  long long v7 = a1[5];
  long long v8 = a1[6];
  long long v9 = a1[7];
  uint64_t v10 = *(uint64_t (**)(_OWORD *))(a3 + 96);
  v12[0]  = *a1;
  v12[1]  = v3;
  v12[2]  = v4;
  v12[3]  = v5;
  v12[4]  = v6;
  v12[5]  = v7;
  v12[6]  = v8;
  v12[7]  = v9;
  return v10(v12);
}

{
  long long v3;
  long long v4;
  long long v5;
  long long v6;
  long long v7;
  long long v8;
  long long v9;
  uint64_t (*v10)(_OWORD *);
  _OWORD v12[8];

  long long v3 = a1[1];
  long long v4 = a1[2];
  long long v5 = a1[3];
  long long v6 = a1[4];
  long long v7 = a1[5];
  long long v8 = a1[6];
  long long v9 = a1[7];
  uint64_t v10 = *(uint64_t (**)(_OWORD *))(a3 + 104);
  v12[0]  = *a1;
  v12[1]  = v3;
  v12[2]  = v4;
  v12[3]  = v5;
  v12[4]  = v6;
  v12[5]  = v7;
  v12[6]  = v8;
  v12[7]  = v9;
  return v10(v12);
}

uint64_t dispatch thunk of Primitive3D.apply(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  long long v3 = *(_OWORD *)(a1 + 16);
  long long v4 = *(_OWORD *)(a1 + 32);
  long long v5 = *(uint64_t (**)(_OWORD *))(a3 + 112);
  uint64_t v6 = *(void *)(a1 + 48);
  uint64_t v7 = *(void *)(a1 + 56);
  v9[0]  = *(_OWORD *)a1;
  v9[1]  = v3;
  uint64_t v10 = v6;
  uint64_t v11 = v7;
  v9[2]  = v4;
  return v5(v9);
}

uint64_t dispatch thunk of Primitive3D.unapplying(_:)(_OWORD *a1, uint64_t a2, uint64_t a3)
{
  long long v3 = a1[1];
  long long v4 = a1[2];
  long long v5 = a1[3];
  long long v6 = a1[4];
  long long v7 = a1[5];
  long long v8 = a1[6];
  long long v9 = a1[7];
  uint64_t v10 = *(uint64_t (**)(_OWORD *))(a3 + 120);
  v12[0]  = *a1;
  v12[1]  = v3;
  v12[2]  = v4;
  v12[3]  = v5;
  v12[4]  = v6;
  v12[5]  = v7;
  v12[6]  = v8;
  v12[7]  = v9;
  return v10(v12);
}

{
  long long v3;
  long long v4;
  long long v5;
  long long v6;
  long long v7;
  long long v8;
  long long v9;
  uint64_t (*v10)(_OWORD *);
  _OWORD v12[8];

  long long v3 = a1[1];
  long long v4 = a1[2];
  long long v5 = a1[3];
  long long v6 = a1[4];
  long long v7 = a1[5];
  long long v8 = a1[6];
  long long v9 = a1[7];
  uint64_t v10 = *(uint64_t (**)(_OWORD *))(a3 + 136);
  v12[0]  = *a1;
  v12[1]  = v3;
  v12[2]  = v4;
  v12[3]  = v5;
  v12[4]  = v6;
  v12[5]  = v7;
  v12[6]  = v8;
  v12[7]  = v9;
  return v10(v12);
}

uint64_t dispatch thunk of Primitive3D.unapply(_:)(_OWORD *a1, uint64_t a2, uint64_t a3)
{
  long long v3 = a1[1];
  long long v4 = a1[2];
  long long v5 = a1[3];
  long long v6 = a1[4];
  long long v7 = a1[5];
  long long v8 = a1[6];
  long long v9 = a1[7];
  uint64_t v10 = *(uint64_t (**)(_OWORD *))(a3 + 128);
  v12[0]  = *a1;
  v12[1]  = v3;
  v12[2]  = v4;
  v12[3]  = v5;
  v12[4]  = v6;
  v12[5]  = v7;
  v12[6]  = v8;
  v12[7]  = v9;
  return v10(v12);
}

{
  long long v3;
  long long v4;
  long long v5;
  long long v6;
  long long v7;
  long long v8;
  long long v9;
  uint64_t (*v10)(_OWORD *);
  _OWORD v12[8];

  long long v3 = a1[1];
  long long v4 = a1[2];
  long long v5 = a1[3];
  long long v6 = a1[4];
  long long v7 = a1[5];
  long long v8 = a1[6];
  long long v9 = a1[7];
  uint64_t v10 = *(uint64_t (**)(_OWORD *))(a3 + 152);
  v12[0]  = *a1;
  v12[1]  = v3;
  v12[2]  = v4;
  v12[3]  = v5;
  v12[4]  = v6;
  v12[5]  = v7;
  v12[6]  = v8;
  v12[7]  = v9;
  return v10(v12);
}

uint64_t dispatch thunk of Primitive3D.unapplying(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  long long v3 = *(_OWORD *)(a1 + 16);
  long long v4 = *(_OWORD *)(a1 + 32);
  long long v5 = *(uint64_t (**)(_OWORD *))(a3 + 144);
  uint64_t v6 = *(void *)(a1 + 48);
  uint64_t v7 = *(void *)(a1 + 56);
  v9[0]  = *(_OWORD *)a1;
  v9[1]  = v3;
  uint64_t v10 = v6;
  uint64_t v11 = v7;
  v9[2]  = v4;
  return v5(v9);
}

uint64_t dispatch thunk of Primitive3D.unapply(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  long long v3 = *(_OWORD *)(a1 + 16);
  long long v4 = *(_OWORD *)(a1 + 32);
  long long v5 = *(uint64_t (**)(_OWORD *))(a3 + 160);
  uint64_t v6 = *(void *)(a1 + 48);
  uint64_t v7 = *(void *)(a1 + 56);
  v9[0]  = *(_OWORD *)a1;
  v9[1]  = v3;
  uint64_t v10 = v6;
  uint64_t v11 = v7;
  v9[2]  = v4;
  return v5(v9);
}

uint64_t dispatch thunk of Translatable3D.translated(by:)(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 8))();
}

{
  return (*(uint64_t (**)(void))(a2 + 24))();
}

uint64_t dispatch thunk of Translatable3D.translate(by:)(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

{
  return (*(uint64_t (**)(void))(a2 + 32))();
}

uint64_t dispatch thunk of Scalable3D.scaledBy(x:y:z:)(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 8))();
}

uint64_t dispatch thunk of Scalable3D.scaleBy(x:y:z:)(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

uint64_t dispatch thunk of Scalable3D.scaled(by:)(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 24))();
}

uint64_t dispatch thunk of Scalable3D.scale(by:)(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 32))();
}

uint64_t dispatch thunk of Scalable3D.uniformlyScaled(by:)(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 40))();
}

uint64_t dispatch thunk of Scalable3D.uniformlyScale(by:)(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 48))();
}

uint64_t dispatch thunk of Rotatable3D.rotated(by:)(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 8))();
}

{
  return (*(uint64_t (**)(void))(a2 + 24))();
}

uint64_t dispatch thunk of Rotatable3D.rotate(by:)(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 16))();
}

{
  return (*(uint64_t (**)(void))(a2 + 32))();
}

__n128 __swift_memcpy17_8(__n128 *a1, __n128 *a2)
{
  __n128 result = *a2;
  a1[1].n128_u8[0]  = a2[1].n128_u8[0];
  *a1  = result;
  return result;
}

uint64_t getEnumTagSinglePayload for AxisWithFactors(uint64_t a1, unsigned int a2)
{
  if (!a2) {
    return 0;
  }
  if (a2 >= 0xFE && *(unsigned char *)(a1 + 17)) {
    return (*(_DWORD *)a1 + 254);
  }
  unsigned int v3 = *(unsigned __int8 *)(a1 + 16);
  if (v3 <= 2) {
    int v4 = -1;
  }
  else {
    int v4 = v3 ^ 0xFF;
  }
  return (v4 + 1);
}

uint64_t storeEnumTagSinglePayload for AxisWithFactors(uint64_t result, unsigned int a2, unsigned int a3)
{
  if (a2 > 0xFD)
  {
    *(unsigned char *)(result + 16)  = 0;
    *(void *)__n128 result = a2 - 254;
    *(void *)(result + 8)  = 0;
    if (a3 >= 0xFE) {
      *(unsigned char *)(result + 17)  = 1;
    }
  }
  else
  {
    if (a3 >= 0xFE) {
      *(unsigned char *)(result + 17)  = 0;
    }
    if (a2) {
      *(unsigned char *)(result + 16)  = -(char)a2;
    }
  }
  return result;
}

uint64_t getEnumTag for AxisWithFactors(uint64_t a1)
{
  return *(unsigned __int8 *)(a1 + 16);
}

uint64_t destructiveInjectEnumTag for AxisWithFactors(uint64_t result, char a2)
{
  *(unsigned char *)(result + 16)  = a2;
  return result;
}

ValueMetadata *type metadata accessor for AxisWithFactors()
{
  return &type metadata for AxisWithFactors;
}

uint64_t dispatch thunk of Shearable3D.sheared(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 8))();
}

uint64_t dispatch thunk of Shearable3D.shear(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 16))();
}

uint64_t dispatch thunk of Volumetric.size.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 8))();
}

uint64_t dispatch thunk of Volumetric.contains(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 16))();
}

uint64_t dispatch thunk of Volumetric.contains(point:)(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 24))();
}

uint64_t dispatch thunk of Volumetric.union(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 32))();
}

uint64_t dispatch thunk of Volumetric.intersection(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 40))();
}

uint64_t dispatch thunk of Volumetric.formIntersection(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 48))();
}

uint64_t dispatch thunk of Volumetric.formUnion(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 56))();
}

uint64_t dispatch thunk of Volumetric.containsAny(of:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 64))();
}

uint64_t dispatch thunk of Volumetric.contains(anyOf:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 72))();
}

uint64_t dispatch thunk of VolumetricLocatable3D.intersects(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 8))();
}

ValueMetadata *type metadata accessor for Dimension3DSet()
{
  return &type metadata for Dimension3DSet;
}

uint64_t dispatch thunk of Transform3D.init(shear:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 40))();
}

uint64_t dispatch thunk of Transform3D.init(scale:)(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 48))();
}

uint64_t dispatch thunk of Transform3D.init(rotation:)(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 56))();
}

uint64_t dispatch thunk of Transform3D.init(translation:)(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 64))();
}

{
  return (*(uint64_t (**)(void))(a2 + 72))();
}

uint64_t dispatch thunk of Transform3D.init(scale:rotation:translation:)(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 80))();
}

{
  return (*(uint64_t (**)(void))(a2 + 88))();
}

uint64_t dispatch thunk of Transform3D.isIdentity.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 96))();
}

uint64_t dispatch thunk of static Transform3D.identity.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 104))();
}

uint64_t dispatch thunk of Transform3D.isTranslation.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 112))();
}

uint64_t dispatch thunk of Transform3D.isUniform.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 120))();
}

uint64_t dispatch thunk of Transform3D.isUniform(overDimensions:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 128))();
}

uint64_t dispatch thunk of Transform3D.isRectilinear.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 136))();
}

uint64_t dispatch thunk of Transform3D.concatenating(_:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 144))();
}

uint64_t dispatch thunk of Transform3D.inverse.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 152))();
}

uint64_t dispatch thunk of Transform3D.isApproximatelyEqual(to:tolerance:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 160))();
}

uint64_t dispatch thunk of Transform3D.translation.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 168))();
}

uint64_t dispatch thunk of Transform3D.translation.setter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 176))();
}

uint64_t dispatch thunk of Transform3D.translation.modify(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 184))();
}

uint64_t dispatch thunk of Transform3D.rotation.getter(uint64_t a1, uint64_t a2)
{
  return (*(uint64_t (**)(void))(a2 + 192))();
}

uint64_t dispatch thunk of Transform3D.flip(along:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 200))();
}

uint64_t dispatch thunk of Transform3D.flipped(along:)(uint64_t a1, uint64_t a2, uint64_t a3)
{
  return (*(uint64_t (**)(void))(a3 + 208))();
}

uint64_t protocol witness for Primitive3D.apply(_:) in conformance SPSize3D(long long *a1, uint64_t a2, uint64_t a3)
{
  return protocol witness for Primitive3D.apply(_:) in conformance SPSize3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPSize3DApplyAffineTransform);
}

{
  return protocol witness for Primitive3D.apply(_:) in conformance SPSize3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPSize3DApplyProjectiveTransform);
}

uint64_t protocol witness for Primitive3D.apply(_:) in conformance SPSize3D(long long *a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(long long *, _OWORD *))
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  long long v13 = *v4;
  uint64_t v14 = *((void *)v4 + 3);
  uint64_t v18 = *((void *)v4 + 2);
  uint64_t v19 = v14;
  long long v17 = v13;
  v16[0]  = v5;
  v16[1]  = v6;
  v16[2]  = v7;
  _OWORD v16[3] = v8;
  v16[4]  = v9;
  v16[5]  = v10;
  v16[6]  = v11;
  v16[7]  = v12;
  return a4(&v17, v16);
}

BOOL protocol witness for Primitive3D.unapply(_:) in conformance SPSize3D(long long *a1, uint64_t a2, uint64_t a3)
{
  return protocol witness for Primitive3D.unapply(_:) in conformance SPSize3D(a1, a2, a3, (void (*)(SPSize3D *__return_ptr, void *, SPSize3D *))SPSize3DUnapplyAffineTransform);
}

{
  return protocol witness for Primitive3D.unapply(_:) in conformance SPSize3D(a1, a2, a3, (void (*)(SPSize3D *__return_ptr, void *, SPSize3D *))SPSize3DUnapplyProjectiveTransform);
}

BOOL protocol witness for Primitive3D.unapply(_:) in conformance SPSize3D(long long *a1, uint64_t a2, uint64_t a3, void (*a4)(SPSize3D *__return_ptr, void *, SPSize3D *))
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  double v13 = *v4;
  uint64_t v14 = *((void *)v4 + 1);
  uint64_t v15 = *((void *)v4 + 2);
  double v16 = v4[3];
  v33[2]  = v15;
  *(double *)&v33[3]  = v16;
  *(double *)uint64_t v33 = v13;
  v33[1]  = v14;
  *(_OWORD *)&v26.width  = v5;
  *(_OWORD *)&v26.vector.f64[2]  = v6;
  long long v27 = v7;
  long long v28 = v8;
  long long v29 = v9;
  long long v30 = v10;
  long long v31 = v11;
  long long v32 = v12;
  a4(&v25, v33, &v26);
  width  = v25.width;
  long long v19 = *(_OWORD *)&v25.vector.f64[1];
  double v18 = v25.vector.f64[3];
  *(SPSize3D *)&v26.width  = *(SPSize3D *)&v25.width;
  BOOL result = SPSize3DIsValid(&v26);
  if (result) {
    double v21 = width;
  }
  else {
    double v21 = v13;
  }
  if (result) {
    uint64_t v22 = *((void *)&v19 + 1);
  }
  else {
    uint64_t v22 = v15;
  }
  if (result) {
    double v23 = v18;
  }
  else {
    double v23 = v16;
  }
  *((void *)v4 + 2)  = v22;
  v4[3]  = v23;
  if (result) {
    uint64_t v24 = v19;
  }
  else {
    uint64_t v24 = v14;
  }
  *int v4 = v21;
  *((void *)v4 + 1)  = v24;
  return result;
}

double protocol witness for Scalable3D.scaleBy(x:y:z:) in conformance SPVector3D(float64x2_t a1, float64_t a2, float64x2_t a3)
{
  long long v4 = *(_OWORD *)v3;
  double v5 = *(double *)(v3 + 24);
  v7.double z = *(double *)(v3 + 16);
  v7.vector.f64[3]  = v5;
  *(_OWORD *)&v7.x  = v4;
  *(void *)&double result = *(_OWORD *)&SPVector3DScaleBy(&v7, a1, a2, a3, v3);
  return result;
}

void *protocol witness for Shearable3D.shear(_:) in conformance SPSize3D(uint64_t a1, uint64_t a2, uint64_t a3, __n128 a4, __n128 a5)
{
  return protocol witness for Shearable3D.shear(_:) in conformance SPVector3D(a1, a4, a5, a2, a3, (void *(*)(void *__return_ptr, uint64_t *, uint64_t, __n128, __n128))SPSize3DShear);
}

void *protocol witness for Shearable3D.shear(_:) in conformance SPVector3D(uint64_t a1, __n128 a2, __n128 a3, uint64_t a4, uint64_t a5, void *(*a6)(void *__return_ptr, uint64_t *, uint64_t, __n128, __n128))
{
  uint64_t v8 = v6[2];
  uint64_t v7 = v6[3];
  uint64_t v9 = v6[1];
  a2.n128_u64[0]  = *(void *)a1;
  a3.n128_u64[0]  = *(void *)(a1 + 8);
  if (*(unsigned char *)(a1 + 16))
  {
    if (*(unsigned char *)(a1 + 16) == 1)
    {
      uint64_t v16 = *v6;
      uint64_t v17 = v9;
      uint64_t v18 = v8;
      uint64_t v19 = v7;
      uint64_t v10 = 2;
    }
    else
    {
      uint64_t v16 = *v6;
      uint64_t v17 = v9;
      uint64_t v18 = v8;
      uint64_t v19 = v7;
      uint64_t v10 = 4;
    }
  }
  else
  {
    uint64_t v16 = *v6;
    uint64_t v17 = v9;
    uint64_t v18 = v8;
    uint64_t v19 = v7;
    uint64_t v10 = 1;
  }
  double result = a6(v15, &v16, v10, a2, a3);
  uint64_t v13 = v15[2];
  uint64_t v12 = v15[3];
  uint64_t v14 = v15[1];
  *long long v6 = v15[0];
  v6[1]  = v14;
  v6[2]  = v13;
  v6[3]  = v12;
  return result;
}

float64x2_t *protocol witness for Volumetric.formIntersection(_:) in conformance SPSize3D(double *a1)
{
  long long v2 = *(_OWORD *)a1;
  double v3 = a1[2];
  double v4 = a1[3];
  long long v5 = *(_OWORD *)v1;
  double v6 = v1[3];
  v16.depth  = v1[2];
  v16.vector.f64[3]  = v6;
  *(_OWORD *)&v16.width  = v5;
  v15.depth  = v3;
  v15.vector.f64[3]  = v4;
  *(_OWORD *)&v15.width  = v2;
  double result = SPSize3DIntersection((float64x2_t *)&v16, &v15, &v11);
  double v9 = v13;
  uint64_t v8 = v14;
  double v10 = v12;
  if (v11 != 0.0 || v12 != 0.0 || v13 != 0.0)
  {
    double *v1 = v11;
    v1[1]  = v10;
    v1[2]  = v9;
    *((void *)v1 + 3)  = v8;
  }
  return result;
}

double protocol witness for Volumetric.formUnion(_:) in conformance SPSize3D(double *a1)
{
  long long v2 = *(_OWORD *)a1;
  double v3 = a1[2];
  double v4 = a1[3];
  long long v5 = *(_OWORD *)v1;
  double v6 = *(double *)(v1 + 24);
  v9.depth  = *(double *)(v1 + 16);
  v9.vector.f64[3]  = v6;
  *(_OWORD *)&v9.width  = v5;
  v8.depth  = v3;
  v8.vector.f64[3]  = v4;
  *(_OWORD *)&v8.width  = v2;
  *(void *)&double result = *(_OWORD *)&SPSize3DUnion(&v9, &v8, v1);
  return result;
}

double protocol witness for Rotatable3D.rotate(by:) in conformance SPRotation3D(float64x2_t a1, float64x2_t a2)
{
  float64x2_t v3 = *v2;
  float64x2_t v4 = v2[1];
  v8[0]  = a1;
  v8[1]  = a2;
  v7[0]  = v3;
  v7[1]  = v4;
  *(float64x2_t *)v9.vector.f64  = simd_mul(v8, v7, v6);
  *(void *)&double result = SPRotation3DMakeWithQuaternion(v9, (uint64_t)v6, v2).n128_u64[0];
  return result;
}

double protocol witness for Translatable3D.translate(by:) in conformance SPAffineTransform3D(uint64_t a1, uint64_t a2, double a3, double a4, double a5)
{
  return protocol witness for Translatable3D.translate(by:) in conformance SPProjectiveTransform3D(a3, a4, a5, a1, a2, (void (*)(_OWORD *__return_ptr, SPSize3D *, unsigned char *))SPAffineTransform3DTranslate);
}

{
  return protocol witness for Translatable3D.translate(by:) in conformance SPProjectiveTransform3D(a3, a4, a5, a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, void *))SPAffineTransform3DTranslate);
}

double protocol witness for Translatable3D.translate(by:) in conformance SPProjectiveTransform3D(double a1, double a2, double a3, uint64_t a4, uint64_t a5, void (*a6)(_OWORD *__return_ptr, SPSize3D *, unsigned char *))
{
  long long v22 = v6[2];
  long long v23 = *v6;
  long long v18 = v6[3];
  long long v19 = v6[1];
  long long v20 = v6[6];
  long long v21 = v6[4];
  long long v16 = v6[7];
  long long v17 = v6[5];
  v26.width  = a1;
  v26.height  = a2;
  v26.depth  = a3;
  SPVector3DMakeWithSize(&v26, (uint64_t)v25);
  *(_OWORD *)&v26.width  = v23;
  *(_OWORD *)&v26.vector.f64[2]  = v19;
  long long v27 = v22;
  long long v28 = v18;
  long long v29 = v21;
  long long v30 = v17;
  long long v31 = v20;
  long long v32 = v16;
  a6(v24, &v26, v25);
  double result = *(double *)v24;
  long long v9 = v24[1];
  long long v10 = v24[2];
  long long v11 = v24[3];
  long long v12 = v24[4];
  long long v13 = v24[5];
  long long v14 = v24[6];
  long long v15 = v24[7];
  *double v6 = v24[0];
  v6[1]  = v9;
  v6[2]  = v10;
  v6[3]  = v11;
  v6[4]  = v12;
  v6[5]  = v13;
  v6[6]  = v14;
  v6[7]  = v15;
  return result;
}

double protocol witness for Rotatable3D.rotate(by:) in conformance SPAffineTransform3D(uint64_t a1, uint64_t a2, __n128 a3, __n128 a4)
{
  return protocol witness for Rotatable3D.rotate(by:) in conformance SPProjectiveTransform3D(a3, a4, a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPAffineTransform3DRotate);
}

{
  return protocol witness for Rotatable3D.rotate(by:) in conformance SPProjectiveTransform3D(a3, a4, a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, _OWORD *))SPAffineTransform3DRotateByQuaternion);
}

double protocol witness for Rotatable3D.rotate(by:) in conformance SPProjectiveTransform3D(__n128 a1, __n128 a2, uint64_t a3, uint64_t a4, void (*a5)(_OWORD *__return_ptr, _OWORD *, _OWORD *))
{
  long long v6 = v5[1];
  long long v7 = v5[2];
  long long v8 = v5[3];
  long long v9 = v5[4];
  long long v10 = v5[5];
  long long v11 = v5[6];
  long long v12 = v5[7];
  v23[0]  = *v5;
  v23[1]  = v6;
  v23[2]  = v7;
  v23[3]  = v8;
  v23[4]  = v9;
  v23[5]  = v10;
  v23[6]  = v11;
  v23[7]  = v12;
  v22[0]  = a1;
  v22[1]  = a2;
  a5(v21, v23, v22);
  double result = *(double *)v21;
  long long v14 = v21[1];
  long long v15 = v21[2];
  long long v16 = v21[3];
  long long v17 = v21[4];
  long long v18 = v21[5];
  long long v19 = v21[6];
  long long v20 = v21[7];
  *long long v5 = v21[0];
  v5[1]  = v14;
  v5[2]  = v15;
  v5[3]  = v16;
  void v5[4] = v17;
  v5[5]  = v18;
  v5[6]  = v19;
  v5[7]  = v20;
  return result;
}

{
  _OWORD *v5;
  long long v6;
  long long v7;
  long long v8;
  long long v9;
  long long v10;
  long long v11;
  long long v12;
  double result;
  long long v14;
  long long v15;
  long long v16;
  long long v17;
  long long v18;
  long long v19;
  long long v20;
  _OWORD v21[8];
  _OWORD v22[2];
  _OWORD v23[8];

  long long v6 = v5[1];
  long long v7 = v5[2];
  long long v8 = v5[3];
  long long v9 = v5[4];
  long long v10 = v5[5];
  long long v11 = v5[6];
  long long v12 = v5[7];
  v23[0]  = *v5;
  v23[1]  = v6;
  v23[2]  = v7;
  v23[3]  = v8;
  v23[4]  = v9;
  v23[5]  = v10;
  v23[6]  = v11;
  v23[7]  = v12;
  v22[0]  = a1;
  v22[1]  = a2;
  a5(v21, v23, v22);
  double result = *(double *)v21;
  long long v14 = v21[1];
  long long v15 = v21[2];
  long long v16 = v21[3];
  long long v17 = v21[4];
  long long v18 = v21[5];
  long long v19 = v21[6];
  long long v20 = v21[7];
  *long long v5 = v21[0];
  v5[1]  = v14;
  v5[2]  = v15;
  v5[3]  = v16;
  void v5[4] = v17;
  v5[5]  = v18;
  v5[6]  = v19;
  v5[7]  = v20;
  return result;
}

double protocol witness for Scalable3D.scaleBy(x:y:z:) in conformance SPAffineTransform3D(uint64_t a1, uint64_t a2)
{
  return protocol witness for Scalable3D.scaleBy(x:y:z:) in conformance SPProjectiveTransform3D(a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *))SPAffineTransform3DScaleBy);
}

double protocol witness for Scalable3D.scaleBy(x:y:z:) in conformance SPProjectiveTransform3D(uint64_t a1, uint64_t a2, void (*a3)(_OWORD *__return_ptr, _OWORD *))
{
  long long v4 = v3[1];
  long long v5 = v3[2];
  long long v6 = v3[3];
  long long v7 = v3[4];
  long long v8 = v3[5];
  long long v9 = v3[6];
  long long v10 = v3[7];
  v20[0]  = *v3;
  v20[1]  = v4;
  v20[2]  = v5;
  v20[3]  = v6;
  v20[4]  = v7;
  v20[5]  = v8;
  v20[6]  = v9;
  v20[7]  = v10;
  a3(v19, v20);
  double result = *(double *)v19;
  long long v12 = v19[1];
  long long v13 = v19[2];
  long long v14 = v19[3];
  long long v15 = v19[4];
  long long v16 = v19[5];
  long long v17 = v19[6];
  long long v18 = v19[7];
  *float64x2_t v3 = v19[0];
  v3[1]  = v12;
  v3[2]  = v13;
  v3[3]  = v14;
  v3[4]  = v15;
  v3[5]  = v16;
  v3[6]  = v17;
  v3[7]  = v18;
  return result;
}

double protocol witness for Scalable3D.scale(by:) in conformance SPAffineTransform3D(uint64_t a1, uint64_t a2, double a3, double a4, double a5)
{
  return protocol witness for Translatable3D.translate(by:) in conformance SPProjectiveTransform3D(a3, a4, a5, a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *, void *))SPAffineTransform3DScaleBySize);
}

double protocol witness for Translatable3D.translate(by:) in conformance SPProjectiveTransform3D(double a1, double a2, double a3, uint64_t a4, uint64_t a5, void (*a6)(_OWORD *__return_ptr, _OWORD *, void *))
{
  long long v7 = v6[1];
  long long v8 = v6[2];
  long long v9 = v6[3];
  long long v10 = v6[4];
  long long v11 = v6[5];
  long long v12 = v6[6];
  long long v13 = v6[7];
  v24[0]  = *v6;
  v24[1]  = v7;
  v24[2]  = v8;
  v24[3]  = v9;
  v24[4]  = v10;
  v24[5]  = v11;
  v24[6]  = v12;
  v24[7]  = v13;
  *(double *)long long v23 = a1;
  *(double *)&v23[1]  = a2;
  *(double *)&v23[2]  = a3;
  a6(v22, v24, v23);
  double result = *(double *)v22;
  long long v15 = v22[1];
  long long v16 = v22[2];
  long long v17 = v22[3];
  long long v18 = v22[4];
  long long v19 = v22[5];
  long long v20 = v22[6];
  long long v21 = v22[7];
  *long long v6 = v22[0];
  v6[1]  = v15;
  v6[2]  = v16;
  v6[3]  = v17;
  v6[4]  = v18;
  v6[5]  = v19;
  v6[6]  = v20;
  v6[7]  = v21;
  return result;
}

double protocol witness for Scalable3D.uniformlyScale(by:) in conformance SPAffineTransform3D(uint64_t a1, uint64_t a2)
{
  return protocol witness for Scalable3D.uniformlyScale(by:) in conformance SPProjectiveTransform3D(a1, a2, (void (*)(_OWORD *__return_ptr, _OWORD *))SPAffineTransform3DScaleUniform);
}

double protocol witness for Scalable3D.uniformlyScale(by:) in conformance SPProjectiveTransform3D(uint64_t a1, uint64_t a2, void (*a3)(_OWORD *__return_ptr, _OWORD *))
{
  long long v4 = v3[1];
  long long v5 = v3[2];
  long long v6 = v3[3];
  long long v7 = v3[4];
  long long v8 = v3[5];
  long long v9 = v3[6];
  long long v10 = v3[7];
  v20[0]  = *v3;
  v20[1]  = v4;
  v20[2]  = v5;
  v20[3]  = v6;
  v20[4]  = v7;
  v20[5]  = v8;
  v20[6]  = v9;
  v20[7]  = v10;
  a3(v19, v20);
  double result = *(double *)v19;
  long long v12 = v19[1];
  long long v13 = v19[2];
  long long v14 = v19[3];
  long long v15 = v19[4];
  long long v16 = v19[5];
  long long v17 = v19[6];
  long long v18 = v19[7];
  *float64x2_t v3 = v19[0];
  v3[1]  = v12;
  v3[2]  = v13;
  v3[3]  = v14;
  v3[4]  = v15;
  v3[5]  = v16;
  v3[6]  = v17;
  v3[7]  = v18;
  return result;
}

__n128 protocol witness for Shearable3D.shear(_:) in conformance SPAffineTransform3D(uint64_t a1, float64x2_t a2, float64x2_t a3)
{
  long long v4 = *(_OWORD *)(v3 + 16);
  long long v6 = *(_OWORD *)(v3 + 32);
  long long v5 = *(_OWORD *)(v3 + 48);
  long long v8 = *(_OWORD *)(v3 + 64);
  long long v7 = *(_OWORD *)(v3 + 80);
  long long v10 = *(_OWORD *)(v3 + 96);
  long long v9 = *(_OWORD *)(v3 + 112);
  a2.f64[0]  = *(float64_t *)a1;
  a3.f64[0]  = *(float64_t *)(a1 + 8);
  if (*(unsigned char *)(a1 + 16))
  {
    if (*(unsigned char *)(a1 + 16) == 1)
    {
      float64x2_t v20 = *(float64x2_t *)v3;
      long long v21 = v4;
      long long v22 = v6;
      long long v23 = v5;
      long long v24 = v8;
      long long v25 = v7;
      long long v26 = v10;
      long long v27 = v9;
      int v11 = 2;
    }
    else
    {
      float64x2_t v20 = *(float64x2_t *)v3;
      long long v21 = v4;
      long long v22 = v6;
      long long v23 = v5;
      long long v24 = v8;
      long long v25 = v7;
      long long v26 = v10;
      long long v27 = v9;
      int v11 = 4;
    }
  }
  else
  {
    float64x2_t v20 = *(float64x2_t *)v3;
    long long v21 = v4;
    long long v22 = v6;
    long long v23 = v5;
    long long v24 = v8;
    long long v25 = v7;
    long long v26 = v10;
    long long v27 = v9;
    int v11 = 1;
  }
  SPAffineTransform3DShear(&v20, v11, v19, a2, a3);
  __n128 result = (__n128)v19[1];
  float64x2_t v14 = v19[2];
  float64x2_t v13 = v19[3];
  float64x2_t v16 = v19[4];
  float64x2_t v15 = v19[5];
  float64x2_t v18 = v19[6];
  float64x2_t v17 = v19[7];
  *(float64x2_t *)uint64_t v3 = v19[0];
  *(__n128 *)(v3 + 16)  = result;
  *(float64x2_t *)(v3 + 32)  = v14;
  *(float64x2_t *)(v3 + 48)  = v13;
  *(float64x2_t *)(v3 + 64)  = v16;
  *(float64x2_t *)(v3 + 80)  = v15;
  *(float64x2_t *)(v3 + 96)  = v18;
  *(float64x2_t *)(v3 + 112)  = v17;
  return result;
}

float64x2_t *SPAffineTransform3DShear@<X0>(float64x2_t *result@<X0>, int a2@<W1>, float64x2_t *a3@<X8>, float64x2_t a4@<Q0>, float64x2_t a5@<Q1>)
{
  if (a2 == 1)
  {
    __asm { FMOV            V18.2D, #1.0 }
    v5.f64[0]  = _Q18.f64[0];
    v5.f64[1]  = a4.f64[0];
    float64x2_t v6 = (float64x2_t)xmmword_228C1F7A0;
    a4  = 0uLL;
    float64x2_t v7 = 0uLL;
  }
  else
  {
    if (a2 == 2)
    {
      __asm { FMOV            V18.2D, #1.0 }
      v6.f64[1]  = _Q18.f64[1];
      v6.f64[0]  = a4.f64[0];
      float64x2_t v5 = (float64x2_t)xmmword_228C1F7D0;
      a4  = 0uLL;
      float64x2_t v7 = a5;
    }
    else
    {
      if (a2 != 4)
      {
        float64x2_t v41 = result[5];
        a3[4]  = result[4];
        a3[5]  = v41;
        float64x2_t v42 = result[7];
        a3[6]  = result[6];
        a3[7]  = v42;
        float64x2_t v43 = result[1];
        *a3  = *result;
        a3[1]  = v43;
        float64x2_t v44 = result[3];
        a3[2]  = result[2];
        a3[3]  = v44;
        return result;
      }
      a4.f64[1]  = a5.f64[0];
      float64x2_t v5 = (float64x2_t)xmmword_228C1F7D0;
      float64x2_t v6 = (float64x2_t)xmmword_228C1F7A0;
      float64x2_t v7 = 0uLL;
      __asm { FMOV            V18.2D, #1.0 }
    }
    a5  = 0uLL;
  }
  float64x2_t v14 = *result;
  float64x2_t v13 = result[1];
  float64x2_t v16 = result[2];
  float64x2_t v15 = result[3];
  float64x2_t v18 = result[4];
  float64x2_t v17 = result[5];
  float64x2_t v20 = result[6];
  float64x2_t v19 = result[7];
  float64x2_t v21 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v23 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v22 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v25 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v24 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v26 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v23, v16), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], *result)), (int8x16_t)vceqq_f64(v25, v18));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v26, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v22, v15), (int8x16_t)vceqq_f64(v21, v13)), (int8x16_t)vceqq_f64(v24, v17)), (int8x16_t)v26)).u64[0] & 0x8000000000000000) != 0&& (int8x16_t v27 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v22, v7), (int8x16_t)vceqq_f64(v21, a5)), (int8x16_t)vceqq_f64(v24, _Q18)), v28 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v23, v6), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v5)), (int8x16_t)vceqq_f64(v25, a4)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v28, 1), vandq_s8(v27, (int8x16_t)v28)).u64[0] & 0x8000000000000000) != 0))
  {
    *a3  = v14;
    a3[1]  = v13;
    a3[2]  = v16;
    a3[3]  = v15;
    a3[4]  = v18;
    a3[5]  = v17;
    a3[6]  = vaddq_f64(v20, (float64x2_t)0);
    a3[7]  = vaddq_f64(v19, (float64x2_t)0);
  }
  else
  {
    int64x2_t v29 = vceqzq_f64(v20);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v29, 1), vandq_s8((int8x16_t)vceqzq_f64(v19), (int8x16_t)v29)).u64[0] & 0x8000000000000000) != 0)
    {
      uint64_t v45 = 0;
      float64x2_t v54 = v5;
      float64x2_t v55 = a5;
      float64x2_t v56 = v6;
      float64x2_t v57 = v7;
      float64x2_t v58 = a4;
      float64x2_t v59 = _Q18;
      float64x2_t v63 = 0u;
      v64  = 0u;
      v65  = 0u;
      float64x2_t v66 = 0u;
      float64x2_t v67 = 0u;
      float64x2_t v68 = 0u;
      *(void *)&v14.f64[1]  = vextq_s8((int8x16_t)v14, (int8x16_t)v14, 8uLL).u64[0];
      *(void *)&v16.f64[1]  = vextq_s8((int8x16_t)v16, (int8x16_t)v16, 8uLL).u64[0];
      *(void *)&v18.f64[1]  = vextq_s8((int8x16_t)v18, (int8x16_t)v18, 8uLL).u64[0];
      do
      {
        float64x2_t v47 = *(float64x2_t *)((char *)&v54 + v45);
        float64x2_t v46 = *(float64x2_t *)((char *)&v54 + v45 + 16);
        float64x2_t v48 = (float64x2_t *)((char *)&v63 + v45);
        *float64x2_t v48 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v14, v47.f64[0]), v16, v47, 1), v18, v46.f64[0]);
        v48[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v13, v47), v15, v47, 1), v46, v17);
        v45 += 32;
      }
      while (v45 != 96);
      float64x2_t v49 = v64;
      float64x2_t v50 = v65;
      float64x2_t v51 = v66;
      float64x2_t v52 = v67;
      float64x2_t v53 = v68;
      *a3  = v63;
      a3[1]  = v49;
      a3[2]  = v50;
      a3[3]  = v51;
      a3[4]  = v52;
      a3[5]  = v53;
      a3[6]  = v20;
      a3[7]  = v19;
    }
    else
    {
      uint64_t v30 = 0;
      v13.f64[1]  = 0.0;
      v15.f64[1]  = 0.0;
      v17.f64[1]  = 0.0;
      v19.f64[1]  = 1.0;
      float64x2_t v54 = v5;
      float64x2_t v55 = (float64x2_t)*(unint64_t *)&a5.f64[0];
      float64x2_t v56 = v6;
      float64x2_t v57 = (float64x2_t)*(unint64_t *)&v7.f64[0];
      float64x2_t v58 = a4;
      float64x2_t v59 = (float64x2_t)*(unint64_t *)&_Q18.f64[0];
      uint64_t v60 = 0;
      uint64_t v61 = 0;
      long long v62 = xmmword_228C1F7A0;
      float64x2_t v63 = 0u;
      v64  = 0u;
      v65  = 0u;
      float64x2_t v66 = 0u;
      float64x2_t v67 = 0u;
      float64x2_t v68 = 0u;
      float64x2_t v69 = 0u;
      float64x2_t v70 = 0u;
      do
      {
        float64x2_t v32 = *(float64x2_t *)((char *)&v54 + v30);
        float64x2_t v31 = *(float64x2_t *)((char *)&v54 + v30 + 16);
        uint64_t v33 = (float64x2_t *)((char *)&v63 + v30);
        *uint64_t v33 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v14, v32.f64[0]), v16, v32, 1), v18, v31.f64[0]), v20, v31, 1);
        v33[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v13, v32.f64[0]), v15, v32, 1), v17, v31.f64[0]), v19, v31, 1);
        v30 += 32;
      }
      while (v30 != 128);
      float64x2_t v34 = v64;
      float64x2_t v35 = v65;
      float64x2_t v36 = v66;
      float64x2_t v37 = v67;
      float64x2_t v38 = v68;
      float64x2_t v39 = v69;
      float64x2_t v40 = v70;
      *a3  = v63;
      a3[1]  = v34;
      a3[2]  = v35;
      a3[3]  = v36;
      a3[4]  = v37;
      a3[5]  = v38;
      a3[6]  = v39;
      a3[7]  = v40;
    }
  }
  return result;
}

float64x2_t SPAffineTransform3DScaleUniform@<Q0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X8>, float64x2_t a3@<Q0>)
{
  v3.f64[0]  = 0.0;
  v3.f64[1]  = a3.f64[0];
  float64x2_t v5 = *a1;
  float64x2_t v4 = a1[1];
  float64x2_t v7 = a1[2];
  float64x2_t v6 = a1[3];
  float64x2_t v9 = a1[4];
  float64x2_t v8 = a1[5];
  float64x2_t v11 = a1[6];
  float64x2_t v10 = a1[7];
  float64x2_t v12 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v14 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v13 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v17 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v7), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], *a1)), (int8x16_t)vceqq_f64(v16, v9));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v17, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v13, v6), (int8x16_t)vceqq_f64(v12, v4)), (int8x16_t)vceqq_f64(v15, v8)), (int8x16_t)v17)).u64[0] & 0x8000000000000000) != 0&& (int8x16_t v18 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v13, (float64x2_t)0), (int8x16_t)vceqq_f64(v12, (float64x2_t)0)), (int8x16_t)vceqq_f64(v15, a3)), v19 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v3), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], (float64x2_t)*(unint64_t *)&a3.f64[0])), (int8x16_t)vceqq_f64(v16, (float64x2_t)0)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v19, 1), vandq_s8(v18, (int8x16_t)v19)).u64[0] & 0x8000000000000000) != 0))
  {
    float64x2_t result = vaddq_f64(v11, (float64x2_t)0);
    *a2  = v5;
    a2[1]  = v4;
    a2[2]  = v7;
    a2[3]  = v6;
    a2[4]  = v9;
    a2[5]  = v8;
    a2[6]  = result;
    a2[7]  = vaddq_f64(v10, (float64x2_t)0);
  }
  else
  {
    int64x2_t v20 = vceqzq_f64(v11);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v20, 1), vandq_s8((int8x16_t)vceqzq_f64(v10), (int8x16_t)v20)).u64[0] & 0x8000000000000000) != 0)
    {
      uint64_t v32 = 0;
      long long v40 = *(unint64_t *)&a3.f64[0];
      long long v41 = 0uLL;
      float64x2_t v42 = v3;
      long long v43 = 0uLL;
      long long v44 = 0uLL;
      float64x2_t v45 = a3;
      float64x2_t v49 = 0u;
      float64x2_t v50 = 0u;
      float64x2_t v51 = 0u;
      float64x2_t v52 = 0u;
      float64x2_t v53 = 0u;
      float64x2_t v54 = 0u;
      *(void *)&v5.f64[1]  = vextq_s8((int8x16_t)v5, (int8x16_t)v5, 8uLL).u64[0];
      *(void *)&v7.f64[1]  = vextq_s8((int8x16_t)v7, (int8x16_t)v7, 8uLL).u64[0];
      *(void *)&v9.f64[1]  = vextq_s8((int8x16_t)v9, (int8x16_t)v9, 8uLL).u64[0];
      do
      {
        float64x2_t v33 = *(float64x2_t *)((char *)&v40 + v32);
        float64x2_t result = *(float64x2_t *)((char *)&v40 + v32 + 16);
        float64x2_t v34 = (float64x2_t *)((char *)&v49 + v32);
        *float64x2_t v34 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v5, v33.f64[0]), v7, v33, 1), v9, result.f64[0]);
        v34[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v4, v33), v6, v33, 1), result, v8);
        v32 += 32;
      }
      while (v32 != 96);
      result.f64[0]  = v49.f64[0];
      float64x2_t v35 = v50;
      float64x2_t v36 = v51;
      float64x2_t v37 = v52;
      float64x2_t v38 = v53;
      float64x2_t v39 = v54;
      *a2  = v49;
      a2[1]  = v35;
      a2[2]  = v36;
      a2[3]  = v37;
      a2[4]  = v38;
      a2[5]  = v39;
      a2[6]  = v11;
      a2[7]  = v10;
    }
    else
    {
      uint64_t v21 = 0;
      v4.f64[1]  = 0.0;
      v6.f64[1]  = 0.0;
      v8.f64[1]  = 0.0;
      v10.f64[1]  = 1.0;
      long long v40 = *(unint64_t *)&a3.f64[0];
      long long v41 = 0u;
      float64x2_t v42 = v3;
      long long v43 = 0u;
      long long v44 = 0uLL;
      float64x2_t v45 = (float64x2_t)*(unint64_t *)&a3.f64[0];
      uint64_t v46 = 0;
      uint64_t v47 = 0;
      long long v48 = xmmword_228C1F7A0;
      float64x2_t v49 = 0u;
      float64x2_t v50 = 0u;
      float64x2_t v51 = 0u;
      float64x2_t v52 = 0u;
      float64x2_t v53 = 0u;
      float64x2_t v54 = 0u;
      float64x2_t v55 = 0u;
      float64x2_t v56 = 0u;
      do
      {
        float64x2_t v23 = *(float64x2_t *)((char *)&v40 + v21);
        float64x2_t result = *(float64x2_t *)((char *)&v40 + v21 + 16);
        float64x2_t v24 = (float64x2_t *)((char *)&v49 + v21);
        *float64x2_t v24 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v5, v23.f64[0]), v7, v23, 1), v9, result.f64[0]), v11, result, 1);
        v24[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v4, v23.f64[0]), v6, v23, 1), v8, result.f64[0]), v10, result, 1);
        v21 += 32;
      }
      while (v21 != 128);
      result.f64[0]  = v49.f64[0];
      float64x2_t v25 = v50;
      float64x2_t v26 = v51;
      float64x2_t v27 = v52;
      float64x2_t v28 = v53;
      float64x2_t v29 = v54;
      float64x2_t v30 = v55;
      float64x2_t v31 = v56;
      *a2  = v49;
      a2[1]  = v25;
      a2[2]  = v26;
      a2[3]  = v27;
      a2[4]  = v28;
      a2[5]  = v29;
      a2[6]  = v30;
      a2[7]  = v31;
    }
  }
  return result;
}

{
  float64x2_t v3;
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  int64x2_t v17;
  int8x16_t v18;
  int64x2_t v19;
  int64x2_t v20;
  uint64_t v21;
  float64x2_t result;
  float64x2_t v23;
  float64x2_t *v24;
  float64x2_t v25;
  float64x2_t v26;
  float64x2_t v27;
  float64x2_t v28;
  float64x2_t v29;
  float64x2_t v30;
  float64x2_t v31;
  uint64_t v32;
  float64x2_t v33;
  float64x2_t *v34;
  float64x2_t v35;
  float64x2_t v36;
  float64x2_t v37;
  float64x2_t v38;
  float64x2_t v39;
  long long v40;
  long long v41;
  float64x2_t v42;
  long long v43;
  long long v44;
  float64x2_t v45;
  uint64_t v46;
  uint64_t v47;
  long long v48;
  float64x2_t v49;
  float64x2_t v50;
  float64x2_t v51;
  float64x2_t v52;
  float64x2_t v53;
  float64x2_t v54;
  float64x2_t v55;
  float64x2_t v56;

  v3.f64[0]  = 0.0;
  v3.f64[1]  = a3.f64[0];
  float64x2_t v5 = *a1;
  float64x2_t v4 = a1[1];
  float64x2_t v7 = a1[2];
  float64x2_t v6 = a1[3];
  float64x2_t v9 = a1[4];
  float64x2_t v8 = a1[5];
  float64x2_t v11 = a1[6];
  float64x2_t v10 = a1[7];
  float64x2_t v12 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v14 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v13 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v17 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v7), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], *a1)), (int8x16_t)vceqq_f64(v16, v9));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v17, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v13, v6), (int8x16_t)vceqq_f64(v12, v4)), (int8x16_t)vceqq_f64(v15, v8)), (int8x16_t)v17)).u64[0] & 0x8000000000000000) != 0&& (int8x16_t v18 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v13, (float64x2_t)0), (int8x16_t)vceqq_f64(v12, (float64x2_t)0)), (int8x16_t)vceqq_f64(v15, a3)), v19 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v3), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], (float64x2_t)*(unint64_t *)&a3.f64[0])), (int8x16_t)vceqq_f64(v16, (float64x2_t)0)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v19, 1), vandq_s8(v18, (int8x16_t)v19)).u64[0] & 0x8000000000000000) != 0))
  {
    float64x2_t result = vaddq_f64(v11, (float64x2_t)0);
    *a2  = v5;
    a2[1]  = v4;
    a2[2]  = v7;
    a2[3]  = v6;
    a2[4]  = v9;
    a2[5]  = v8;
    a2[6]  = result;
    a2[7]  = vaddq_f64(v10, (float64x2_t)0);
  }
  else
  {
    int64x2_t v20 = vceqzq_f64(v11);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v20, 1), vandq_s8((int8x16_t)vceqzq_f64(v10), (int8x16_t)v20)).u64[0] & 0x8000000000000000) != 0)
    {
      uint64_t v32 = 0;
      long long v40 = *(unint64_t *)&a3.f64[0];
      long long v41 = 0uLL;
      float64x2_t v42 = v3;
      long long v43 = 0uLL;
      long long v44 = 0uLL;
      float64x2_t v45 = a3;
      float64x2_t v49 = 0u;
      float64x2_t v50 = 0u;
      float64x2_t v51 = 0u;
      float64x2_t v52 = 0u;
      float64x2_t v53 = 0u;
      float64x2_t v54 = 0u;
      *(void *)&v5.f64[1]  = vextq_s8((int8x16_t)v5, (int8x16_t)v5, 8uLL).u64[0];
      *(void *)&v7.f64[1]  = vextq_s8((int8x16_t)v7, (int8x16_t)v7, 8uLL).u64[0];
      *(void *)&v9.f64[1]  = vextq_s8((int8x16_t)v9, (int8x16_t)v9, 8uLL).u64[0];
      do
      {
        float64x2_t v33 = *(float64x2_t *)((char *)&v40 + v32);
        float64x2_t result = *(float64x2_t *)((char *)&v40 + v32 + 16);
        float64x2_t v34 = (float64x2_t *)((char *)&v49 + v32);
        *float64x2_t v34 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v5, v33.f64[0]), v7, v33, 1), v9, result.f64[0]);
        v34[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v4, v33), v6, v33, 1), result, v8);
        v32 += 32;
      }
      while (v32 != 96);
      result.f64[0]  = v49.f64[0];
      float64x2_t v35 = v50;
      float64x2_t v36 = v51;
      float64x2_t v37 = v52;
      float64x2_t v38 = v53;
      float64x2_t v39 = v54;
      *a2  = v49;
      a2[1]  = v35;
      a2[2]  = v36;
      a2[3]  = v37;
      a2[4]  = v38;
      a2[5]  = v39;
      a2[6]  = v11;
      a2[7]  = v10;
    }
    else
    {
      uint64_t v21 = 0;
      v4.f64[1]  = 0.0;
      v6.f64[1]  = 0.0;
      v8.f64[1]  = 0.0;
      v10.f64[1]  = 1.0;
      long long v40 = *(unint64_t *)&a3.f64[0];
      long long v41 = 0u;
      float64x2_t v42 = v3;
      long long v43 = 0u;
      long long v44 = 0uLL;
      float64x2_t v45 = (float64x2_t)*(unint64_t *)&a3.f64[0];
      uint64_t v46 = 0;
      uint64_t v47 = 0;
      long long v48 = xmmword_228C1F7A0;
      float64x2_t v49 = 0u;
      float64x2_t v50 = 0u;
      float64x2_t v51 = 0u;
      float64x2_t v52 = 0u;
      float64x2_t v53 = 0u;
      float64x2_t v54 = 0u;
      float64x2_t v55 = 0u;
      float64x2_t v56 = 0u;
      do
      {
        float64x2_t v23 = *(float64x2_t *)((char *)&v40 + v21);
        float64x2_t result = *(float64x2_t *)((char *)&v40 + v21 + 16);
        float64x2_t v24 = (float64x2_t *)((char *)&v49 + v21);
        *float64x2_t v24 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v5, v23.f64[0]), v7, v23, 1), v9, result.f64[0]), v11, result, 1);
        v24[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v4, v23.f64[0]), v6, v23, 1), v8, result.f64[0]), v10, result, 1);
        v21 += 32;
      }
      while (v21 != 128);
      result.f64[0]  = v49.f64[0];
      float64x2_t v25 = v50;
      float64x2_t v26 = v51;
      float64x2_t v27 = v52;
      float64x2_t v28 = v53;
      float64x2_t v29 = v54;
      float64x2_t v30 = v55;
      float64x2_t v31 = v56;
      *a2  = v49;
      a2[1]  = v25;
      a2[2]  = v26;
      a2[3]  = v27;
      a2[4]  = v28;
      a2[5]  = v29;
      a2[6]  = v30;
      a2[7]  = v31;
    }
  }
  return result;
}

float64x2_t *SPAffineTransform3DScaleBySize@<X0>(float64x2_t *result@<X0>, unint64_t *a2@<X1>, float64x2_t *a3@<X8>)
{
  *(void *)&v3.f64[0]  = a2[2];
  v4.f64[0]  = 0.0;
  *(void *)&v4.f64[1]  = a2[1];
  float64x2_t v6 = *result;
  float64x2_t v5 = result[1];
  float64x2_t v8 = result[2];
  float64x2_t v7 = result[3];
  float64x2_t v10 = result[4];
  float64x2_t v9 = result[5];
  float64x2_t v12 = result[6];
  float64x2_t v11 = result[7];
  float64x2_t v13 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v14 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v17 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v18 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, v8), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], *result)), (int8x16_t)vceqq_f64(v17, v10));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v18, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v7), (int8x16_t)vceqq_f64(v13, v5)), (int8x16_t)vceqq_f64(v16, v9)), (int8x16_t)v18)).u64[0] & 0x8000000000000000) != 0)
  {
    int8x16_t v19 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, (float64x2_t)0), (int8x16_t)vceqq_f64(v13, (float64x2_t)0)), (int8x16_t)vceqq_f64(v16, v3));
    int64x2_t v20 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(v15, v4), vandq_s8((int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], (float64x2_t)*a2), (int8x16_t)vceqq_f64(v17, (float64x2_t)0)));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v20, 1), vandq_s8(v19, (int8x16_t)v20)).u64[0] & 0x8000000000000000) != 0)
    {
      float64x2_t v11 = vaddq_f64(v11, (float64x2_t)0);
      float64x2_t v12 = vaddq_f64(v12, (float64x2_t)0);
      *a3  = v6;
      a3[1]  = v5;
      a3[2]  = v8;
      a3[3]  = v7;
      a3[4]  = v10;
      a3[5]  = v9;
LABEL_11:
      a3[6]  = v12;
      a3[7]  = v11;
      return result;
    }
  }
  int64x2_t v21 = vceqzq_f64(v12);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v21, 1), vandq_s8((int8x16_t)vceqzq_f64(v11), (int8x16_t)v21)).u64[0] & 0x8000000000000000) != 0)
  {
    uint64_t v33 = 0;
    long long v42 = *a2;
    long long v43 = 0uLL;
    float64x2_t v44 = v4;
    long long v45 = 0uLL;
    long long v46 = 0uLL;
    float64x2_t v47 = v3;
    float64x2_t v51 = 0u;
    float64x2_t v52 = 0u;
    float64x2_t v53 = 0u;
    float64x2_t v54 = 0u;
    float64x2_t v55 = 0u;
    float64x2_t v56 = 0u;
    *(void *)&v6.f64[1]  = vextq_s8((int8x16_t)v6, (int8x16_t)v6, 8uLL).u64[0];
    *(void *)&v8.f64[1]  = vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL).u64[0];
    *(void *)&v10.f64[1]  = vextq_s8((int8x16_t)v10, (int8x16_t)v10, 8uLL).u64[0];
    do
    {
      float64x2_t v35 = *(float64x2_t *)((char *)&v42 + v33);
      float64x2_t v34 = *(float64x2_t *)((char *)&v42 + v33 + 16);
      float64x2_t v36 = (float64x2_t *)((char *)&v51 + v33);
      *float64x2_t v36 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v35.f64[0]), v8, v35, 1), v10, v34.f64[0]);
      v36[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v5, v35), v7, v35, 1), v34, v9);
      v33 += 32;
    }
    while (v33 != 96);
    float64x2_t v37 = v52;
    float64x2_t v38 = v53;
    float64x2_t v39 = v54;
    float64x2_t v40 = v55;
    float64x2_t v41 = v56;
    *a3  = v51;
    a3[1]  = v37;
    a3[2]  = v38;
    a3[3]  = v39;
    a3[4]  = v40;
    a3[5]  = v41;
    goto LABEL_11;
  }
  uint64_t v22 = 0;
  v5.f64[1]  = 0.0;
  v7.f64[1]  = 0.0;
  v9.f64[1]  = 0.0;
  v11.f64[1]  = 1.0;
  long long v42 = *a2;
  long long v43 = 0u;
  float64x2_t v44 = v4;
  long long v45 = 0u;
  long long v46 = 0uLL;
  float64x2_t v47 = (float64x2_t)*(unint64_t *)&v3.f64[0];
  uint64_t v48 = 0;
  uint64_t v49 = 0;
  long long v50 = xmmword_228C1F7A0;
  float64x2_t v51 = 0u;
  float64x2_t v52 = 0u;
  float64x2_t v53 = 0u;
  float64x2_t v54 = 0u;
  float64x2_t v55 = 0u;
  float64x2_t v56 = 0u;
  float64x2_t v57 = 0u;
  float64x2_t v58 = 0u;
  do
  {
    float64x2_t v24 = *(float64x2_t *)((char *)&v42 + v22);
    float64x2_t v23 = *(float64x2_t *)((char *)&v42 + v22 + 16);
    float64x2_t v25 = (float64x2_t *)((char *)&v51 + v22);
    *float64x2_t v25 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v24.f64[0]), v8, v24, 1), v10, v23.f64[0]), v12, v23, 1);
    v25[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v5, v24.f64[0]), v7, v24, 1), v9, v23.f64[0]), v11, v23, 1);
    v22 += 32;
  }
  while (v22 != 128);
  float64x2_t v26 = v52;
  float64x2_t v27 = v53;
  float64x2_t v28 = v54;
  float64x2_t v29 = v55;
  float64x2_t v30 = v56;
  float64x2_t v31 = v57;
  float64x2_t v32 = v58;
  *a3  = v51;
  a3[1]  = v26;
  a3[2]  = v27;
  a3[3]  = v28;
  a3[4]  = v29;
  a3[5]  = v30;
  a3[6]  = v31;
  a3[7]  = v32;
  return result;
}

{
  float64x2_t v3;
  float64x2_t v4;
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  float64x2_t v17;
  int64x2_t v18;
  int8x16_t v19;
  int64x2_t v20;
  int64x2_t v21;
  uint64_t v22;
  float64x2_t v23;
  float64x2_t v24;
  float64x2_t *v25;
  float64x2_t v26;
  float64x2_t v27;
  float64x2_t v28;
  float64x2_t v29;
  float64x2_t v30;
  float64x2_t v31;
  float64x2_t v32;
  uint64_t v33;
  float64x2_t v34;
  float64x2_t v35;
  float64x2_t *v36;
  float64x2_t v37;
  float64x2_t v38;
  float64x2_t v39;
  float64x2_t v40;
  float64x2_t v41;
  long long v42;
  long long v43;
  float64x2_t v44;
  long long v45;
  long long v46;
  float64x2_t v47;
  uint64_t v48;
  uint64_t v49;
  long long v50;
  float64x2_t v51;
  float64x2_t v52;
  float64x2_t v53;
  float64x2_t v54;
  float64x2_t v55;
  float64x2_t v56;
  float64x2_t v57;
  float64x2_t v58;

  *(void *)&v3.f64[0]  = a2[2];
  v4.f64[0]  = 0.0;
  *(void *)&v4.f64[1]  = a2[1];
  float64x2_t v6 = *result;
  float64x2_t v5 = result[1];
  float64x2_t v8 = result[2];
  float64x2_t v7 = result[3];
  float64x2_t v10 = result[4];
  float64x2_t v9 = result[5];
  float64x2_t v12 = result[6];
  float64x2_t v11 = result[7];
  float64x2_t v13 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v14 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v17 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v18 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, v8), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], *result)), (int8x16_t)vceqq_f64(v17, v10));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v18, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v7), (int8x16_t)vceqq_f64(v13, v5)), (int8x16_t)vceqq_f64(v16, v9)), (int8x16_t)v18)).u64[0] & 0x8000000000000000) != 0)
  {
    int8x16_t v19 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, (float64x2_t)0), (int8x16_t)vceqq_f64(v13, (float64x2_t)0)), (int8x16_t)vceqq_f64(v16, v3));
    int64x2_t v20 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(v15, v4), vandq_s8((int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], (float64x2_t)*a2), (int8x16_t)vceqq_f64(v17, (float64x2_t)0)));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v20, 1), vandq_s8(v19, (int8x16_t)v20)).u64[0] & 0x8000000000000000) != 0)
    {
      float64x2_t v11 = vaddq_f64(v11, (float64x2_t)0);
      float64x2_t v12 = vaddq_f64(v12, (float64x2_t)0);
      *a3  = v6;
      a3[1]  = v5;
      a3[2]  = v8;
      a3[3]  = v7;
      a3[4]  = v10;
      a3[5]  = v9;
LABEL_11:
      a3[6]  = v12;
      a3[7]  = v11;
      return result;
    }
  }
  int64x2_t v21 = vceqzq_f64(v12);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v21, 1), vandq_s8((int8x16_t)vceqzq_f64(v11), (int8x16_t)v21)).u64[0] & 0x8000000000000000) != 0)
  {
    uint64_t v33 = 0;
    long long v42 = *a2;
    long long v43 = 0uLL;
    float64x2_t v44 = v4;
    long long v45 = 0uLL;
    long long v46 = 0uLL;
    float64x2_t v47 = v3;
    float64x2_t v51 = 0u;
    float64x2_t v52 = 0u;
    float64x2_t v53 = 0u;
    float64x2_t v54 = 0u;
    float64x2_t v55 = 0u;
    float64x2_t v56 = 0u;
    *(void *)&v6.f64[1]  = vextq_s8((int8x16_t)v6, (int8x16_t)v6, 8uLL).u64[0];
    *(void *)&v8.f64[1]  = vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL).u64[0];
    *(void *)&v10.f64[1]  = vextq_s8((int8x16_t)v10, (int8x16_t)v10, 8uLL).u64[0];
    do
    {
      float64x2_t v35 = *(float64x2_t *)((char *)&v42 + v33);
      float64x2_t v34 = *(float64x2_t *)((char *)&v42 + v33 + 16);
      float64x2_t v36 = (float64x2_t *)((char *)&v51 + v33);
      *float64x2_t v36 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v35.f64[0]), v8, v35, 1), v10, v34.f64[0]);
      v36[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v5, v35), v7, v35, 1), v34, v9);
      v33 += 32;
    }
    while (v33 != 96);
    float64x2_t v37 = v52;
    float64x2_t v38 = v53;
    float64x2_t v39 = v54;
    float64x2_t v40 = v55;
    float64x2_t v41 = v56;
    *a3  = v51;
    a3[1]  = v37;
    a3[2]  = v38;
    a3[3]  = v39;
    a3[4]  = v40;
    a3[5]  = v41;
    goto LABEL_11;
  }
  uint64_t v22 = 0;
  v5.f64[1]  = 0.0;
  v7.f64[1]  = 0.0;
  v9.f64[1]  = 0.0;
  v11.f64[1]  = 1.0;
  long long v42 = *a2;
  long long v43 = 0u;
  float64x2_t v44 = v4;
  long long v45 = 0u;
  long long v46 = 0uLL;
  float64x2_t v47 = (float64x2_t)*(unint64_t *)&v3.f64[0];
  uint64_t v48 = 0;
  uint64_t v49 = 0;
  long long v50 = xmmword_228C1F7A0;
  float64x2_t v51 = 0u;
  float64x2_t v52 = 0u;
  float64x2_t v53 = 0u;
  float64x2_t v54 = 0u;
  float64x2_t v55 = 0u;
  float64x2_t v56 = 0u;
  float64x2_t v57 = 0u;
  float64x2_t v58 = 0u;
  do
  {
    float64x2_t v24 = *(float64x2_t *)((char *)&v42 + v22);
    float64x2_t v23 = *(float64x2_t *)((char *)&v42 + v22 + 16);
    float64x2_t v25 = (float64x2_t *)((char *)&v51 + v22);
    *float64x2_t v25 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v24.f64[0]), v8, v24, 1), v10, v23.f64[0]), v12, v23, 1);
    v25[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v5, v24.f64[0]), v7, v24, 1), v9, v23.f64[0]), v11, v23, 1);
    v22 += 32;
  }
  while (v22 != 128);
  float64x2_t v26 = v52;
  float64x2_t v27 = v53;
  float64x2_t v28 = v54;
  float64x2_t v29 = v55;
  float64x2_t v30 = v56;
  float64x2_t v31 = v57;
  float64x2_t v32 = v58;
  *a3  = v51;
  a3[1]  = v26;
  a3[2]  = v27;
  a3[3]  = v28;
  a3[4]  = v29;
  a3[5]  = v30;
  a3[6]  = v31;
  a3[7]  = v32;
  return result;
}

float64x2_t *SPAffineTransform3DScaleBy@<X0>(float64x2_t *result@<X0>, float64x2_t *a2@<X8>, unint64_t a3@<D0>, float64_t a4@<D1>, float64x2_t a5@<Q2>)
{
  float64x2_t v5 = (float64x2_t)a3;
  v6.f64[0]  = 0.0;
  v6.f64[1]  = a4;
  float64x2_t v8 = *result;
  float64x2_t v7 = result[1];
  float64x2_t v10 = result[2];
  float64x2_t v9 = result[3];
  float64x2_t v12 = result[4];
  float64x2_t v11 = result[5];
  float64x2_t v14 = result[6];
  float64x2_t v13 = result[7];
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v17 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v19 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v18 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v20 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v17, v10), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], *result)), (int8x16_t)vceqq_f64(v19, v12));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v20, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v16, v9), (int8x16_t)vceqq_f64(v15, v7)), (int8x16_t)vceqq_f64(v18, v11)), (int8x16_t)v20)).u64[0] & 0x8000000000000000) != 0)
  {
    int8x16_t v21 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v16, (float64x2_t)0), (int8x16_t)vceqq_f64(v15, (float64x2_t)0)), (int8x16_t)vceqq_f64(v18, a5));
    int64x2_t v22 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v17, v6), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v5)), (int8x16_t)vceqq_f64(v19, (float64x2_t)0));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v22, 1), vandq_s8(v21, (int8x16_t)v22)).u64[0] & 0x8000000000000000) != 0)
    {
      float64x2_t v13 = vaddq_f64(v13, (float64x2_t)0);
      float64x2_t v14 = vaddq_f64(v14, (float64x2_t)0);
      *a2  = v8;
      a2[1]  = v7;
      a2[2]  = v10;
      a2[3]  = v9;
      a2[4]  = v12;
      a2[5]  = v11;
LABEL_11:
      a2[6]  = v14;
      a2[7]  = v13;
      return result;
    }
  }
  int64x2_t v23 = vceqzq_f64(v14);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v23, 1), vandq_s8((int8x16_t)vceqzq_f64(v13), (int8x16_t)v23)).u64[0] & 0x8000000000000000) != 0)
  {
    uint64_t v35 = 0;
    float64x2_t v44 = v5;
    long long v45 = 0uLL;
    float64x2_t v46 = v6;
    long long v47 = 0uLL;
    long long v48 = 0uLL;
    float64x2_t v49 = a5;
    float64x2_t v53 = 0u;
    float64x2_t v54 = 0u;
    float64x2_t v55 = 0u;
    float64x2_t v56 = 0u;
    float64x2_t v57 = 0u;
    float64x2_t v58 = 0u;
    *(void *)&v8.f64[1]  = vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL).u64[0];
    *(void *)&v10.f64[1]  = vextq_s8((int8x16_t)v10, (int8x16_t)v10, 8uLL).u64[0];
    *(void *)&v12.f64[1]  = vextq_s8((int8x16_t)v12, (int8x16_t)v12, 8uLL).u64[0];
    do
    {
      float64x2_t v37 = *(float64x2_t *)((char *)&v44 + v35);
      float64x2_t v36 = *(float64x2_t *)((char *)&v44 + v35 + 16);
      float64x2_t v38 = (float64x2_t *)((char *)&v53 + v35);
      *float64x2_t v38 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v8, v37.f64[0]), v10, v37, 1), v12, v36.f64[0]);
      v38[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v7, v37), v9, v37, 1), v36, v11);
      v35 += 32;
    }
    while (v35 != 96);
    float64x2_t v39 = v54;
    float64x2_t v40 = v55;
    float64x2_t v41 = v56;
    float64x2_t v42 = v57;
    float64x2_t v43 = v58;
    *a2  = v53;
    a2[1]  = v39;
    a2[2]  = v40;
    a2[3]  = v41;
    a2[4]  = v42;
    a2[5]  = v43;
    goto LABEL_11;
  }
  uint64_t v24 = 0;
  v7.f64[1]  = 0.0;
  v9.f64[1]  = 0.0;
  v11.f64[1]  = 0.0;
  v13.f64[1]  = 1.0;
  float64x2_t v44 = v5;
  long long v45 = 0u;
  float64x2_t v46 = v6;
  long long v47 = 0u;
  long long v48 = 0uLL;
  float64x2_t v49 = (float64x2_t)*(unint64_t *)&a5.f64[0];
  uint64_t v50 = 0;
  uint64_t v51 = 0;
  long long v52 = xmmword_228C1F7A0;
  float64x2_t v53 = 0u;
  float64x2_t v54 = 0u;
  float64x2_t v55 = 0u;
  float64x2_t v56 = 0u;
  float64x2_t v57 = 0u;
  float64x2_t v58 = 0u;
  float64x2_t v59 = 0u;
  float64x2_t v60 = 0u;
  do
  {
    float64x2_t v26 = *(float64x2_t *)((char *)&v44 + v24);
    float64x2_t v25 = *(float64x2_t *)((char *)&v44 + v24 + 16);
    float64x2_t v27 = (float64x2_t *)((char *)&v53 + v24);
    *float64x2_t v27 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v8, v26.f64[0]), v10, v26, 1), v12, v25.f64[0]), v14, v25, 1);
    v27[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v7, v26.f64[0]), v9, v26, 1), v11, v25.f64[0]), v13, v25, 1);
    v24 += 32;
  }
  while (v24 != 128);
  float64x2_t v28 = v54;
  float64x2_t v29 = v55;
  float64x2_t v30 = v56;
  float64x2_t v31 = v57;
  float64x2_t v32 = v58;
  float64x2_t v33 = v59;
  float64x2_t v34 = v60;
  *a2  = v53;
  a2[1]  = v28;
  a2[2]  = v29;
  a2[3]  = v30;
  a2[4]  = v31;
  a2[5]  = v32;
  a2[6]  = v33;
  a2[7]  = v34;
  return result;
}

{
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  float64x2_t v17;
  float64x2_t v18;
  float64x2_t v19;
  int64x2_t v20;
  int8x16_t v21;
  int64x2_t v22;
  int64x2_t v23;
  uint64_t v24;
  float64x2_t v25;
  float64x2_t v26;
  float64x2_t *v27;
  float64x2_t v28;
  float64x2_t v29;
  float64x2_t v30;
  float64x2_t v31;
  float64x2_t v32;
  float64x2_t v33;
  float64x2_t v34;
  uint64_t v35;
  float64x2_t v36;
  float64x2_t v37;
  float64x2_t *v38;
  float64x2_t v39;
  float64x2_t v40;
  float64x2_t v41;
  float64x2_t v42;
  float64x2_t v43;
  float64x2_t v44;
  long long v45;
  float64x2_t v46;
  long long v47;
  long long v48;
  float64x2_t v49;
  uint64_t v50;
  uint64_t v51;
  long long v52;
  float64x2_t v53;
  float64x2_t v54;
  float64x2_t v55;
  float64x2_t v56;
  float64x2_t v57;
  float64x2_t v58;
  float64x2_t v59;
  float64x2_t v60;

  float64x2_t v5 = (float64x2_t)a3;
  v6.f64[0]  = 0.0;
  v6.f64[1]  = a4;
  float64x2_t v8 = *result;
  float64x2_t v7 = result[1];
  float64x2_t v10 = result[2];
  float64x2_t v9 = result[3];
  float64x2_t v12 = result[4];
  float64x2_t v11 = result[5];
  float64x2_t v14 = result[6];
  float64x2_t v13 = result[7];
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v17 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v19 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v18 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v20 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v17, v10), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], *result)), (int8x16_t)vceqq_f64(v19, v12));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v20, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v16, v9), (int8x16_t)vceqq_f64(v15, v7)), (int8x16_t)vceqq_f64(v18, v11)), (int8x16_t)v20)).u64[0] & 0x8000000000000000) != 0)
  {
    int8x16_t v21 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v16, (float64x2_t)0), (int8x16_t)vceqq_f64(v15, (float64x2_t)0)), (int8x16_t)vceqq_f64(v18, a5));
    int64x2_t v22 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v17, v6), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v5)), (int8x16_t)vceqq_f64(v19, (float64x2_t)0));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v22, 1), vandq_s8(v21, (int8x16_t)v22)).u64[0] & 0x8000000000000000) != 0)
    {
      float64x2_t v13 = vaddq_f64(v13, (float64x2_t)0);
      float64x2_t v14 = vaddq_f64(v14, (float64x2_t)0);
      *a2  = v8;
      a2[1]  = v7;
      a2[2]  = v10;
      a2[3]  = v9;
      a2[4]  = v12;
      a2[5]  = v11;
LABEL_11:
      a2[6]  = v14;
      a2[7]  = v13;
      return result;
    }
  }
  int64x2_t v23 = vceqzq_f64(v14);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v23, 1), vandq_s8((int8x16_t)vceqzq_f64(v13), (int8x16_t)v23)).u64[0] & 0x8000000000000000) != 0)
  {
    uint64_t v35 = 0;
    float64x2_t v44 = v5;
    long long v45 = 0uLL;
    float64x2_t v46 = v6;
    long long v47 = 0uLL;
    long long v48 = 0uLL;
    float64x2_t v49 = a5;
    float64x2_t v53 = 0u;
    float64x2_t v54 = 0u;
    float64x2_t v55 = 0u;
    float64x2_t v56 = 0u;
    float64x2_t v57 = 0u;
    float64x2_t v58 = 0u;
    *(void *)&v8.f64[1]  = vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL).u64[0];
    *(void *)&v10.f64[1]  = vextq_s8((int8x16_t)v10, (int8x16_t)v10, 8uLL).u64[0];
    *(void *)&v12.f64[1]  = vextq_s8((int8x16_t)v12, (int8x16_t)v12, 8uLL).u64[0];
    do
    {
      float64x2_t v37 = *(float64x2_t *)((char *)&v44 + v35);
      float64x2_t v36 = *(float64x2_t *)((char *)&v44 + v35 + 16);
      float64x2_t v38 = (float64x2_t *)((char *)&v53 + v35);
      *float64x2_t v38 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v8, v37.f64[0]), v10, v37, 1), v12, v36.f64[0]);
      v38[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v7, v37), v9, v37, 1), v36, v11);
      v35 += 32;
    }
    while (v35 != 96);
    float64x2_t v39 = v54;
    float64x2_t v40 = v55;
    float64x2_t v41 = v56;
    float64x2_t v42 = v57;
    float64x2_t v43 = v58;
    *a2  = v53;
    a2[1]  = v39;
    a2[2]  = v40;
    a2[3]  = v41;
    a2[4]  = v42;
    a2[5]  = v43;
    goto LABEL_11;
  }
  uint64_t v24 = 0;
  v7.f64[1]  = 0.0;
  v9.f64[1]  = 0.0;
  v11.f64[1]  = 0.0;
  v13.f64[1]  = 1.0;
  float64x2_t v44 = v5;
  long long v45 = 0u;
  float64x2_t v46 = v6;
  long long v47 = 0u;
  long long v48 = 0uLL;
  float64x2_t v49 = (float64x2_t)*(unint64_t *)&a5.f64[0];
  uint64_t v50 = 0;
  uint64_t v51 = 0;
  long long v52 = xmmword_228C1F7A0;
  float64x2_t v53 = 0u;
  float64x2_t v54 = 0u;
  float64x2_t v55 = 0u;
  float64x2_t v56 = 0u;
  float64x2_t v57 = 0u;
  float64x2_t v58 = 0u;
  float64x2_t v59 = 0u;
  float64x2_t v60 = 0u;
  do
  {
    float64x2_t v26 = *(float64x2_t *)((char *)&v44 + v24);
    float64x2_t v25 = *(float64x2_t *)((char *)&v44 + v24 + 16);
    float64x2_t v27 = (float64x2_t *)((char *)&v53 + v24);
    *float64x2_t v27 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v8, v26.f64[0]), v10, v26, 1), v12, v25.f64[0]), v14, v25, 1);
    v27[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v7, v26.f64[0]), v9, v26, 1), v11, v25.f64[0]), v13, v25, 1);
    v24 += 32;
  }
  while (v24 != 128);
  float64x2_t v28 = v54;
  float64x2_t v29 = v55;
  float64x2_t v30 = v56;
  float64x2_t v31 = v57;
  float64x2_t v32 = v58;
  float64x2_t v33 = v59;
  float64x2_t v34 = v60;
  *a2  = v53;
  a2[1]  = v28;
  a2[2]  = v29;
  a2[3]  = v30;
  a2[4]  = v31;
  a2[5]  = v32;
  a2[6]  = v33;
  a2[7]  = v34;
  return result;
}

void SPAffineTransform3DRotateByQuaternion(float64x2_t *a1@<X0>, simd_quatd *a2@<X1>, float64x2_t *a3@<X8>)
{
  simd_quatd v59 = *a2;
  float64x2_t v57 = 0u;
  float64x2_t v58 = 0u;
  float64x2_t v55 = 0u;
  float64x2_t v56 = 0u;
  float64x2_t v53 = 0u;
  float64x2_t v54 = 0u;
  float64x2_t v51 = 0u;
  float64x2_t v52 = 0u;
  float64x2_t v42 = *(float64x2_t *)v59.vector.f64;
  float64x2_t v43 = *(float64x2_t *)&v59.vector.f64[2];
  simd_matrix4x4(v59, &v42, (uint64_t)&v51);
  float64x2_t v6 = *a1;
  float64x2_t v5 = a1[1];
  float64x2_t v8 = a1[2];
  float64x2_t v7 = a1[3];
  float64x2_t v10 = a1[4];
  float64x2_t v9 = a1[5];
  float64x2_t v12 = a1[6];
  float64x2_t v11 = a1[7];
  float64x2_t v13 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v14 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v17 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v18 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, v8), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], *a1)), (int8x16_t)vceqq_f64(v17, v10));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v18, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v7), (int8x16_t)vceqq_f64(v13, v5)), (int8x16_t)vceqq_f64(v16, v9)), (int8x16_t)v18)).u64[0] & 0x8000000000000000) != 0)
  {
    int8x16_t v19 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v54), (int8x16_t)vceqq_f64(v13, v52)), (int8x16_t)vceqq_f64(v16, v56));
    int64x2_t v20 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, v53), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v51)), (int8x16_t)vceqq_f64(v17, v55));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v20, 1), vandq_s8(v19, (int8x16_t)v20)).u64[0] & 0x8000000000000000) != 0)
    {
      float64x2_t v11 = vaddq_f64(v11, (float64x2_t)0);
      float64x2_t v12 = vaddq_f64(v12, (float64x2_t)0);
      *a3  = v6;
      a3[1]  = v5;
      a3[2]  = v8;
      a3[3]  = v7;
      a3[4]  = v10;
      a3[5]  = v9;
LABEL_11:
      a3[6]  = v12;
      a3[7]  = v11;
      return;
    }
  }
  int64x2_t v21 = vceqzq_f64(v12);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v21, 1), vandq_s8((int8x16_t)vceqzq_f64(v11), (int8x16_t)v21)).u64[0] & 0x8000000000000000) != 0)
  {
    uint64_t v33 = 0;
    float64x2_t v42 = v51;
    float64x2_t v43 = v52;
    float64x2_t v44 = v53;
    float64x2_t v45 = v54;
    float64x2_t v46 = v55;
    float64x2_t v47 = v56;
    float64x2_t v51 = 0u;
    float64x2_t v52 = 0u;
    float64x2_t v53 = 0u;
    float64x2_t v54 = 0u;
    float64x2_t v55 = 0u;
    float64x2_t v56 = 0u;
    *(void *)&v6.f64[1]  = vextq_s8((int8x16_t)v6, (int8x16_t)v6, 8uLL).u64[0];
    *(void *)&v8.f64[1]  = vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL).u64[0];
    *(void *)&v10.f64[1]  = vextq_s8((int8x16_t)v10, (int8x16_t)v10, 8uLL).u64[0];
    do
    {
      float64x2_t v35 = *(float64x2_t *)((char *)&v42 + v33);
      float64x2_t v34 = *(float64x2_t *)((char *)&v42 + v33 + 16);
      float64x2_t v36 = (float64x2_t *)((char *)&v51 + v33);
      *float64x2_t v36 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v35.f64[0]), v8, v35, 1), v10, v34.f64[0]);
      v36[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v5, v35), v7, v35, 1), v34, v9);
      v33 += 32;
    }
    while (v33 != 96);
    float64x2_t v37 = v52;
    float64x2_t v38 = v53;
    float64x2_t v39 = v54;
    float64x2_t v40 = v55;
    float64x2_t v41 = v56;
    *a3  = v51;
    a3[1]  = v37;
    a3[2]  = v38;
    a3[3]  = v39;
    a3[4]  = v40;
    a3[5]  = v41;
    goto LABEL_11;
  }
  uint64_t v22 = 0;
  v5.f64[1]  = 0.0;
  v7.f64[1]  = 0.0;
  v9.f64[1]  = 0.0;
  v11.f64[1]  = 1.0;
  float64x2_t v42 = v51;
  float64x2_t v43 = (float64x2_t)*(unint64_t *)&v52.f64[0];
  float64x2_t v44 = v53;
  float64x2_t v45 = (float64x2_t)*(unint64_t *)&v54.f64[0];
  float64x2_t v46 = v55;
  float64x2_t v47 = (float64x2_t)*(unint64_t *)&v56.f64[0];
  uint64_t v48 = 0;
  uint64_t v49 = 0;
  long long v50 = xmmword_228C1F7A0;
  float64x2_t v51 = 0u;
  float64x2_t v52 = 0u;
  float64x2_t v53 = 0u;
  float64x2_t v54 = 0u;
  float64x2_t v55 = 0u;
  float64x2_t v56 = 0u;
  float64x2_t v57 = 0u;
  float64x2_t v58 = 0u;
  do
  {
    float64x2_t v24 = *(float64x2_t *)((char *)&v42 + v22);
    float64x2_t v23 = *(float64x2_t *)((char *)&v42 + v22 + 16);
    float64x2_t v25 = (float64x2_t *)((char *)&v51 + v22);
    *float64x2_t v25 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v24.f64[0]), v8, v24, 1), v10, v23.f64[0]), v12, v23, 1);
    v25[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v5, v24.f64[0]), v7, v24, 1), v9, v23.f64[0]), v11, v23, 1);
    v22 += 32;
  }
  while (v22 != 128);
  float64x2_t v26 = v52;
  float64x2_t v27 = v53;
  float64x2_t v28 = v54;
  float64x2_t v29 = v55;
  float64x2_t v30 = v56;
  float64x2_t v31 = v57;
  float64x2_t v32 = v58;
  *a3  = v51;
  a3[1]  = v26;
  a3[2]  = v27;
  a3[3]  = v28;
  a3[4]  = v29;
  a3[5]  = v30;
  a3[6]  = v31;
  a3[7]  = v32;
}

void SPAffineTransform3DRotate(float64x2_t *a1@<X0>, simd_quatd *a2@<X1>, float64x2_t *a3@<X8>)
{
  simd_quatd v59 = *a2;
  float64x2_t v57 = 0u;
  float64x2_t v58 = 0u;
  float64x2_t v55 = 0u;
  float64x2_t v56 = 0u;
  float64x2_t v53 = 0u;
  float64x2_t v54 = 0u;
  float64x2_t v51 = 0u;
  float64x2_t v52 = 0u;
  float64x2_t v42 = *(float64x2_t *)v59.vector.f64;
  float64x2_t v43 = *(float64x2_t *)&v59.vector.f64[2];
  simd_matrix4x4(v59, &v42, (uint64_t)&v51);
  float64x2_t v6 = *a1;
  float64x2_t v5 = a1[1];
  float64x2_t v8 = a1[2];
  float64x2_t v7 = a1[3];
  float64x2_t v10 = a1[4];
  float64x2_t v9 = a1[5];
  float64x2_t v12 = a1[6];
  float64x2_t v11 = a1[7];
  float64x2_t v13 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v15 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v14 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v17 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v18 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, v8), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], *a1)), (int8x16_t)vceqq_f64(v17, v10));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v18, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v7), (int8x16_t)vceqq_f64(v13, v5)), (int8x16_t)vceqq_f64(v16, v9)), (int8x16_t)v18)).u64[0] & 0x8000000000000000) != 0)
  {
    int8x16_t v19 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v14, v54), (int8x16_t)vceqq_f64(v13, v52)), (int8x16_t)vceqq_f64(v16, v56));
    int64x2_t v20 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v15, v53), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v51)), (int8x16_t)vceqq_f64(v17, v55));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v20, 1), vandq_s8(v19, (int8x16_t)v20)).u64[0] & 0x8000000000000000) != 0)
    {
      float64x2_t v11 = vaddq_f64(v11, (float64x2_t)0);
      float64x2_t v12 = vaddq_f64(v12, (float64x2_t)0);
      *a3  = v6;
      a3[1]  = v5;
      a3[2]  = v8;
      a3[3]  = v7;
      a3[4]  = v10;
      a3[5]  = v9;
LABEL_11:
      a3[6]  = v12;
      a3[7]  = v11;
      return;
    }
  }
  int64x2_t v21 = vceqzq_f64(v12);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v21, 1), vandq_s8((int8x16_t)vceqzq_f64(v11), (int8x16_t)v21)).u64[0] & 0x8000000000000000) != 0)
  {
    uint64_t v33 = 0;
    float64x2_t v42 = v51;
    float64x2_t v43 = v52;
    float64x2_t v44 = v53;
    float64x2_t v45 = v54;
    float64x2_t v46 = v55;
    float64x2_t v47 = v56;
    float64x2_t v51 = 0u;
    float64x2_t v52 = 0u;
    float64x2_t v53 = 0u;
    float64x2_t v54 = 0u;
    float64x2_t v55 = 0u;
    float64x2_t v56 = 0u;
    *(void *)&v6.f64[1]  = vextq_s8((int8x16_t)v6, (int8x16_t)v6, 8uLL).u64[0];
    *(void *)&v8.f64[1]  = vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL).u64[0];
    *(void *)&v10.f64[1]  = vextq_s8((int8x16_t)v10, (int8x16_t)v10, 8uLL).u64[0];
    do
    {
      float64x2_t v35 = *(float64x2_t *)((char *)&v42 + v33);
      float64x2_t v34 = *(float64x2_t *)((char *)&v42 + v33 + 16);
      float64x2_t v36 = (float64x2_t *)((char *)&v51 + v33);
      *float64x2_t v36 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v35.f64[0]), v8, v35, 1), v10, v34.f64[0]);
      v36[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v5, v35), v7, v35, 1), v34, v9);
      v33 += 32;
    }
    while (v33 != 96);
    float64x2_t v37 = v52;
    float64x2_t v38 = v53;
    float64x2_t v39 = v54;
    float64x2_t v40 = v55;
    float64x2_t v41 = v56;
    *a3  = v51;
    a3[1]  = v37;
    a3[2]  = v38;
    a3[3]  = v39;
    a3[4]  = v40;
    a3[5]  = v41;
    goto LABEL_11;
  }
  uint64_t v22 = 0;
  v5.f64[1]  = 0.0;
  v7.f64[1]  = 0.0;
  v9.f64[1]  = 0.0;
  v11.f64[1]  = 1.0;
  float64x2_t v42 = v51;
  float64x2_t v43 = (float64x2_t)*(unint64_t *)&v52.f64[0];
  float64x2_t v44 = v53;
  float64x2_t v45 = (float64x2_t)*(unint64_t *)&v54.f64[0];
  float64x2_t v46 = v55;
  float64x2_t v47 = (float64x2_t)*(unint64_t *)&v56.f64[0];
  uint64_t v48 = 0;
  uint64_t v49 = 0;
  long long v50 = xmmword_228C1F7A0;
  float64x2_t v51 = 0u;
  float64x2_t v52 = 0u;
  float64x2_t v53 = 0u;
  float64x2_t v54 = 0u;
  float64x2_t v55 = 0u;
  float64x2_t v56 = 0u;
  float64x2_t v57 = 0u;
  float64x2_t v58 = 0u;
  do
  {
    float64x2_t v24 = *(float64x2_t *)((char *)&v42 + v22);
    float64x2_t v23 = *(float64x2_t *)((char *)&v42 + v22 + 16);
    float64x2_t v25 = (float64x2_t *)((char *)&v51 + v22);
    *float64x2_t v25 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v24.f64[0]), v8, v24, 1), v10, v23.f64[0]), v12, v23, 1);
    v25[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v5, v24.f64[0]), v7, v24, 1), v9, v23.f64[0]), v11, v23, 1);
    v22 += 32;
  }
  while (v22 != 128);
  float64x2_t v26 = v52;
  float64x2_t v27 = v53;
  float64x2_t v28 = v54;
  float64x2_t v29 = v55;
  float64x2_t v30 = v56;
  float64x2_t v31 = v57;
  float64x2_t v32 = v58;
  *a3  = v51;
  a3[1]  = v26;
  a3[2]  = v27;
  a3[3]  = v28;
  a3[4]  = v29;
  a3[5]  = v30;
  a3[6]  = v31;
  a3[7]  = v32;
}

float64x2_t simd_mul@<Q0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>)
{
  float64x2_t v3 = a1[1];
  float64x2_t v4 = a2[1];
  float64x2_t v5 = vnegq_f64(*a2);
  float64x2_t v6 = (float64x2_t)vextq_s8((int8x16_t)v4, (int8x16_t)vnegq_f64(v4), 8uLL);
  float64x2_t v7 = vmlaq_n_f64(vmulq_laneq_f64(v4, *a1, 1), v6, a1->f64[0]);
  float64x2_t v8 = vmlaq_n_f64(vmulq_laneq_f64(v4, v3, 1), v6, v3.f64[0]);
  float64x2_t result = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(*a2, v3, 1), (float64x2_t)vextq_s8((int8x16_t)v5, *(int8x16_t *)a2, 8uLL), v3.f64[0]), v7);
  float64x2_t v10 = vaddq_f64(v8, vmlaq_n_f64(vmulq_laneq_f64(v5, *a1, 1), (float64x2_t)vextq_s8(*(int8x16_t *)a2, (int8x16_t)v5, 8uLL), a1->f64[0]));
  *a3  = result;
  a3[1]  = v10;
  return result;
}

float64x2_t SPSize3DUnion@<Q0>(SPSize3D *a1@<X0>, SPSize3D *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v3 = *(float64x2_t *)&a1->vector.f64[2];
  float64x2_t v4 = *(float64x2_t *)&a2->vector.f64[2];
  unint64_t v5 = *(_OWORD *)&vabsq_f64(v3);
  *(void *)&v3.f64[0]  = *(_OWORD *)&vaddq_f64(vminnmq_f64((float64x2_t)*(unint64_t *)&v3.f64[0], (float64x2_t)0), (float64x2_t)0);
  float64x2_t v6 = vaddq_f64(vminnmq_f64(*(float64x2_t *)&a1->width, (float64x2_t)0), (float64x2_t)0);
  unint64_t v7 = *(_OWORD *)&vabsq_f64(v4);
  *(void *)&v4.f64[0]  = *(_OWORD *)&vaddq_f64(vminnmq_f64((float64x2_t)*(unint64_t *)&v4.f64[0], (float64x2_t)0), (float64x2_t)0);
  float64x2_t v8 = vaddq_f64(vminnmq_f64(*(float64x2_t *)&a2->width, (float64x2_t)0), (float64x2_t)0);
  float64x2_t result = vsubq_f64(vmaxnmq_f64(vaddq_f64(v6, vmaxnmq_f64(vabsq_f64(*(float64x2_t *)&a1->width), (float64x2_t)0)), vaddq_f64(v8, vmaxnmq_f64(vabsq_f64(*(float64x2_t *)&a2->width), (float64x2_t)0))), vminnmq_f64(v6, v8));
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = *(_OWORD *)&vsubq_f64(vmaxnmq_f64((float64x2_t)(unint64_t)*(_OWORD *)&vaddq_f64((float64x2_t)*(unint64_t *)&v3.f64[0], vmaxnmq_f64((float64x2_t)v5, (float64x2_t)0)), (float64x2_t)(unint64_t)*(_OWORD *)&vaddq_f64((float64x2_t)*(unint64_t *)&v4.f64[0], vmaxnmq_f64((float64x2_t)v7, (float64x2_t)0))), vminnmq_f64((float64x2_t)*(unint64_t *)&v3.f64[0], (float64x2_t)*(unint64_t *)&v4.f64[0]));
  return result;
}

float64x2_t *SPSize3DIntersection@<X0>(float64x2_t *result@<X0>, SPSize3D *a2@<X1>, float64_t *a3@<X8>)
{
  float64x2_t v3 = result[1];
  float64x2_t v4 = *(float64x2_t *)&a2->vector.f64[2];
  unint64_t v5 = *(_OWORD *)&vabsq_f64(v3);
  *(void *)&v3.f64[0]  = *(_OWORD *)&vaddq_f64(vminnmq_f64((float64x2_t)*(unint64_t *)&v3.f64[0], (float64x2_t)0), (float64x2_t)0);
  unint64_t v6 = *(_OWORD *)&vabsq_f64(v4);
  float64x2_t v7 = vaddq_f64(vminnmq_f64(*result, (float64x2_t)0), (float64x2_t)0);
  *(void *)&v4.f64[0]  = *(_OWORD *)&vaddq_f64(vminnmq_f64((float64x2_t)*(unint64_t *)&v4.f64[0], (float64x2_t)0), (float64x2_t)0);
  float64x2_t v8 = vaddq_f64(vminnmq_f64(*(float64x2_t *)&a2->width, (float64x2_t)0), (float64x2_t)0);
  float64x2_t v9 = vsubq_f64(vminnmq_f64(vaddq_f64(v7, vmaxnmq_f64(vabsq_f64(*result), (float64x2_t)0)), vaddq_f64(v8, vmaxnmq_f64(vabsq_f64(*(float64x2_t *)&a2->width), (float64x2_t)0))), vmaxnmq_f64(v7, v8));
  *(void *)&double v10 = *(_OWORD *)&vsubq_f64(vminnmq_f64((float64x2_t)(unint64_t)*(_OWORD *)&vaddq_f64((float64x2_t)*(unint64_t *)&v3.f64[0], vmaxnmq_f64((float64x2_t)v5, (float64x2_t)0)), (float64x2_t)(unint64_t)*(_OWORD *)&vaddq_f64((float64x2_t)*(unint64_t *)&v4.f64[0], vmaxnmq_f64((float64x2_t)v6, (float64x2_t)0))), vmaxnmq_f64((float64x2_t)*(unint64_t *)&v3.f64[0], (float64x2_t)*(unint64_t *)&v4.f64[0]));
  double v11 = v9.f64[1];
  if (fmin(fmin(v9.f64[0], v10), v9.f64[1]) < 0.0)
  {
    double v10 = 0.0;
    double v11 = 0.0;
    v9.f64[0]  = 0.0;
  }
  *a3  = v9.f64[0];
  a3[1]  = v11;
  a3[2]  = v10;
  return result;
}

double SPSize3DShear@<D0>(SPSize3D *a1@<X0>, SPAxis a2@<W1>, float64x2_t a3@<Q0>, float64x2_t a4@<Q1>, _OWORD *a5@<X8>)
{
  long long v6 = *(_OWORD *)&a1->vector.f64[2];
  long long v15 = *(_OWORD *)&a1->width;
  long long v16 = v6;
  long long v13 = 0u;
  long long v14 = 0u;
  memset(&v9, 0, 32);
  *(_OWORD *)&v9.size.width  = v15;
  *(_OWORD *)&v9.size.vector.f64[2]  = v6;
  SPRect3DShear(&v9, a2, a3, a4, v10);
  double result = *(double *)&v11;
  long long v8 = v12;
  *a5  = v11;
  a5[1]  = v8;
  return result;
}

float64x2_t SPSize3DScaleBySize@<Q0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>)
{
  uint64_t v3 = *(_OWORD *)&vmulq_f64(a1[1], a2[1]);
  float64x2_t result = vmulq_f64(*a1, *a2);
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = v3;
  return result;
}

float64_t SPSize3DUnapplyPose@<D0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>)
{
  float64x2_t v4 = a2[2];
  float64x2_t v5 = a2[3];
  float64x2_t v6 = vnegq_f64(v4);
  v4.f64[0]  = 1.0 / vaddvq_f64(vaddq_f64(vmulq_f64(v4, v4), vmulq_f64(v5, v5)));
  *(float64x2_t *)&v19.vector.f64[2]  = vmulq_n_f64(vmulq_f64(v5, (float64x2_t)xmmword_228C1FC40), v4.f64[0]);
  float64x2_t v8 = *a1;
  float64x2_t v9 = a1[1];
  *(float64x2_t *)v19.vector.f64  = vmulq_n_f64(v6, v4.f64[0]);
  long long v17 = 0u;
  long long v18 = 0u;
  float64x2_t v15 = 0u;
  float64x2_t v16 = 0u;
  float64x2_t v13 = 0u;
  float64x2_t v14 = 0u;
  float64x2_t v11 = 0u;
  float64x2_t v12 = 0u;
  v10[0]  = *(float64x2_t *)v19.vector.f64;
  v10[1]  = *(float64x2_t *)&v19.vector.f64[2];
  simd_matrix4x4(v19, v10, (uint64_t)&v11);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v8, v12), v14, v8, 1), v9, v16), (float64x2_t)0);
  *a3  = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v11, v8.f64[0]), v13, v8, 1), v15, v9.f64[0]), (float64x2_t)0);
  a3[1].f64[0]  = result;
  return result;
}

float64x2_t SPSize3DUnapplyProjectiveTransform@<Q0>(uint64_t a1@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v5 = a2[2];
  float64x2_t v6 = a2[3];
  float64x2_t v7 = a2[4];
  float64x2_t v8 = a2[5];
  float64x2_t v9 = a2[6];
  float64x2_t v10 = a2[7];
  float64x2_t v11 = (float64x2_t)vextq_s8((int8x16_t)v5, (int8x16_t)v6, 8uLL);
  float64x2_t v12 = (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v5, 8uLL);
  float64x2_t v13 = (float64x2_t)vextq_s8((int8x16_t)v9, (int8x16_t)v10, 8uLL);
  float64x2_t v14 = (float64x2_t)vextq_s8((int8x16_t)v10, (int8x16_t)v9, 8uLL);
  float64x2_t v15 = (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v8, 8uLL);
  float64x2_t v16 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)v7, 8uLL);
  float64x2_t v17 = vnegq_f64(v16);
  float64x2_t v18 = vnegq_f64(v15);
  float64x2_t v19 = vmlaq_f64(vmulq_f64(v9, v18), v13, v7);
  float64x2_t v20 = vmlaq_f64(vmulq_f64(v6, vmlaq_f64(vmulq_f64(v14, v18), v13, v16)), vmlaq_f64(vmulq_f64(v10, v17), v14, v8), v11);
  int64x2_t v21 = (int64x2_t)vmulq_f64(a2[1], vmlaq_f64(vmlaq_f64(vmulq_f64(v5, vmlaq_f64(vmulq_f64(v13, v17), v14, v15)), v19, v12), vmlaq_f64(vmulq_f64(v14, vnegq_f64(v7)), v9, v16), v11));
  int64x2_t v22 = (int64x2_t)vmulq_f64(*a2, vmlaq_f64(v20, vmlaq_f64(vmulq_f64(v13, vnegq_f64(v8)), v10, v15), v12));
  if (vaddvq_f64(vsubq_f64((float64x2_t)vzip1q_s64(v22, v21), (float64x2_t)vzip2q_s64(v22, v21))) == 0.0)
  {
    float64x2_t v23 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    float64x2_t v24 = v23;
    float64x2_t v25 = v23;
    float64x2_t v26 = v23;
    float64x2_t v28 = v23;
    float64x2_t v27 = v23;
    float64x2_t v29 = v23;
    float64x2_t v30 = v23;
  }
  else
  {
    __invert_d4();
    float64x2_t v23 = 0u;
    float64x2_t v24 = 0u;
    float64x2_t v25 = 0u;
    float64x2_t v26 = 0u;
    float64x2_t v28 = 0u;
    float64x2_t v27 = 0u;
    float64x2_t v29 = 0u;
    float64x2_t v30 = 0u;
  }
  long long v31 = *(_OWORD *)(a1 + 16);
  uint64_t v32 = *(_OWORD *)&vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v24, *(double *)a1), v26, *(float64x2_t *)a1, 1), v27, *(double *)&v31), (float64x2_t)0, v30);
  float64x2_t result = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v23, *(double *)a1), v25, *(float64x2_t *)a1, 1), v28, *(double *)&v31), (float64x2_t)0, v29);
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = v32;
  return result;
}

{
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  float64x2_t v17;
  float64x2_t v18;
  float64x2_t v19;
  float64x2_t v20;
  int64x2_t v21;
  int64x2_t v22;
  float64x2_t v23;
  float64x2_t v24;
  float64x2_t v25;
  float64x2_t v26;
  float64x2_t v27;
  float64x2_t v28;
  float64x2_t v29;
  float64x2_t v30;
  long long v31;
  uint64_t v32;
  float64x2_t result;

  float64x2_t v5 = a2[2];
  float64x2_t v6 = a2[3];
  float64x2_t v7 = a2[4];
  float64x2_t v8 = a2[5];
  float64x2_t v9 = a2[6];
  float64x2_t v10 = a2[7];
  float64x2_t v11 = (float64x2_t)vextq_s8((int8x16_t)v5, (int8x16_t)v6, 8uLL);
  float64x2_t v12 = (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v5, 8uLL);
  float64x2_t v13 = (float64x2_t)vextq_s8((int8x16_t)v9, (int8x16_t)v10, 8uLL);
  float64x2_t v14 = (float64x2_t)vextq_s8((int8x16_t)v10, (int8x16_t)v9, 8uLL);
  float64x2_t v15 = (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v8, 8uLL);
  float64x2_t v16 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)v7, 8uLL);
  float64x2_t v17 = vnegq_f64(v16);
  float64x2_t v18 = vnegq_f64(v15);
  float64x2_t v19 = vmlaq_f64(vmulq_f64(v9, v18), v13, v7);
  float64x2_t v20 = vmlaq_f64(vmulq_f64(v6, vmlaq_f64(vmulq_f64(v14, v18), v13, v16)), vmlaq_f64(vmulq_f64(v10, v17), v14, v8), v11);
  int64x2_t v21 = (int64x2_t)vmulq_f64(a2[1], vmlaq_f64(vmlaq_f64(vmulq_f64(v5, vmlaq_f64(vmulq_f64(v13, v17), v14, v15)), v19, v12), vmlaq_f64(vmulq_f64(v14, vnegq_f64(v7)), v9, v16), v11));
  int64x2_t v22 = (int64x2_t)vmulq_f64(*a2, vmlaq_f64(v20, vmlaq_f64(vmulq_f64(v13, vnegq_f64(v8)), v10, v15), v12));
  if (vaddvq_f64(vsubq_f64((float64x2_t)vzip1q_s64(v22, v21), (float64x2_t)vzip2q_s64(v22, v21))) == 0.0)
  {
    float64x2_t v23 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    float64x2_t v24 = v23;
    float64x2_t v25 = v23;
    float64x2_t v26 = v23;
    float64x2_t v28 = v23;
    float64x2_t v27 = v23;
    float64x2_t v29 = v23;
    float64x2_t v30 = v23;
  }
  else
  {
    __invert_d4();
    float64x2_t v23 = 0u;
    float64x2_t v24 = 0u;
    float64x2_t v25 = 0u;
    float64x2_t v26 = 0u;
    float64x2_t v28 = 0u;
    float64x2_t v27 = 0u;
    float64x2_t v29 = 0u;
    float64x2_t v30 = 0u;
  }
  long long v31 = *(_OWORD *)(a1 + 16);
  uint64_t v32 = *(_OWORD *)&vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v24, *(double *)a1), v26, *(float64x2_t *)a1, 1), v27, *(double *)&v31), (float64x2_t)0, v30);
  float64x2_t result = vmlaq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v23, *(double *)a1), v25, *(float64x2_t *)a1, 1), v28, *(double *)&v31), (float64x2_t)0, v29);
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = v32;
  return result;
}

double SPSize3DUnapplyAffineTransform@<D0>(float64x2_t *a1@<X0>, uint64_t a2@<X1>, uint64_t a3@<X8>)
{
  v5.f64[0]  = *(float64_t *)(a2 + 80);
  v5.f64[1]  = *(float64_t *)(a2 + 64);
  v6.f64[0]  = *(float64_t *)(a2 + 48);
  v6.f64[1]  = *(float64_t *)(a2 + 32);
  if (vmulq_f64(*(float64x2_t *)(a2 + 16), vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)(a2 + 64), vnegq_f64((float64x2_t)vdupq_laneq_s64(*(int64x2_t *)(a2 + 32), 1))), *(float64x2_t *)(a2 + 32), *(float64x2_t *)(a2 + 64), 1)).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)a2, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 64), *(int8x16_t *)(a2 + 80), 8uLL), vnegq_f64(v6)), v5, (float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 32), *(int8x16_t *)(a2 + 48), 8uLL)))) == 0.0)
  {
    float64x2_t v7 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    float64x2_t v9 = v7;
    float64x2_t v8 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v10 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v12 = v7;
    float64x2_t v11 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v14 = v7;
    float64x2_t v13 = (float64x2_t)xmmword_228C1FC60;
  }
  else
  {
    float64x2_t v19 = *(float64x2_t *)(a2 + 96);
    float64x2_t v20 = *(float64x2_t *)(a2 + 112);
    __invert_d3();
    float64x2_t v7 = 0u;
    float64x2_t v8 = 0u;
    float64x2_t v9 = 0u;
    float64x2_t v10 = 0u;
    float64x2_t v12 = 0u;
    float64x2_t v11 = 0u;
    float64x2_t v13 = vnegq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v19, (float64x2_t)0), (float64x2_t)0, v19, 1), v20, (float64x2_t)0));
    float64x2_t v14 = vnegq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v19.f64[0]), (float64x2_t)0, v19, 1), (float64x2_t)0, v20.f64[0]));
  }
  float64x2_t v15 = a1[1];
  float64x2_t v16 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v7, a1->f64[0]), v9, *a1, 1), v12, v15.f64[0]);
  double result = 0.0;
  uint64_t v18 = *(_OWORD *)&vmlaq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v8, *a1), v10, *a1, 1), v15, v11), (float64x2_t)0, v13);
  *(float64x2_t *)a3  = vmlaq_f64(v16, (float64x2_t)0, v14);
  *(void *)(a3 + 16)  = v18;
  return result;
}

{
  float64x2_t v5;
  float64x2_t v6;
  float64x2_t v7;
  float64x2_t v8;
  float64x2_t v9;
  float64x2_t v10;
  float64x2_t v11;
  float64x2_t v12;
  float64x2_t v13;
  float64x2_t v14;
  float64x2_t v15;
  float64x2_t v16;
  double result;
  uint64_t v18;
  float64x2_t v19;
  float64x2_t v20;

  v5.f64[0]  = *(float64_t *)(a2 + 80);
  v5.f64[1]  = *(float64_t *)(a2 + 64);
  v6.f64[0]  = *(float64_t *)(a2 + 48);
  v6.f64[1]  = *(float64_t *)(a2 + 32);
  if (vmulq_f64(*(float64x2_t *)(a2 + 16), vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)(a2 + 64), vnegq_f64((float64x2_t)vdupq_laneq_s64(*(int64x2_t *)(a2 + 32), 1))), *(float64x2_t *)(a2 + 32), *(float64x2_t *)(a2 + 64), 1)).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)a2, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 64), *(int8x16_t *)(a2 + 80), 8uLL), vnegq_f64(v6)), v5, (float64x2_t)vextq_s8(*(int8x16_t *)(a2 + 32), *(int8x16_t *)(a2 + 48), 8uLL)))) == 0.0)
  {
    float64x2_t v7 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
    float64x2_t v9 = v7;
    float64x2_t v8 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v10 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v12 = v7;
    float64x2_t v11 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v14 = v7;
    float64x2_t v13 = (float64x2_t)xmmword_228C1FC60;
  }
  else
  {
    float64x2_t v19 = *(float64x2_t *)(a2 + 96);
    float64x2_t v20 = *(float64x2_t *)(a2 + 112);
    __invert_d3();
    float64x2_t v7 = 0u;
    float64x2_t v8 = 0u;
    float64x2_t v9 = 0u;
    float64x2_t v10 = 0u;
    float64x2_t v12 = 0u;
    float64x2_t v11 = 0u;
    float64x2_t v13 = vnegq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v19, (float64x2_t)0), (float64x2_t)0, v19, 1), v20, (float64x2_t)0));
    float64x2_t v14 = vnegq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v19.f64[0]), (float64x2_t)0, v19, 1), (float64x2_t)0, v20.f64[0]));
  }
  float64x2_t v15 = a1[1];
  float64x2_t v16 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v7, a1->f64[0]), v9, *a1, 1), v12, v15.f64[0]);
  double result = 0.0;
  uint64_t v18 = *(_OWORD *)&vmlaq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v8, *a1), v10, *a1, 1), v15, v11), (float64x2_t)0, v13);
  *(float64x2_t *)a3  = vmlaq_f64(v16, (float64x2_t)0, v14);
  *(void *)(a3 + 16)  = v18;
  return result;
}

float64_t SPSize3DApplyPose@<D0>(float64x2_t *a1@<X0>, uint64_t a2@<X1>, float64x2_t *a3@<X8>)
{
  float64x2_t v5 = *a1;
  float64x2_t v6 = a1[1];
  simd_quatd v16 = *(simd_quatd *)(a2 + 32);
  long long v14 = 0u;
  long long v15 = 0u;
  float64x2_t v12 = 0u;
  float64x2_t v13 = 0u;
  float64x2_t v10 = 0u;
  float64x2_t v11 = 0u;
  float64x2_t v8 = 0u;
  float64x2_t v9 = 0u;
  v7[0]  = *(float64x2_t *)v16.vector.f64;
  v7[1]  = *(float64x2_t *)&v16.vector.f64[2];
  simd_matrix4x4(v16, v7, (uint64_t)&v8);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v5, v9), v11, v5, 1), v6, v13), (float64x2_t)0);
  *a3  = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v8, v5.f64[0]), v10, v5, 1), v12, v6.f64[0]), (float64x2_t)0);
  a3[1].f64[0]  = result;
  return result;
}

void SPRay3DRotateByQuaternion(SPRay3D *a1@<X0>, simd_quatd a2@<0:Q0, 16:Q1>, _OWORD *a3@<X1>, uint64_t a4@<X8>)
{
  *(_OWORD *)&v23.vector.f64[2]  = *a3;
  long long v10 = a3[1];
  float64x2_t v11 = *(float64x2_t *)&v23.vector.f64[2];
  SPRay3D v12 = *a1;
  *(_OWORD *)v23.vector.f64  = 0uLL;
  long long v21 = 0u;
  long long v22 = 0u;
  float64x2_t v19 = 0u;
  float64x2_t v20 = 0u;
  float64x2_t v17 = 0u;
  float64x2_t v18 = 0u;
  float64x2_t v15 = 0u;
  float64x2_t v16 = 0u;
  float64x2_t v13 = *(float64x2_t *)&v23.vector.f64[2];
  long long v14 = v10;
  simd_matrix4x4(v23, &v13, (uint64_t)&v15);
  *(void *)&double v5 = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)&v12.origin.x, v16), v18, *(float64x2_t *)&v12.origin.x, 1), *(float64x2_t *)&v12.origin.vector.f64[2], v20), (float64x2_t)0);
  *(float64x2_t *)&v12.origin.x  = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v15, v12.origin.x), v17, *(float64x2_t *)&v12.origin.x, 1), v19, v12.origin.z), (float64x2_t)0);
  v12.origin.double z = v5;
  long long v21 = 0u;
  long long v22 = 0u;
  float64x2_t v19 = 0u;
  float64x2_t v20 = 0u;
  float64x2_t v17 = 0u;
  float64x2_t v18 = 0u;
  float64x2_t v15 = 0u;
  float64x2_t v16 = 0u;
  *(_OWORD *)&v24.vector.f64[2]  = v10;
  *(float64x2_t *)v24.vector.f64  = v11;
  float64x2_t v13 = v11;
  long long v14 = v10;
  simd_matrix4x4(v24, &v13, (uint64_t)&v15);
  float64x2_t v6 = vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)&v12.direction.x, v16), v18, *(float64x2_t *)&v12.direction.x, 1), *(float64x2_t *)&v12.direction.vector.f64[2], v20), (float64x2_t)0);
  float64x2_t v7 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v15, v12.direction.x), v17, *(float64x2_t *)&v12.direction.x, 1), v19, v12.direction.z), (float64x2_t)0);
  *(void *)(a4 + 48)  = 0;
  *(void *)(a4 + 56)  = 0;
  int64x2_t v8 = vceqzq_f64(v7);
  *(_OWORD *)a4  = *(_OWORD *)&v12.origin.x;
  *(_OWORD *)(a4 + 16)  = *(unint64_t *)&v12.origin.z;
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v8, 1), vandq_s8((int8x16_t)vceqzq_f64(v6), (int8x16_t)v8)).u64[0] & 0x8000000000000000) != 0)
  {
    *(float64x2_t *)(a4 + 32)  = v7;
    *(_OWORD *)(a4 + 48)  = *(unint64_t *)&v6.f64[0];
  }
  else
  {
    float64x2_t v9 = vmulq_f64((float64x2_t)*(unint64_t *)&v6.f64[0], (float64x2_t)*(unint64_t *)&v6.f64[0]);
    v9.f64[0]  = 1.0 / sqrt(v9.f64[0] + vaddvq_f64(vmulq_f64(v7, v7)));
    *(void *)&v7.f64[1]  = vextq_s8((int8x16_t)v7, (int8x16_t)v7, 8uLL).u64[0];
    *(float64x2_t *)(a4 + 32)  = vmulq_n_f64(v7, v9.f64[0]);
    *(void *)(a4 + 48)  = *(_OWORD *)&vmulq_f64(v6, v9);
  }
}

void SPRay3DRotate(SPRay3D *a1@<X0>, SPRotation3D a2@<0:Q0, 16:Q1>, _OWORD *a3@<X1>, float64x2_t *a4@<X8>)
{
  float64x2_t v14 = *(float64x2_t *)&a1->origin.x;
  float64x2_t v16 = *(float64x2_t *)&a1->origin.vector.f64[2];
  *(_OWORD *)&v27.vector.f64[2]  = *a3;
  long long v12 = a3[1];
  float64x2_t v13 = *(float64x2_t *)&v27.vector.f64[2];
  *(_OWORD *)v27.vector.f64  = 0uLL;
  long long v25 = 0u;
  long long v26 = 0u;
  float64x2_t v23 = 0u;
  float64x2_t v24 = 0u;
  float64x2_t v21 = 0u;
  float64x2_t v22 = 0u;
  float64x2_t v19 = 0u;
  float64x2_t v20 = 0u;
  float64x2_t v17 = *(float64x2_t *)&v27.vector.f64[2];
  long long v18 = v12;
  simd_matrix4x4(v27, &v17, (uint64_t)&v19);
  *(void *)&float64_t v6 = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v14, v20), v22, v14, 1), v16, v24), (float64x2_t)0);
  float64x2_t v15 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v19, v14.f64[0]), v21, v14, 1), v23, v16.f64[0]), (float64x2_t)0);
  v16.f64[0]  = v6;
  *(SPVector3D *)&v11.x  = *(SPVector3D *)&a1->direction.x;
  long long v25 = 0u;
  long long v26 = 0u;
  float64x2_t v23 = 0u;
  float64x2_t v24 = 0u;
  float64x2_t v21 = 0u;
  float64x2_t v22 = 0u;
  float64x2_t v19 = 0u;
  float64x2_t v20 = 0u;
  *(_OWORD *)&v28.vector.f64[2]  = v12;
  *(float64x2_t *)v28.vector.f64  = v13;
  float64x2_t v17 = v13;
  long long v18 = v12;
  simd_matrix4x4(v28, &v17, (uint64_t)&v19);
  float64x2_t v7 = vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)&v11.x, v20), v22, *(float64x2_t *)&v11.x, 1), *(float64x2_t *)&v11.vector.f64[2], v24), (float64x2_t)0);
  float64x2_t v8 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v19, v11.x), v21, *(float64x2_t *)&v11.x, 1), v23, v11.z), (float64x2_t)0);
  a4[2]  = 0u;
  a4[3]  = 0u;
  int64x2_t v9 = vceqzq_f64(v8);
  *a4  = v15;
  a4[1]  = (float64x2_t)*(unint64_t *)&v16.f64[0];
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v9, 1), vandq_s8((int8x16_t)vceqzq_f64(v7), (int8x16_t)v9)).u64[0] & 0x8000000000000000) != 0)
  {
    a4[2]  = v8;
    a4[3]  = (float64x2_t)*(unint64_t *)&v7.f64[0];
  }
  else
  {
    float64x2_t v10 = vmulq_f64((float64x2_t)*(unint64_t *)&v7.f64[0], (float64x2_t)*(unint64_t *)&v7.f64[0]);
    v10.f64[0]  = 1.0 / sqrt(v10.f64[0] + vaddvq_f64(vmulq_f64(v8, v8)));
    *(void *)&v8.f64[1]  = vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL).u64[0];
    a4[2]  = vmulq_n_f64(v8, v10.f64[0]);
    *(void *)&a4[3].f64[0]  = *(_OWORD *)&vmulq_f64(v7, v10);
  }
}

void SPRay3DUnapplyPose(SPRay3D *a1@<X0>, SPPose3D *a2@<X1>, uint64_t a3@<X8>)
{
  long long v4 = *(_OWORD *)&a1->origin.vector.f64[2];
  long long v5 = *(_OWORD *)&a2->position.vector.f64[2];
  float64x2_t v6 = *(float64x2_t *)a2->rotation.vector.f64;
  float64x2_t v7 = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  float64x2_t v8 = vmulq_f64(v7, (float64x2_t)xmmword_228C1FC40);
  float64x2_t v9 = vnegq_f64(v6);
  v7.f64[0]  = 1.0 / vaddvq_f64(vaddq_f64(vmulq_f64(v6, v6), vmulq_f64(v7, v7)));
  *(float64x2_t *)v43.vector.f64  = vmulq_n_f64(v8, v7.f64[0]);
  *(float64x2_t *)&v43.vector.f64[2]  = vmulq_n_f64(v9, v7.f64[0]);
  float64x2_t v10 = vmulq_f64(*(float64x2_t *)v43.vector.f64, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v11 = (int8x16_t)vnegq_f64(*(float64x2_t *)&v43.vector.f64[2]);
  float64x2_t v12 = vnegq_f64(*(float64x2_t *)&a2->position.x);
  float64x2_t v13 = (float64x2_t)vextq_s8((int8x16_t)v10, (int8x16_t)vnegq_f64(v10), 8uLL);
  float64x2_t v14 = (float64x2_t)vextq_s8(v11, *(int8x16_t *)&v43.vector.f64[2], 8uLL);
  float64x2_t v15 = vmulq_laneq_f64(v10, v12, 1);
  float64x2_t v16 = (float64x2_t)vextq_s8(*(int8x16_t *)&v43.vector.f64[2], v11, 8uLL);
  float64x2_t v17 = vmlsq_lane_f64(vmlsq_lane_f64(vmulq_laneq_f64(*(float64x2_t *)&v43.vector.f64[2], v12, 1), v14, a2->position.x, 0), v13, *(double *)&v5, 0);
  float64x2_t v18 = vmlsq_lane_f64(vmlsq_lane_f64(v15, v13, a2->position.x, 0), v16, *(double *)&v5, 0);
  float64x2_t v19 = vnegq_f64(v18);
  float64x2_t v20 = (float64x2_t)vextq_s8((int8x16_t)v17, (int8x16_t)vnegq_f64(v17), 8uLL);
  float64x2_t v21 = vmlaq_n_f64(vmulq_laneq_f64(v17, *(float64x2_t *)&v43.vector.f64[2], 1), v20, v43.vector.f64[2]);
  float64x2_t v22 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v17, *(float64x2_t *)v43.vector.f64, 1), v20, v43.vector.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v19, *(float64x2_t *)&v43.vector.f64[2], 1), (float64x2_t)vextq_s8((int8x16_t)v18, (int8x16_t)v19, 8uLL), v43.vector.f64[2]));
  float64x2_t v23 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(*(float64x2_t *)&v43.vector.f64[2], *(float64x2_t *)&a1->origin.x, 1), v14, a1->origin.x), v13, *(double *)&v4);
  float64x2_t v24 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v10, *(float64x2_t *)&a1->origin.x, 1), v13, a1->origin.x), v16, *(double *)&v4);
  float64x2_t v25 = vnegq_f64(v24);
  float64x2_t v26 = (float64x2_t)vextq_s8((int8x16_t)v23, (int8x16_t)vnegq_f64(v23), 8uLL);
  *(SPVector3D *)&v31.x  = *(SPVector3D *)&a1->direction.x;
  long long v41 = 0u;
  long long v42 = 0u;
  float64x2_t v39 = 0u;
  float64x2_t v40 = 0u;
  float64x2_t v37 = 0u;
  float64x2_t v38 = 0u;
  float64x2_t v35 = 0u;
  float64x2_t v36 = 0u;
  float64x2_t v32 = vaddq_f64(vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v24, *(float64x2_t *)v43.vector.f64, 1), (float64x2_t)vextq_s8((int8x16_t)v25, (int8x16_t)v24, 8uLL), v43.vector.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v23, *(float64x2_t *)&v43.vector.f64[2], 1), v26, v43.vector.f64[2])), vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v18, *(float64x2_t *)v43.vector.f64, 1), (float64x2_t)vextq_s8((int8x16_t)v19, (int8x16_t)v18, 8uLL), v43.vector.f64[0]), v21));
  uint64_t v33 = *(_OWORD *)&vaddq_f64(vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v23, *(float64x2_t *)v43.vector.f64, 1), v26, v43.vector.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v25, *(float64x2_t *)&v43.vector.f64[2], 1), (float64x2_t)vextq_s8((int8x16_t)v24, (int8x16_t)v25, 8uLL), v43.vector.f64[2])), v22);
  v34[0]  = *(float64x2_t *)&v43.vector.f64[2];
  v34[1]  = *(float64x2_t *)v43.vector.f64;
  simd_matrix4x4(v43, v34, (uint64_t)&v35);
  float64x2_t v27 = vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)&v31.x, v36), v38, *(float64x2_t *)&v31.x, 1), *(float64x2_t *)&v31.vector.f64[2], v40), (float64x2_t)0);
  float64x2_t v29 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v35, v31.x), v37, *(float64x2_t *)&v31.x, 1), v39, v31.z), (float64x2_t)0);
  int64x2_t v28 = vceqzq_f64(v29);
  *(float64x2_t *)a3  = v32;
  *(void *)(a3 + 16)  = v33;
  *(void *)&v29.f64[1]  = vextq_s8((int8x16_t)v29, (int8x16_t)v29, 8uLL).u64[0];
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v28, 1), vandq_s8((int8x16_t)vceqzq_f64(v27), (int8x16_t)v28)).u64[0] & 0x8000000000000000) != 0)
  {
    *(float64x2_t *)(a3 + 32)  = v29;
    *(float64x2_t *)(a3 + 48)  = v27;
  }
  else
  {
    float64x2_t v30 = vmulq_f64(v29, v29);
    v30.f64[0]  = 1.0 / sqrt(vmulq_f64(v27, v27).f64[0] + vaddvq_f64(v30));
    *(float64x2_t *)(a3 + 32)  = vmulq_n_f64(v29, v30.f64[0]);
    *(void *)(a3 + 48)  = *(_OWORD *)&vmulq_f64(v27, v30);
  }
}

void SPRect3DRotateByQuaternion(SPRect3D *a1@<X0>, simd_quatd a2@<0:Q0, 16:Q1>, simd_quatd *a3@<X1>, float64x2_t *a4@<X8>)
{
  uint64_t v46 = *MEMORY[0x263EF8340];
  simd_quatd v47 = *a3;
  width  = a1->size.width;
  height  = a1->size.height;
  depth  = a1->size.depth;
  long long v36 = 0u;
  long long v37 = 0u;
  float64x2_t v34 = 0u;
  float64x2_t v35 = 0u;
  float64x2_t v32 = 0u;
  float64x2_t v33 = 0u;
  float64x2_t v30 = 0u;
  float64x2_t v31 = 0u;
  v29[0]  = *(float64x2_t *)v47.vector.f64;
  v29[1]  = *(float64x2_t *)&v47.vector.f64[2];
  simd_matrix4x4(v47, v29, (uint64_t)&v30);
  uint64_t v9 = 0;
  float64x2_t v11 = v30;
  float64x2_t v10 = v31;
  float64x2_t v13 = v32;
  float64x2_t v12 = v33;
  float64x2_t v15 = v34;
  float64x2_t v14 = v35;
  long long v44 = 0u;
  *(_OWORD *)float64x2_t v45 = 0u;
  long long v42 = 0u;
  long long v43 = 0u;
  long long v40 = 0u;
  long long v41 = 0u;
  long long v38 = 0u;
  long long v39 = 0u;
  long long v36 = 0u;
  long long v37 = 0u;
  float64x2_t v34 = 0u;
  float64x2_t v35 = 0u;
  float64x2_t v32 = 0u;
  float64x2_t v33 = 0u;
  float64x2_t v30 = 0u;
  float64x2_t v31 = 0u;
  do
  {
    float64x2_t v16 = (_OWORD *)&v30.f64[v9];
    long long v17 = *(_OWORD *)&a1->origin.vector.f64[2];
    *float64x2_t v16 = *(_OWORD *)&a1->origin.x;
    v16[1]  = v17;
    v9 += 4;
  }
  while (v9 != 32);
  uint64_t v18 = 0;
  v32.f64[1]  = height + v32.f64[1];
  v34.f64[0]  = width + v34.f64[0];
  v34.f64[1]  = height + v34.f64[1];
  *(double *)&long long v36 = width + *(double *)&v36;
  *(double *)&long long v39 = depth + *(double *)&v39;
  *((double *)&v40 + 1)  = height + *((double *)&v40 + 1);
  *(double *)&long long v41 = depth + *(double *)&v41;
  *(double *)&long long v42 = width + *(double *)&v42;
  *((double *)&v42 + 1)  = height + *((double *)&v42 + 1);
  *(double *)&long long v43 = depth + *(double *)&v43;
  *(double *)&long long v44 = width + *(double *)&v44;
  v45[0]  = depth + v45[0];
  *(void *)&v11.f64[1]  = vextq_s8((int8x16_t)v11, (int8x16_t)v11, 8uLL).u64[0];
  *(void *)&v13.f64[1]  = vextq_s8((int8x16_t)v13, (int8x16_t)v13, 8uLL).u64[0];
  *(void *)&v15.f64[1]  = vextq_s8((int8x16_t)v15, (int8x16_t)v15, 8uLL).u64[0];
  do
  {
    float64x2_t v19 = (char *)&v30 + v18;
    float64x2_t v21 = *(float64x2_t *)((char *)&v30 + v18);
    float64x2_t v20 = *(float64x2_t *)((char *)&v30 + v18 + 16);
    *(float64x2_t *)float64x2_t v19 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v11, v21.f64[0]), v13, v21, 1), v15, v20.f64[0]), (float64x2_t)0);
    *((void *)v19 + 2)  = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v10, v21), v12, v21, 1), v20, v14), (float64x2_t)0);
    v18 += 32;
  }
  while (v18 != 256);
  uint64_t v22 = 0;
  a4[2]  = 0u;
  a4[3]  = 0u;
  a4[1]  = 0u;
  float64x2_t v23 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v24.f64[0]  = INFINITY;
  float64x2_t v25 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
  v26.f64[0]  = -INFINITY;
  do
  {
    float64x2_t v27 = *(float64x2_t *)((char *)&v30 + v22);
    long long v28 = *(long long *)((char *)&v30 + v22 + 16);
    float64x2_t v23 = vminnmq_f64(v23, v27);
    float64x2_t v24 = vminnmq_f64((float64x2_t)*(unint64_t *)&v24.f64[0], (float64x2_t)(unint64_t)v28);
    float64x2_t v25 = vmaxnmq_f64(v25, v27);
    float64x2_t v26 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v26.f64[0], (float64x2_t)(unint64_t)v28);
    v22 += 32;
  }
  while (v22 != 256);
  *a4  = v23;
  a4[1].f64[0]  = v24.f64[0];
  a4[2]  = vsubq_f64(v25, v23);
  *(void *)&a4[3].f64[0]  = *(_OWORD *)&vsubq_f64(v26, v24);
}

float64x2_t SPRect3DRotate@<Q0>(SPRect3D *a1@<X0>, SPRotation3D a2@<0:Q0, 16:Q1>, simd_quatd *a3@<X1>, float64x2_t *a4@<X8>)
{
  uint64_t v46 = *MEMORY[0x263EF8340];
  simd_quatd v47 = *a3;
  long long v36 = 0u;
  long long v37 = 0u;
  float64x2_t v34 = 0u;
  float64x2_t v35 = 0u;
  float64x2_t v32 = 0u;
  float64x2_t v33 = 0u;
  float64x2_t v30 = 0u;
  float64x2_t v31 = 0u;
  v29[0]  = *(float64x2_t *)v47.vector.f64;
  v29[1]  = *(float64x2_t *)&v47.vector.f64[2];
  simd_matrix4x4(v47, v29, (uint64_t)&v30);
  uint64_t v6 = 0;
  float64x2_t v8 = v30;
  float64x2_t v7 = v31;
  float64x2_t v10 = v32;
  float64x2_t v9 = v33;
  float64x2_t v12 = v34;
  float64x2_t v11 = v35;
  float64x2_t v13 = *(float64x2_t *)&a1->size.width;
  depth  = a1->size.depth;
  long long v44 = 0u;
  *(_OWORD *)float64x2_t v45 = 0u;
  float64x2_t v42 = 0u;
  long long v43 = 0u;
  long long v40 = 0u;
  long long v41 = 0u;
  long long v38 = 0u;
  long long v39 = 0u;
  long long v36 = 0u;
  long long v37 = 0u;
  float64x2_t v34 = 0u;
  float64x2_t v35 = 0u;
  float64x2_t v32 = 0u;
  float64x2_t v33 = 0u;
  float64x2_t v30 = 0u;
  float64x2_t v31 = 0u;
  do
  {
    float64x2_t v15 = (_OWORD *)&v30.f64[v6];
    long long v16 = *(_OWORD *)&a1->origin.vector.f64[2];
    *float64x2_t v15 = *(_OWORD *)&a1->origin.x;
    v15[1]  = v16;
    v6 += 4;
  }
  while (v6 != 32);
  uint64_t v17 = 0;
  v32.f64[1]  = v13.f64[1] + v32.f64[1];
  float64x2_t v34 = vaddq_f64(v13, v34);
  *(double *)&long long v36 = v13.f64[0] + *(double *)&v36;
  *(double *)&long long v39 = depth + *(double *)&v39;
  *((double *)&v40 + 1)  = v13.f64[1] + *((double *)&v40 + 1);
  *(double *)&long long v41 = depth + *(double *)&v41;
  float64x2_t v42 = vaddq_f64(v13, v42);
  *(double *)&long long v43 = depth + *(double *)&v43;
  *(double *)&long long v44 = v13.f64[0] + *(double *)&v44;
  v45[0]  = depth + v45[0];
  *(void *)&v8.f64[1]  = vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL).u64[0];
  *(void *)&v10.f64[1]  = vextq_s8((int8x16_t)v10, (int8x16_t)v10, 8uLL).u64[0];
  *(void *)&v12.f64[1]  = vextq_s8((int8x16_t)v12, (int8x16_t)v12, 8uLL).u64[0];
  do
  {
    uint64_t v18 = (char *)&v30 + v17;
    float64x2_t v20 = *(float64x2_t *)((char *)&v30 + v17);
    float64x2_t v19 = *(float64x2_t *)((char *)&v30 + v17 + 16);
    *(float64x2_t *)uint64_t v18 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v8, v20.f64[0]), v10, v20, 1), v12, v19.f64[0]), (float64x2_t)0);
    *((void *)v18 + 2)  = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v7, v20), v9, v20, 1), v19, v11), (float64x2_t)0);
    v17 += 32;
  }
  while (v17 != 256);
  uint64_t v21 = 0;
  a4[2]  = 0u;
  a4[3]  = 0u;
  a4[1]  = 0u;
  float64x2_t v22 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v23.f64[0]  = INFINITY;
  float64x2_t v24 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
  v25.f64[0]  = -INFINITY;
  do
  {
    float64x2_t v26 = *(float64x2_t *)((char *)&v30 + v21);
    long long v27 = *(long long *)((char *)&v30 + v21 + 16);
    float64x2_t v22 = vminnmq_f64(v22, v26);
    float64x2_t v23 = vminnmq_f64((float64x2_t)*(unint64_t *)&v23.f64[0], (float64x2_t)(unint64_t)v27);
    float64x2_t v24 = vmaxnmq_f64(v24, v26);
    float64x2_t v25 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v25.f64[0], (float64x2_t)(unint64_t)v27);
    v21 += 32;
  }
  while (v21 != 256);
  *a4  = v22;
  a4[1].f64[0]  = v23.f64[0];
  float64x2_t result = vsubq_f64(v24, v22);
  a4[2]  = result;
  *(void *)&a4[3].f64[0]  = *(_OWORD *)&vsubq_f64(v25, v23);
  return result;
}

float64x2_t *SPProjectiveTransform3DRotateByQuaternion@<X0>(uint64_t a1@<X0>, simd_quatd *a2@<X1>, float64x2_t *a3@<X8>, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15, float64x2_t a16, long long a17, long long a18, long long a19, long long a20,long long a21,long long a22,long long a23)
{
  simd_quatd v33 = *a2;
  a22  = 0u;
  a23  = 0u;
  a20  = 0u;
  a21  = 0u;
  a18  = 0u;
  a19  = 0u;
  a16  = 0u;
  a17  = 0u;
  float64x2_t v30 = *(float64x2_t *)v33.vector.f64;
  long long v31 = *(_OWORD *)&v33.vector.f64[2];
  simd_matrix4x4(v33, &v30, (uint64_t)&a16);
  float64x2_t v30 = a16;
  long long v31 = a17;
  vars0  = a18;
  long long v25 = *(_OWORD *)(a1 + 80);
  a20  = *(_OWORD *)(a1 + 64);
  a21  = v25;
  long long v26 = *(_OWORD *)(a1 + 112);
  a22  = *(_OWORD *)(a1 + 96);
  a23  = v26;
  long long v27 = *(_OWORD *)(a1 + 16);
  a16  = *(float64x2_t *)a1;
  a17  = v27;
  long long v28 = *(_OWORD *)(a1 + 48);
  a18  = *(_OWORD *)(a1 + 32);
  a19  = v28;
  return SPProjectiveTransform3DConcatenation(&a16, &v30, a3);
}

float64x2_t *SPProjectiveTransform3DRotate@<X0>(uint64_t a1@<X0>, simd_quatd *a2@<X1>, float64x2_t *a3@<X8>, uint64_t a4, uint64_t a5, uint64_t a6, uint64_t a7, uint64_t a8, uint64_t a9, uint64_t a10, uint64_t a11, uint64_t a12, uint64_t a13, uint64_t a14, uint64_t a15, float64x2_t a16, long long a17, long long a18, long long a19, long long a20,long long a21,long long a22,long long a23)
{
  simd_quatd v33 = *a2;
  a22  = 0u;
  a23  = 0u;
  a20  = 0u;
  a21  = 0u;
  a18  = 0u;
  a19  = 0u;
  a16  = 0u;
  a17  = 0u;
  float64x2_t v30 = *(float64x2_t *)v33.vector.f64;
  long long v31 = *(_OWORD *)&v33.vector.f64[2];
  simd_matrix4x4(v33, &v30, (uint64_t)&a16);
  float64x2_t v30 = a16;
  long long v31 = a17;
  vars0  = a18;
  long long v25 = *(_OWORD *)(a1 + 80);
  a20  = *(_OWORD *)(a1 + 64);
  a21  = v25;
  long long v26 = *(_OWORD *)(a1 + 112);
  a22  = *(_OWORD *)(a1 + 96);
  a23  = v26;
  long long v27 = *(_OWORD *)(a1 + 16);
  a16  = *(float64x2_t *)a1;
  a17  = v27;
  long long v28 = *(_OWORD *)(a1 + 48);
  a18  = *(_OWORD *)(a1 + 32);
  a19  = v28;
  return SPProjectiveTransform3DConcatenation(&a16, &v30, a3);
}

float64_t SPSize3DRotateByQuaternion@<D0>(float64x2_t *a1@<X0>, simd_quatd *a2@<X1>, float64x2_t *a3@<X8>)
{
  simd_quatd v16 = *a2;
  float64x2_t v5 = *a1;
  float64x2_t v6 = a1[1];
  long long v14 = 0u;
  long long v15 = 0u;
  float64x2_t v12 = 0u;
  float64x2_t v13 = 0u;
  float64x2_t v10 = 0u;
  float64x2_t v11 = 0u;
  float64x2_t v8 = 0u;
  float64x2_t v9 = 0u;
  v7[0]  = *(float64x2_t *)v16.vector.f64;
  v7[1]  = *(float64x2_t *)&v16.vector.f64[2];
  simd_matrix4x4(v16, v7, (uint64_t)&v8);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v5, v9), v11, v5, 1), v6, v13), (float64x2_t)0);
  *a3  = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v8, v5.f64[0]), v10, v5, 1), v12, v6.f64[0]), (float64x2_t)0);
  a3[1].f64[0]  = result;
  return result;
}

float64_t SPSize3DRotate@<D0>(float64x2_t *a1@<X0>, simd_quatd *a2@<X1>, float64x2_t *a3@<X8>)
{
  simd_quatd v16 = *a2;
  long long v14 = 0u;
  long long v15 = 0u;
  float64x2_t v12 = 0u;
  float64x2_t v13 = 0u;
  float64x2_t v10 = 0u;
  float64x2_t v11 = 0u;
  float64x2_t v8 = 0u;
  float64x2_t v9 = 0u;
  v7[0]  = *(float64x2_t *)v16.vector.f64;
  v7[1]  = *(float64x2_t *)&v16.vector.f64[2];
  simd_matrix4x4(v16, v7, (uint64_t)&v8);
  float64x2_t v5 = a1[1];
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v9, *a1), v11, *a1, 1), v5, v13), (float64x2_t)0);
  *a3  = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v8, a1->f64[0]), v10, *a1, 1), v12, v5.f64[0]), (float64x2_t)0);
  a3[1].f64[0]  = result;
  return result;
}

double SPSize3D.init(_:)(__n128 a1, __n128 a2)
{
  v4[0]  = a1;
  v4[1]  = a2;
  SPPoint3DMakeWithVector(v4, &v3);
  return v3.n128_f64[0];
}

void __swiftcall SPSize3D.applying(_:)(SPSize3D *__return_ptr retstr, SPPose3D *a2)
{
  SPSize3D.applying(_:)((long long *)a2, (void (*)(double *__return_ptr, void *, _OWORD *))SPSize3DApplyPose, v2, v3, v4);
}

float64_t SPSize3DApplyPose@<D0>(SPSize3D *a1@<X0>, SPPose3D *a2@<X1>, float64x2_t *a3@<X8>, float64x2_t _Q7@<Q7>)
{
  float64x2_t v6 = *(float64x2_t *)&a1->vector.f64[2];
  _Q3  = *(float64x2_t *)a2->rotation.vector.f64;
  _Q2  = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  _D5  = a2->rotation.vector.f64[1];
  __asm { FMLS            D4, D2, V2.D[0] }
  _Q7.f64[0]  = a2->rotation.vector.f64[3];
  __asm { FMLA            D4, D7, V2.D[1] }
  double v16 = vmlad_n_f64(vmuld_lane_f64(_Q2.f64[0], _Q2, 1), _D5, _Q3.f64[0]);
  v17.f64[0]  = vmuld_lane_f64(_D5, _Q2, 1);
  v4.f64[0]  = vmlad_n_f64(-(_D5 * _Q7.f64[0]), _Q2.f64[0], _Q3.f64[0]);
  v4.f64[0]  = v4.f64[0] + v4.f64[0];
  _Q4.f64[1]  = v16 + v16;
  double v18 = vmlad_n_f64(-(_Q2.f64[0] * _Q7.f64[0]), _D5, _Q3.f64[0]);
  v19.f64[0]  = v18 + v18;
  __asm
  {
    FMLA            D6, D5, V3.D[1]
    FMLA            D6, D7, V2.D[1]
    FMLS            D6, D3, V3.D[0]
    FMLA            D19, D2, V3.D[1]
  }
  _Q19.f64[0]  = _Q19.f64[0] + _Q19.f64[0];
  v19.f64[1]  = _D6;
  float64_t v23 = -(_Q3.f64[0] * _Q7.f64[0]);
  float64x2_t v24 = (float64x2_t)vzip1q_s64((int64x2_t)_Q3, (int64x2_t)_Q2);
  __asm
  {
    FMLS            D7, D3, V3.D[0]
    FMLS            D7, D5, V3.D[1]
  }
  _Q3.f64[0]  = a2->rotation.vector.f64[2];
  v17.f64[1]  = v23;
  float64x2_t v25 = vmlaq_f64(v17, v24, _Q3);
  float64x2_t v26 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q4, a1->width), v19, *(float64x2_t *)&a1->width, 1), vaddq_f64(v25, v25), v6.f64[0]), (float64x2_t)0);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)&a1->width, v4), _Q19, *(float64x2_t *)&a1->width, 1), v6, _Q7), (float64x2_t)0);
  *a3  = v26;
  a3[1].f64[0]  = result;
  return result;
}

void __swiftcall SPSize3D.init(width:height:depth:)(SPSize3D *__return_ptr retstr, Swift::Double width, Swift::Double height, Swift::Double depth)
{
  SPPoint3DMake(width, height, depth, (double *)&v4);
}

BOOL static SPSize3D.== infix(_:_:)(double a1, double a2, double a3, double a4, double a5, double a6)
{
  BOOL v6 = a1 == a4;
  if (a2 != a5) {
    BOOL v6 = 0;
  }
  return a3 == a6 && v6;
}

uint64_t SPSize3D.description.getter()
{
  _StringGuts.grow(_:)(34);
  v0._countAndFlagsBits  = 0x203A687464697728;
  v0._object  = (void *)0xE800000000000000;
  String.append(_:)(v0);
  Double.write<A>(to:)();
  v1._countAndFlagsBits  = 0x746867696568202CLL;
  v1._object  = (void *)0xEA0000000000203ALL;
  String.append(_:)(v1);
  Double.write<A>(to:)();
  v2._countAndFlagsBits  = 0x3A6874706564202CLL;
  v2._object  = (void *)0xE900000000000020;
  String.append(_:)(v2);
  Double.write<A>(to:)();
  v3._countAndFlagsBits  = 41;
  v3._object  = (void *)0xE100000000000000;
  String.append(_:)(v3);
  return 0;
}

void one-time initialization function for one()
{
}

double static SPSize3D.one.getter()
{
  if (one-time initialization token for one != -1) {
    swift_once();
  }
  return *(double *)&static SPSize3D.one;
}

void __swiftcall SPSize3D.init(_:)(SPSize3D *__return_ptr retstr, SPPoint3D *a2)
{
  SPSize3D.init(_:)((void (*)(double *__return_ptr, void *))SPSize3DMakeWithPoint, v2, v3, v4);
}

double SPSize3DMakeWithPoint@<D0>(SPPoint3D *a1@<X0>, uint64_t a2@<X8>)
{
  double result = a1->z;
  *(_OWORD *)a2  = *(_OWORD *)&a1->x;
  *(double *)(a2 + 16)  = result;
  return result;
}

void __swiftcall SPSize3D.init(_:)(SPSize3D *__return_ptr retstr, SPVector3D *a2)
{
  SPSize3D.init(_:)((void (*)(double *__return_ptr, void *))SPSize3DMakeWithPoint, v2, v3, v4);
}

double SPSize3D.init(_:)(void (*a1)(double *__return_ptr, void *), double a2, double a3, double a4)
{
  *(double *)BOOL v6 = a2;
  *(double *)&v6[1]  = a3;
  *(double *)&v6[2]  = a4;
  a1(&v5, v6);
  return v5;
}

void __swiftcall SPSize3D.unapplying(_:)(SPSize3D *__return_ptr retstr, SPAffineTransform3D *a2)
{
  SPSize3D.unapplying(_:)((long long *)a2, (void (*)(SPSize3D *__return_ptr, void *, SPSize3D *))SPSize3DUnapplyAffineTransform, v2, v3, v4);
}

void __swiftcall SPSize3D.unapplying(_:)(SPSize3D *__return_ptr retstr, SPProjectiveTransform3D *a2)
{
  SPSize3D.unapplying(_:)((long long *)a2, (void (*)(SPSize3D *__return_ptr, void *, SPSize3D *))SPSize3DUnapplyProjectiveTransform, v2, v3, v4);
}

double SPSize3D.unapplying(_:)(long long *a1, void (*a2)(SPSize3D *__return_ptr, void *, SPSize3D *), double a3, double a4, double a5)
{
  *(double *)&v25[1]  = a4;
  *(double *)&v25[2]  = a5;
  *(double *)float64x2_t v25 = a3;
  long long v6 = *a1;
  long long v7 = a1[1];
  long long v8 = a1[2];
  long long v9 = a1[3];
  long long v10 = a1[4];
  long long v11 = a1[5];
  long long v12 = a1[6];
  long long v13 = a1[7];
  v25[3]  = 0;
  *(_OWORD *)&v18.width  = v6;
  *(_OWORD *)&v18.vector.f64[2]  = v7;
  long long v19 = v8;
  long long v20 = v9;
  long long v21 = v10;
  long long v22 = v11;
  long long v23 = v12;
  long long v24 = v13;
  a2(&v17, v25, &v18);
  width  = v17.width;
  *(SPSize3D *)&v18.width  = *(SPSize3D *)&v17.width;
  if (SPSize3DIsValid(&v18)) {
    return width;
  }
  else {
    return a3;
  }
}

void __swiftcall SPSize3D.unapplying(_:)(SPSize3D *__return_ptr retstr, SPPose3D *a2)
{
  SPSize3D.applying(_:)((long long *)a2, (void (*)(double *__return_ptr, void *, _OWORD *))SPSize3DUnapplyPose, v2, v3, v4);
}

double SPSize3D.applying(_:)(long long *a1, void (*a2)(double *__return_ptr, void *, _OWORD *), double a3, double a4, double a5)
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  uint64_t v8 = *((void *)a1 + 6);
  uint64_t v9 = *((void *)a1 + 7);
  *(double *)long long v15 = a3;
  *(double *)&v15[1]  = a4;
  *(double *)&v15[2]  = a5;
  v12[0]  = v5;
  v12[1]  = v6;
  uint64_t v13 = v8;
  uint64_t v14 = v9;
  v12[2]  = v7;
  a2(&v11, v15, v12);
  return v11;
}

{
  long long v5;
  long long v6;
  long long v7;
  uint64_t v8;
  uint64_t v9;
  uint64_t v10;
  double v12;
  _OWORD v13[3];
  uint64_t v14;
  uint64_t v15;
  uint64_t v16;
  void v17[4];

  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  uint64_t v8 = *((void *)a1 + 6);
  uint64_t v9 = *((void *)a1 + 7);
  long long v10 = *((void *)a1 + 8);
  *(double *)SPSize3D v17 = a3;
  *(double *)&v17[1]  = a4;
  *(double *)&v17[2]  = a5;
  v13[0]  = v5;
  v13[1]  = v6;
  uint64_t v14 = v8;
  long long v15 = v9;
  v13[2]  = v7;
  double v16 = v10;
  a2(&v12, v17, v13);
  return v12;
}

float64_t SPSize3DUnapplyPose@<D0>(SPSize3D *a1@<X0>, SPPose3D *a2@<X1>, float64x2_t *a3@<X8>, float64x2_t _Q7@<Q7>)
{
  float64x2_t v6 = *(float64x2_t *)a2->rotation.vector.f64;
  float64x2_t v7 = *(float64x2_t *)&a2->rotation.quaternion.vector.f64[2];
  float64x2_t v8 = vnegq_f64(v6);
  v6.f64[0]  = 1.0 / vaddvq_f64(vaddq_f64(vmulq_f64(v6, v6), vmulq_f64(v7, v7)));
  _Q2  = vmulq_n_f64(vmulq_f64(v7, (float64x2_t)xmmword_228C1FC40), v6.f64[0]);
  _Q3  = vmulq_n_f64(v8, v6.f64[0]);
  float64x2_t v11 = *(float64x2_t *)&a1->vector.f64[2];
  _D5  = _Q3.f64[1];
  __asm { FMLS            D4, D2, V2.D[0] }
  _Q7.f64[0]  = _Q2.f64[1];
  __asm { FMLA            D4, D7, V2.D[1] }
  double v19 = vmlad_n_f64(vmuld_lane_f64(_Q2.f64[0], _Q2, 1), _Q3.f64[1], _Q3.f64[0]);
  v20.f64[0]  = vmuld_lane_f64(_Q3.f64[1], _Q2, 1);
  v4.f64[0]  = vmlad_n_f64(-(_Q3.f64[1] * _Q2.f64[1]), _Q2.f64[0], _Q3.f64[0]);
  v4.f64[0]  = v4.f64[0] + v4.f64[0];
  _Q4.f64[1]  = v19 + v19;
  double v21 = vmlad_n_f64(-(_Q2.f64[0] * _Q2.f64[1]), _Q3.f64[1], _Q3.f64[0]);
  v22.f64[0]  = v21 + v21;
  __asm
  {
    FMLA            D6, D5, V3.D[1]
    FMLA            D6, D7, V2.D[1]
    FMLS            D6, D3, V3.D[0]
    FMLA            D19, D2, V3.D[1]
  }
  _Q19.f64[0]  = _Q19.f64[0] + _Q19.f64[0];
  v22.f64[1]  = _D6;
  float64_t v26 = -(_Q3.f64[0] * _Q2.f64[1]);
  float64x2_t v27 = (float64x2_t)vzip1q_s64((int64x2_t)_Q3, (int64x2_t)_Q2);
  __asm
  {
    FMLS            D7, D3, V3.D[0]
    FMLS            D7, D5, V3.D[1]
  }
  _Q3.f64[0]  = _Q2.f64[0];
  v20.f64[1]  = v26;
  float64x2_t v28 = vmlaq_f64(v20, v27, _Q3);
  float64x2_t v29 = vaddq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q4, a1->width), v22, *(float64x2_t *)&a1->width, 1), vaddq_f64(v28, v28), v11.f64[0]), (float64x2_t)0);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(*(float64x2_t *)&a1->width, v4), _Q19, *(float64x2_t *)&a1->width, 1), v11, _Q7), (float64x2_t)0);
  *a3  = v29;
  a3[1].f64[0]  = result;
  return result;
}

uint64_t protocol witness for Primitive3D.applying(_:) in conformance SPSize3D(long long *a1, uint64_t a2, uint64_t a3)
{
  return protocol witness for Primitive3D.applying(_:) in conformance SPSize3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPSize3DApplyAffineTransform);
}

{
  return protocol witness for Primitive3D.applying(_:) in conformance SPSize3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPSize3DApplyProjectiveTransform);
}

{
  return protocol witness for Primitive3D.applying(_:) in conformance SPSize3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPSize3DApplyPose);
}

uint64_t protocol witness for Primitive3D.applying(_:) in conformance SPSize3D(long long *a1, uint64_t a2, uint64_t a3, uint64_t (*a4)(long long *, _OWORD *))
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  long long v13 = *v4;
  uint64_t v14 = *((void *)v4 + 3);
  uint64_t v18 = *((void *)v4 + 2);
  uint64_t v19 = v14;
  long long v17 = v13;
  v16[0]  = v5;
  v16[1]  = v6;
  v16[2]  = v7;
  _OWORD v16[3] = v8;
  v16[4]  = v9;
  v16[5]  = v10;
  v16[6]  = v11;
  v16[7]  = v12;
  return a4(&v17, v16);
}

{
  long long *v4;
  long long v5;
  long long v6;
  long long v7;
  uint64_t v8;
  uint64_t v9;
  long long v10;
  uint64_t v11;
  _OWORD v13[3];
  uint64_t v14;
  uint64_t v15;
  long long v16;
  uint64_t v17;
  uint64_t v18;

  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = *((void *)a1 + 6);
  long long v9 = *((void *)a1 + 7);
  long long v10 = *v4;
  long long v11 = *((void *)v4 + 3);
  long long v17 = *((void *)v4 + 2);
  uint64_t v18 = v11;
  double v16 = v10;
  v13[0]  = v5;
  v13[1]  = v6;
  uint64_t v14 = v8;
  long long v15 = v9;
  v13[2]  = v7;
  return a4(&v16, v13);
}

BOOL protocol witness for Primitive3D.unapplying(_:) in conformance SPSize3D@<W0>(long long *a1@<X0>, double *a2@<X8>)
{
  return protocol witness for Primitive3D.unapplying(_:) in conformance SPSize3D(a1, (void (*)(SPSize3D *__return_ptr, void *, SPSize3D *))SPSize3DUnapplyAffineTransform, a2);
}

{
  return protocol witness for Primitive3D.unapplying(_:) in conformance SPSize3D(a1, (void (*)(SPSize3D *__return_ptr, void *, SPSize3D *))SPSize3DUnapplyProjectiveTransform, a2);
}

BOOL protocol witness for Primitive3D.unapplying(_:) in conformance SPSize3D@<W0>(long long *a1@<X0>, void (*a2)(SPSize3D *__return_ptr, void *, SPSize3D *)@<X3>, double *a3@<X8>)
{
  long long v5 = *a1;
  long long v6 = a1[1];
  long long v7 = a1[2];
  long long v8 = a1[3];
  long long v9 = a1[4];
  long long v10 = a1[5];
  long long v11 = a1[6];
  long long v12 = a1[7];
  double v13 = *v3;
  uint64_t v14 = *((void *)v3 + 1);
  uint64_t v16 = *((void *)v3 + 2);
  double v15 = v3[3];
  v33[2]  = v16;
  *(double *)&v33[3]  = v15;
  *(double *)simd_quatd v33 = v13;
  v33[1]  = v14;
  *(_OWORD *)&v26.width  = v5;
  *(_OWORD *)&v26.vector.f64[2]  = v6;
  long long v27 = v7;
  long long v28 = v8;
  long long v29 = v9;
  long long v30 = v10;
  long long v31 = v11;
  long long v32 = v12;
  a2(&v25, v33, &v26);
  width  = v25.width;
  long long v19 = *(_OWORD *)&v25.vector.f64[1];
  double v18 = v25.vector.f64[3];
  *(SPSize3D *)&v26.width  = *(SPSize3D *)&v25.width;
  BOOL result = SPSize3DIsValid(&v26);
  if (result) {
    double v21 = width;
  }
  else {
    double v21 = v13;
  }
  if (result) {
    uint64_t v22 = *((void *)&v19 + 1);
  }
  else {
    uint64_t v22 = v16;
  }
  if (result) {
    double v23 = v18;
  }
  else {
    double v23 = v15;
  }
  *((void *)a3 + 2)  = v22;
  a3[3]  = v23;
  if (result) {
    uint64_t v24 = v19;
  }
  else {
    uint64_t v24 = v14;
  }
  *a3  = v21;
  *((void *)a3 + 1)  = v24;
  return result;
}

uint64_t protocol witness for Primitive3D.unapplying(_:) in conformance SPSize3D(long long *a1, uint64_t a2, uint64_t a3)
{
  return protocol witness for Primitive3D.applying(_:) in conformance SPSize3D(a1, a2, a3, (uint64_t (*)(long long *, _OWORD *))SPSize3DUnapplyPose);
}

double SPSize3D.applying(_:)(long long *a1, double a2, double a3, double a4)
{
  return SPSize3D.applying(_:)(a1, (void (*)(double *__return_ptr, void *, _OWORD *))SPSize3DApplyScaledPose, a2, a3, a4);
}

float64_t SPSize3DApplyScaledPose@<D0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>, float64x2_t a4@<Q2>)
{
  a4.f64[0]  = a2[4].f64[0];
  *(void *)&double v4 = *(_OWORD *)&vmulq_f64(a1[1], a4);
  float64x2_t v5 = vmulq_n_f64(*a1, a4.f64[0]);
  float64x2_t v6 = a2[2];
  float64x2_t v7 = a2[3];
  float64x2_t v8 = vmulq_f64(v7, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v9 = (int8x16_t)vnegq_f64(v6);
  float64x2_t v10 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)vnegq_f64(v8), 8uLL);
  float64x2_t v11 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v8, v5, 1), v10, v5.f64[0]), (float64x2_t)vextq_s8((int8x16_t)v6, v9, 8uLL), v4);
  float64x2_t v12 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v6, v5, 1), (float64x2_t)vextq_s8(v9, (int8x16_t)v6, 8uLL), v5.f64[0]), v10, v4);
  float64x2_t v13 = vnegq_f64(v11);
  float64x2_t v14 = (float64x2_t)vextq_s8((int8x16_t)v12, (int8x16_t)vnegq_f64(v12), 8uLL);
  float64x2_t v15 = vmlaq_n_f64(vmulq_laneq_f64(v12, v6, 1), v14, v6.f64[0]);
  *(void *)&float64_t result = *(_OWORD *)&vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v12, v7, 1), v14, v7.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v13, v6, 1), (float64x2_t)vextq_s8((int8x16_t)v11, (int8x16_t)v13, 8uLL), v6.f64[0]));
  *a3  = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v11, v7, 1), (float64x2_t)vextq_s8((int8x16_t)v13, (int8x16_t)v11, 8uLL), v7.f64[0]), v15);
  a3[1].f64[0]  = result;
  return result;
}

double SPSize3D.unapplying(_:)(long long *a1, double a2, double a3, double a4)
{
  return SPSize3D.applying(_:)(a1, (void (*)(double *__return_ptr, void *, _OWORD *))SPSize3DUnapplyScaledPose, a2, a3, a4);
}

float64x2_t SPSize3DUnapplyScaledPose@<Q0>(uint64_t a1@<X0>, float64x2_t *a2@<X1>, uint64_t a3@<X8>)
{
  float64x2_t v3 = a2[2];
  float64x2_t v4 = a2[3];
  float64x2_t v5 = vnegq_f64(v3);
  v3.f64[0]  = 1.0 / vaddvq_f64(vaddq_f64(vmulq_f64(v3, v3), vmulq_f64(v4, v4)));
  float64x2_t v6 = vmulq_n_f64(vmulq_f64(v4, (float64x2_t)xmmword_228C1FC40), v3.f64[0]);
  float64x2_t v7 = vmulq_n_f64(v5, v3.f64[0]);
  float64x2_t v8 = vmulq_f64(v6, (float64x2_t)xmmword_228C1FC40);
  int8x16_t v9 = (int8x16_t)vnegq_f64(v7);
  float64x2_t v10 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)vnegq_f64(v8), 8uLL);
  long long v11 = *(_OWORD *)(a1 + 16);
  float64x2_t v12 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v7, *(float64x2_t *)a1, 1), (float64x2_t)vextq_s8(v9, (int8x16_t)v7, 8uLL), *(double *)a1), v10, *(double *)&v11);
  float64x2_t v13 = vmlaq_n_f64(vmlaq_n_f64(vmulq_laneq_f64(v8, *(float64x2_t *)a1, 1), v10, *(double *)a1), (float64x2_t)vextq_s8((int8x16_t)v7, v9, 8uLL), *(double *)&v11);
  float64x2_t v14 = vnegq_f64(v13);
  float64x2_t v15 = (float64x2_t)vextq_s8((int8x16_t)v12, (int8x16_t)vnegq_f64(v12), 8uLL);
  float64x2_t v16 = vmlaq_n_f64(vmulq_laneq_f64(v14, v7, 1), (float64x2_t)vextq_s8((int8x16_t)v13, (int8x16_t)v14, 8uLL), v7.f64[0]);
  float64x2_t v17 = vmlaq_n_f64(vmulq_laneq_f64(v13, v6, 1), (float64x2_t)vextq_s8((int8x16_t)v14, (int8x16_t)v13, 8uLL), v6.f64[0]);
  float64x2_t v18 = vaddq_f64(v17, vmlaq_n_f64(vmulq_laneq_f64(v12, v7, 1), v15, v7.f64[0]));
  v17.f64[0]  = a2[4].f64[0];
  float64x2_t result = vdivq_f64(v18, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v17.f64[0], 0));
  *(float64x2_t *)a3  = result;
  *(void *)(a3 + 16)  = *(_OWORD *)&vdivq_f64(vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v12, v6, 1), v15, v6.f64[0]), v16), v17);
  return result;
}

void __swiftcall SPSize3D.scaledBy(x:y:z:)(SPSize3D *__return_ptr retstr, Swift::Double x, Swift::Double y, Swift::Double z)
{
  v8.x  = v4;
  v8.y  = v5;
  v8.double z = v6;
  SPVector3DScaleBy(&v8, *(float64x2_t *)&x, y, *(float64x2_t *)&z, (uint64_t)&v7);
}

double protocol witness for Scalable3D.scaled(by:) in conformance SPSize3D@<D0>(uint64_t a1@<X8>, float64_t a2@<D0>, float64_t a3@<D1>, double a4@<D2>)
{
  float64x2_t v5 = *(float64x2_t *)v4;
  uint64_t v6 = *(void *)(v4 + 24);
  uint64_t v11 = *(void *)(v4 + 16);
  uint64_t v12 = v6;
  float64x2_t v10 = v5;
  v8.f64[0]  = a2;
  v8.f64[1]  = a3;
  double v9 = a4;
  *(void *)&double result = *(_OWORD *)&SPSize3DScaleBySize(&v10, &v8, a1);
  return result;
}

double SPSize3D.sheared(_:)(uint64_t a1, float64x2_t a2, float64x2_t a3, double a4)
{
  if (*(unsigned char *)(a1 + 16))
  {
    unint64_t v4 = *(void *)a1;
    unint64_t v5 = *(void *)(a1 + 8);
    if (*(unsigned char *)(a1 + 16) == 1)
    {
      v9.width  = a2.f64[0];
      v9.height  = a3.f64[0];
      v9.depth  = a4;
      SPAxis v6 = SPAxisY;
    }
    else
    {
      v9.width  = a2.f64[0];
      v9.height  = a3.f64[0];
      v9.depth  = a4;
      SPAxis v6 = SPAxisZ;
    }
  }
  else
  {
    unint64_t v4 = *(void *)a1;
    unint64_t v5 = *(void *)(a1 + 8);
    v9.width  = a2.f64[0];
    v9.height  = a3.f64[0];
    v9.depth  = a4;
    SPAxis v6 = SPAxisX;
  }
  *(void *)&a2.f64[0]  = v4;
  *(void *)&a3.f64[0]  = v5;
  SPSize3DShear(&v9, v6, a2, a3, &v8);
  return *(double *)&v8;
}

float64x2_t SPSize3DShear@<Q0>(SPSize3D *a1@<X0>, SPAxis a2@<W1>, float64x2_t a3@<Q0>, float64x2_t a4@<Q1>, _OWORD *a5@<X8>)
{
  uint64_t v67 = *MEMORY[0x263EF8340];
  float64x2_t v5 = *(float64x2_t *)&a1->width;
  depth  = a1->depth;
  float64x2_t v7 = 0uLL;
  *a5  = 0u;
  a5[1]  = 0u;
  switch(a2)
  {
    case SPAxisZ:
      a3.f64[1]  = a4.f64[0];
      float64x2_t v13 = (float64x2_t)xmmword_228C1F7D0;
      float64x2_t v15 = 0uLL;
      __asm { FMOV            V6.2D, #1.0 }
      a4  = 0uLL;
      float64x2_t v14 = (float64x2_t)xmmword_228C1F7A0;
      break;
    case SPAxisY:
      __asm { FMOV            V6.2D, #1.0 }
      v14.f64[1]  = _Q6.f64[1];
      v14.f64[0]  = a3.f64[0];
      float64x2_t v13 = (float64x2_t)xmmword_228C1F7D0;
      a3  = 0uLL;
      float64x2_t v15 = a4;
      a4  = 0uLL;
      break;
    case SPAxisX:
      __asm { FMOV            V6.2D, #1.0 }
      v13.f64[0]  = _Q6.f64[0];
      v13.f64[1]  = a3.f64[0];
      float64x2_t v14 = (float64x2_t)xmmword_228C1F7A0;
      a3  = 0uLL;
      float64x2_t v15 = 0uLL;
      break;
    default:
      float64x2_t v31 = (float64x2_t)xmmword_228C1F7D0;
      float64x2_t v22 = (float64x2_t)xmmword_228C1F7A0;
      float64x2_t v32 = 0uLL;
      float64x2_t v35 = 0uLL;
      float64x2_t v21 = (float64x2_t)xmmword_228C1F7D0;
LABEL_14:
      float64x2_t v33 = 0uLL;
      float64x2_t v34 = 0uLL;
      goto LABEL_15;
  }
  float64x2_t v16 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v18 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v17 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v19 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v20 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  float64x2_t v21 = (float64x2_t)xmmword_228C1F7D0;
  float64x2_t v22 = (float64x2_t)xmmword_228C1F7A0;
  __asm { FMOV            V17.2D, #1.0 }
  int64x2_t v24 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v18, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], (float64x2_t)xmmword_228C1F7D0)), (int8x16_t)vceqzq_f64(v19));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v24, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqzq_f64(v17), (int8x16_t)vceqzq_f64(v16)), (int8x16_t)vceqq_f64(v20, _Q17)), (int8x16_t)v24)).u64[0] & 0x8000000000000000) != 0)
  {
    int8x16_t v25 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v17, v15), (int8x16_t)vceqq_f64(v16, a4)), (int8x16_t)vceqq_f64(v20, _Q6));
    int64x2_t v26 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v18, v14), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v13)), (int8x16_t)vceqq_f64(v19, a3));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v26, 1), vandq_s8(v25, (int8x16_t)v26)).u64[0] & 0x8000000000000000) != 0)
    {
      float64x2_t v7 = 0uLL;
      float64x2_t v31 = (float64x2_t)xmmword_228C1F7D0;
      float64x2_t v32 = 0uLL;
      float64x2_t v35 = 0uLL;
      goto LABEL_14;
    }
  }
  uint64_t v27 = 0;
  v50[0]  = v13;
  v50[1]  = a4;
  v50[2]  = v14;
  v50[3]  = v15;
  v50[4]  = a3;
  v50[5]  = _Q6;
  float64x2_t v51 = 0u;
  float64x2_t v52 = 0u;
  float64x2_t v53 = 0u;
  float64x2_t v54 = 0u;
  float64x2_t v55 = 0u;
  float64x2_t v56 = 0u;
  do
  {
    float64x2_t v29 = (float64x2_t)v50[v27];
    float64x2_t v28 = (float64x2_t)v50[v27 + 1];
    long long v30 = (float64x2_t *)((char *)&v51 + v27 * 16);
    *long long v30 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)xmmword_228C1F7D0, v29.f64[0]), (float64x2_t)xmmword_228C1F7A0, v29, 1), (float64x2_t)0, v28.f64[0]);
    v30[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v29, (float64x2_t)0), (float64x2_t)0, v29, 1), _Q17, v28);
    v27 += 2;
  }
  while (v27 != 6);
  float64x2_t v31 = v51;
  float64x2_t v7 = v52;
  float64x2_t v22 = v53;
  float64x2_t v32 = v54;
  float64x2_t v33 = 0uLL;
  float64x2_t v34 = 0uLL;
  float64x2_t v35 = v55;
  float64x2_t v21 = v56;
LABEL_15:
  uint64_t v36 = 0;
  v65  = 0u;
  *(_OWORD *)float64x2_t v66 = 0u;
  float64x2_t v63 = 0u;
  v64  = 0u;
  long long v61 = 0u;
  long long v62 = 0u;
  long long v59 = 0u;
  long long v60 = 0u;
  long long v57 = 0u;
  long long v58 = 0u;
  float64x2_t v55 = 0u;
  float64x2_t v56 = 0u;
  float64x2_t v53 = 0u;
  float64x2_t v54 = 0u;
  float64x2_t v51 = 0u;
  float64x2_t v52 = 0u;
  do
  {
    long long v37 = (_OWORD *)&v51.f64[v36];
    *long long v37 = 0uLL;
    v37[1]  = 0uLL;
    v36 += 4;
  }
  while (v36 != 32);
  uint64_t v38 = 0;
  v53.f64[1]  = v5.f64[1] + v53.f64[1];
  float64x2_t v55 = vaddq_f64(v5, v55);
  *(double *)&long long v57 = v5.f64[0] + *(double *)&v57;
  *(double *)&long long v60 = depth + *(double *)&v60;
  *((double *)&v61 + 1)  = v5.f64[1] + *((double *)&v61 + 1);
  *(double *)&long long v62 = depth + *(double *)&v62;
  float64x2_t v63 = vaddq_f64(v5, v63);
  *(double *)&v64  = depth + *(double *)&v64;
  *(double *)&v65  = v5.f64[0] + *(double *)&v65;
  v66[0]  = depth + v66[0];
  *(void *)&v31.f64[1]  = vextq_s8((int8x16_t)v31, (int8x16_t)v31, 8uLL).u64[0];
  *(void *)&v22.f64[1]  = vextq_s8((int8x16_t)v22, (int8x16_t)v22, 8uLL).u64[0];
  *(void *)&v35.f64[1]  = vextq_s8((int8x16_t)v35, (int8x16_t)v35, 8uLL).u64[0];
  *(void *)&v33.f64[1]  = vextq_s8((int8x16_t)v33, (int8x16_t)v33, 8uLL).u64[0];
  do
  {
    long long v39 = (char *)&v51 + v38;
    float64x2_t v41 = *(float64x2_t *)((char *)&v51 + v38);
    float64x2_t v40 = *(float64x2_t *)((char *)&v51 + v38 + 16);
    *(float64x2_t *)long long v39 = vaddq_f64(v33, vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v31, v41.f64[0]), v22, v41, 1), v35, v40.f64[0]));
    *((void *)v39 + 2)  = *(_OWORD *)&vaddq_f64(v34, vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v7, v41), v32, v41, 1), v40, v21));
    v38 += 32;
  }
  while (v38 != 256);
  uint64_t v42 = 0;
  float64x2_t v43 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v44.f64[0]  = INFINITY;
  float64x2_t v45 = (float64x2_t)vdupq_n_s64(0xFFF0000000000000);
  v46.f64[0]  = -INFINITY;
  do
  {
    float64x2_t v47 = *(float64x2_t *)((char *)&v51 + v42);
    long long v48 = *(long long *)((char *)&v51 + v42 + 16);
    float64x2_t v43 = vminnmq_f64(v43, v47);
    float64x2_t v44 = vminnmq_f64((float64x2_t)*(unint64_t *)&v44.f64[0], (float64x2_t)(unint64_t)v48);
    float64x2_t v45 = vmaxnmq_f64(v45, v47);
    float64x2_t v46 = vmaxnmq_f64((float64x2_t)*(unint64_t *)&v46.f64[0], (float64x2_t)(unint64_t)v48);
    v42 += 32;
  }
  while (v42 != 256);
  float64x2_t result = vsubq_f64(v45, v43);
  *a5  = result;
  a5[1]  = (unint64_t)*(_OWORD *)&vsubq_f64(v46, v44);
  return result;
}

void protocol witness for Shearable3D.sheared(_:) in conformance SPSize3D(uint64_t a1@<X0>, void *a2@<X8>, float64x2_t a3@<Q0>, float64x2_t a4@<Q1>)
{
  double v7 = v4[2];
  double v6 = v4[3];
  double v8 = v4[1];
  if (*(unsigned char *)(a1 + 16))
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    if (*(unsigned char *)(a1 + 16) == 1)
    {
      v16.width  = *v4;
      v16.height  = v8;
      v16.depth  = v7;
      v16.vector.f64[3]  = v6;
      SPAxis v9 = SPAxisY;
    }
    else
    {
      v16.width  = *v4;
      v16.height  = v8;
      v16.depth  = v7;
      v16.vector.f64[3]  = v6;
      SPAxis v9 = SPAxisZ;
    }
  }
  else
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    v16.width  = *v4;
    v16.height  = v8;
    v16.depth  = v7;
    v16.vector.f64[3]  = v6;
    SPAxis v9 = SPAxisX;
  }
  SPSize3DShear(&v16, v9, a3, a4, &v13);
  uint64_t v11 = v14;
  uint64_t v10 = v15;
  uint64_t v12 = *((void *)&v13 + 1);
  *a2  = v13;
  a2[1]  = v12;
  a2[2]  = v11;
  a2[3]  = v10;
}

Swift::Bool __swiftcall SPSize3D.contains(anyOf:)(Swift::OpaquePointer anyOf)
{
  unint64_t v4 = *((void *)anyOf._rawValue + 2);
  if (v4 >> 31)
  {
    __break(1u);
  }
  else
  {
    v6.width  = v1;
    v6.height  = v2;
    v6.depth  = v3;
    LOBYTE(anyOf._rawValue)  = SPSize3DContainsAnyPoint(&v6, (const SPPoint3D *)anyOf._rawValue + 1, v4);
  }
  return (Swift::Bool)anyOf._rawValue;
}

void __swiftcall SPSize3D.intersection(_:)(SPSize3D_optional *__return_ptr retstr, SPSize3D *a2)
{
  v19.width  = v5;
  v19.height  = v6;
  v19.depth  = v7;
  v18.width  = v2;
  v18.height  = v3;
  v18.depth  = v4;
  SPSize3DIntersection((float64x2_t *)&v19, &v18, &v14);
  double v10 = v16;
  double v9 = v17;
  double v12 = v14;
  double v11 = v15;
  Swift::Bool v13 = v14 == 0.0;
  if (v15 != 0.0) {
    Swift::Bool v13 = 0;
  }
  if (v16 != 0.0) {
    Swift::Bool v13 = 0;
  }
  if (v13)
  {
    double v12 = 0.0;
    double v11 = 0.0;
    double v10 = 0.0;
    double v9 = 0.0;
  }
  retstr->value.width  = v12;
  retstr->value.height  = v11;
  retstr->value.depth  = v10;
  retstr->value.vector.f64[3]  = v9;
  retstr->is_nil  = v13;
}

double protocol witness for Volumetric.size.getter in conformance SPSize3D()
{
  return *(double *)v0;
}

unint64_t protocol witness for Volumetric.contains(_:) in conformance SPSize3D(double *a1)
{
  long long v2 = *(_OWORD *)a1;
  double v3 = a1[2];
  double v4 = a1[3];
  long long v5 = *(_OWORD *)v1;
  double v6 = v1[3];
  v9.depth  = v1[2];
  v9.vector.f64[3]  = v6;
  *(_OWORD *)&v9.width  = v5;
  v8.depth  = v3;
  v8.vector.f64[3]  = v4;
  *(_OWORD *)&v8.width  = v2;
  return SPSize3DContainsSize(&v9, &v8);
}

unint64_t protocol witness for Volumetric.contains(point:) in conformance SPSize3D(double a1, double a2, double a3)
{
  long long v4 = *(_OWORD *)v3;
  double v5 = v3[3];
  v8.depth  = v3[2];
  v8.vector.f64[3]  = v5;
  *(_OWORD *)&v8.width  = v4;
  v7.x  = a1;
  v7.y  = a2;
  v7.double z = a3;
  return SPSize3DContainsPoint(&v8, &v7);
}

double protocol witness for Volumetric.union(_:) in conformance SPSize3D@<D0>(double *a1@<X0>, uint64_t a2@<X8>)
{
  long long v3 = *(_OWORD *)a1;
  double v4 = a1[2];
  double v5 = a1[3];
  long long v6 = *(_OWORD *)v2;
  double v7 = v2[3];
  v10.depth  = v2[2];
  v10.vector.f64[3]  = v7;
  *(_OWORD *)&v10.width  = v6;
  v9.depth  = v4;
  v9.vector.f64[3]  = v5;
  *(_OWORD *)&v9.width  = v3;
  *(void *)&double result = *(_OWORD *)&SPSize3DUnion(&v10, &v9, a2);
  return result;
}

float64x2_t *protocol witness for Volumetric.intersection(_:) in conformance SPSize3D@<X0>(double *a1@<X0>, uint64_t a2@<X8>)
{
  long long v4 = *(_OWORD *)a1;
  double v5 = a1[2];
  double v6 = a1[3];
  long long v7 = *(_OWORD *)v2;
  double v8 = v2[3];
  v20.depth  = v2[2];
  v20.vector.f64[3]  = v8;
  *(_OWORD *)&v20.width  = v7;
  v19.depth  = v5;
  v19.vector.f64[3]  = v6;
  *(_OWORD *)&v19.width  = v4;
  double result = SPSize3DIntersection((float64x2_t *)&v20, &v19, &v15);
  double v11 = v17;
  uint64_t v10 = v18;
  double v13 = v15;
  double v12 = v16;
  BOOL v14 = v15 == 0.0;
  if (v16 != 0.0) {
    BOOL v14 = 0;
  }
  if (v17 != 0.0) {
    BOOL v14 = 0;
  }
  if (v14)
  {
    double v13 = 0.0;
    double v12 = 0.0;
    double v11 = 0.0;
    uint64_t v10 = 0;
  }
  *(double *)a2  = v13;
  *(double *)(a2 + 8)  = v12;
  *(double *)(a2 + 16)  = v11;
  *(void *)(a2 + 24)  = v10;
  *(unsigned char *)(a2 + 32)  = v14;
  return result;
}

void SPSize3D.hash(into:)(__n128 a1, double a2, double a3)
{
  a1.n128_f64[1]  = a2;
  specialized SIMD.hash(into:)(a1, a3);
}

Swift::Int SPSize3D.hashValue.getter(double a1, double a2, double a3)
{
  Hasher.init(_seed:)();
  v3.n128_f64[0]  = a1;
  v3.n128_f64[1]  = a2;
  specialized SIMD.hash(into:)(v3, a3);
  return Hasher._finalize()();
}

uint64_t protocol witness for static Equatable.== infix(_:_:) in conformance SPSize3D.CodingKeys(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return specialized == infix<A>(_:_:)(*a1, *a2);
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPSize3D.CodingKeys()
{
  return Hasher._finalize()();
}

uint64_t protocol witness for Hashable.hash(into:) in conformance SPSize3D.CodingKeys()
{
  String.hash(into:)();

  return swift_bridgeObjectRelease();
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPSize3D.CodingKeys()
{
  return Hasher._finalize()();
}

unint64_t protocol witness for RawRepresentable.init(rawValue:) in conformance SPSize3D.CodingKeys@<X0>(Swift::String *a1@<X0>, unsigned char *a2@<X8>)
{
  unint64_t result = specialized SPSize3D.CodingKeys.init(rawValue:)(*a1);
  *a2  = result;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance SPSize3D.CodingKeys(uint64_t *a1@<X8>)
{
  int v2 = *v1;
  unint64_t v3 = 0xE500000000000000;
  unint64_t v4 = 0xE600000000000000;
  uint64_t v5 = 0x746867696568;
  if (v2 != 1)
  {
    uint64_t v5 = 0x6874706564;
    unint64_t v4 = 0xE500000000000000;
  }
  BOOL v6 = v2 == 0;
  if (*v1) {
    uint64_t v7 = v5;
  }
  else {
    uint64_t v7 = 0x6874646977;
  }
  if (!v6) {
    unint64_t v3 = v4;
  }
  *a1  = v7;
  a1[1]  = v3;
}

uint64_t protocol witness for CodingKey.stringValue.getter in conformance SPSize3D.CodingKeys()
{
  uint64_t v1 = 0x746867696568;
  if (*v0 != 1) {
    uint64_t v1 = 0x6874706564;
  }
  if (*v0) {
    return v1;
  }
  else {
    return 0x6874646977;
  }
}

unint64_t protocol witness for CodingKey.init(stringValue:) in conformance SPSize3D.CodingKeys@<X0>(Swift::String a1@<X1:X0>, unsigned char *a2@<X8>)
{
  unint64_t result = specialized SPSize3D.CodingKeys.init(rawValue:)(a1);
  *a2  = result;
  return result;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPSize3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys();

  return MEMORY[0x270FA00B0](a1, v2);
}

uint64_t protocol witness for CustomDebugStringConvertible.debugDescription.getter in conformance SPSize3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys();

  return MEMORY[0x270FA00B8](a1, v2);
}

uint64_t SPSize3D.encode(to:)(void *a1)
{
  uint64_t v3 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedEncodingContainer<SPSize3D.CodingKeys>);
  uint64_t v4 = *(void *)(v3 - 8);
  MEMORY[0x270FA5388](v3);
  BOOL v6 = &v8[-((v5 + 15) & 0xFFFFFFFFFFFFFFF0)];
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys();
  dispatch thunk of Encoder.container<A>(keyedBy:)();
  v8[15]  = 0;
  KeyedEncodingContainer.encode(_:forKey:)();
  if (!v1)
  {
    v8[14]  = 1;
    KeyedEncodingContainer.encode(_:forKey:)();
    v8[13]  = 2;
    KeyedEncodingContainer.encode(_:forKey:)();
  }
  return (*(uint64_t (**)(unsigned char *, uint64_t))(v4 + 8))(v6, v3);
}

double SPSize3D.init(from:)(void *a1)
{
  return specialized SPSize3D.init(from:)(a1);
}

void protocol witness for Decodable.init(from:) in conformance SPSize3D(void *a1@<X0>, uint64_t a2@<X8>)
{
  double v4 = specialized SPSize3D.init(from:)(a1);
  if (!v2)
  {
    *(void *)(a2 + 24)  = 0;
    *(double *)a2  = v4;
    *(void *)(a2 + 8)  = v5;
    *(void *)(a2 + 16)  = v6;
  }
}

uint64_t protocol witness for Encodable.encode(to:) in conformance SPSize3D(void *a1)
{
  return SPSize3D.encode(to:)(a1);
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPSize3D()
{
  return SPSize3D.description.getter();
}

uint64_t SPSize3D.customMirror.getter(double a1, double a2, double a3)
{
  uint64_t v6 = type metadata accessor for Mirror.AncestorRepresentation();
  uint64_t v7 = *(void *)(v6 - 8);
  MEMORY[0x270FA5388](v6);
  SPSize3D v9 = (char *)v19 - ((v8 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v10 = __swift_instantiateConcreteTypeFromMangledName((uint64_t *)&demangling cache variable for type metadata for Mirror.DisplayStyle?);
  MEMORY[0x270FA5388](v10 - 8);
  double v12 = (char *)v19 - ((v11 + 15) & 0xFFFFFFFFFFFFFFF0);
  *(double *)SPSize3D v19 = a1;
  *(double *)&v19[1]  = a2;
  *(double *)&v19[2]  = a3;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<(String, Any)>);
  uint64_t v13 = swift_allocObject();
  *(_OWORD *)(v13 + 16)  = xmmword_228C1FC50;
  *(void *)(v13 + 32)  = 0x6874646977;
  *(void *)(v13 + 40)  = 0xE500000000000000;
  uint64_t v14 = MEMORY[0x263F8D538];
  *(double *)(v13 + 48)  = a1;
  *(void *)(v13 + 72)  = v14;
  *(void *)(v13 + 80)  = 0x746867696568;
  *(void *)(v13 + 88)  = 0xE600000000000000;
  *(double *)(v13 + 96)  = a2;
  *(void *)(v13 + 120)  = v14;
  *(void *)(v13 + 128)  = 0x6874706564;
  *(void *)(v13 + 136)  = 0xE500000000000000;
  *(void *)(v13 + 168)  = v14;
  *(double *)(v13 + 144)  = a3;
  uint64_t v15 = *MEMORY[0x263F8E808];
  uint64_t v16 = type metadata accessor for Mirror.DisplayStyle();
  uint64_t v17 = *(void *)(v16 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v17 + 104))(v12, v15, v16);
  (*(void (**)(char *, void, uint64_t, uint64_t))(v17 + 56))(v12, 0, 1, v16);
  (*(void (**)(char *, void, uint64_t))(v7 + 104))(v9, *MEMORY[0x263F8E830], v6);
  type metadata accessor for SPSize3D(0);
  return Mirror.init<A>(_:children:displayStyle:ancestorRepresentation:)();
}

uint64_t protocol witness for CustomReflectable.customMirror.getter in conformance SPSize3D()
{
  return SPSize3D.customMirror.getter(*v0, v0[1], v0[2]);
}

unint64_t lazy protocol witness table accessor for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys()
{
  unint64_t result = lazy protocol witness table cache variable for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys);
  }
  return result;
}

unint64_t specialized SPSize3D.CodingKeys.init(rawValue:)(Swift::String string)
{
  object  = string._object;
  v2._countAndFlagsBits  = string._countAndFlagsBits;
  v2._object  = object;
  unint64_t v3 = _findStringSwitchCase(cases:string:)((Swift::OpaquePointer)&outlined read-only object #0 of SPSize3D.CodingKeys.init(rawValue:), v2);
  swift_bridgeObjectRelease();
  if (v3 >= 3) {
    return 3;
  }
  else {
    return v3;
  }
}

double specialized SPSize3D.init(from:)(void *a1)
{
  uint64_t v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedDecodingContainer<SPSize3D.CodingKeys>);
  uint64_t v5 = *(void *)(v4 - 8);
  MEMORY[0x270FA5388](v4);
  uint64_t v7 = (char *)&v14 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPSize3D.CodingKeys and conformance SPSize3D.CodingKeys();
  dispatch thunk of Decoder.container<A>(keyedBy:)();
  if (!v1)
  {
    LOBYTE(v14)  = 0;
    KeyedDecodingContainer.decode(_:forKey:)();
    double v9 = v8;
    LOBYTE(v14)  = 1;
    KeyedDecodingContainer.decode(_:forKey:)();
    double v11 = v10;
    LOBYTE(v14)  = 2;
    KeyedDecodingContainer.decode(_:forKey:)();
    SPPoint3DMake(v9, v11, v13, &v14);
    double v2 = v14;
    (*(void (**)(char *, uint64_t))(v5 + 8))(v7, v4);
  }
  __swift_destroy_boxed_opaque_existential_1(a1);
  return v2;
}

uint64_t instantiation function for generic protocol witness table for SPSize3D(void *a1)
{
  a1[1]  = lazy protocol witness table accessor for type SPSize3D and conformance SPSize3D((unint64_t *)&lazy protocol witness table cache variable for type SPSize3D and conformance SPSize3D);
  a1[2]  = lazy protocol witness table accessor for type SPSize3D and conformance SPSize3D(&lazy protocol witness table cache variable for type SPSize3D and conformance SPSize3D);
  uint64_t result = lazy protocol witness table accessor for type SPSize3D and conformance SPSize3D(&lazy protocol witness table cache variable for type SPSize3D and conformance SPSize3D);
  a1[3]  = result;
  return result;
}

uint64_t lazy protocol witness table accessor for type SPSize3D and conformance SPSize3D(unint64_t *a1)
{
  uint64_t result = *a1;
  if (!result)
  {
    type metadata accessor for SPSize3D(255);
    uint64_t result = swift_getWitnessTable();
    atomic_store(result, a1);
  }
  return result;
}

unsigned char *storeEnumTagSinglePayload for SPSize3D.CodingKeys(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 2 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 2) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFE) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFD)
  {
    unsigned int v6 = ((a2 - 254) >> 8) + 1;
    *uint64_t result = a2 + 2;
    switch(v5)
    {
      case 1:
        result[1]  = v6;
        break;
      case 2:
        *(_WORD *)(result + 1)  = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x228C16A58);
      case 4:
        *(_DWORD *)(result + 1)  = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1]  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1)  = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1)  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *uint64_t result = a2 + 2;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for SPSize3D.CodingKeys()
{
  return &type metadata for SPSize3D.CodingKeys;
}

unint64_t SPSize3DContainsPoint(SPSize3D *a1, SPPoint3D *a2)
{
  int64x2_t v2 = (int64x2_t)vandq_s8((int8x16_t)vcgeq_f64(vaddq_f64(vmaxnmq_f64(*(float64x2_t *)&a1->width, (float64x2_t)0), (float64x2_t)0), *(float64x2_t *)&a2->x), (int8x16_t)vcgeq_f64(*(float64x2_t *)&a2->x, vaddq_f64(vminnmq_f64(*(float64x2_t *)&a1->width, (float64x2_t)0), (float64x2_t)0)));
  return vandq_s8(vandq_s8((int8x16_t)vdupq_laneq_s64(v2, 1), (int8x16_t)v2), (int8x16_t)vshlq_n_s64((int64x2_t)vandq_s8((int8x16_t)vcgeq_f64(vaddq_f64(vmaxnmq_f64((float64x2_t)*(unint64_t *)&a1->depth, (float64x2_t)0), (float64x2_t)0), *(float64x2_t *)&a2->vector.f64[2]), (int8x16_t)vcgeq_f64(*(float64x2_t *)&a2->vector.f64[2], vaddq_f64(vminnmq_f64((float64x2_t)*(unint64_t *)&a1->depth, (float64x2_t)0), (float64x2_t)0))), 0x3FuLL)).u64[0] >> 63;
}

unint64_t SPSize3DContainsSize(SPSize3D *a1, SPSize3D *a2)
{
  v2.i64[0]  = -1;
  v2.i64[1]  = -1;
  int64x2_t v3 = vceqq_s64(vcgeq_f64(*(float64x2_t *)&a1->width, *(float64x2_t *)&a2->width), v2);
  return vandq_s8((int8x16_t)vdupq_laneq_s64(v3, 1), vandq_s8((int8x16_t)vceqq_s64(vcgeq_f64(*(float64x2_t *)&a1->vector.f64[2], *(float64x2_t *)&a2->vector.f64[2]), v2), (int8x16_t)v3)).u64[0] >> 63;
}

void __swiftcall SPRotation3D.init(_:)(SPRotation3D *__return_ptr retstr, simd_quatd *a2)
{
  simd_quatd v4 = v2;
  SPRotation3DMakeWithQuaternion(v2, (uint64_t)&v4, &v3);
}

double static SPRotation3D.slerp(from:to:t:along:)(unsigned char *a1, float64x2_t a2, float64x2_t a3, float64x2_t a4, float64x2_t a5, double a6)
{
  if (*a1 && (*a1 == 1 || vaddvq_f64(vaddq_f64(vmulq_f64(a2, a4), vmulq_f64(a3, a5))) < 0.0))
  {
    float64x2_t v10 = a2;
    float64x2_t v11 = a3;
    float64x2_t v9 = a5;
    float64x2_t v8 = a4;
    SPRotation3DSlerpLongest(&v10, &v8, (uint64_t)&v7, a6);
  }
  else
  {
    float64x2_t v10 = a2;
    float64x2_t v11 = a3;
    float64x2_t v9 = a5;
    float64x2_t v8 = a4;
    SPRotation3DSlerp(&v10, &v8, (uint64_t)&v7, a6);
  }
  return v7;
}

void __swiftcall SPRotation3D.rotated(by:)(SPRotation3D *__return_ptr retstr, SPRotation3D *by)
{
  float64x2_t v6 = vnegq_f64(v4);
  float64x2_t v7 = (float64x2_t)vextq_s8((int8x16_t)v5, (int8x16_t)vnegq_f64(v5), 8uLL);
  float64x2_t v8 = vmlaq_n_f64(vmulq_laneq_f64(v6, v2, 1), (float64x2_t)vextq_s8((int8x16_t)v4, (int8x16_t)v6, 8uLL), v2.f64[0]);
  *(float64x2_t *)v11.vector.f64  = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v4, v3, 1), (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v4, 8uLL), v3.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v5, v2, 1), v7, v2.f64[0]));
  *(float64x2_t *)&v11.vector.f64[2]  = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v5, v3, 1), v7, v3.f64[0]), v8);
  simd_quatd v10 = v11;
  SPRotation3DMakeWithQuaternion(v11, (uint64_t)&v10, &v9);
}

uint64_t SPRotation3D.description.getter()
{
  _StringGuts.grow(_:)(16);
  v0._countAndFlagsBits  = 0x6E72657461757128;
  v0._object  = (void *)0xED0000203A6E6F69;
  String.append(_:)(v0);
  type metadata accessor for simd_quatd(0);
  _print_unlocked<A, B>(_:_:)();
  v1._countAndFlagsBits  = 41;
  v1._object  = (void *)0xE100000000000000;
  String.append(_:)(v1);
  return 0;
}

long double SPRotation3D.angle.getter(SPRotation3D a1)
{
  v2[0]  = *(float64x2_t *)a1.vector.f64;
  v2[1]  = *(float64x2_t *)&a1.quaternion.vector.f64[2];
  return SPRotation3DGetAngle(a1, v2);
}

long double SPRotation3DGetAngle(SPRotation3D a1, float64x2_t *a2)
{
  long double v2 = atan2(sqrt(vmulq_f64(a2[1], a2[1]).f64[0] + vaddvq_f64(vmulq_f64(*a2, *a2))), a2[1].f64[1]);
  return v2 + v2;
}

double SPRotation3D.angle.setter(SPAngle a1)
{
  *(void *)&double result = *(_OWORD *)&SPRotation3DSetAngle(v1, a1);
  return result;
}

float64x2_t SPRotation3DSetAngle(SPRotation3D *a1, SPAngle a2)
{
  float64x2_t v3 = *(float64x2_t *)&a1->quaternion.vector.f64[2];
  float64x2_t v4 = vmulq_f64(v3, v3);
  v4.f64[0]  = 1.0
            / sqrt(v4.f64[0] + vaddvq_f64(vmulq_f64(*(float64x2_t *)a1->vector.f64, *(float64x2_t *)a1->vector.f64)));
  float64x2_t v9 = vmulq_f64(v3, v4);
  float64x2_t v10 = vmulq_n_f64(*(float64x2_t *)a1->vector.f64, v4.f64[0]);
  __double2 v6 = __sincos_stret(a2.radians * 0.5);
  v5.f64[0]  = v6.__sinval;
  *(void *)&long long v7 = *(_OWORD *)&vmulq_f64(v5, v9);
  *((void *)&v7 + 1)  = *(void *)&v6.__cosval;
  float64x2_t result = vmulq_n_f64(v10, v6.__sinval);
  *(float64x2_t *)a1->vector.f64  = result;
  *(_OWORD *)&a1->quaternion.vector.f64[2]  = v7;
  return result;
}

double (*SPRotation3D.angle.modify(uint64_t a1, SPRotation3D a2))(void *a1)
{
  *(void *)(a1 + 8)  = v2;
  *(_OWORD *)a2.vector.f64  = *(_OWORD *)v2;
  uint64_t v3 = *(void *)(v2 + 24);
  uint64_t v6 = *(void *)(v2 + 16);
  uint64_t v7 = v3;
  float64x2_t v5 = *(float64x2_t *)a2.vector.f64;
  *(long double *)a1  = SPRotation3DGetAngle(a2, &v5);
  return SPRotation3D.angle.modify;
}

double SPRotation3D.angle.modify(void *a1)
{
  *(SPAngle *)&v1.radians  = (SPAngle)*a1;
  *(void *)&double result = *(_OWORD *)&SPRotation3DSetAngle((SPRotation3D *)a1[1], v1);
  return result;
}

double SPRotation3D.axis.getter(SPRotation3D a1, double a2, double a3, double a4, double a5, double a6, float64x2_t a7)
{
  v9[0]  = *(float64x2_t *)a1.vector.f64;
  v9[1]  = *(float64x2_t *)&a1.quaternion.vector.f64[2];
  SPRotation3DGetAxis(a1, v9, &v8, a7);
  return *(double *)&v8;
}

__n128 SPRotation3DGetAxis@<Q0>(SPRotation3D a1@<0:Q0, 16:Q1>, float64x2_t *a2@<X0>, _OWORD *a3@<X8>, float64x2_t a4@<Q7>)
{
  a1.vector.f64[0]  = 0.0;
  float64x2_t v4 = a2[1];
  float64x2_t v5 = vmulq_f64(v4, v4);
  v5.f64[0]  = 1.0 / sqrt(v5.f64[0] + vaddvq_f64(vmulq_f64(*a2, *a2)));
  float64x2_t v6 = vmulq_f64(v4, v5);
  *(float64x2_t *)&a1.quaternion.vector.f64[2]  = vmulq_n_f64(*a2, v5.f64[0]);
  a4.f64[0]  = INFINITY;
  int64x2_t v7 = (int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(*(float64x2_t *)&a1.quaternion.vector.f64[2]), (int8x16_t)vcgezq_f64(*(float64x2_t *)&a1.quaternion.vector.f64[2])), (int8x16_t)vceqq_f64(vabsq_f64(*(float64x2_t *)&a1.quaternion.vector.f64[2]), (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL)));
  *(int64x2_t *)a1.vector.f64  = vdupq_lane_s64(vcgtq_s64(*(int64x2_t *)a1.vector.f64, (int64x2_t)vandq_s8((int8x16_t)vdupq_laneq_s64(v7, 1), vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v6), (int8x16_t)vcgezq_f64(v6)), (int8x16_t)vceqq_f64(vabsq_f64(v6), a4)), (int8x16_t)v7))).i64[0], 0);
  *(int8x16_t *)&a1.quaternion.vector.f64[2]  = vandq_s8(*(int8x16_t *)&a1.quaternion.vector.f64[2], *(int8x16_t *)a1.vector.f64);
  *(int8x16_t *)a1.vector.f64  = vbslq_s8(*(int8x16_t *)a1.vector.f64, (int8x16_t)v6, (int8x16_t)0);
  *a3  = *(_OWORD *)&a1.quaternion.vector.f64[2];
  a3[1]  = *(_OWORD *)a1.vector.f64;
  return *(__n128 *)a1.vector.f64;
}

double SPRotation3D.axis.setter(double a1, double a2, double a3)
{
  v5.x  = a1;
  v5.y  = a2;
  v5.double z = a3;
  *(void *)&double result = *(_OWORD *)&SPRotation3DSetAxis(v3, &v5);
  return result;
}

float64x2_t SPRotation3DSetAxis(SPRotation3D *a1, SPRotationAxis3D *a2)
{
  long double v4 = atan2(sqrt(vmulq_f64(*(float64x2_t *)&a1->quaternion.vector.f64[2], *(float64x2_t *)&a1->quaternion.vector.f64[2]).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)a1->vector.f64, *(float64x2_t *)a1->vector.f64))), a1->vector.f64[3]);
  float64x2_t v5 = *(float64x2_t *)&a2->vector.f64[2];
  float64x2_t v6 = vmulq_f64(v5, v5);
  v6.f64[0]  = 1.0 / sqrt(v6.f64[0] + vaddvq_f64(vmulq_f64(*(float64x2_t *)&a2->x, *(float64x2_t *)&a2->x)));
  float64x2_t v11 = vmulq_f64(v5, v6);
  float64x2_t v12 = vmulq_n_f64(*(float64x2_t *)&a2->x, v6.f64[0]);
  __double2 v8 = __sincos_stret((v4 + v4) * 0.5);
  v7.f64[0]  = v8.__sinval;
  *(void *)&long long v9 = *(_OWORD *)&vmulq_f64(v7, v11);
  *((void *)&v9 + 1)  = *(void *)&v8.__cosval;
  float64x2_t result = vmulq_n_f64(v12, v8.__sinval);
  *(float64x2_t *)a1->vector.f64  = result;
  *(_OWORD *)&a1->quaternion.vector.f64[2]  = v9;
  return result;
}

void (*SPRotation3D.axis.modify(void *a1))(double **a1)
{
  uint64_t v3 = malloc(0x28uLL);
  *a1  = v3;
  v3[4]  = v1;
  *(_OWORD *)v10.vector.f64  = *(_OWORD *)v1;
  uint64_t v4 = *(void *)(v1 + 24);
  uint64_t v8 = *(void *)(v1 + 16);
  uint64_t v9 = v4;
  float64x2_t v7 = *(float64x2_t *)v10.vector.f64;
  SPRotation3DGetAxis(v10, &v7, v3, v5);
  return SPRotation3D.axis.modify;
}

void SPRotation3D.axis.modify(double **a1)
{
  uint64_t v1 = *a1;
  double v3 = (*a1)[2];
  double v2 = (*a1)[3];
  double v5 = **a1;
  double v4 = (*a1)[1];
  float64x2_t v6 = (SPRotation3D *)*((void *)*a1 + 4);
  v7.x  = v5;
  v7.y  = v4;
  v7.double z = v3;
  v7.vector.f64[3]  = v2;
  SPRotation3DSetAxis(v6, &v7);

  free(v1);
}

void __swiftcall SPRotation3D.init(position:target:up:)(SPRotation3D *__return_ptr retstr, SPPoint3D *position, SPPoint3D *target, SPVector3D *up)
{
  v15.x  = v4;
  v15.y  = v5;
  v15.double z = v6;
  v14.x  = v7;
  v14.y  = v8;
  v14.double z = v9;
  v13.x  = v10;
  v13.y  = v11;
  v13.double z = v16;
  SPRotation3DMakeLookAt((float64x2_t *)&v15, &v14, &v13, &v12);
}

float64x2_t *SPRotation3DMakeLookAt@<X0>(float64x2_t *result@<X0>, SPPoint3D *a2@<X1>, SPVector3D *a3@<X2>, float64x2_t *a4@<X8>)
{
  float64x2_t v4 = vsubq_f64(*(float64x2_t *)&a2->x, *result);
  float64x2_t v5 = vsubq_f64(*(float64x2_t *)&a2->vector.f64[2], result[1]);
  float64x2_t v6 = vmulq_f64(v4, v4);
  v6.f64[0]  = 1.0 / sqrt(vmulq_f64(v5, v5).f64[0] + vaddvq_f64(v6));
  float64x2_t v7 = vmulq_f64(v5, v6);
  float64x2_t v8 = vmulq_n_f64(v4, v6.f64[0]);
  float64x2_t v9 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL);
  float64x2_t v10 = *(float64x2_t *)&a3->vector.f64[2];
  float64x2_t v11 = vmulq_f64(v10, v10);
  v11.f64[0]  = 1.0 / sqrt(v11.f64[0] + vaddvq_f64(vmulq_f64(*(float64x2_t *)&a3->x, *(float64x2_t *)&a3->x)));
  float64x2_t v12 = vmulq_n_f64(*(float64x2_t *)&a3->x, v11.f64[0]);
  float64x2_t v13 = vmulq_f64(v10, v11);
  v11.f64[0]  = v7.f64[0];
  v11.f64[1]  = v8.f64[0];
  float64x2_t v14 = vmlaq_laneq_f64(vmulq_laneq_f64(vnegq_f64(v7), v12, 1), v13, v8, 1);
  v13.f64[1]  = v12.f64[0];
  float64x2_t v15 = vmlaq_f64(vmulq_f64(v13, vnegq_f64(v8)), v12, v11);
  float64x2_t v16 = vmulq_f64(v15, v15);
  double v17 = vmulq_f64(v14, v14).f64[0];
  v14.f64[1]  = v15.f64[0];
  v16.f64[0]  = 1.0 / sqrt(v16.f64[1] + v17 + v16.f64[0]);
  float64x2_t v18 = vmulq_n_f64(v14, v16.f64[0]);
  float64x2_t v19 = vmulq_laneq_f64(v16, v15, 1);
  float64x2_t v20 = vnegq_f64(v18);
  float64x2_t v21 = vnegq_f64(v19);
  v19.f64[1]  = v18.f64[0];
  float64x2_t v22 = vmlaq_f64(vmulq_f64(v11, v20), v8, v19);
  float64x2_t v23 = vmlaq_laneq_f64(vmulq_laneq_f64(v21, v8, 1), v7, v18, 1);
  v18.f64[0]  = vmulq_f64(v23, v23).f64[0];
  v23.f64[1]  = v22.f64[0];
  float64x2_t v24 = vmulq_f64(v22, v22);
  v24.f64[0]  = 1.0 / sqrt(v24.f64[1] + v18.f64[0] + v24.f64[0]);
  float64x2_t v25 = vmulq_n_f64(v23, v24.f64[0]);
  float64x2_t v26 = vmulq_laneq_f64(v24, v22, 1);
  double v27 = v7.f64[0] + v20.f64[0] + v25.f64[1];
  if (v27 >= 0.0)
  {
    double v33 = sqrt(v27 + 1.0);
    double v34 = v33 + v33;
    double v35 = 1.0 / (v33 + v33);
    v7.f64[0]  = v35 * vsubq_f64(v26, v9).f64[0];
    double v31 = (v8.f64[0] - v21.f64[0]) * v35;
    v8.f64[0]  = (v20.f64[1] - v25.f64[0]) * v35;
    double v32 = v34 * 0.25;
  }
  else if (v20.f64[0] < v7.f64[0] || v20.f64[0] < v25.f64[1])
  {
    BOOL v36 = v25.f64[1] < v7.f64[0];
    double v37 = sqrt(v7.f64[0] + 1.0 - v20.f64[0] - v25.f64[1]);
    double v38 = v37 + v37;
    double v39 = 1.0 / v38;
    double v40 = (v8.f64[0] + v21.f64[0]) * (1.0 / v38);
    double v41 = 1.0 / v38 * vaddq_f64(v9, v26).f64[0];
    double v42 = v38 * 0.25;
    double v43 = (v20.f64[1] - v25.f64[0]) * v39;
    double v44 = sqrt(1.0 - v20.f64[0] - v7.f64[0] + v25.f64[1]);
    double v45 = v44 + v44;
    double v46 = (v20.f64[1] + v25.f64[0]) * (1.0 / v45);
    double v47 = v45 * 0.25;
    double v48 = 1.0 / v45 * vaddq_f64(v9, v26).f64[0];
    double v49 = (v8.f64[0] - v21.f64[0]) * (1.0 / v45);
    if (v36) {
      v7.f64[0]  = v40;
    }
    else {
      v7.f64[0]  = v46;
    }
    if (v36) {
      double v31 = v41;
    }
    else {
      double v31 = v47;
    }
    if (v36) {
      v8.f64[0]  = v42;
    }
    else {
      v8.f64[0]  = v48;
    }
    if (v36) {
      double v32 = v43;
    }
    else {
      double v32 = v49;
    }
  }
  else
  {
    double v28 = sqrt(v20.f64[0] + 1.0 - v25.f64[1] - v7.f64[0]);
    double v29 = v28 + v28;
    double v30 = 1.0 / v29;
    v7.f64[0]  = v29 * 0.25;
    double v31 = (v20.f64[1] + v25.f64[0]) * v30;
    v8.f64[0]  = (v8.f64[0] + v21.f64[0]) * v30;
    double v32 = v30 * vsubq_f64(v26, v9).f64[0];
  }
  v7.f64[1]  = v31;
  v8.f64[1]  = v32;
  double v50 = vaddvq_f64(vaddq_f64(vmulq_f64(v7, v7), vmulq_f64(v8, v8)));
  if (v50 == 0.0)
  {
    float64x2_t v51 = (float64x2_t)xmmword_228C1F7A0;
    float64x2_t v52 = 0uLL;
  }
  else
  {
    double v53 = 1.0 / sqrt(v50);
    float64x2_t v51 = vmulq_n_f64(v8, v53);
    float64x2_t v52 = vmulq_n_f64(v7, v53);
  }
  *a4  = v52;
  a4[1]  = v51;
  return result;
}

void __swiftcall SPRotation3D.init(forward:)(SPRotation3D *__return_ptr retstr, SPVector3D *forward)
{
  double v5 = v4;
  double v6 = v3;
  double v7 = v2;
  SPVector3DMake((uint64_t)&v9);
  v10.x  = v7;
  v10.y  = v6;
  v10.double z = v5;
  SPRotation3DMakeLookAt(&v10, &v9, &v8);
}

double SPVector3DMake@<D0>(uint64_t a1@<X8>)
{
  double result = 0.0;
  *(_OWORD *)a1  = xmmword_228C1F7A0;
  *(void *)(a1 + 16)  = 0;
  return result;
}

float64x2_t *SPRotation3DMakeLookAt@<X0>(SPVector3D *a1@<X0>, SPVector3D *a2@<X1>, float64x2_t *a3@<X8>)
{
  memset(&v8, 0, sizeof(v8));
  long long v3 = *(_OWORD *)&a1->x;
  *(_OWORD *)&v7.vector.f64[2]  = *(unint64_t *)&a1->z;
  long long v4 = *(_OWORD *)&a2->vector.f64[2];
  *(_OWORD *)&v6.x  = *(_OWORD *)&a2->x;
  *(_OWORD *)&v6.vector.f64[2]  = v4;
  *(_OWORD *)&v7.x  = v3;
  return SPRotation3DMakeLookAt((float64x2_t *)&v8, &v7, &v6, a3);
}

void __swiftcall SPRotation3D.init()(SPRotation3D *__return_ptr retstr)
{
  v3[0]  = 0;
  v3[1]  = 0;
  *(_OWORD *)v1.vector.f64  = xmmword_228C1F7A0;
  long long v4 = xmmword_228C1F7A0;
  SPRotation3DMakeWithQuaternion(v1, (uint64_t)v3, &v2);
}

void __swiftcall SPRotation3D.rotated(by:)(SPRotation3D *__return_ptr retstr, simd_quatd *by)
{
  float64x2_t v6 = vnegq_f64(v4);
  float64x2_t v7 = (float64x2_t)vextq_s8((int8x16_t)v5, (int8x16_t)vnegq_f64(v5), 8uLL);
  float64x2_t v8 = vmlaq_n_f64(vmulq_laneq_f64(v6, v2, 1), (float64x2_t)vextq_s8((int8x16_t)v4, (int8x16_t)v6, 8uLL), v2.f64[0]);
  *(float64x2_t *)v11.vector.f64  = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v4, v3, 1), (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v4, 8uLL), v3.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v5, v2, 1), v7, v2.f64[0]));
  *(float64x2_t *)&v11.vector.f64[2]  = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v5, v3, 1), v7, v3.f64[0]), v8);
  simd_quatd v10 = v11;
  SPRotation3DMakeWithQuaternion(v11, (uint64_t)&v10, &v9);
}

BOOL static SPRotation3D.SlerpPath.== infix(_:_:)(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

void SPRotation3D.SlerpPath.hash(into:)()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int SPRotation3D.SlerpPath.hashValue.getter()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

BOOL protocol witness for static Equatable.== infix(_:_:) in conformance SPRotation3D.SlerpPath(unsigned __int8 *a1, unsigned __int8 *a2)
{
  return *a1 == *a2;
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPRotation3D.SlerpPath()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance SPRotation3D.SlerpPath()
{
  Hasher._combine(_:)(*v0);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPRotation3D.SlerpPath()
{
  Swift::UInt v1 = *v0;
  Hasher.init(_seed:)();
  Hasher._combine(_:)(v1);
  return Hasher._finalize()();
}

double protocol witness for Rotatable3D.rotated(by:) in conformance SPRotation3D@<D0>(_OWORD *a1@<X8>, float64x2_t a2@<Q0>, float64x2_t a3@<Q1>)
{
  float64x2_t v5 = *v3;
  float64x2_t v6 = v3[1];
  v10[0]  = a2;
  v10[1]  = a3;
  v9[0]  = v5;
  v9[1]  = v6;
  *(float64x2_t *)v11.vector.f64  = simd_mul(v10, v9, v8);
  *(void *)&double result = SPRotation3DMakeWithQuaternion(v11, (uint64_t)v8, a1).n128_u64[0];
  return result;
}

double static SPRotation3D.spline(leftEndpoint:from:to:rightEndpoint:t:)(float64x2_t a1, float64x2_t a2, float64x2_t a3, float64x2_t a4, float64x2_t a5, float64x2_t a6, float64x2_t a7, float64x2_t a8, double a9)
{
  v22[0]  = a1;
  v22[1]  = a2;
  v21[0]  = a3;
  v21[1]  = a4;
  v20[0]  = a5;
  v20[1]  = a6;
  v19[0]  = a7;
  v19[1]  = a8;
  SPRotation3DSpline(v22, v21, v20, v19, (uint64_t)&v18, a9);
  return *(double *)&v18;
}

float64_t SPRotation3DSpline@<D0>(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X2>, float64x2_t *a4@<X3>, uint64_t a5@<X8>, double a6@<D0>)
{
  float64x2_t v22 = 0u;
  long long v23 = 0u;
  float64x2_t v8 = *a1;
  float64x2_t v9 = a1[1];
  float64x2_t v20 = *a3;
  float64x2_t v21 = *a2;
  float64x2_t v18 = a3[1];
  float64x2_t v19 = a2[1];
  float64x2_t v16 = *a4;
  float64x2_t v14 = a4[1];
  float64x2_t v26 = 0u;
  float64x2_t v27 = 0u;
  float64x2_t v34 = v8;
  float64x2_t v35 = v9;
  float64x2_t v32 = v21;
  float64x2_t v33 = v19;
  float64x2_t v30 = v20;
  float64x2_t v31 = v18;
  _simd_intermediate(&v34, &v32, &v30, &v26);
  float64x2_t v24 = 0u;
  float64x2_t v25 = 0u;
  float64x2_t v34 = v21;
  float64x2_t v35 = v19;
  float64x2_t v32 = v20;
  float64x2_t v33 = v18;
  float64x2_t v30 = v16;
  float64x2_t v31 = v14;
  _simd_intermediate(&v34, &v32, &v30, &v24);
  float64x2_t v13 = v27;
  float64x2_t v15 = v24;
  float64x2_t v17 = v26;
  float64x2_t v12 = v25;
  float64x2_t v34 = 0u;
  float64x2_t v35 = 0u;
  float64x2_t v32 = v21;
  float64x2_t v33 = v19;
  float64x2_t v30 = v20;
  float64x2_t v31 = v18;
  _simd_slerp_internal(&v32, &v30, &v34, a6);
  float64x2_t v32 = 0u;
  float64x2_t v33 = 0u;
  float64x2_t v30 = v17;
  float64x2_t v31 = v13;
  float64x2_t v28 = v15;
  float64x2_t v29 = v12;
  _simd_slerp_internal(&v30, &v28, &v32, a6);
  float64x2_t v30 = v34;
  float64x2_t v31 = v35;
  float64x2_t v28 = v32;
  float64x2_t v29 = v33;
  _simd_slerp_internal(&v30, &v28, &v22, (a6 + a6) * (1.0 - a6));
  float64_t result = v22.f64[0];
  long long v11 = v23;
  *(float64x2_t *)a5  = v22;
  *(_OWORD *)(a5 + 16)  = v11;
  return result;
}

unint64_t static SPRotation3D.== infix(_:_:)(float64x2_t a1, float64x2_t a2, float64x2_t a3, float64x2_t a4)
{
  v6[0]  = a1;
  v6[1]  = a2;
  v5[0]  = a3;
  v5[1]  = a4;
  return SPRotation3DEqualToRotation(v6, v5);
}

unint64_t SPRotation3DEqualToRotation(float64x2_t *a1, float64x2_t *a2)
{
  float64x2_t v2 = a1[1];
  float64x2_t v3 = a2[1];
  int64x2_t v4 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(*a1, *a2), (int8x16_t)vceqq_f64(v2, v3));
  if ((vandq_s8((int8x16_t)v4, (int8x16_t)vdupq_laneq_s64(v4, 1)).u64[0] & 0x8000000000000000) != 0) {
    return 1;
  }
  int64x2_t v5 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(*a1, vnegq_f64(*a2)), (int8x16_t)vceqq_f64(v2, vnegq_f64(v3)));
  return vandq_s8((int8x16_t)v5, (int8x16_t)vdupq_laneq_s64(v5, 1)).u64[0] >> 63;
}

unint64_t protocol witness for static Equatable.== infix(_:_:) in conformance SPRotation3D(uint64_t a1, uint64_t a2)
{
  float64x2_t v2 = *(float64x2_t *)a1;
  uint64_t v3 = *(void *)(a1 + 24);
  float64x2_t v4 = *(float64x2_t *)a2;
  uint64_t v5 = *(void *)(a2 + 16);
  uint64_t v6 = *(void *)(a2 + 24);
  uint64_t v12 = *(void *)(a1 + 16);
  uint64_t v13 = v3;
  float64x2_t v11 = v2;
  uint64_t v9 = v5;
  uint64_t v10 = v6;
  float64x2_t v8 = v4;
  return SPRotation3DEqualToRotation(&v11, &v8);
}

Swift::Bool __swiftcall SPRotation3D.isApproximatelyEqual(to:tolerance:)(SPRotation3D *to, Swift::Double tolerance)
{
  float64x2_t v9 = v4;
  long long v10 = v5;
  float64x2_t v7 = *(float64x2_t *)&tolerance;
  long long v8 = v2;
  return SPRotation3DAlmostEqualToRotation(&v9, &v7, v3);
}

BOOL SPRotation3DAlmostEqualToRotation(float64x2_t *a1, float64x2_t *a2, double a3)
{
  if (vabdd_f64(a1->f64[0], a2->f64[0]) < a3
    && (float64x2_t v3 = vsubq_f64(*a1, *a2),
        float64x2_t v4 = vsubq_f64(a1[1], a2[1]),
        v3.f64[0]  = v4.f64[0],
        *(int32x2_t *)&v3.f64[0]  = vmovn_s64(vcgtq_f64((float64x2_t)vdupq_lane_s64(*(uint64_t *)&a3, 0), vabsq_f64(v3))),
        (HIDWORD(v3.f64[0]) & LODWORD(v3.f64[0]) & 1) != 0))
  {
    return fabs(v4.f64[1]) < a3;
  }
  else
  {
    return 0;
  }
}

void SPRotation3D.hash(into:)(int8x16_t a1, int8x16_t a2)
{
  int8x16_t v2 = (int8x16_t)vdupq_n_s64((uint64_t)((unint64_t)(a1.i64[0] >= 0) << 63) >> 63);
  specialized SIMD.hash(into:)((__n128)vbslq_s8(v2, a1, (int8x16_t)vnegq_f64((float64x2_t)a1)), (__n128)vbslq_s8(v2, a2, (int8x16_t)vnegq_f64((float64x2_t)a2)));
}

Swift::Int SPRotation3D.hashValue.getter(int8x16_t a1, int8x16_t a2)
{
  int64_t v2 = a1.i64[0];
  Hasher.init(_seed:)();
  int8x16_t v3 = (int8x16_t)vdupq_n_s64((uint64_t)((unint64_t)(v2 >= 0) << 63) >> 63);
  specialized SIMD.hash(into:)((__n128)vbslq_s8(v3, a1, (int8x16_t)vnegq_f64((float64x2_t)a1)), (__n128)vbslq_s8(v3, a2, (int8x16_t)vnegq_f64((float64x2_t)a2)));
  return Hasher._finalize()();
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPRotation3D()
{
  float64x2_t v3 = *v0;
  float64x2_t v4 = v0[1];
  Hasher.init(_seed:)();
  *(float64x2_t *)v1.vector.f64  = v3;
  if ((*(void *)&v3.f64[0] & 0x8000000000000000) != 0)
  {
    v6[0]  = v3;
    *(float64x2_t *)v1.vector.f64  = v4;
    v6[1]  = v4;
    simd_negate(v1, v6, (float64x2_t *)&v5);
    simd_quatd v1 = v5;
  }
  else
  {
    *(float64x2_t *)&v1.vector.f64[2]  = v4;
  }
  specialized SIMD.hash(into:)(*(__n128 *)v1.vector.f64, *(__n128 *)&v1.vector.f64[2]);
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance SPRotation3D()
{
  simd_quatd v1 = *v0;
  if ((*(void *)&v0->vector.f64[0] & 0x8000000000000000) != 0)
  {
    v3[0]  = *(float64x2_t *)v0->vector.f64;
    v3[1]  = *(float64x2_t *)&v1.vector.f64[2];
    simd_negate(v1, v3, v2);
    *(float64x2_t *)v1.vector.f64  = v2[0];
    *(float64x2_t *)&v1.vector.f64[2]  = v2[1];
  }

  specialized SIMD.hash(into:)(*(__n128 *)v1.vector.f64, *(__n128 *)&v1.vector.f64[2]);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPRotation3D()
{
  float64x2_t v3 = *v0;
  float64x2_t v4 = v0[1];
  Hasher.init(_seed:)();
  *(float64x2_t *)v1.vector.f64  = v3;
  if ((*(void *)&v3.f64[0] & 0x8000000000000000) != 0)
  {
    v6[0]  = v3;
    *(float64x2_t *)v1.vector.f64  = v4;
    v6[1]  = v4;
    simd_negate(v1, v6, (float64x2_t *)&v5);
    simd_quatd v1 = v5;
  }
  else
  {
    *(float64x2_t *)&v1.vector.f64[2]  = v4;
  }
  specialized SIMD.hash(into:)(*(__n128 *)v1.vector.f64, *(__n128 *)&v1.vector.f64[2]);
  return Hasher._finalize()();
}

uint64_t protocol witness for static Equatable.== infix(_:_:) in conformance SPRotation3D.CodingKeys()
{
  return 1;
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPRotation3D.CodingKeys()
{
  return Hasher._finalize()();
}

uint64_t protocol witness for Hashable.hash(into:) in conformance SPRotation3D.CodingKeys()
{
  return String.hash(into:)();
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPRotation3D.CodingKeys()
{
  return Hasher._finalize()();
}

uint64_t protocol witness for RawRepresentable.init(rawValue:) in conformance SPRotation3D.CodingKeys@<X0>(Swift::String *a1@<X0>, BOOL *a2@<X8>)
{
  Swift::Int v3 = _findStringSwitchCase(cases:string:)((Swift::OpaquePointer)&outlined read-only object #0 of protocol witness for RawRepresentable.init(rawValue:) in conformance SPRotation3D.CodingKeys, *a1);
  uint64_t result = swift_bridgeObjectRelease();
  *a2  = v3 != 0;
  return result;
}

void protocol witness for RawRepresentable.rawValue.getter in conformance SPRotation3D.CodingKeys(void *a1@<X8>)
{
  *a1  = 0x726F74636576;
  a1[1]  = 0xE600000000000000;
}

uint64_t protocol witness for CodingKey.stringValue.getter in conformance SPRotation3D.CodingKeys()
{
  return 0x726F74636576;
}

uint64_t protocol witness for CodingKey.init(stringValue:) in conformance SPRotation3D.CodingKeys@<X0>(Swift::String string@<0:X0, 8:X1>, BOOL *a2@<X8>)
{
  object  = string._object;
  v3._countAndFlagsBits  = string._countAndFlagsBits;
  v3._object  = object;
  Swift::Int v5 = _findStringSwitchCase(cases:string:)((Swift::OpaquePointer)&outlined read-only object #0 of protocol witness for CodingKey.init(stringValue:) in conformance SPRotation3D.CodingKeys, v3);
  uint64_t result = swift_bridgeObjectRelease();
  *a2  = v5 != 0;
  return result;
}

void protocol witness for CodingKey.init(intValue:) in conformance SPRotation3D.CodingKeys(unsigned char *a1@<X8>)
{
  *a1  = 1;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPRotation3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys();

  return MEMORY[0x270FA00B0](a1, v2);
}

uint64_t protocol witness for CustomDebugStringConvertible.debugDescription.getter in conformance SPRotation3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys();

  return MEMORY[0x270FA00B8](a1, v2);
}

uint64_t SPRotation3D.encode(to:)(void *a1, __n128 a2, __n128 a3)
{
  __n128 v9 = a3;
  __n128 v10 = a2;
  uint64_t v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedEncodingContainer<SPRotation3D.CodingKeys>);
  uint64_t v5 = *(void *)(v4 - 8);
  MEMORY[0x270FA5388](v4);
  float64x2_t v7 = (char *)&v9 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys();
  dispatch thunk of Encoder.container<A>(keyedBy:)();
  __n128 v11 = v10;
  __n128 v12 = v9;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for SIMD4<Double>);
  lazy protocol witness table accessor for type SIMD4<Double> and conformance SIMD4<A>(&lazy protocol witness table cache variable for type SIMD4<Double> and conformance SIMD4<A>);
  KeyedEncodingContainer.encode<A>(_:forKey:)();
  return (*(uint64_t (**)(char *, uint64_t))(v5 + 8))(v7, v4);
}

uint64_t SPRotation3D.init(from:)(void *a1)
{
  return specialized SPRotation3D.init(from:)(a1);
}

uint64_t protocol witness for Decodable.init(from:) in conformance SPRotation3D@<X0>(void *a1@<X0>, _OWORD *a2@<X8>)
{
  uint64_t result = specialized SPRotation3D.init(from:)(a1);
  if (!v2)
  {
    *a2  = v5;
    a2[1]  = v6;
  }
  return result;
}

uint64_t protocol witness for Encodable.encode(to:) in conformance SPRotation3D(void *a1)
{
  return SPRotation3D.encode(to:)(a1, *v1, v1[1]);
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPRotation3D()
{
  _StringGuts.grow(_:)(16);
  v0._countAndFlagsBits  = 0x6E72657461757128;
  v0._object  = (void *)0xED0000203A6E6F69;
  String.append(_:)(v0);
  type metadata accessor for simd_quatd(0);
  _print_unlocked<A, B>(_:_:)();
  v1._countAndFlagsBits  = 41;
  v1._object  = (void *)0xE100000000000000;
  String.append(_:)(v1);
  return 0;
}

uint64_t SPRotation3D.customMirror.getter(__n128 a1, __n128 a2)
{
  __n128 v17 = a2;
  __n128 v18 = a1;
  uint64_t v2 = type metadata accessor for Mirror.AncestorRepresentation();
  uint64_t v3 = *(void *)(v2 - 8);
  MEMORY[0x270FA5388](v2);
  long long v5 = (char *)&v17 - ((v4 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v6 = __swift_instantiateConcreteTypeFromMangledName((uint64_t *)&demangling cache variable for type metadata for Mirror.DisplayStyle?);
  MEMORY[0x270FA5388](v6 - 8);
  long long v8 = (char *)&v17 - ((v7 + 15) & 0xFFFFFFFFFFFFFFF0);
  __n128 v19 = v18;
  __n128 v20 = v17;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<(String, Any)>);
  uint64_t v9 = swift_allocObject();
  *(_OWORD *)(v9 + 16)  = xmmword_228C20870;
  *(void *)(v9 + 32)  = 0x696E726574617571;
  *(void *)(v9 + 40)  = 0xEA00000000006E6FLL;
  type metadata accessor for simd_quatd(0);
  *(void *)(v9 + 72)  = v10;
  uint64_t v11 = swift_allocObject();
  *(void *)(v9 + 48)  = v11;
  __n128 v12 = v17;
  *(__n128 *)(v11 + 16)  = v18;
  *(__n128 *)(v11 + 32)  = v12;
  uint64_t v13 = *MEMORY[0x263F8E808];
  uint64_t v14 = type metadata accessor for Mirror.DisplayStyle();
  uint64_t v15 = *(void *)(v14 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v15 + 104))(v8, v13, v14);
  (*(void (**)(char *, void, uint64_t, uint64_t))(v15 + 56))(v8, 0, 1, v14);
  (*(void (**)(char *, void, uint64_t))(v3 + 104))(v5, *MEMORY[0x263F8E830], v2);
  type metadata accessor for SPRotation3D(0);
  return Mirror.init<A>(_:children:displayStyle:ancestorRepresentation:)();
}

uint64_t protocol witness for CustomReflectable.customMirror.getter in conformance SPRotation3D()
{
  return SPRotation3D.customMirror.getter(*v0, v0[1]);
}

double SPEulerAngles.init(angles:order:)(float32x2_t a1)
{
  *(void *)&double result = *(_OWORD *)&vcvtq_f64_f32(a1);
  return result;
}

void __swiftcall SPEulerAngles.init(x:y:z:order:)(SPEulerAngles *__return_ptr retstr, SPAngle x, SPAngle y, SPAngle z, SPEulerAngleOrder order)
{
}

void __swiftcall SPRotation3D.swingTwist(twistAxis:)(Swift::tuple_swing_SPRotation3D_twist_SPRotation3D *__return_ptr retstr, SPRotationAxis3D *twistAxis)
{
  float64x2_t v17 = v4;
  float64x2_t v18 = v5;
  *(float64x2_t *)&v23.x  = v4;
  *(float64x2_t *)&v23.vector.f64[2]  = v5;
  v21.f64[0]  = v2.vector.f64[0];
  v21.f64[1]  = v2.vector.f64[2];
  *(void *)&long long v22 = v3;
  SPRotation3DTwist(v2, (float64x2_t *)&v23, &v21, &v19);
  double v6 = 1.0 / vaddvq_f64(vaddq_f64(vmulq_f64(v19, v19), vmulq_f64(v20, v20)));
  float64x2_t v7 = vmulq_n_f64(vmulq_f64(v20, (float64x2_t)xmmword_228C1FC40), v6);
  float64x2_t v8 = vmulq_n_f64(vnegq_f64(v19), v6);
  float64x2_t v9 = vnegq_f64(v8);
  float64x2_t v10 = (float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)vnegq_f64(v7), 8uLL);
  float64x2_t v11 = vmulq_laneq_f64(v7, v18, 1);
  float64x2_t v12 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v8, v18, 1), (float64x2_t)vextq_s8((int8x16_t)v9, (int8x16_t)v8, 8uLL), v18.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v7, v17, 1), v10, v17.f64[0]));
  float64x2_t v13 = vaddq_f64(vmlaq_n_f64(v11, v10, v18.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v9, v17, 1), (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)v9, 8uLL), v17.f64[0]));
  double v14 = vaddvq_f64(vaddq_f64(vmulq_f64(v12, v12), vmulq_f64(v13, v13)));
  if (v14 == 0.0)
  {
    *(_OWORD *)v15.vector.f64  = xmmword_228C1F7A0;
    v15.vector.f64[2]  = 0.0;
    v15.vector.f64[3]  = 0.0;
  }
  else
  {
    double v16 = 1.0 / sqrt(v14);
    *(float64x2_t *)v15.vector.f64  = vmulq_n_f64(v13, v16);
    *(float64x2_t *)&v15.vector.f64[2]  = vmulq_n_f64(v12, v16);
  }
  *(_OWORD *)&v23.x  = *(_OWORD *)&v15.vector.f64[2];
  *(_OWORD *)&v23.vector.f64[2]  = *(_OWORD *)v15.vector.f64;
  SPRotation3DMakeWithQuaternion(v15, (uint64_t)&v23, &v21);
}

uint64_t static SPEulerAngleOrder.pitchYawRoll.getter()
{
  return 1;
}

uint64_t static SPEulerAngleOrder.xyz.getter()
{
  return 1;
}

uint64_t static SPEulerAngleOrder.zxy.getter()
{
  return 2;
}

unint64_t SPEulerAngleOrder.description.getter(int a1)
{
  unint64_t v1 = 0xD000000000000012;
  if (a1 == 2) {
    unint64_t v1 = 0xD0000000000000D7;
  }
  if (a1 == 1) {
    return 0xD0000000000000D7;
  }
  else {
    return v1;
  }
}

unint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPEulerAngleOrder()
{
  return SPEulerAngleOrder.description.getter(*v0);
}

unint64_t lazy protocol witness table accessor for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys()
{
  unint64_t result = lazy protocol witness table cache variable for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys);
  }
  return result;
}

uint64_t specialized SPRotation3D.init(from:)(void *a1)
{
  uint64_t v3 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedDecodingContainer<SPRotation3D.CodingKeys>);
  uint64_t v4 = *(void *)(v3 - 8);
  MEMORY[0x270FA5388](v3);
  double v6 = (char *)v8 - ((v5 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPRotation3D.CodingKeys and conformance SPRotation3D.CodingKeys();
  dispatch thunk of Decoder.container<A>(keyedBy:)();
  if (!v1)
  {
    __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for SIMD4<Double>);
    lazy protocol witness table accessor for type SIMD4<Double> and conformance SIMD4<A>(&lazy protocol witness table cache variable for type SIMD4<Double> and conformance SIMD4<A>);
    KeyedDecodingContainer.decode<A>(_:forKey:)();
    SPRotation3DMakeWithQuaternion(v10, (uint64_t)&v10, v9);
    v8[0]  = v9[1];
    v8[1]  = v9[0];
    (*(void (**)(char *, uint64_t))(v4 + 8))(v6, v3);
  }
  return __swift_destroy_boxed_opaque_existential_1(a1);
}

uint64_t sub_228C18818()
{
  return MEMORY[0x270FA0238](v0, 48, 15);
}

float64x2_t *SPRotation3DTwist@<X0>(SPRotation3D a1@<0:Q0, 16:Q1>, float64x2_t *result@<X0>, float64x2_t *a3@<X1>, float64x2_t *a4@<X8>)
{
  float64x2_t v4 = a3[1];
  float64x2_t v5 = vmulq_f64(v4, v4);
  v5.f64[0]  = (vmulq_f64(result[1], v4).f64[0] + vaddvq_f64(vmulq_f64(*result, *a3)))
            / (v5.f64[0] + vaddvq_f64(vmulq_f64(*a3, *a3)));
  *(void *)&v6.f64[0]  = *(_OWORD *)&vmulq_f64(v4, v5);
  v6.f64[1]  = result[1].f64[1];
  float64x2_t v7 = vmulq_n_f64(*a3, v5.f64[0]);
  double v8 = vaddvq_f64(vaddq_f64(vmulq_f64(v7, v7), vmulq_f64(v6, v6)));
  if (v8 == 0.0)
  {
    float64x2_t v9 = (float64x2_t)xmmword_228C1F7A0;
    float64x2_t v10 = 0uLL;
  }
  else
  {
    double v11 = 1.0 / sqrt(v8);
    float64x2_t v9 = vmulq_n_f64(v6, v11);
    float64x2_t v10 = vmulq_n_f64(v7, v11);
  }
  *a4  = v10;
  a4[1]  = v9;
  return result;
}

unint64_t lazy protocol witness table accessor for type SPRotation3D.SlerpPath and conformance SPRotation3D.SlerpPath()
{
  unint64_t result = lazy protocol witness table cache variable for type SPRotation3D.SlerpPath and conformance SPRotation3D.SlerpPath;
  if (!lazy protocol witness table cache variable for type SPRotation3D.SlerpPath and conformance SPRotation3D.SlerpPath)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRotation3D.SlerpPath and conformance SPRotation3D.SlerpPath);
  }
  return result;
}

unint64_t lazy protocol witness table accessor for type SPRotation3D and conformance SPRotation3D()
{
  unint64_t result = lazy protocol witness table cache variable for type SPRotation3D and conformance SPRotation3D;
  if (!lazy protocol witness table cache variable for type SPRotation3D and conformance SPRotation3D)
  {
    type metadata accessor for SPRotation3D(255);
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRotation3D and conformance SPRotation3D);
  }
  return result;
}

void sub_228C18958(uint64_t a1@<X0>, long double *a2@<X8>, SPRotation3D a3@<Q1:Q0>)
{
  *(_OWORD *)a3.vector.f64  = *(_OWORD *)a1;
  uint64_t v3 = *(void *)(a1 + 24);
  uint64_t v5 = *(void *)(a1 + 16);
  uint64_t v6 = v3;
  float64x2_t v4 = *(float64x2_t *)a3.vector.f64;
  *a2  = SPRotation3DGetAngle(a3, &v4);
}

double sub_228C1899C(void *a1, SPRotation3D *a2)
{
  *(SPAngle *)&v2.radians  = (SPAngle)*a1;
  *(void *)&double result = *(_OWORD *)&SPRotation3DSetAngle(a2, v2);
  return result;
}

double sub_228C189A8@<D0>(uint64_t a1@<X0>, _OWORD *a2@<X8>, SPRotation3D a3@<Q1:Q0>, float64x2_t a4@<Q7>)
{
  *(_OWORD *)a3.vector.f64  = *(_OWORD *)a1;
  uint64_t v4 = *(void *)(a1 + 24);
  uint64_t v7 = *(void *)(a1 + 16);
  uint64_t v8 = v4;
  float64x2_t v6 = *(float64x2_t *)a3.vector.f64;
  *(void *)&double result = SPRotation3DGetAxis(a3, &v6, a2, a4).n128_u64[0];
  return result;
}

double sub_228C189DC(double *a1, SPRotation3D *a2)
{
  long long v2 = *(_OWORD *)a1;
  double v3 = a1[3];
  v5.double z = a1[2];
  v5.vector.f64[3]  = v3;
  *(_OWORD *)&v5.x  = v2;
  *(void *)&double result = *(_OWORD *)&SPRotation3DSetAxis(a2, &v5);
  return result;
}

unsigned char *storeEnumTagSinglePayload for SPRotation3D.SlerpPath(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 2 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 2) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFE) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFD)
  {
    unsigned int v6 = ((a2 - 254) >> 8) + 1;
    *double result = a2 + 2;
    switch(v5)
    {
      case 1:
        result[1]  = v6;
        break;
      case 2:
        *(_WORD *)(result + 1)  = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x228C18AE4);
      case 4:
        *(_DWORD *)(result + 1)  = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1]  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1)  = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1)  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *double result = a2 + 2;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for SPRotation3D.SlerpPath()
{
  return &type metadata for SPRotation3D.SlerpPath;
}

uint64_t getEnumTagSinglePayload for SPRotation3D.CodingKeys(unsigned int *a1, int a2)
{
  if (!a2) {
    return 0;
  }
  if ((a2 + 1) >= 0x10000) {
    int v2 = 4;
  }
  else {
    int v2 = 2;
  }
  if ((a2 + 1) < 0x100) {
    int v3 = 1;
  }
  else {
    int v3 = v2;
  }
  if (v3 == 4) {
    return *a1;
  }
  if (v3 == 2) {
    return *(unsigned __int16 *)a1;
  }
  return *(unsigned __int8 *)a1;
}

unsigned char *storeEnumTagSinglePayload for SPRotation3D.CodingKeys(unsigned char *result, int a2, int a3)
{
  if ((a3 + 1) >= 0x10000) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 1) < 0x100) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2)
  {
    switch(v5)
    {
      case 1:
        *double result = a2;
        return result;
      case 2:
        *(_WORD *)double result = a2;
        return result;
      case 3:
        goto LABEL_19;
      case 4:
        *(_DWORD *)double result = a2;
        return result;
      default:
        return result;
    }
  }
  switch(v5)
  {
    case 1:
      *double result = 0;
      break;
    case 2:
      *(_WORD *)double result = 0;
      break;
    case 3:
LABEL_19:
      __break(1u);
      JUMPOUT(0x228C18C0CLL);
    case 4:
      *(_DWORD *)double result = 0;
      break;
    default:
      return result;
  }
  return result;
}

uint64_t getEnumTag for SPRotation3D.CodingKeys()
{
  return 0;
}

ValueMetadata *type metadata accessor for SPRotation3D.CodingKeys()
{
  return &type metadata for SPRotation3D.CodingKeys;
}

void _simd_intermediate(float64x2_t *a1@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X2>, float64x2_t *a4@<X8>)
{
  float64x2_t v54 = 0u;
  float64x2_t v55 = 0u;
  float64x2_t v6 = a2[1];
  float64x2_t v7 = vmulq_f64(v6, (float64x2_t)xmmword_228C1FC40);
  float64x2_t v48 = *a2;
  float64x2_t v49 = v6;
  v6.f64[0]  = 1.0 / vaddvq_f64(vaddq_f64(vmulq_f64(v48, v48), vmulq_f64(v6, v6)));
  float64x2_t v8 = vmulq_n_f64(v7, v6.f64[0]);
  float64x2_t v9 = vmulq_n_f64(vnegq_f64(*a2), v6.f64[0]);
  float64x2_t v10 = a1[1];
  float64x2_t v39 = v8;
  float64x2_t v40 = vnegq_f64(v9);
  float64x2_t v38 = (float64x2_t)vextq_s8((int8x16_t)v8, (int8x16_t)vnegq_f64(v8), 8uLL);
  float64x2_t v43 = v9;
  float64x2_t v45 = (float64x2_t)vextq_s8((int8x16_t)v9, (int8x16_t)v40, 8uLL);
  float64x2_t v41 = (float64x2_t)vextq_s8((int8x16_t)v40, (int8x16_t)v9, 8uLL);
  float64x2_t v11 = vmlaq_n_f64(vmulq_laneq_f64(v8, v10, 1), v38, v10.f64[0]);
  *(float64x2_t *)v56.vector.f64  = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v9, v10, 1), v41, v10.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v8, *a1, 1), v38, a1->f64[0]));
  *(float64x2_t *)&v56.vector.f64[2]  = vaddq_f64(v11, vmlaq_n_f64(vmulq_laneq_f64(v40, *a1, 1), v45, a1->f64[0]));
  v53[0]  = *(float64x2_t *)v56.vector.f64;
  v53[1]  = *(float64x2_t *)&v56.vector.f64[2];
  __tg_log(v56, v53, &v54);
  float64x2_t v51 = 0u;
  float64x2_t v52 = 0u;
  float64x2_t v12 = a3[1];
  float64x2_t v13 = vmlaq_n_f64(vmulq_laneq_f64(v39, v12, 1), v38, v12.f64[0]);
  *(float64x2_t *)v57.vector.f64  = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v43, v12, 1), v41, v12.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v39, *a3, 1), v38, a3->f64[0]));
  *(float64x2_t *)&v57.vector.f64[2]  = vaddq_f64(v13, vmlaq_n_f64(vmulq_laneq_f64(v40, *a3, 1), v45, a3->f64[0]));
  v50[0]  = *(float64x2_t *)v57.vector.f64;
  v50[1]  = *(float64x2_t *)&v57.vector.f64[2];
  __tg_log(v57, v50, &v51);
  __asm { FMOV            V2.2D, #-0.25 }
  float64x2_t v19 = vmulq_f64(vaddq_f64(v54, v51), _Q2);
  float64x2_t v20 = vmulq_f64(vaddq_f64(v55, v52), _Q2);
  float64x2_t v21 = vmulq_f64(v19, v19);
  double v22 = sqrt(vmulq_f64(v20, v20).f64[0] + vaddvq_f64(v21));
  if (v22 == 0.0)
  {
    long double v23 = exp(v20.f64[1]);
    v24.f64[0]  = 0.0;
    v24.f64[1]  = v23;
    float64x2_t v25 = 0uLL;
  }
  else
  {
    v21.f64[0]  = 1.0 / v22;
    float64x2_t v44 = vmulq_n_f64(v19, 1.0 / v22);
    float64x2_t v46 = vmulq_f64(v20, v21);
    long double v42 = v20.f64[1];
    __double2 v27 = __sincos_stret(v22);
    v26.f64[0]  = v27.__sinval;
    *(void *)&v26.f64[0]  = *(_OWORD *)&vmulq_f64(v26, v46);
    v26.f64[1]  = v27.__cosval;
    float64x2_t v47 = v26;
    double v28 = exp(v42);
    float64x2_t v25 = vmulq_n_f64(vmulq_n_f64(v44, v27.__sinval), v28);
    float64x2_t v24 = vmulq_n_f64(v47, v28);
  }
  float64x2_t v29 = vnegq_f64(v25);
  float64x2_t v30 = (float64x2_t)vextq_s8((int8x16_t)v24, (int8x16_t)vnegq_f64(v24), 8uLL);
  float64x2_t v31 = vmlaq_n_f64(vmulq_laneq_f64(v29, v48, 1), (float64x2_t)vextq_s8((int8x16_t)v25, (int8x16_t)v29, 8uLL), v48.f64[0]);
  float64x2_t v32 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v25, v49, 1), (float64x2_t)vextq_s8((int8x16_t)v29, (int8x16_t)v25, 8uLL), v49.f64[0]), vmlaq_n_f64(vmulq_laneq_f64(v24, v48, 1), v30, v48.f64[0]));
  float64x2_t v33 = vaddq_f64(vmlaq_n_f64(vmulq_laneq_f64(v24, v49, 1), v30, v49.f64[0]), v31);
  double v34 = vaddvq_f64(vaddq_f64(vmulq_f64(v32, v32), vmulq_f64(v33, v33)));
  if (v34 == 0.0)
  {
    float64x2_t v35 = (float64x2_t)xmmword_228C1F7A0;
    float64x2_t v36 = 0uLL;
  }
  else
  {
    double v37 = 1.0 / sqrt(v34);
    float64x2_t v35 = vmulq_n_f64(v33, v37);
    float64x2_t v36 = vmulq_n_f64(v32, v37);
  }
  *a4  = v36;
  a4[1]  = v35;
}

__n128 __tg_log@<Q0>(simd_quatd a1@<0:Q0, 16:Q1>, float64x2_t *a2@<X0>, float64x2_t *a3@<X8>)
{
  float64x2_t v4 = a2[1];
  float64x2_t v5 = vmulq_f64(v4, v4);
  float64x2_t v6 = vmulq_f64(*a2, *a2);
  double v7 = vaddvq_f64(vaddq_f64(v6, v5));
  int64x2_t v8 = vceqzq_f64(*a2);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v8, 1), vandq_s8((int8x16_t)vceqzq_f64(v4), (int8x16_t)v8)).u64[0] & 0x8000000000000000) != 0)
  {
    float64_t v18 = 0.0;
    float64x2_t v20 = 0u;
  }
  else
  {
    float64x2_t v17 = *a2;
    float64x2_t v19 = a2[1];
    float64x2_t v15 = v6;
    double v16 = v5.f64[0];
    v9.f64[0]  = acos(v4.f64[1] / sqrt(v7));
    v10.f64[1]  = v15.f64[1];
    v10.f64[0]  = 1.0 / sqrt(v16 + vaddvq_f64(v15));
    float64x2_t v11 = vmulq_n_f64(v17, v10.f64[0]);
    *(void *)&float64_t v18 = *(_OWORD *)&vmulq_f64(vmulq_f64(v19, v10), v9);
    float64x2_t v20 = vmulq_n_f64(v11, v9.f64[0]);
  }
  long double v12 = log(v7);
  v13.f64[0]  = v18;
  v13.f64[1]  = v12 * 0.5;
  __n128 result = (__n128)v20;
  *a3  = v20;
  a3[1]  = v13;
  return result;
}

uint64_t SPAffineTransform3D.inverse.getter@<X0>(uint64_t a1@<X8>)
{
  long long v3 = *(_OWORD *)(v1 + 16);
  float64x2_t v4 = *(float64x2_t *)(v1 + 32);
  long long v5 = *(_OWORD *)(v1 + 48);
  long long v6 = *(_OWORD *)(v1 + 64);
  long long v7 = *(_OWORD *)(v1 + 80);
  long long v8 = *(_OWORD *)(v1 + 96);
  long long v9 = *(_OWORD *)(v1 + 112);
  float64x2_t v27 = *(float64x2_t *)v1;
  long long v28 = v3;
  float64x2_t v29 = v4;
  long long v30 = v5;
  long long v31 = v6;
  long long v32 = v7;
  long long v33 = v8;
  long long v34 = v9;
  SPAffineTransform3DInverted((uint64_t)&v27, &v19);
  long long v17 = v22;
  long long v18 = v20;
  float64x2_t v27 = v19;
  long long v28 = v20;
  float64x2_t v13 = v21;
  float64x2_t v14 = v19;
  float64x2_t v29 = v21;
  long long v30 = v22;
  long long v15 = *(_OWORD *)v26;
  long long v16 = v24;
  long long v31 = v23;
  long long v32 = v24;
  long long v11 = v25;
  long long v12 = v23;
  long long v33 = v25;
  long long v34 = *(_OWORD *)v26;
  if (SPAffineTransform3DIsValid(&v27, *(double *)&v20, *(double *)&v22, *(double *)&v24, v26[0], v19.f64[0], v21))
  {
    float64x2_t v19 = v14;
    long long v20 = v18;
    float64x2_t v21 = v13;
    long long v22 = v17;
    long long v23 = v12;
    long long v24 = v16;
    long long v25 = v11;
    *(_OWORD *)float64x2_t v26 = v15;
    _sSo19SPAffineTransform3DaSgWOi_((uint64_t)&v19);
  }
  else
  {
    _sSo19SPAffineTransform3DaSgWOi0_((uint64_t)&v19);
  }
  outlined init with take of SPAffineTransform3D?((uint64_t)&v19, (uint64_t)&v27);
  return outlined init with take of SPAffineTransform3D?((uint64_t)&v27, a1);
}

uint64_t outlined init with take of SPAffineTransform3D?(uint64_t a1, uint64_t a2)
{
  uint64_t v4 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for SPAffineTransform3D?);
  (*(void (**)(uint64_t, uint64_t, uint64_t))(*(void *)(v4 - 8) + 32))(a2, a1, v4);
  return a2;
}

double SPAffineTransform3D.translation.getter()
{
  __n128 v1 = v0[1];
  __n128 v2 = v0[2];
  __n128 v3 = v0[3];
  __n128 v4 = v0[4];
  __n128 v5 = v0[5];
  __n128 v6 = v0[6];
  __n128 v7 = v0[7];
  v10[0]  = *v0;
  v10[1]  = v1;
  v10[2]  = v2;
  v10[3]  = v3;
  v10[4]  = v4;
  v10[5]  = v5;
  v10[6]  = v6;
  v10[7]  = v7;
  SPAffineTransform3DGetTranslation(v10, &v9);
  return v9.n128_f64[0];
}

double SPAffineTransform3D.translation.setter(double a1, double a2, double a3)
{
  v5.x  = a1;
  v5.y  = a2;
  v5.double z = a3;
  *(void *)&double result = SPAffineTransform3DSetTranslation(v3, &v5).n128_u64[0];
  return result;
}

void (*SPAffineTransform3D.translation.modify(__n128 **a1))(double **a1)
{
  __n128 v3 = (__n128 *)malloc(0x28uLL);
  *a1  = v3;
  v3[2].n128_u64[0]  = (unint64_t)v1;
  __n128 v4 = v1[1];
  __n128 v5 = v1[2];
  __n128 v6 = v1[3];
  __n128 v7 = v1[4];
  __n128 v8 = v1[5];
  __n128 v9 = v1[6];
  __n128 v10 = v1[7];
  v12[0]  = *v1;
  v12[1]  = v4;
  v12[2]  = v5;
  v12[3]  = v6;
  v12[4]  = v7;
  v12[5]  = v8;
  v12[6]  = v9;
  v12[7]  = v10;
  SPAffineTransform3DGetTranslation(v12, v3);
  return SPAffineTransform3D.translation.modify;
}

void SPAffineTransform3D.translation.modify(double **a1)
{
  __n128 v1 = *a1;
  double v3 = (*a1)[2];
  double v2 = (*a1)[3];
  double v5 = **a1;
  double v4 = (*a1)[1];
  __n128 v6 = (SPAffineTransform3D *)*((void *)*a1 + 4);
  v7.x  = v5;
  v7.y  = v4;
  v7.double z = v3;
  v7.vector.f64[3]  = v2;
  SPAffineTransform3DSetTranslation(v6, &v7);

  free(v1);
}

Swift::Bool __swiftcall SPAffineTransform3D.isApproximatelyEqual(to:tolerance:)(SPAffineTransform3D *to, Swift::Double tolerance)
{
  float64x2_t v3 = *(float64x2_t *)to->matrix.columns[0].f64;
  float64x2_t v4 = *(float64x2_t *)&to->matrix.columns[0].f64[2];
  float64x2_t v5 = *(float64x2_t *)to->matrix.columns[1].f64;
  float64x2_t v6 = *(float64x2_t *)&to->matrix.columns[1].f64[2];
  float64x2_t v7 = *(float64x2_t *)to->matrix.columns[2].f64;
  float64x2_t v8 = *(float64x2_t *)&to->matrix.columns[2].f64[2];
  float64x2_t v9 = *(float64x2_t *)to->matrix.columns[3].f64;
  float64x2_t v10 = *(float64x2_t *)&to->matrix.columns[3].f64[2];
  float64x2_t v11 = v2[1];
  float64x2_t v12 = v2[2];
  float64x2_t v13 = v2[3];
  float64x2_t v14 = v2[4];
  float64x2_t v15 = v2[5];
  float64x2_t v16 = v2[6];
  float64x2_t v17 = v2[7];
  v20[0]  = *v2;
  v20[1]  = v11;
  v20[2]  = v12;
  v20[3]  = v13;
  v20[4]  = v14;
  v20[5]  = v15;
  v20[6]  = v16;
  v20[7]  = v17;
  v19[0]  = v3;
  v19[1]  = v4;
  v19[2]  = v5;
  v19[3]  = v6;
  v19[4]  = v7;
  v19[5]  = v8;
  v19[6]  = v9;
  v19[7]  = v10;
  return SPAffineTransform3DAlmostEqualToTransform(v20, v19, *(float64x2_t *)&tolerance);
}

unint64_t SPAffineTransform3DAlmostEqualToTransform(float64x2_t *a1, float64x2_t *a2, float64x2_t a3)
{
  float64x2_t v3 = (float64x2_t)vdupq_lane_s64(*(uint64_t *)&a3.f64[0], 0);
  int64x2_t v4 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vcgeq_f64(v3, vabdq_f64(a1[2], a2[2])), (int8x16_t)vcgeq_f64(v3, vabdq_f64(*a1, *a2))), vandq_s8((int8x16_t)vcgeq_f64(v3, vabdq_f64(a1[4], a2[4])), (int8x16_t)vcgeq_f64(v3, vabdq_f64(a1[6], a2[6]))));
  return vandq_s8((int8x16_t)vdupq_laneq_s64(v4, 1), vandq_s8((int8x16_t)vshlq_n_s64((int64x2_t)vandq_s8(vandq_s8((int8x16_t)vcgeq_f64(a3, vabdq_f64(a1[3], a2[3])), (int8x16_t)vcgeq_f64(a3, vabdq_f64(a1[1], a2[1]))), vandq_s8((int8x16_t)vcgeq_f64(a3, vabdq_f64(a1[5], a2[5])), (int8x16_t)vcgeq_f64(a3, vabdq_f64(a1[7], a2[7])))), 0x3FuLL), (int8x16_t)v4)).u64[0] >> 63;
}

void __swiftcall SPAffineTransform3D.init()(SPAffineTransform3D *__return_ptr retstr)
{
  SPPoint3DMake(1.0, 1.0, 1.0, &v11.width);
  SPAffineTransform3DMakeScale(&v11, (uint64_t)v10, v2);
  long long v3 = v10[1];
  long long v4 = v10[2];
  long long v5 = v10[3];
  long long v6 = v10[4];
  long long v7 = v10[5];
  long long v8 = v10[6];
  long long v9 = v10[7];
  *(_OWORD *)retstr->matrix.columns[0].f64  = v10[0];
  *(_OWORD *)&retstr->matrix.columns[0].f64[2]  = v3;
  *(_OWORD *)retstr->matrix.columns[1].f64  = v4;
  *(_OWORD *)&retstr->matrix.columns[1].f64[2]  = v5;
  *(_OWORD *)retstr->matrix.columns[2].f64  = v6;
  *(_OWORD *)&retstr->matrix.columns[2].f64[2]  = v7;
  *(_OWORD *)retstr->matrix.columns[3].f64  = v8;
  *(_OWORD *)&retstr->matrix.columns[3].f64[2]  = v9;
}

double SPAffineTransform3DMakeScale@<D0>(SPSize3D *a1@<X0>, uint64_t a2@<X8>, __n128 a3@<Q2>)
{
  double result = a1->width;
  *(void *)&long long v4 = 0;
  *((void *)&v4 + 1)  = *(void *)&a1->height;
  a3.n128_u64[0]  = *(void *)&a1->depth;
  *(_OWORD *)a2  = *(unint64_t *)&a1->width;
  *(_OWORD *)(a2 + 16)  = 0u;
  *(_OWORD *)(a2 + 32)  = v4;
  *(_OWORD *)(a2 + 48)  = 0u;
  *(_OWORD *)(a2 + 64)  = 0u;
  *(__n128 *)(a2 + 80)  = a3;
  *(_OWORD *)(a2 + 96)  = 0u;
  *(_OWORD *)(a2 + 112)  = 0u;
  return result;
}

void __swiftcall SPAffineTransform3D.init(scale:rotation:translation:)(SPAffineTransform3D *__return_ptr retstr, SPSize3D *scale, SPRotation3D *rotation, SPSize3D *translation)
{
  *(_OWORD *)&v26.x  = v7;
  *(_OWORD *)&v26.vector.f64[2]  = v8;
  double v12 = v6;
  double v13 = v5;
  double v14 = v4;
  v27.vector.f64[0]  = v9;
  v27.vector.f64[1]  = v10;
  v27.vector.f64[2]  = v11;
  SPVector3DMakeWithSize((SPSize3D *)&v27, (uint64_t)v34);
  v36.width  = v14;
  v36.height  = v13;
  v36.depth  = v12;
  *(_OWORD *)v37.vector.f64  = *(_OWORD *)&v26.vector.f64[2];
  *(SPVector3D *)&v35.x  = *(SPVector3D *)&v26.x;
  SPAffineTransform3DMake(&v36, v37, &v35, v34, (long long *)&v27, v16, v17, v18);
  long long v19 = *(_OWORD *)&v27.quaternion.vector.f64[2];
  long long v20 = v28;
  long long v21 = v29;
  long long v22 = v30;
  long long v23 = v31;
  long long v24 = v32;
  long long v25 = v33;
  *(_OWORD *)retstr->matrix.columns[0].f64  = *(_OWORD *)v27.vector.f64;
  *(_OWORD *)&retstr->matrix.columns[0].f64[2]  = v19;
  *(_OWORD *)retstr->matrix.columns[1].f64  = v20;
  *(_OWORD *)&retstr->matrix.columns[1].f64[2]  = v21;
  *(_OWORD *)retstr->matrix.columns[2].f64  = v22;
  *(_OWORD *)&retstr->matrix.columns[2].f64[2]  = v23;
  *(_OWORD *)retstr->matrix.columns[3].f64  = v24;
  *(_OWORD *)&retstr->matrix.columns[3].f64[2]  = v25;
}

float64_t *SPAffineTransform3DMake@<X0>(SPSize3D *a1@<X0>, SPRotation3D a2@<0:Q0, 16:Q1>, SPVector3D *a3@<X1>, float64x2_t *a4@<X2>, long long *a5@<X8>, float64x2_t a6@<Q3>, float64x2_t _Q5@<Q5>, float64x2_t _Q6@<Q6>)
{
  v8.f64[0]  = a1->depth;
  float64x2_t v10 = (float64x2_t)*(unint64_t *)&a1->width;
  double result = &a1->height;
  v11.f64[0]  = 0.0;
  v11.f64[1]  = *result;
  _Q1  = *(float64x2_t *)&a3->x;
  _Q0  = *(float64x2_t *)&a3->vector.f64[2];
  _Q6.f64[0]  = a3->y;
  __asm { FMLS            D2, D0, V0.D[0] }
  _D7  = a3->vector.f64[3];
  __asm { FMLA            D2, D7, V0.D[1] }
  a6.f64[0]  = vmlad_n_f64(vmuld_lane_f64(_Q0.f64[0], _Q0, 1), _Q6.f64[0], a3->x);
  float64_t v19 = a6.f64[0] + a6.f64[0];
  v20.f64[0]  = vmuld_lane_f64(_Q6.f64[0], _Q0, 1);
  a6.f64[0]  = vmlad_n_f64(-(_Q6.f64[0] * _D7), _Q0.f64[0], a3->x);
  a6.f64[0]  = a6.f64[0] + a6.f64[0];
  _Q2.f64[1]  = v19;
  double v22 = vmlad_n_f64(-(_Q0.f64[0] * _D7), _Q6.f64[0], a3->x);
  v23.f64[0]  = v22 + v22;
  __asm
  {
    FMLA            D5, D6, V1.D[1]
    FMLA            D5, D7, V0.D[1]
    FMLS            D5, D1, V1.D[0]
  }
  v23.f64[1]  = _Q5.f64[0];
  __asm
  {
    FMLA            D19, D0, V1.D[1]
    FMLS            D5, D1, V1.D[0]
    FMLS            D5, D6, V1.D[1]
  }
  _Q1.f64[0]  = a3->z;
  v20.f64[1]  = -(a3->x * _D7);
  _Q6.f64[0]  = _D19 + _D19;
  float64x2_t v25 = vmlaq_f64(v20, (float64x2_t)vzip1q_s64(*(int64x2_t *)&a3->x, (int64x2_t)_Q0), _Q1);
  float64x2_t v26 = vaddq_f64(v25, v25);
  float64x2_t v27 = *a4;
  _Q0.f64[0]  = a4[1].f64[0];
  *a5  = 0u;
  a5[1]  = 0u;
  a5[2]  = 0u;
  a5[3]  = 0u;
  a5[4]  = 0u;
  a5[5]  = 0u;
  a5[6]  = 0u;
  a5[7]  = 0u;
  float64x2_t v30 = *(float64x2_t *)MEMORY[0x263EF8988];
  float64x2_t v29 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v32 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v31 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v34 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v33 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v35 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v34, v26), (int8x16_t)vceqq_f64(v32, v23)), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], _Q2));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v35, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v33, _Q5), (int8x16_t)vceqq_f64(v31, _Q6)), (int8x16_t)vceqq_f64(v29, a6)), (int8x16_t)v35)).u64[0] & 0x8000000000000000) == 0|| (int64x2_t v36 = (int64x2_t)vandq_s8((int8x16_t)vceqq_f64(v32, v11), vandq_s8((int8x16_t)vceqq_f64(v30, v10), (int8x16_t)vceqq_f64(v34, (float64x2_t)0))), (vandq_s8((int8x16_t)vdupq_laneq_s64(v36, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v31, (float64x2_t)0), (int8x16_t)vceqq_f64(v29, (float64x2_t)0)), (int8x16_t)vceqq_f64(v33, v8)), (int8x16_t)v36)).u64[0] & 0x8000000000000000) == 0))
  {
    uint64_t v37 = 0;
    float64x2_t v59 = v10;
    float64x2_t v60 = 0uLL;
    float64x2_t v61 = v11;
    float64x2_t v62 = 0uLL;
    float64x2_t v63 = 0uLL;
    v64  = v8;
    long long v68 = 0uLL;
    float64x2_t v69 = 0uLL;
    long long v70 = 0uLL;
    float64x2_t v71 = 0uLL;
    float64x2_t v72 = 0uLL;
    v73  = 0uLL;
    *(void *)&_Q2.f64[1]  = vextq_s8((int8x16_t)_Q2, (int8x16_t)_Q2, 8uLL).u64[0];
    *(void *)&v23.f64[1]  = vextq_s8((int8x16_t)v23, (int8x16_t)v23, 8uLL).u64[0];
    *(void *)&v26.f64[1]  = vextq_s8((int8x16_t)v26, (int8x16_t)v26, 8uLL).u64[0];
    do
    {
      float64x2_t v39 = *(float64x2_t *)((char *)&v59 + v37);
      float64x2_t v38 = *(float64x2_t *)((char *)&v59 + v37 + 16);
      float64x2_t v40 = (float64x2_t *)((char *)&v68 + v37);
      *float64x2_t v40 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(_Q2, v39.f64[0]), v23, v39, 1), v26, v38.f64[0]);
      v40[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(a6, v39), _Q6, v39, 1), v38, _Q5);
      v37 += 32;
    }
    while (v37 != 96);
    _Q2  = (float64x2_t)v68;
    a6  = v69;
    float64x2_t v23 = (float64x2_t)v70;
    _Q6  = v71;
    float64x2_t v26 = v72;
    _Q5  = v73;
    *a5  = v68;
    a5[1]  = (__int128)a6;
  }
  long long v41 = xmmword_228C1F7D0;
  long long v42 = xmmword_228C1F7A0;
  __asm { FMOV            V24.2D, #1.0 }
  int64x2_t v44 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v32, (float64x2_t)xmmword_228C1F7A0), (int8x16_t)vceqq_f64(v30, (float64x2_t)xmmword_228C1F7D0)), (int8x16_t)vceqzq_f64(v34));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v44, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqzq_f64(v31), (int8x16_t)vceqzq_f64(v29)), (int8x16_t)vceqq_f64(v33, _Q24)), (int8x16_t)v44)).u64[0] & 0x8000000000000000) != 0&& (int8x16_t v45 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v31, _Q6), (int8x16_t)vceqq_f64(v29, a6)), (int8x16_t)vceqq_f64(v33, _Q5)), v46 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v32, v23), (int8x16_t)vceqq_f64(v30, _Q2)), (int8x16_t)vceqq_f64(v34, v26)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v46, 1), vandq_s8(v45, (int8x16_t)v46)).u64[0] & 0x8000000000000000) != 0))
  {
    float64x2_t v54 = 0uLL;
    float64x2_t v27 = vaddq_f64(v27, (float64x2_t)0);
    _Q0  = vaddq_f64(_Q0, (float64x2_t)0);
    float64x2_t v53 = 0uLL;
    float64x2_t v52 = 0uLL;
  }
  else
  {
    int64x2_t v47 = vceqzq_f64(v27);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v47, 1), vandq_s8((int8x16_t)vceqzq_f64(_Q0), (int8x16_t)v47)).u64[0] & 0x8000000000000000) != 0)
    {
      uint64_t v55 = 0;
      float64x2_t v59 = _Q2;
      float64x2_t v60 = a6;
      float64x2_t v61 = v23;
      float64x2_t v62 = _Q6;
      float64x2_t v63 = v26;
      v64  = _Q5;
      long long v68 = 0u;
      float64x2_t v69 = 0u;
      long long v70 = 0u;
      float64x2_t v71 = 0u;
      float64x2_t v72 = 0u;
      v73  = 0u;
      do
      {
        float64x2_t v57 = *(float64x2_t *)((char *)&v59 + v55);
        float64x2_t v56 = *(float64x2_t *)((char *)&v59 + v55 + 16);
        long long v58 = (float64x2_t *)((char *)&v68 + v55);
        *long long v58 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)xmmword_228C1F7D0, v57.f64[0]), (float64x2_t)xmmword_228C1F7A0, v57, 1), (float64x2_t)0, v56.f64[0]);
        v58[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v57, (float64x2_t)0), (float64x2_t)0, v57, 1), _Q24, v56);
        v55 += 32;
      }
      while (v55 != 96);
      long long v41 = v68;
      float64x2_t v52 = v69;
      long long v42 = v70;
      float64x2_t v53 = v71;
      float64x2_t v54 = v72;
      _Q24  = v73;
    }
    else
    {
      uint64_t v48 = 0;
      _Q0.f64[1]  = 1.0;
      float64x2_t v59 = _Q2;
      float64x2_t v60 = (float64x2_t)*(unint64_t *)&a6.f64[0];
      float64x2_t v61 = v23;
      float64x2_t v62 = (float64x2_t)*(unint64_t *)&_Q6.f64[0];
      float64x2_t v63 = v26;
      v64  = (float64x2_t)*(unint64_t *)&_Q5.f64[0];
      v65  = 0;
      uint64_t v66 = 0;
      long long v67 = xmmword_228C1F7A0;
      long long v68 = 0u;
      float64x2_t v69 = 0u;
      long long v70 = 0u;
      float64x2_t v71 = 0u;
      float64x2_t v72 = 0u;
      v73  = 0u;
      v74  = 0u;
      v75  = 0u;
      do
      {
        float64x2_t v50 = *(float64x2_t *)((char *)&v59 + v48);
        float64x2_t v49 = *(float64x2_t *)((char *)&v59 + v48 + 16);
        float64x2_t v51 = (float64x2_t *)((char *)&v68 + v48);
        *float64x2_t v51 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)xmmword_228C1F7D0, v50.f64[0]), (float64x2_t)xmmword_228C1F7A0, v50, 1), (float64x2_t)0, v49.f64[0]), v27, v49, 1);
        v51[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v50.f64[0]), (float64x2_t)0, v50, 1), (float64x2_t)xmmword_228C1F7D0, v49.f64[0]), _Q0, v49, 1);
        v48 += 32;
      }
      while (v48 != 128);
      long long v41 = v68;
      float64x2_t v52 = v69;
      long long v42 = v70;
      float64x2_t v53 = v71;
      float64x2_t v54 = v72;
      _Q24  = v73;
      float64x2_t v27 = v74;
      _Q0  = v75;
    }
  }
  *a5  = v41;
  a5[1]  = (__int128)v52;
  a5[2]  = v42;
  a5[3]  = (__int128)v53;
  a5[4]  = (__int128)v54;
  a5[5]  = (__int128)_Q24;
  a5[6]  = (__int128)v27;
  a5[7]  = (__int128)_Q0;
  return result;
}

void __swiftcall SPAffineTransform3D.init(scale:rotation:translation:)(SPAffineTransform3D *__return_ptr retstr, SPSize3D *scale, SPRotation3D *rotation, SPVector3D *translation)
{
  v23.width  = v4.vector.f64[0];
  v23.height  = v4.vector.f64[2];
  v23.depth  = v5;
  *(float64x2_t *)&v22.x  = v6;
  *(_OWORD *)&v22.vector.f64[2]  = v7;
  v20.f64[0]  = v8.f64[0];
  v20.f64[1]  = v9.f64[0];
  uint64_t v21 = v10;
  SPAffineTransform3DMake(&v23, v4, &v22, &v20, v19, v6, v8, v9);
  long long v12 = v19[1];
  long long v13 = v19[2];
  long long v14 = v19[3];
  long long v15 = v19[4];
  long long v16 = v19[5];
  long long v17 = v19[6];
  long long v18 = v19[7];
  *(_OWORD *)retstr->matrix.columns[0].f64  = v19[0];
  *(_OWORD *)&retstr->matrix.columns[0].f64[2]  = v12;
  *(_OWORD *)retstr->matrix.columns[1].f64  = v13;
  *(_OWORD *)&retstr->matrix.columns[1].f64[2]  = v14;
  *(_OWORD *)retstr->matrix.columns[2].f64  = v15;
  *(_OWORD *)&retstr->matrix.columns[2].f64[2]  = v16;
  *(_OWORD *)retstr->matrix.columns[3].f64  = v17;
  *(_OWORD *)&retstr->matrix.columns[3].f64[2]  = v18;
}

uint64_t SPAffineTransform3D.init(shear:)@<X0>(uint64_t a1@<X0>, _OWORD *a2@<X8>, float64x2_t a3@<Q0>, float64x2_t a4@<Q1>)
{
  int v5 = *(unsigned __int8 *)(a1 + 16);
  if (v5 == 2)
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    uint64_t v6 = 4;
    goto LABEL_7;
  }
  if (v5 == 1)
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    uint64_t v6 = 2;
LABEL_7:
    uint64_t result = SPAffineTransform3DMakeShear(v6, a3, a4, (uint64_t)v15);
    long long v8 = v15[1];
    long long v10 = v15[2];
    long long v9 = v15[3];
    long long v12 = v15[4];
    long long v11 = v15[5];
    long long v14 = v15[6];
    long long v13 = v15[7];
    *a2  = v15[0];
    a2[1]  = v8;
    a2[2]  = v10;
    a2[3]  = v9;
    a2[4]  = v12;
    a2[5]  = v11;
    a2[6]  = v14;
    a2[7]  = v13;
    return result;
  }
  if (!*(unsigned char *)(a1 + 16))
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    uint64_t v6 = 1;
    goto LABEL_7;
  }
  uint64_t result = _assertionFailure(_:_:file:line:flags:)();
  __break(1u);
  return result;
}

uint64_t SPAffineTransform3DMakeShear@<X0>(uint64_t result@<X0>, float64x2_t a2@<Q0>, float64x2_t a3@<Q1>, uint64_t a4@<X8>)
{
  long long v4 = 0uLL;
  *(_OWORD *)a4  = 0u;
  *(_OWORD *)(a4 + 16)  = 0u;
  *(_OWORD *)(a4 + 32)  = 0u;
  *(_OWORD *)(a4 + 48)  = 0u;
  *(_OWORD *)(a4 + 64)  = 0u;
  *(_OWORD *)(a4 + 80)  = 0u;
  *(_OWORD *)(a4 + 96)  = 0u;
  *(_OWORD *)(a4 + 112)  = 0u;
  *(void *)a4  = 0x3FF0000000000000;
  *(void *)(a4 + 40)  = 0x3FF0000000000000;
  *(void *)(a4 + 80)  = 0x3FF0000000000000;
  float64x2_t v6 = *(float64x2_t *)a4;
  float64x2_t v5 = *(float64x2_t *)(a4 + 16);
  float64x2_t v8 = *(float64x2_t *)(a4 + 32);
  float64x2_t v7 = *(float64x2_t *)(a4 + 48);
  float64x2_t v10 = *(float64x2_t *)(a4 + 64);
  float64x2_t v9 = *(float64x2_t *)(a4 + 80);
  if (result == 4)
  {
    a2.f64[1]  = a3.f64[0];
    float64x2_t v16 = (float64x2_t)xmmword_228C1F7D0;
    float64x2_t v17 = (float64x2_t)xmmword_228C1F7A0;
    float64x2_t v18 = 0uLL;
    __asm { FMOV            V16.2D, #1.0 }
LABEL_7:
    a3  = 0uLL;
    goto LABEL_8;
  }
  if (result == 2)
  {
    __asm { FMOV            V16.2D, #1.0 }
    v17.f64[1]  = _Q16.f64[1];
    v17.f64[0]  = a2.f64[0];
    float64x2_t v16 = (float64x2_t)xmmword_228C1F7D0;
    a2  = 0uLL;
    float64x2_t v18 = a3;
    goto LABEL_7;
  }
  if (result != 1) {
    goto LABEL_13;
  }
  __asm { FMOV            V16.2D, #1.0 }
  v16.f64[0]  = _Q16.f64[0];
  v16.f64[1]  = a2.f64[0];
  float64x2_t v17 = (float64x2_t)xmmword_228C1F7A0;
  a2  = 0uLL;
  float64x2_t v18 = 0uLL;
LABEL_8:
  float64x2_t v19 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v21 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v20 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v23 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v22 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v24 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v21, v8), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v6)), (int8x16_t)vceqq_f64(v23, v10));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v24, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v20, v7), (int8x16_t)vceqq_f64(v19, v5)), (int8x16_t)vceqq_f64(v22, v9)), (int8x16_t)v24)).u64[0] & 0x8000000000000000) != 0&& (int8x16_t v25 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v20, v18), (int8x16_t)vceqq_f64(v19, a3)), (int8x16_t)vceqq_f64(v22, _Q16)), v26 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v21, v17), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v16)), (int8x16_t)vceqq_f64(v23, a2)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v26, 1), vandq_s8(v25, (int8x16_t)v26)).u64[0] & 0x8000000000000000) != 0))
  {
    long long v4 = 0uLL;
  }
  else
  {
    uint64_t v27 = 0;
    v31[0]  = v16;
    v31[1]  = a3;
    v31[2]  = v17;
    v31[3]  = v18;
    v31[4]  = a2;
    v31[5]  = _Q16;
    float64x2_t v32 = 0u;
    float64x2_t v33 = 0u;
    float64x2_t v34 = 0u;
    float64x2_t v35 = 0u;
    float64x2_t v36 = 0u;
    float64x2_t v37 = 0u;
    *(void *)&v6.f64[1]  = vextq_s8((int8x16_t)v6, (int8x16_t)v6, 8uLL).u64[0];
    *(void *)&v8.f64[1]  = vextq_s8((int8x16_t)v8, (int8x16_t)v8, 8uLL).u64[0];
    *(void *)&v10.f64[1]  = vextq_s8((int8x16_t)v10, (int8x16_t)v10, 8uLL).u64[0];
    do
    {
      float64x2_t v29 = (float64x2_t)v31[v27];
      float64x2_t v28 = (float64x2_t)v31[v27 + 1];
      float64x2_t v30 = (float64x2_t *)((char *)&v32 + v27 * 16);
      *float64x2_t v30 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v29.f64[0]), v8, v29, 1), v10, v28.f64[0]);
      v30[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v5, v29), v7, v29, 1), v28, v9);
      v27 += 2;
    }
    while (v27 != 6);
    float64x2_t v6 = v32;
    float64x2_t v5 = v33;
    float64x2_t v8 = v34;
    float64x2_t v7 = v35;
    long long v4 = 0uLL;
    float64x2_t v10 = v36;
    float64x2_t v9 = v37;
  }
LABEL_13:
  *(float64x2_t *)a4  = v6;
  *(float64x2_t *)(a4 + 16)  = v5;
  *(float64x2_t *)(a4 + 32)  = v8;
  *(float64x2_t *)(a4 + 48)  = v7;
  *(float64x2_t *)(a4 + 64)  = v10;
  *(float64x2_t *)(a4 + 80)  = v9;
  *(_OWORD *)(a4 + 96)  = v4;
  *(_OWORD *)(a4 + 112)  = v4;
  return result;
}

void __swiftcall SPAffineTransform3D.init(pose:)(SPAffineTransform3D *__return_ptr retstr, SPPose3D *pose)
{
  long long v6 = *(_OWORD *)&pose->position.vector.f64[2];
  long long v7 = *(_OWORD *)pose->rotation.vector.f64;
  double v8 = pose->rotation.vector.f64[2];
  double v9 = pose->rotation.vector.f64[3];
  *(_OWORD *)&v18.position.x  = *(_OWORD *)&pose->position.x;
  *(_OWORD *)&v18.position.vector.f64[2]  = v6;
  v18.rotation.vector.f64[2]  = v8;
  v18.rotation.vector.f64[3]  = v9;
  *(_OWORD *)v18.rotation.vector.f64  = v7;
  SPAffineTransform3DMakeWithPose(&v18, v17, v2, v3, v4);
  long long v10 = v17[1];
  long long v11 = v17[2];
  long long v12 = v17[3];
  long long v13 = v17[4];
  long long v14 = v17[5];
  long long v15 = v17[6];
  long long v16 = v17[7];
  *(_OWORD *)retstr->matrix.columns[0].f64  = v17[0];
  *(_OWORD *)&retstr->matrix.columns[0].f64[2]  = v10;
  *(_OWORD *)retstr->matrix.columns[1].f64  = v11;
  *(_OWORD *)&retstr->matrix.columns[1].f64[2]  = v12;
  *(_OWORD *)retstr->matrix.columns[2].f64  = v13;
  *(_OWORD *)&retstr->matrix.columns[2].f64[2]  = v14;
  *(_OWORD *)retstr->matrix.columns[3].f64  = v15;
  *(_OWORD *)&retstr->matrix.columns[3].f64[2]  = v16;
}

float64_t *SPAffineTransform3DMakeWithPose@<X0>(SPPose3D *a1@<X0>, long long *a2@<X8>, float64x2_t a3@<Q3>, float64x2_t a4@<Q5>, float64x2_t a5@<Q6>)
{
  __asm { FMOV            V0.2D, #1.0 }
  *(_OWORD *)&v15.width  = _Q0;
  v15.depth  = 1.0;
  float64x2_t v10 = *(float64x2_t *)&a1->position.x;
  double z = a1->position.z;
  *(_OWORD *)&v16.quaternion.vector.f64[2]  = *(_OWORD *)&a1->rotation.quaternion.vector.f64[2];
  *(_OWORD *)&v12.x  = *(_OWORD *)a1->rotation.vector.f64;
  *(_OWORD *)v16.vector.f64  = *(_OWORD *)&v12.x;
  *(_OWORD *)&v12.vector.f64[2]  = *(_OWORD *)&v16.quaternion.vector.f64[2];
  float64x2_t v13 = v10;
  return SPAffineTransform3DMake(&v15, v16, &v12, &v13, a2, a3, a4, a5);
}

double SPAffineTransform3D.init(scaledPose:)@<D0>(uint64_t a1@<X0>, _OWORD *a2@<X8>, float64x2_t a3@<Q3>, float64x2_t a4@<Q5>, float64x2_t a5@<Q6>)
{
  long long v6 = *(_OWORD *)(a1 + 16);
  long long v7 = *(_OWORD *)(a1 + 32);
  uint64_t v8 = *(void *)(a1 + 48);
  uint64_t v9 = *(void *)(a1 + 56);
  a3.f64[0]  = *(float64_t *)(a1 + 64);
  v19[0]  = *(_OWORD *)a1;
  v19[1]  = v6;
  uint64_t v20 = v8;
  uint64_t v21 = v9;
  v19[2]  = v7;
  unint64_t v22 = *(void *)&a3.f64[0];
  SPAffineTransform3DMakeWithScaledPose((uint64_t)v19, v18, a3, a4, a5);
  double result = *(double *)v18;
  long long v11 = v18[1];
  long long v12 = v18[2];
  long long v13 = v18[3];
  long long v14 = v18[4];
  long long v15 = v18[5];
  long long v16 = v18[6];
  long long v17 = v18[7];
  *a2  = v18[0];
  a2[1]  = v11;
  a2[2]  = v12;
  a2[3]  = v13;
  a2[4]  = v14;
  a2[5]  = v15;
  a2[6]  = v16;
  a2[7]  = v17;
  return result;
}

float64_t *SPAffineTransform3DMakeWithScaledPose@<X0>(uint64_t a1@<X0>, long long *a2@<X8>, float64x2_t a3@<Q3>, float64x2_t a4@<Q5>, float64x2_t a5@<Q6>)
{
  v10.width  = *(double *)(a1 + 64);
  v10.height  = v10.width;
  v10.depth  = v10.width;
  float64x2_t v5 = *(float64x2_t *)a1;
  uint64_t v9 = *(void *)(a1 + 16);
  *(_OWORD *)&v11.quaternion.vector.f64[2]  = *(_OWORD *)(a1 + 48);
  *(_OWORD *)&v7.x  = *(_OWORD *)(a1 + 32);
  *(_OWORD *)v11.vector.f64  = *(_OWORD *)&v7.x;
  *(_OWORD *)&v7.vector.f64[2]  = *(_OWORD *)&v11.quaternion.vector.f64[2];
  float64x2_t v8 = v5;
  return SPAffineTransform3DMake(&v10, v11, &v7, &v8, a2, a3, a4, a5);
}

__n128 SPAffineTransform3D.sheared(_:)@<Q0>(uint64_t a1@<X0>, uint64_t a2@<X8>, float64x2_t a3@<Q0>, float64x2_t a4@<Q1>)
{
  long long v6 = *(_OWORD *)(v4 + 16);
  long long v8 = *(_OWORD *)(v4 + 32);
  long long v7 = *(_OWORD *)(v4 + 48);
  long long v10 = *(_OWORD *)(v4 + 64);
  long long v9 = *(_OWORD *)(v4 + 80);
  long long v12 = *(_OWORD *)(v4 + 96);
  long long v11 = *(_OWORD *)(v4 + 112);
  if (*(unsigned char *)(a1 + 16))
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    if (*(unsigned char *)(a1 + 16) == 1)
    {
      float64x2_t v22 = *(float64x2_t *)v4;
      long long v23 = v6;
      long long v24 = v8;
      long long v25 = v7;
      long long v26 = v10;
      long long v27 = v9;
      long long v28 = v12;
      long long v29 = v11;
      int v13 = 2;
    }
    else
    {
      float64x2_t v22 = *(float64x2_t *)v4;
      long long v23 = v6;
      long long v24 = v8;
      long long v25 = v7;
      long long v26 = v10;
      long long v27 = v9;
      long long v28 = v12;
      long long v29 = v11;
      int v13 = 4;
    }
  }
  else
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    float64x2_t v22 = *(float64x2_t *)v4;
    long long v23 = v6;
    long long v24 = v8;
    long long v25 = v7;
    long long v26 = v10;
    long long v27 = v9;
    long long v28 = v12;
    long long v29 = v11;
    int v13 = 1;
  }
  SPAffineTransform3DShear(&v22, v13, v21, a3, a4);
  __n128 result = (__n128)v21[1];
  float64x2_t v16 = v21[2];
  float64x2_t v15 = v21[3];
  float64x2_t v18 = v21[4];
  float64x2_t v17 = v21[5];
  float64x2_t v20 = v21[6];
  float64x2_t v19 = v21[7];
  *(float64x2_t *)a2  = v21[0];
  *(__n128 *)(a2 + 16)  = result;
  *(float64x2_t *)(a2 + 32)  = v16;
  *(float64x2_t *)(a2 + 48)  = v15;
  *(float64x2_t *)(a2 + 64)  = v18;
  *(float64x2_t *)(a2 + 80)  = v17;
  *(float64x2_t *)(a2 + 96)  = v20;
  *(float64x2_t *)(a2 + 112)  = v19;
  return result;
}

void __swiftcall SPAffineTransform3D.scaledBy(x:y:z:)(SPAffineTransform3D *__return_ptr retstr, Swift::Double x, Swift::Double y, Swift::Double z)
{
  float64x2_t v6 = v4[1];
  float64x2_t v7 = v4[2];
  float64x2_t v8 = v4[3];
  float64x2_t v9 = v4[4];
  float64x2_t v10 = v4[5];
  float64x2_t v11 = v4[6];
  float64x2_t v12 = v4[7];
  v21[0]  = *v4;
  v21[1]  = v6;
  v21[2]  = v7;
  v21[3]  = v8;
  v21[4]  = v9;
  v21[5]  = v10;
  v21[6]  = v11;
  v21[7]  = v12;
  SPAffineTransform3DScaleBy(v21, v20, *(unint64_t *)&x, y, *(float64x2_t *)&z);
  float64x2_t v13 = v20[1];
  float64x2_t v14 = v20[2];
  float64x2_t v15 = v20[3];
  float64x2_t v16 = v20[4];
  float64x2_t v17 = v20[5];
  float64x2_t v18 = v20[6];
  float64x2_t v19 = v20[7];
  *(float64x2_t *)retstr->matrix.columns[0].f64  = v20[0];
  *(float64x2_t *)&retstr->matrix.columns[0].f64[2]  = v13;
  *(float64x2_t *)retstr->matrix.columns[1].f64  = v14;
  *(float64x2_t *)&retstr->matrix.columns[1].f64[2]  = v15;
  *(float64x2_t *)retstr->matrix.columns[2].f64  = v16;
  *(float64x2_t *)&retstr->matrix.columns[2].f64[2]  = v17;
  *(float64x2_t *)retstr->matrix.columns[3].f64  = v18;
  *(float64x2_t *)&retstr->matrix.columns[3].f64[2]  = v19;
}

Swift::Bool __swiftcall SPAffineTransform3D.isUniform(overDimensions:)(Spatial::Dimension3DSet overDimensions)
{
  unint64_t v2 = *(void *)overDimensions.rawValue;
  if ((*(void *)overDimensions.rawValue & 0x8000000000000000) != 0)
  {
    __break(1u);
    goto LABEL_5;
  }
  if (HIDWORD(v2))
  {
LABEL_5:
    __break(1u);
    return overDimensions.rawValue;
  }
  float64x2_t v3 = v1[1];
  float64x2_t v5 = v1[2];
  float64x2_t v4 = v1[3];
  float64x2_t v7 = v1[4];
  float64x2_t v6 = v1[5];
  float64x2_t v9 = v1[6];
  float64x2_t v8 = v1[7];
  v11[0]  = *v1;
  v11[1]  = v3;
  v11[2]  = v5;
  v11[3]  = v4;
  v11[4]  = v7;
  v11[5]  = v6;
  v11[6]  = v9;
  v11[7]  = v8;
  LOBYTE(overDimensions.rawValue)  = SPAffineTransform3DIsUniformOverDimensions(v11, v2);
  return overDimensions.rawValue;
}

BOOL SPAffineTransform3DIsUniformOverDimensions(float64x2_t *a1, int a2)
{
  float64x2_t v2 = a1[1];
  float64x2_t v3 = a1[2];
  float64x2_t v4 = a1[3];
  float64x2_t v5 = a1[4];
  float64x2_t v6 = a1[5];
  v7.f64[0]  = a1[5].f64[0];
  v7.f64[1]  = a1[4].f64[0];
  v8.f64[0]  = a1[3].f64[0];
  v8.f64[1]  = a1[2].f64[0];
  float64x2_t v10 = vmulq_f64(*a1, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v5, (int8x16_t)v6, 8uLL), vnegq_f64(v8)), v7, (float64x2_t)vextq_s8((int8x16_t)v3, (int8x16_t)v4, 8uLL)));
  BOOL v9 = vmulq_f64(v2, vmlaq_laneq_f64(vmulq_f64(v5, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v3, 1))), v3, v5, 1)).f64[0]+ vaddvq_f64(v10) < 0.0;
  v10.f64[0]  = -1.0;
  if (!v9) {
    v10.f64[0]  = 1.0;
  }
  v11.f64[0]  = sqrt(vmulq_f64(v2, v2).f64[0] + vaddvq_f64(vmulq_f64(*a1, *a1)));
  float64x2_t v12 = vmulq_f64(v6, v6);
  v12.f64[0]  = sqrt(v12.f64[0] + vaddvq_f64(vmulq_f64(v5, v5)));
  v11.f64[1]  = sqrt(vmulq_f64(v4, v4).f64[0] + vaddvq_f64(vmulq_f64(v3, v3)));
  *(void *)&double v21 = *(_OWORD *)&vmulq_f64(v12, v10);
  float64x2_t v22 = vmulq_n_f64(v11, v10.f64[0]);
  if (a2 == 7)
  {
    float64x2_t v23 = *a1;
    float64x2_t v24 = v2;
    float64x2_t v25 = v3;
    float64x2_t v26 = v4;
    float64x2_t v27 = v5;
    float64x2_t v28 = v6;
    simd_quaternion((uint64_t)&v23, (uint64_t)&v29);
    long double v13 = atan2(sqrt(vmulq_f64(v30, v30).f64[0] + vaddvq_f64(vmulq_f64(v29, v29))), v30.f64[1]);
    return fabs(remainder(v13 + v13, 1.57079633)) < 0.0000000149011612 && v22.f64[0] == v22.f64[1] && v22.f64[1] == v21;
  }
  else
  {
    float64x2_t v23 = *a1;
    float64x2_t v24 = v2;
    float64x2_t v25 = v3;
    float64x2_t v26 = v4;
    float64x2_t v27 = v5;
    float64x2_t v28 = v6;
    simd_quaternion((uint64_t)&v23, (uint64_t)&v29);
    long double v17 = atan2(sqrt(vmulq_f64(v30, v30).f64[0] + vaddvq_f64(vmulq_f64(v29, v29))), v30.f64[1]);
    double v18 = fabs(remainder(v17 + v17, 1.57079633));
    BOOL result = v18 < 0.0000000149011612;
    switch(a2)
    {
      case 6:
        BOOL v19 = v18 < 0.0000000149011612;
        BOOL v20 = v22.f64[1] == v21;
        break;
      case 5:
        BOOL v19 = v18 < 0.0000000149011612;
        BOOL v20 = v22.f64[0] == v21;
        break;
      case 3:
        BOOL v19 = v18 < 0.0000000149011612;
        BOOL v20 = v22.f64[0] == v22.f64[1];
        break;
      default:
        return result;
    }
    return v20 && v19;
  }
}

void __swiftcall SPAffineTransform3D.flipped(along:)(SPAffineTransform3D *__return_ptr retstr, SPAxis *along)
{
  float64x2_t v4 = v2[1];
  float64x2_t v5 = v2[2];
  float64x2_t v6 = v2[3];
  float64x2_t v7 = v2[4];
  float64x2_t v8 = v2[5];
  float64x2_t v9 = v2[6];
  float64x2_t v10 = v2[7];
  v19[0]  = *v2;
  v19[1]  = v4;
  v19[2]  = v5;
  v19[3]  = v6;
  v19[4]  = v7;
  v19[5]  = v8;
  v19[6]  = v9;
  v19[7]  = v10;
  SPAffineTransform3DFlip(v19, (int)along, v18);
  float64x2_t v11 = v18[1];
  float64x2_t v12 = v18[2];
  float64x2_t v13 = v18[3];
  float64x2_t v14 = v18[4];
  float64x2_t v15 = v18[5];
  float64x2_t v16 = v18[6];
  float64x2_t v17 = v18[7];
  *(float64x2_t *)retstr->matrix.columns[0].f64  = v18[0];
  *(float64x2_t *)&retstr->matrix.columns[0].f64[2]  = v11;
  *(float64x2_t *)retstr->matrix.columns[1].f64  = v12;
  *(float64x2_t *)&retstr->matrix.columns[1].f64[2]  = v13;
  *(float64x2_t *)retstr->matrix.columns[2].f64  = v14;
  *(float64x2_t *)&retstr->matrix.columns[2].f64[2]  = v15;
  *(float64x2_t *)retstr->matrix.columns[3].f64  = v16;
  *(float64x2_t *)&retstr->matrix.columns[3].f64[2]  = v17;
}

float64x2_t *SPAffineTransform3DFlip@<X0>(float64x2_t *result@<X0>, int a2@<W1>, float64x2_t *a3@<X8>)
{
  BOOL v3 = __OFSUB__(a2, 1);
  if (a2 == 1)
  {
    unsigned int v4 = 0;
  }
  else
  {
    BOOL v3 = __OFSUB__(a2, 4);
    if (a2 == 4)
    {
      unsigned int v4 = 2;
    }
    else
    {
      BOOL v3 = __OFSUB__(a2, 2);
      if (a2 != 2)
      {
        float64x2_t v55 = result[5];
        a3[4]  = result[4];
        a3[5]  = v55;
        float64x2_t v56 = result[7];
        a3[6]  = result[6];
        a3[7]  = v56;
        float64x2_t v57 = result[1];
        *a3  = *result;
        a3[1]  = v57;
        float64x2_t v58 = result[3];
        a3[2]  = result[2];
        a3[3]  = v58;
        return result;
      }
      unsigned int v4 = 1;
    }
  }
  v76.f64[1]  = 0.0;
  v77  = 0u;
  v74  = 0u;
  v72.f64[0]  = 0.0;
  v73  = 0u;
  float64x2_t v71 = 0u;
  float64x2_t v70 = (float64x2_t)0x3FF0000000000000uLL;
  v72.f64[1]  = 1.0;
  v75  = (float64x2_t)0x3FF0000000000000uLL;
  v70.f64[4 * v4 + v4]  = -1.0;
  v76.f64[0]  = 0.0;
  *a3  = 0u;
  a3[1]  = 0u;
  a3[2]  = 0u;
  a3[3]  = 0u;
  a3[4]  = 0u;
  a3[5]  = 0u;
  a3[6]  = 0u;
  a3[7]  = 0u;
  float64x2_t v6 = v70;
  float64x2_t v5 = v71;
  float64x2_t v8 = v72;
  float64x2_t v7 = v73;
  float64x2_t v10 = v74;
  float64x2_t v9 = v75;
  float64x2_t v12 = v76;
  float64x2_t v11 = v77;
  float64x2_t v13 = result[1];
  float64x2_t v15 = result[2];
  float64x2_t v14 = result[3];
  float64x2_t v17 = result[4];
  float64x2_t v16 = result[5];
  float64x2_t v19 = result[6];
  float64x2_t v18 = result[7];
  float64x2_t v21 = *(float64x2_t *)MEMORY[0x263EF8988];
  float64x2_t v20 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v23 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v22 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v25 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v24 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v26 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v23, v72), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], v70)), (int8x16_t)vceqq_f64(v25, v74));
  unint64_t v27 = vandq_s8((int8x16_t)vdupq_laneq_s64(v26, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v22, v73), (int8x16_t)vceqq_f64(v20, v71)), (int8x16_t)vceqq_f64(v24, v75)), (int8x16_t)v26)).u64[0];
  if ((v27 & 0x8000000000000000) != 0
    && (int64x2_t v28 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v23, v15), (int8x16_t)vceqq_f64(v21, *result)), (int8x16_t)vceqq_f64(v25, v17)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v28, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v22, v14), (int8x16_t)vceqq_f64(v20, v13)), (int8x16_t)vceqq_f64(v24, v16)), (int8x16_t)v28)).u64[0] & 0x8000000000000000) != 0))
  {
    float64x2_t v45 = vaddq_f64(v77, v18);
    float64x2_t v46 = vaddq_f64(v76, v19);
    *a3  = v70;
    a3[1]  = v5;
    a3[2]  = v8;
    a3[3]  = v7;
    a3[4]  = v10;
    a3[5]  = v9;
    float64x2_t v43 = v10;
    float64x2_t v44 = v9;
    a3[6]  = v46;
    a3[7]  = v45;
    float64x2_t v41 = v8;
    float64x2_t v42 = v7;
    float64x2_t v40 = v6;
    float64x2_t v39 = v5;
  }
  else
  {
    int64x2_t v29 = vceqzq_f64(v76);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v29, 1), vandq_s8((int8x16_t)vceqzq_f64(v77), (int8x16_t)v29)).u64[0] & 0x8000000000000000) != 0
      && (int64x2_t v30 = vceqzq_f64(v19),
          (vandq_s8((int8x16_t)vdupq_laneq_s64(v30, 1), vandq_s8((int8x16_t)vceqzq_f64(v18), (int8x16_t)v30)).u64[0] & 0x8000000000000000) != 0))
    {
      uint64_t v59 = 0;
      v78  = *result;
      v79  = v13;
      v80  = v15;
      v81  = v14;
      v82  = v17;
      v83  = v16;
      v86  = 0u;
      v87  = 0u;
      v88  = 0u;
      v89  = 0u;
      v90  = 0u;
      v91  = 0u;
      v60.f64[0]  = v70.f64[0];
      *(void *)&v60.f64[1]  = vextq_s8((int8x16_t)v70, (int8x16_t)v70, 8uLL).u64[0];
      v61.f64[0]  = v72.f64[0];
      *(void *)&v61.f64[1]  = vextq_s8((int8x16_t)v72, (int8x16_t)v72, 8uLL).u64[0];
      v62.f64[0]  = v74.f64[0];
      *(void *)&v62.f64[1]  = vextq_s8((int8x16_t)v74, (int8x16_t)v74, 8uLL).u64[0];
      do
      {
        v64  = *(float64x2_t *)((char *)&v78 + v59);
        float64x2_t v63 = *(float64x2_t *)((char *)&v78 + v59 + 16);
        v65  = (float64x2_t *)((char *)&v86 + v59);
        *v65  = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v60, v64.f64[0]), v61, v64, 1), v62, v63.f64[0]);
        v65[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v5, v64), v7, v64, 1), v63, v9);
        v59 += 32;
        BOOL v3 = __OFSUB__(v59, 96);
      }
      while (v59 != 96);
      float64x2_t v40 = v86;
      float64x2_t v39 = v87;
      float64x2_t v41 = v88;
      float64x2_t v42 = v89;
      float64x2_t v43 = v90;
      float64x2_t v44 = v91;
      *a3  = v86;
      a3[1]  = v39;
      a3[2]  = v41;
      a3[3]  = v42;
      a3[4]  = v43;
      a3[5]  = v44;
      float64x2_t v46 = v12;
      float64x2_t v45 = v11;
      a3[6]  = v12;
      a3[7]  = v11;
    }
    else
    {
      uint64_t v31 = 0;
      float64x2_t v32 = (float64x2_t)*(unint64_t *)&v71.f64[0];
      float64x2_t v33 = (float64x2_t)*(unint64_t *)&v73.f64[0];
      float64x2_t v34 = (float64x2_t)*(unint64_t *)&v75.f64[0];
      v35.f64[0]  = v77.f64[0];
      v35.f64[1]  = 1.0;
      v18.f64[1]  = 1.0;
      v78  = *result;
      v79  = (float64x2_t)*(unint64_t *)&v13.f64[0];
      v80  = v15;
      v81  = (float64x2_t)*(unint64_t *)&v14.f64[0];
      v82  = v17;
      v83  = (float64x2_t)*(unint64_t *)&v16.f64[0];
      v84  = v19;
      v85  = v18;
      v86  = 0u;
      v87  = 0u;
      v88  = 0u;
      v89  = 0u;
      v90  = 0u;
      v91  = 0u;
      v92  = 0u;
      v93  = 0u;
      do
      {
        float64x2_t v37 = *(float64x2_t *)((char *)&v78 + v31);
        float64x2_t v36 = *(float64x2_t *)((char *)&v78 + v31 + 16);
        float64x2_t v38 = (float64x2_t *)((char *)&v86 + v31);
        *float64x2_t v38 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v6, v37.f64[0]), v8, v37, 1), v10, v36.f64[0]), v12, v36, 1);
        v38[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v32, v37.f64[0]), v33, v37, 1), v34, v36.f64[0]), v35, v36, 1);
        v31 += 32;
        BOOL v3 = __OFSUB__(v31, 128);
      }
      while (v31 != 128);
      float64x2_t v40 = v86;
      float64x2_t v39 = v87;
      float64x2_t v41 = v88;
      float64x2_t v42 = v89;
      float64x2_t v43 = v90;
      float64x2_t v44 = v91;
      float64x2_t v46 = v92;
      float64x2_t v45 = v93;
      *a3  = v86;
      a3[1]  = v39;
      a3[2]  = v41;
      a3[3]  = v42;
      a3[4]  = v43;
      a3[5]  = v44;
      a3[6]  = v46;
      a3[7]  = v45;
    }
  }
  int8x16_t v47 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v22, v42), (int8x16_t)vceqq_f64(v20, v39)), (int8x16_t)vceqq_f64(v24, v44));
  int64x2_t v48 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v23, v41), (int8x16_t)vceqq_f64(v21, v40)), (int8x16_t)vceqq_f64(v25, v43));
  if (((vandq_s8((int8x16_t)vdupq_laneq_s64(v48, 1), vandq_s8(v47, (int8x16_t)v48)).u64[0] & v27 & 0x8000000000000000) != 0) != v3)
  {
    float64x2_t v46 = vaddq_f64(v12, v46);
    float64x2_t v45 = vaddq_f64(v11, v45);
  }
  else
  {
    int64x2_t v49 = vceqzq_f64(v46);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v49, 1), vandq_s8((int8x16_t)vceqzq_f64(v45), (int8x16_t)v49)).u64[0] & 0x8000000000000000) != 0
      && (int64x2_t v50 = vceqzq_f64(v12),
          (vandq_s8((int8x16_t)vdupq_laneq_s64(v50, 1), vandq_s8((int8x16_t)vceqzq_f64(v11), (int8x16_t)v50)).u64[0] & 0x8000000000000000) != 0))
    {
      uint64_t v66 = 0;
      v78  = v6;
      v79  = v5;
      v80  = v8;
      v81  = v7;
      v82  = v10;
      v83  = v9;
      v86  = 0u;
      v87  = 0u;
      v88  = 0u;
      v89  = 0u;
      v90  = 0u;
      v91  = 0u;
      *(void *)&v40.f64[1]  = vextq_s8((int8x16_t)v40, (int8x16_t)v40, 8uLL).u64[0];
      *(void *)&v41.f64[1]  = vextq_s8((int8x16_t)v41, (int8x16_t)v41, 8uLL).u64[0];
      *(void *)&v43.f64[1]  = vextq_s8((int8x16_t)v43, (int8x16_t)v43, 8uLL).u64[0];
      do
      {
        float64x2_t v68 = *(float64x2_t *)((char *)&v78 + v66);
        float64x2_t v67 = *(float64x2_t *)((char *)&v78 + v66 + 16);
        float64x2_t v69 = (float64x2_t *)((char *)&v86 + v66);
        *float64x2_t v69 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v40, v68.f64[0]), v41, v68, 1), v43, v67.f64[0]);
        v69[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v39, v68), v42, v68, 1), v67, v44);
        v66 += 32;
      }
      while (v66 != 96);
      float64x2_t v40 = v86;
      float64x2_t v39 = v87;
      float64x2_t v41 = v88;
      float64x2_t v42 = v89;
      float64x2_t v43 = v90;
      float64x2_t v44 = v91;
    }
    else
    {
      uint64_t v51 = 0;
      v39.f64[1]  = 0.0;
      v42.f64[1]  = 0.0;
      v44.f64[1]  = 0.0;
      v45.f64[1]  = 1.0;
      v11.f64[1]  = 1.0;
      v78  = v6;
      v79  = (float64x2_t)*(unint64_t *)&v5.f64[0];
      v80  = v8;
      v81  = (float64x2_t)*(unint64_t *)&v7.f64[0];
      v82  = v10;
      v83  = (float64x2_t)*(unint64_t *)&v9.f64[0];
      v84  = v12;
      v85  = v11;
      v86  = 0u;
      v87  = 0u;
      v88  = 0u;
      v89  = 0u;
      v90  = 0u;
      v91  = 0u;
      v92  = 0u;
      v93  = 0u;
      do
      {
        float64x2_t v53 = *(float64x2_t *)((char *)&v78 + v51);
        float64x2_t v52 = *(float64x2_t *)((char *)&v78 + v51 + 16);
        float64x2_t v54 = (float64x2_t *)((char *)&v86 + v51);
        *float64x2_t v54 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v40, v53.f64[0]), v41, v53, 1), v43, v52.f64[0]), v46, v52, 1);
        v54[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v39, v53.f64[0]), v42, v53, 1), v44, v52.f64[0]), v45, v52, 1);
        v51 += 32;
      }
      while (v51 != 128);
      float64x2_t v40 = v86;
      float64x2_t v39 = v87;
      float64x2_t v41 = v88;
      float64x2_t v42 = v89;
      float64x2_t v43 = v90;
      float64x2_t v44 = v91;
      float64x2_t v46 = v92;
      float64x2_t v45 = v93;
    }
  }
  *a3  = v40;
  a3[1]  = v39;
  a3[2]  = v41;
  a3[3]  = v42;
  a3[4]  = v43;
  a3[5]  = v44;
  a3[6]  = v46;
  a3[7]  = v45;
  return result;
}

Swift::Void __swiftcall SPAffineTransform3D.flip(along:)(SPAxis *along)
{
  float64x2_t v2 = v1[1];
  float64x2_t v3 = v1[2];
  float64x2_t v4 = v1[3];
  float64x2_t v5 = v1[4];
  float64x2_t v6 = v1[5];
  float64x2_t v7 = v1[6];
  float64x2_t v8 = v1[7];
  v17[0]  = *v1;
  v17[1]  = v2;
  v17[2]  = v3;
  v17[3]  = v4;
  v17[4]  = v5;
  v17[5]  = v6;
  v17[6]  = v7;
  v17[7]  = v8;
  SPAffineTransform3DFlip(v17, (int)along, v16);
  float64x2_t v9 = v16[1];
  float64x2_t v10 = v16[2];
  float64x2_t v11 = v16[3];
  float64x2_t v12 = v16[4];
  float64x2_t v13 = v16[5];
  float64x2_t v14 = v16[6];
  float64x2_t v15 = v16[7];
  float64x2_t *v1 = v16[0];
  v1[1]  = v9;
  v1[2]  = v10;
  v1[3]  = v11;
  v1[4]  = v12;
  v1[5]  = v13;
  v1[6]  = v14;
  v1[7]  = v15;
}

unint64_t SPAffineTransform3D.rotation.getter@<X0>(uint64_t a1@<X8>)
{
  long long v3 = *(_OWORD *)(v1 + 16);
  long long v4 = *(_OWORD *)(v1 + 32);
  long long v5 = *(_OWORD *)(v1 + 48);
  long long v6 = *(_OWORD *)(v1 + 64);
  long long v7 = *(_OWORD *)(v1 + 80);
  long long v8 = *(_OWORD *)(v1 + 96);
  long long v9 = *(_OWORD *)(v1 + 112);
  float64x2_t v17 = *(float64x2_t *)v1;
  long long v18 = v3;
  long long v19 = v4;
  long long v20 = v5;
  long long v21 = v6;
  long long v22 = v7;
  long long v23 = v8;
  long long v24 = v9;
  SPAffineTransform3DGetRotation(&v17, &v15);
  uint64_t v10 = *((void *)&v16 + 1);
  uint64_t v11 = v16;
  float64_t v12 = v15.f64[1];
  float64_t v13 = v15.f64[0];
  long long v18 = v16;
  float64x2_t v17 = v15;
  unint64_t result = SPRotation3DIsValid(&v17);
  if (!result)
  {
    float64_t v13 = 0.0;
    float64_t v12 = 0.0;
    uint64_t v11 = 0;
    uint64_t v10 = 0;
  }
  *(float64_t *)a1  = v13;
  *(float64_t *)(a1 + 8)  = v12;
  *(void *)(a1 + 16)  = v11;
  *(void *)(a1 + 24)  = v10;
  *(unsigned char *)(a1 + 32)  = result ^ 1;
  return result;
}

void SPAffineTransform3DGetRotation(float64x2_t *a1@<X0>, float64x2_t *a2@<X8>)
{
  float64x2_t v4 = *a1;
  float64x2_t v3 = a1[1];
  float64x2_t v6 = a1[2];
  float64x2_t v5 = a1[3];
  float64x2_t v7 = a1[4];
  float64x2_t v8 = a1[5];
  v9.f64[0]  = a1[5].f64[0];
  v9.f64[1]  = a1[4].f64[0];
  v10.f64[0]  = a1[3].f64[0];
  v10.f64[1]  = a1[2].f64[0];
  float64x2_t v11 = vmulq_f64(*a1, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v7, (int8x16_t)v8, 8uLL), vnegq_f64(v10)), v9, (float64x2_t)vextq_s8((int8x16_t)v6, (int8x16_t)v5, 8uLL)));
  float64x2_t v13 = vmulq_f64(v3, vmlaq_laneq_f64(vmulq_f64(v7, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v6, 1))), v6, v7, 1));
  BOOL v12 = v13.f64[0] + vaddvq_f64(v11) < 0.0;
  v13.f64[0]  = -1.0;
  if (!v12) {
    v13.f64[0]  = 1.0;
  }
  v14.f64[0]  = sqrt(vmulq_f64(v3, v3).f64[0] + vaddvq_f64(vmulq_f64(v4, v4)));
  float64x2_t v15 = vmulq_f64(v8, v8);
  v15.f64[0]  = sqrt(v15.f64[0] + vaddvq_f64(vmulq_f64(v7, v7)));
  v14.f64[1]  = sqrt(vmulq_f64(v5, v5).f64[0] + vaddvq_f64(vmulq_f64(v6, v6)));
  float64x2_t v16 = vmulq_n_f64(v14, v13.f64[0]);
  float64x2_t v17 = vmulq_f64(v15, v13);
  float64x2_t v18 = vdivq_f64(v4, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v16.f64[0], 0));
  float64x2_t v19 = vdivq_f64(v3, v16);
  float64x2_t v20 = (float64x2_t)vdupq_laneq_s64((int64x2_t)v16, 1);
  float64x2_t v21 = vdivq_f64(v6, v20);
  float64x2_t v22 = vdivq_f64(v5, v20);
  float64x2_t v23 = vdivq_f64(v7, (float64x2_t)vdupq_lane_s64(*(uint64_t *)&v17.f64[0], 0));
  float64x2_t v24 = vdivq_f64(v8, v17);
  float64x2_t v25 = vmulq_f64(v22, v19);
  float64x2_t v26 = vmulq_f64(v19, v19);
  v26.f64[0]  = v26.f64[0] + vaddvq_f64(vmulq_f64(v18, v18));
  v25.f64[0]  = (v25.f64[0] + vaddvq_f64(vmulq_f64(v21, v18))) / v26.f64[0];
  float64x2_t v27 = vsubq_f64(v21, vmulq_n_f64(v18, v25.f64[0]));
  float64x2_t v28 = vsubq_f64(v22, vmulq_f64(v19, v25));
  float64x2_t v29 = vmulq_f64(v24, v19);
  v29.f64[0]  = (v29.f64[0] + vaddvq_f64(vmulq_f64(v23, v18))) / v26.f64[0];
  float64x2_t v30 = vmulq_n_f64(v18, v29.f64[0]);
  float64x2_t v31 = vsubq_f64(v24, vmulq_f64(v19, v29));
  float64x2_t v32 = vsubq_f64(v23, v30);
  float64x2_t v33 = vmulq_f64(v23, v27);
  v33.f64[0]  = vmulq_f64(v24, v28).f64[0] + vaddvq_f64(v33);
  float64x2_t v34 = vmulq_f64(v28, v28);
  v34.f64[0]  = v34.f64[0] + vaddvq_f64(vmulq_f64(v27, v27));
  v33.f64[0]  = v33.f64[0] / v34.f64[0];
  float64x2_t v35 = vmulq_f64(v28, v33);
  float64x2_t v36 = vsubq_f64(v32, vmulq_n_f64(v27, v33.f64[0]));
  float64x2_t v37 = vsubq_f64(v31, v35);
  v26.f64[0]  = 1.0 / sqrt(v26.f64[0]);
  v34.f64[0]  = 1.0 / sqrt(v34.f64[0]);
  v11.f64[0]  = 1.0 / sqrt(vmulq_f64(v37, v37).f64[0] + vaddvq_f64(vmulq_f64(v36, v36)));
  float64x2_t v44 = 0u;
  float64x2_t v45 = 0u;
  v43[0]  = vmulq_n_f64(v18, v26.f64[0]);
  v43[1]  = vmulq_f64(v19, v26);
  v43[2]  = vmulq_n_f64(v27, v34.f64[0]);
  v43[3]  = vmulq_f64(v28, v34);
  v43[4]  = vmulq_n_f64(v36, v11.f64[0]);
  v43[5]  = vmulq_f64(v37, v11);
  simd_quaternion((uint64_t)v43, (uint64_t)&v44);
  int64x2_t v38 = (int64x2_t)vornq_s8(vmvnq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v44), (int8x16_t)vcgezq_f64(v44))), vorrq_s8((int8x16_t)vcltzq_f64(v45), (int8x16_t)vcgezq_f64(v45)));
  if ((vorrq_s8((int8x16_t)v38, (int8x16_t)vdupq_laneq_s64(v38, 1)).u64[0] & 0x8000000000000000) != 0)
  {
    float64x2_t v41 = (float64x2_t)SPRotation3DInvalid_0;
    float64x2_t v40 = (float64x2_t)unk_228C22190;
  }
  else
  {
    double v39 = vaddvq_f64(vaddq_f64(vmulq_f64(v44, v44), vmulq_f64(v45, v45)));
    if (v39 == 0.0)
    {
      float64x2_t v40 = (float64x2_t)xmmword_228C1F7A0;
      float64x2_t v41 = 0uLL;
    }
    else
    {
      double v42 = 1.0 / sqrt(v39);
      float64x2_t v40 = vmulq_n_f64(v45, v42);
      float64x2_t v41 = vmulq_n_f64(v44, v42);
    }
  }
  *a2  = v41;
  a2[1]  = v40;
}

float64_t protocol witness for Rotatable3D.rotated(by:) in conformance SPAffineTransform3D@<D0>(float64x2_t *a1@<X8>, float64x2_t a2@<Q0>, float64x2_t a3@<Q1>)
{
  float64x2_t v5 = v3[1];
  float64x2_t v6 = v3[2];
  float64x2_t v7 = v3[3];
  float64x2_t v8 = v3[4];
  float64x2_t v9 = v3[5];
  float64x2_t v10 = v3[6];
  float64x2_t v11 = v3[7];
  v22[0]  = *v3;
  v22[1]  = v5;
  v22[2]  = v6;
  v22[3]  = v7;
  v22[4]  = v8;
  v22[5]  = v9;
  v22[6]  = v10;
  v22[7]  = v11;
  v21[0]  = a2;
  v21[1]  = a3;
  SPAffineTransform3DRotateByQuaternion(v22, v21, v20);
  float64_t result = v20[0].f64[0];
  float64x2_t v13 = v20[1];
  float64x2_t v14 = v20[2];
  float64x2_t v15 = v20[3];
  float64x2_t v16 = v20[4];
  float64x2_t v17 = v20[5];
  float64x2_t v18 = v20[6];
  float64x2_t v19 = v20[7];
  *a1  = v20[0];
  a1[1]  = v13;
  a1[2]  = v14;
  a1[3]  = v15;
  a1[4]  = v16;
  a1[5]  = v17;
  a1[6]  = v18;
  a1[7]  = v19;
  return result;
}

float64_t protocol witness for Scalable3D.scaledBy(x:y:z:) in conformance SPAffineTransform3D@<D0>(float64x2_t *a1@<X8>, unint64_t a2@<D0>, float64_t a3@<D1>, float64x2_t a4@<Q2>)
{
  float64x2_t v6 = v4[1];
  float64x2_t v7 = v4[2];
  float64x2_t v8 = v4[3];
  float64x2_t v9 = v4[4];
  float64x2_t v10 = v4[5];
  float64x2_t v11 = v4[6];
  float64x2_t v12 = v4[7];
  v22[0]  = *v4;
  v22[1]  = v6;
  v22[2]  = v7;
  v22[3]  = v8;
  v22[4]  = v9;
  v22[5]  = v10;
  v22[6]  = v11;
  v22[7]  = v12;
  SPAffineTransform3DScaleBy(v22, v21, a2, a3, a4);
  float64_t result = v21[0].f64[0];
  float64x2_t v14 = v21[1];
  float64x2_t v15 = v21[2];
  float64x2_t v16 = v21[3];
  float64x2_t v17 = v21[4];
  float64x2_t v18 = v21[5];
  float64x2_t v19 = v21[6];
  float64x2_t v20 = v21[7];
  *a1  = v21[0];
  a1[1]  = v14;
  a1[2]  = v15;
  a1[3]  = v16;
  a1[4]  = v17;
  a1[5]  = v18;
  a1[6]  = v19;
  a1[7]  = v20;
  return result;
}

double protocol witness for Scalable3D.scaled(by:) in conformance SPAffineTransform3D@<D0>(_OWORD *a1@<X8>, double a2@<D0>, double a3@<D1>, double a4@<D2>)
{
  return protocol witness for Translatable3D.translated(by:) in conformance SPAffineTransform3D((void (*)(_OWORD *__return_ptr, _OWORD *, void *))SPAffineTransform3DScaleBySize, a1, a2, a3, a4);
}

double protocol witness for Translatable3D.translated(by:) in conformance SPAffineTransform3D@<D0>(void (*a1)(_OWORD *__return_ptr, _OWORD *, void *)@<X2>, _OWORD *a2@<X8>, double a3@<D0>, double a4@<D1>, double a5@<D2>)
{
  long long v7 = v5[1];
  long long v8 = v5[2];
  long long v9 = v5[3];
  long long v10 = v5[4];
  long long v11 = v5[5];
  long long v12 = v5[6];
  long long v13 = v5[7];
  v24[0]  = *v5;
  v24[1]  = v7;
  v24[2]  = v8;
  v24[3]  = v9;
  v24[4]  = v10;
  v24[5]  = v11;
  v24[6]  = v12;
  v24[7]  = v13;
  *(double *)float64x2_t v23 = a3;
  *(double *)&v23[1]  = a4;
  *(double *)&v23[2]  = a5;
  a1(v22, v24, v23);
  double result = *(double *)v22;
  long long v15 = v22[1];
  long long v16 = v22[2];
  long long v17 = v22[3];
  long long v18 = v22[4];
  long long v19 = v22[5];
  long long v20 = v22[6];
  long long v21 = v22[7];
  *a2  = v22[0];
  a2[1]  = v15;
  a2[2]  = v16;
  a2[3]  = v17;
  a2[4]  = v18;
  a2[5]  = v19;
  a2[6]  = v20;
  a2[7]  = v21;
  return result;
}

float64_t protocol witness for Scalable3D.uniformlyScaled(by:) in conformance SPAffineTransform3D@<D0>(float64x2_t *a1@<X8>, float64x2_t a2@<Q0>)
{
  float64x2_t v4 = v2[1];
  float64x2_t v5 = v2[2];
  float64x2_t v6 = v2[3];
  float64x2_t v7 = v2[4];
  float64x2_t v8 = v2[5];
  float64x2_t v9 = v2[6];
  float64x2_t v10 = v2[7];
  v20[0]  = *v2;
  v20[1]  = v4;
  v20[2]  = v5;
  v20[3]  = v6;
  v20[4]  = v7;
  v20[5]  = v8;
  v20[6]  = v9;
  v20[7]  = v10;
  SPAffineTransform3DScaleUniform(v20, v19, a2);
  float64_t result = v19[0].f64[0];
  float64x2_t v12 = v19[1];
  float64x2_t v13 = v19[2];
  float64x2_t v14 = v19[3];
  float64x2_t v15 = v19[4];
  float64x2_t v16 = v19[5];
  float64x2_t v17 = v19[6];
  float64x2_t v18 = v19[7];
  *a1  = v19[0];
  a1[1]  = v12;
  a1[2]  = v13;
  a1[3]  = v14;
  a1[4]  = v15;
  a1[5]  = v16;
  a1[6]  = v17;
  a1[7]  = v18;
  return result;
}

__n128 protocol witness for Shearable3D.sheared(_:) in conformance SPAffineTransform3D@<Q0>(uint64_t a1@<X0>, uint64_t a2@<X8>, float64x2_t a3@<Q0>, float64x2_t a4@<Q1>)
{
  long long v6 = *(_OWORD *)(v4 + 16);
  long long v8 = *(_OWORD *)(v4 + 32);
  long long v7 = *(_OWORD *)(v4 + 48);
  long long v10 = *(_OWORD *)(v4 + 64);
  long long v9 = *(_OWORD *)(v4 + 80);
  long long v12 = *(_OWORD *)(v4 + 96);
  long long v11 = *(_OWORD *)(v4 + 112);
  if (*(unsigned char *)(a1 + 16))
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    if (*(unsigned char *)(a1 + 16) == 1)
    {
      float64x2_t v22 = *(float64x2_t *)v4;
      long long v23 = v6;
      long long v24 = v8;
      long long v25 = v7;
      long long v26 = v10;
      long long v27 = v9;
      long long v28 = v12;
      long long v29 = v11;
      int v13 = 2;
    }
    else
    {
      float64x2_t v22 = *(float64x2_t *)v4;
      long long v23 = v6;
      long long v24 = v8;
      long long v25 = v7;
      long long v26 = v10;
      long long v27 = v9;
      long long v28 = v12;
      long long v29 = v11;
      int v13 = 4;
    }
  }
  else
  {
    a3.f64[0]  = *(float64_t *)a1;
    a4.f64[0]  = *(float64_t *)(a1 + 8);
    float64x2_t v22 = *(float64x2_t *)v4;
    long long v23 = v6;
    long long v24 = v8;
    long long v25 = v7;
    long long v26 = v10;
    long long v27 = v9;
    long long v28 = v12;
    long long v29 = v11;
    int v13 = 1;
  }
  SPAffineTransform3DShear(&v22, v13, v21, a3, a4);
  __n128 result = (__n128)v21[1];
  float64x2_t v16 = v21[2];
  float64x2_t v15 = v21[3];
  float64x2_t v18 = v21[4];
  float64x2_t v17 = v21[5];
  float64x2_t v20 = v21[6];
  float64x2_t v19 = v21[7];
  *(float64x2_t *)a2  = v21[0];
  *(__n128 *)(a2 + 16)  = result;
  *(float64x2_t *)(a2 + 32)  = v16;
  *(float64x2_t *)(a2 + 48)  = v15;
  *(float64x2_t *)(a2 + 64)  = v18;
  *(float64x2_t *)(a2 + 80)  = v17;
  *(float64x2_t *)(a2 + 96)  = v20;
  *(float64x2_t *)(a2 + 112)  = v19;
  return result;
}

unint64_t static SPAffineTransform3D.== infix(_:_:)(float64x2_t *a1, float64x2_t *a2)
{
  int64x2_t v2 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(a1[2], a2[2]), (int8x16_t)vceqq_f64(*a1, *a2)), vandq_s8((int8x16_t)vceqq_f64(a1[4], a2[4]), (int8x16_t)vceqq_f64(a1[6], a2[6])));
  return vandq_s8((int8x16_t)vdupq_laneq_s64(v2, 1), vandq_s8((int8x16_t)vshlq_n_s64((int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(a1[3], a2[3]), (int8x16_t)vceqq_f64(a1[1], a2[1])), vandq_s8((int8x16_t)vceqq_f64(a1[5], a2[5]), (int8x16_t)vceqq_f64(a1[7], a2[7]))), 0x3FuLL), (int8x16_t)v2)).u64[0] >> 63;
}

unint64_t protocol witness for static Equatable.== infix(_:_:) in conformance SPAffineTransform3D(float64x2_t *a1, float64x2_t *a2)
{
  float64x2_t v2 = a1[1];
  float64x2_t v3 = a1[2];
  float64x2_t v4 = a1[3];
  float64x2_t v5 = a1[4];
  float64x2_t v6 = a1[5];
  float64x2_t v7 = a1[6];
  float64x2_t v8 = a1[7];
  float64x2_t v9 = *a2;
  float64x2_t v10 = a2[1];
  float64x2_t v11 = a2[2];
  float64x2_t v12 = a2[3];
  float64x2_t v13 = a2[4];
  float64x2_t v14 = a2[5];
  float64x2_t v15 = a2[6];
  float64x2_t v16 = a2[7];
  v19[0]  = *a1;
  v19[1]  = v2;
  v19[2]  = v3;
  v19[3]  = v4;
  v19[4]  = v5;
  v19[5]  = v6;
  v19[6]  = v7;
  v19[7]  = v8;
  v18[0]  = v9;
  v18[1]  = v10;
  v18[2]  = v11;
  _OWORD v18[3] = v12;
  v18[4]  = v13;
  v18[5]  = v14;
  v18[6]  = v15;
  v18[7]  = v16;
  return simd_equal(v19, v18);
}

void SPAffineTransform3D.hash(into:)()
{
  __n128 v1 = *(__n128 *)(v0 + 32);
  long long v2 = *(_OWORD *)(v0 + 48);
  __n128 v3 = *(__n128 *)(v0 + 64);
  long long v4 = *(_OWORD *)(v0 + 80);
  __n128 v5 = *(__n128 *)(v0 + 96);
  long long v6 = *(_OWORD *)(v0 + 112);
  specialized SIMD.hash(into:)(*(__n128 *)v0, *(double *)(v0 + 16));
  specialized SIMD.hash(into:)(v1, *(double *)&v2);
  specialized SIMD.hash(into:)(v3, *(double *)&v4);

  specialized SIMD.hash(into:)(v5, *(double *)&v6);
}

Swift::Int SPAffineTransform3D.hashValue.getter()
{
  Hasher.init(_seed:)();
  __n128 v2 = *(__n128 *)(v0 + 32);
  long long v3 = *(_OWORD *)(v0 + 48);
  __n128 v4 = *(__n128 *)(v0 + 64);
  long long v5 = *(_OWORD *)(v0 + 80);
  __n128 v6 = *(__n128 *)(v0 + 96);
  long long v7 = *(_OWORD *)(v0 + 112);
  specialized SIMD.hash(into:)(*(__n128 *)v0, *(double *)(v0 + 16));
  specialized SIMD.hash(into:)(v2, *(double *)&v3);
  specialized SIMD.hash(into:)(v4, *(double *)&v5);
  specialized SIMD.hash(into:)(v6, *(double *)&v7);
  return Hasher._finalize()();
}

Swift::Int protocol witness for Hashable.hashValue.getter in conformance SPAffineTransform3D()
{
  __n128 v2 = *v0;
  __n128 v3 = v0[1];
  __n128 v4 = v0[2];
  __n128 v5 = v0[3];
  __n128 v6 = v0[4];
  __n128 v7 = v0[5];
  __n128 v8 = v0[6];
  __n128 v9 = v0[7];
  Hasher.init(_seed:)();
  specialized SIMD.hash(into:)(v2, v3.n128_f64[0]);
  specialized SIMD.hash(into:)(v4, v5.n128_f64[0]);
  specialized SIMD.hash(into:)(v6, v7.n128_f64[0]);
  specialized SIMD.hash(into:)(v8, v9.n128_f64[0]);
  return Hasher._finalize()();
}

void protocol witness for Hashable.hash(into:) in conformance SPAffineTransform3D()
{
  __n128 v1 = *(__n128 *)(v0 + 32);
  long long v2 = *(_OWORD *)(v0 + 48);
  __n128 v3 = *(__n128 *)(v0 + 64);
  long long v4 = *(_OWORD *)(v0 + 80);
  __n128 v5 = *(__n128 *)(v0 + 96);
  long long v6 = *(_OWORD *)(v0 + 112);
  specialized SIMD.hash(into:)(*(__n128 *)v0, *(double *)(v0 + 16));
  specialized SIMD.hash(into:)(v1, *(double *)&v2);
  specialized SIMD.hash(into:)(v3, *(double *)&v4);

  specialized SIMD.hash(into:)(v5, *(double *)&v6);
}

Swift::Int protocol witness for Hashable._rawHashValue(seed:) in conformance SPAffineTransform3D()
{
  __n128 v2 = *v0;
  __n128 v3 = v0[1];
  __n128 v4 = v0[2];
  __n128 v5 = v0[3];
  __n128 v6 = v0[4];
  __n128 v7 = v0[5];
  __n128 v8 = v0[6];
  __n128 v9 = v0[7];
  Hasher.init(_seed:)();
  specialized SIMD.hash(into:)(v2, v3.n128_f64[0]);
  specialized SIMD.hash(into:)(v4, v5.n128_f64[0]);
  specialized SIMD.hash(into:)(v6, v7.n128_f64[0]);
  specialized SIMD.hash(into:)(v8, v9.n128_f64[0]);
  return Hasher._finalize()();
}

unint64_t protocol witness for RawRepresentable.init(rawValue:) in conformance SPAffineTransform3D.CodingKeys@<X0>(Swift::String *a1@<X0>, unsigned char *a2@<X8>)
{
  unint64_t result = specialized SPAffineTransform3D.CodingKeys.init(rawValue:)(*a1);
  *a2  = result;
  return result;
}

unint64_t protocol witness for CodingKey.init(stringValue:) in conformance SPAffineTransform3D.CodingKeys@<X0>(Swift::String a1@<X1:X0>, unsigned char *a2@<X8>)
{
  unint64_t result = specialized SPAffineTransform3D.CodingKeys.init(rawValue:)(a1);
  *a2  = result;
  return result;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPAffineTransform3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys();

  return MEMORY[0x270FA00B0](a1, v2);
}

uint64_t protocol witness for CustomDebugStringConvertible.debugDescription.getter in conformance SPAffineTransform3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys();

  return MEMORY[0x270FA00B8](a1, v2);
}

uint64_t SPAffineTransform3D.encode(to:)(void *a1)
{
  __n128 v3 = v1;
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedEncodingContainer<SPAffineTransform3D.CodingKeys>);
  uint64_t v6 = *(void *)(v5 - 8);
  MEMORY[0x270FA5388](v5);
  __n128 v8 = (char *)&v15 - ((v7 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys();
  dispatch thunk of Encoder.container<A>(keyedBy:)();
  long long v9 = *v3;
  long long v10 = v3[1];
  long long v11 = v3[2];
  long long v19 = v3[3];
  long long v20 = v11;
  long long v12 = v3[4];
  long long v17 = v3[5];
  long long v18 = v12;
  long long v13 = v3[6];
  long long v15 = v3[7];
  long long v16 = v13;
  long long v22 = v9;
  long long v23 = v10;
  char v21 = 0;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for SIMD3<Double>);
  lazy protocol witness table accessor for type SIMD3<Double> and conformance SIMD3<A>(&lazy protocol witness table cache variable for type SIMD3<Double> and conformance SIMD3<A>);
  KeyedEncodingContainer.encode<A>(_:forKey:)();
  if (!v2)
  {
    long long v22 = v20;
    long long v23 = v19;
    char v21 = 1;
    KeyedEncodingContainer.encode<A>(_:forKey:)();
    long long v22 = v18;
    long long v23 = v17;
    char v21 = 2;
    KeyedEncodingContainer.encode<A>(_:forKey:)();
    long long v22 = v16;
    long long v23 = v15;
    char v21 = 3;
    KeyedEncodingContainer.encode<A>(_:forKey:)();
  }
  return (*(uint64_t (**)(char *, uint64_t))(v6 + 8))(v8, v5);
}

double SPAffineTransform3D.init(from:)@<D0>(void *a1@<X0>, _OWORD *a2@<X8>)
{
  specialized SPAffineTransform3D.init(from:)(a1, v9);
  if (!v2)
  {
    long long v5 = v13;
    a2[4]  = v12;
    a2[5]  = v5;
    long long v6 = v15;
    a2[6]  = v14;
    a2[7]  = v6;
    long long v7 = v9[1];
    *a2  = v9[0];
    a2[1]  = v7;
    double result = *(double *)&v10;
    long long v8 = v11;
    a2[2]  = v10;
    a2[3]  = v8;
  }
  return result;
}

double protocol witness for Decodable.init(from:) in conformance SPAffineTransform3D@<D0>(void *a1@<X0>, _OWORD *a2@<X8>)
{
  specialized SPAffineTransform3D.init(from:)(a1, v9);
  if (!v2)
  {
    long long v5 = v13;
    a2[4]  = v12;
    a2[5]  = v5;
    long long v6 = v15;
    a2[6]  = v14;
    a2[7]  = v6;
    long long v7 = v9[1];
    *a2  = v9[0];
    a2[1]  = v7;
    double result = *(double *)&v10;
    long long v8 = v11;
    a2[2]  = v10;
    a2[3]  = v8;
  }
  return result;
}

uint64_t protocol witness for Encodable.encode(to:) in conformance SPAffineTransform3D(void *a1)
{
  return SPAffineTransform3D.encode(to:)(a1);
}

void __swiftcall SPAffineTransform3D.changeBasis(from:to:)(SPAffineTransform3D_optional *__return_ptr retstr, SPAffineTransform3D *from, SPAffineTransform3D *to)
{
  long long v5 = *(_OWORD *)to->matrix.columns[0].f64;
  long long v6 = *(_OWORD *)&to->matrix.columns[0].f64[2];
  long long v7 = *(_OWORD *)to->matrix.columns[1].f64;
  long long v8 = *(_OWORD *)&to->matrix.columns[1].f64[2];
  long long v9 = *(_OWORD *)to->matrix.columns[2].f64;
  long long v10 = *(_OWORD *)&to->matrix.columns[2].f64[2];
  long long v11 = *(_OWORD *)to->matrix.columns[3].f64;
  long long v12 = *(_OWORD *)&to->matrix.columns[3].f64[2];
  float64x2_t v13 = *(float64x2_t *)from->matrix.columns[0].f64;
  long long v14 = *(_OWORD *)&from->matrix.columns[0].f64[2];
  float64x2_t v15 = *(float64x2_t *)from->matrix.columns[1].f64;
  long long v16 = *(_OWORD *)&from->matrix.columns[1].f64[2];
  long long v17 = *(_OWORD *)from->matrix.columns[2].f64;
  long long v18 = *(_OWORD *)&from->matrix.columns[2].f64[2];
  long long v19 = *(_OWORD *)from->matrix.columns[3].f64;
  long long v20 = *(_OWORD *)&from->matrix.columns[3].f64[2];
  long long v21 = *(_OWORD *)(v3 + 16);
  float64x2_t v22 = *(float64x2_t *)(v3 + 32);
  long long v23 = *(_OWORD *)(v3 + 48);
  long long v24 = *(_OWORD *)(v3 + 64);
  long long v25 = *(_OWORD *)(v3 + 80);
  long long v26 = *(_OWORD *)(v3 + 96);
  long long v27 = *(_OWORD *)(v3 + 112);
  float64x2_t v53 = *(float64x2_t *)v3;
  long long v54 = v21;
  float64x2_t v55 = v22;
  long long v56 = v23;
  long long v57 = v24;
  long long v58 = v25;
  long long v59 = v26;
  long long v60 = v27;
  float64x2_t v36 = v13;
  long long v37 = v14;
  float64x2_t v38 = v15;
  long long v39 = v16;
  long long v40 = v17;
  long long v41 = v18;
  long long v42 = v19;
  long long v43 = v20;
  v52[0]  = v5;
  v52[1]  = v6;
  v52[2]  = v7;
  v52[3]  = v8;
  v52[4]  = v9;
  v52[5]  = v10;
  v52[6]  = v11;
  v52[7]  = v12;
  SPAffineTransform3DChangeBasis((uint64_t)&v53, (uint64_t)&v36, (uint64_t)v52, &v44);
  long long v34 = v47;
  long long v35 = v45;
  float64x2_t v53 = v44;
  long long v54 = v45;
  float64x2_t v30 = v46;
  float64x2_t v31 = v44;
  float64x2_t v55 = v46;
  long long v56 = v47;
  long long v32 = v51;
  long long v33 = v49;
  long long v57 = v48;
  long long v58 = v49;
  long long v28 = v50;
  long long v29 = v48;
  long long v59 = v50;
  long long v60 = v51;
  if (SPAffineTransform3DIsValid(&v53, *(double *)&v45, *(double *)&v47, *(double *)&v49, *(double *)&v51, v44.f64[0], v46))
  {
    float64x2_t v36 = v31;
    long long v37 = v35;
    float64x2_t v38 = v30;
    long long v39 = v34;
    long long v40 = v29;
    long long v41 = v33;
    long long v42 = v28;
    long long v43 = v32;
    _sSo19SPAffineTransform3DaSgWOi_((uint64_t)&v36);
  }
  else
  {
    _sSo19SPAffineTransform3DaSgWOi0_((uint64_t)&v36);
  }
  outlined init with take of SPAffineTransform3D?((uint64_t)&v36, (uint64_t)&v53);
  outlined init with take of SPAffineTransform3D?((uint64_t)&v53, (uint64_t)retstr);
}

uint64_t SPAffineTransform3D.description.getter()
{
  v0._countAndFlagsBits  = 0x3A78697274616D28;
  v0._object  = (void *)0xE900000000000020;
  String.append(_:)(v0);
  type metadata accessor for simd_double4x3(0);
  _print_unlocked<A, B>(_:_:)();
  v1._countAndFlagsBits  = 41;
  v1._object  = (void *)0xE100000000000000;
  String.append(_:)(v1);
  return 0;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPAffineTransform3D()
{
  v0._countAndFlagsBits  = 0x3A78697274616D28;
  v0._object  = (void *)0xE900000000000020;
  String.append(_:)(v0);
  type metadata accessor for simd_double4x3(0);
  _print_unlocked<A, B>(_:_:)();
  v1._countAndFlagsBits  = 41;
  v1._object  = (void *)0xE100000000000000;
  String.append(_:)(v1);
  return 0;
}

uint64_t SPAffineTransform3D.customMirror.getter()
{
  uint64_t v1 = type metadata accessor for Mirror.AncestorRepresentation();
  uint64_t v2 = *(void *)(v1 - 8);
  MEMORY[0x270FA5388](v1);
  __n128 v4 = (char *)v23 - ((v3 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName((uint64_t *)&demangling cache variable for type metadata for Mirror.DisplayStyle?);
  MEMORY[0x270FA5388](v5 - 8);
  long long v7 = (char *)v23 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  long long v8 = v0[5];
  v23[4]  = v0[4];
  v23[5]  = v8;
  long long v9 = v0[7];
  v23[6]  = v0[6];
  v23[7]  = v9;
  long long v10 = v0[1];
  v23[0]  = *v0;
  v23[1]  = v10;
  long long v11 = v0[3];
  v23[2]  = v0[2];
  v23[3]  = v11;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<(String, Any)>);
  uint64_t v12 = swift_allocObject();
  *(_OWORD *)(v12 + 16)  = xmmword_228C20870;
  *(void *)(v12 + 32)  = 0x78697274616DLL;
  *(void *)(v12 + 40)  = 0xE600000000000000;
  type metadata accessor for simd_double4x3(0);
  *(void *)(v12 + 72)  = v13;
  long long v14 = (_OWORD *)swift_allocObject();
  *(void *)(v12 + 48)  = v14;
  long long v15 = v0[5];
  v14[5]  = v0[4];
  v14[6]  = v15;
  long long v16 = v0[7];
  v14[7]  = v0[6];
  v14[8]  = v16;
  long long v17 = v0[1];
  v14[1]  = *v0;
  v14[2]  = v17;
  long long v18 = v0[3];
  v14[3]  = v0[2];
  v14[4]  = v18;
  uint64_t v19 = *MEMORY[0x263F8E808];
  uint64_t v20 = type metadata accessor for Mirror.DisplayStyle();
  uint64_t v21 = *(void *)(v20 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v21 + 104))(v7, v19, v20);
  (*(void (**)(char *, void, uint64_t, uint64_t))(v21 + 56))(v7, 0, 1, v20);
  (*(void (**)(char *, void, uint64_t))(v2 + 104))(v4, *MEMORY[0x263F8E830], v1);
  type metadata accessor for SPAffineTransform3D(0);
  return Mirror.init<A>(_:children:displayStyle:ancestorRepresentation:)();
}

unint64_t lazy protocol witness table accessor for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys()
{
  unint64_t result = lazy protocol witness table cache variable for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys);
  }
  return result;
}

unint64_t specialized SPAffineTransform3D.CodingKeys.init(rawValue:)(Swift::String string)
{
  object  = string._object;
  v2._countAndFlagsBits  = string._countAndFlagsBits;
  v2._object  = object;
  unint64_t v3 = _findStringSwitchCase(cases:string:)((Swift::OpaquePointer)&outlined read-only object #0 of SPAffineTransform3D.CodingKeys.init(rawValue:), v2);
  swift_bridgeObjectRelease();
  if (v3 >= 4) {
    return 4;
  }
  else {
    return v3;
  }
}

uint64_t specialized SPAffineTransform3D.init(from:)@<X0>(void *a1@<X0>, _OWORD *a2@<X8>)
{
  uint64_t v5 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedDecodingContainer<SPAffineTransform3D.CodingKeys>);
  uint64_t v6 = *(void *)(v5 - 8);
  MEMORY[0x270FA5388](v5);
  long long v8 = (char *)&v20 - ((v7 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPAffineTransform3D.CodingKeys and conformance SPAffineTransform3D.CodingKeys();
  dispatch thunk of Decoder.container<A>(keyedBy:)();
  if (v2) {
    return __swift_destroy_boxed_opaque_existential_1(a1);
  }
  long long v28 = a2;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<SIMD3<Double>>);
  long long v9 = (_OWORD *)swift_allocObject();
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for SIMD3<Double>);
  char v32 = 0;
  lazy protocol witness table accessor for type SIMD3<Double> and conformance SIMD3<A>(&lazy protocol witness table cache variable for type SIMD3<Double> and conformance SIMD3<A>);
  KeyedDecodingContainer.decode<A>(_:forKey:)();
  char v31 = 1;
  KeyedDecodingContainer.decode<A>(_:forKey:)();
  char v30 = 2;
  KeyedDecodingContainer.decode<A>(_:forKey:)();
  char v29 = 3;
  KeyedDecodingContainer.decode<A>(_:forKey:)();
  long long v10 = v9[2];
  long long v11 = v9[3];
  long long v26 = v9[4];
  long long v27 = v10;
  long long v22 = v9[5];
  long long v23 = v11;
  long long v12 = v9[6];
  long long v13 = v9[7];
  long long v24 = v9[8];
  long long v25 = v12;
  long long v20 = v9[9];
  long long v21 = v13;
  swift_setDeallocating();
  swift_deallocClassInstance();
  (*(void (**)(char *, uint64_t))(v6 + 8))(v8, v5);
  uint64_t result = __swift_destroy_boxed_opaque_existential_1(a1);
  long long v15 = v28;
  long long v16 = v23;
  *long long v28 = v27;
  v15[1]  = v16;
  long long v17 = v22;
  v15[2]  = v26;
  v15[3]  = v17;
  long long v18 = v21;
  v15[4]  = v25;
  v15[5]  = v18;
  long long v19 = v20;
  v15[6]  = v24;
  v15[7]  = v19;
  return result;
}

uint64_t SPAffineTransform3DChangeBasis@<X0>(uint64_t result@<X0>, uint64_t a2@<X1>, uint64_t a3@<X2>, float64x2_t *a4@<X8>)
{
  uint64_t v6 = (float64x2_t *)result;
  float64x2_t v8 = *(float64x2_t *)a2;
  float64x2_t v9 = *(float64x2_t *)(a2 + 16);
  float64x2_t v10 = *(float64x2_t *)(a2 + 32);
  int8x16_t v11 = *(int8x16_t *)(a2 + 48);
  float64x2_t v12 = *(float64x2_t *)(a2 + 64);
  float64x2_t v13 = *(float64x2_t *)(a2 + 80);
  v14.f64[0]  = *(float64_t *)(a2 + 80);
  v14.f64[1]  = *(float64_t *)(a2 + 64);
  v15.f64[0]  = *(float64_t *)(a2 + 48);
  v15.f64[1]  = *(float64_t *)(a2 + 32);
  float64x2_t v16 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  if (vmulq_f64(v9, vmlaq_laneq_f64(vmulq_f64(v12, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v10, 1))), v10, v12, 1)).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)a2, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v12, (int8x16_t)v13, 8uLL), vnegq_f64(v15)), v14, (float64x2_t)vextq_s8((int8x16_t)v10, v11, 8uLL)))) == 0.0)
  {
    float64x2_t v17 = v16;
    float64x2_t v18 = v16;
    float64x2_t v19 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v21 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v20 = v16;
    float64x2_t v22 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v24 = v16;
    float64x2_t v23 = (float64x2_t)xmmword_228C1FC60;
  }
  else
  {
    v151  = *(float64x2_t *)(a2 + 96);
    v153  = *(float64x2_t *)(a2 + 112);
    v161  = 0u;
    v162  = 0u;
    v159  = 0u;
    v160  = 0u;
    v157  = 0u;
    v158  = 0u;
    v165  = v8;
    v166  = v9;
    v167  = v10;
    v168  = (float64x2_t)v11;
    v169  = v12;
    v170  = v13;
    v155  = v16;
    uint64_t result = __invert_d3();
    float64x2_t v16 = v155;
    float64x2_t v17 = 0u;
    float64x2_t v19 = 0u;
    float64x2_t v18 = 0u;
    float64x2_t v21 = 0u;
    float64x2_t v20 = 0u;
    float64x2_t v22 = 0u;
    float64x2_t v23 = vnegq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v151, (float64x2_t)0), (float64x2_t)0, v151, 1), v153, (float64x2_t)0));
    float64x2_t v24 = vnegq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v151.f64[0]), (float64x2_t)0, v151, 1), (float64x2_t)0, v153.f64[0]));
  }
  v12.f64[0]  = INFINITY;
  int64x2_t v25 = (int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v17), (int8x16_t)vcgezq_f64(v17)), (int8x16_t)vceqq_f64(vabsq_f64(v17), v16));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v25, 1), vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v19), (int8x16_t)vcgezq_f64(v19)), (int8x16_t)vceqq_f64(vabsq_f64(v19), v12)), (int8x16_t)v25)).u64[0] & 0x8000000000000000) == 0)goto LABEL_15; {
  float64x2_t v26 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  }
  v13.f64[0]  = INFINITY;
  int64x2_t v27 = (int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v18), (int8x16_t)vcgezq_f64(v18)), (int8x16_t)vceqq_f64(vabsq_f64(v18), v26));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v27, 1), vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v21), (int8x16_t)vcgezq_f64(v21)), (int8x16_t)vceqq_f64(vabsq_f64(v21), v13)), (int8x16_t)v27)).u64[0] & 0x8000000000000000) == 0)goto LABEL_15; {
  float64x2_t v29 = vabsq_f64(v20);
  }
  int8x16_t v28 = (int8x16_t)vceqq_f64(v29, v26);
  v29.f64[0]  = INFINITY;
  int64x2_t v30 = (int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v20), (int8x16_t)vcgezq_f64(v20)), v28);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v30, 1), vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v22), (int8x16_t)vcgezq_f64(v22)), (int8x16_t)vceqq_f64(vabsq_f64(v22), v29)), (int8x16_t)v30)).u64[0] & 0x8000000000000000) == 0)goto LABEL_15; {
  float64x2_t v31 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  }
  v29.f64[0]  = INFINITY;
  int64x2_t v32 = (int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v24), (int8x16_t)vcgezq_f64(v24)), (int8x16_t)vceqq_f64(vabsq_f64(v24), v31));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v32, 1), vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v23), (int8x16_t)vcgezq_f64(v23)), (int8x16_t)vceqq_f64(vabsq_f64(v23), v29)), (int8x16_t)v32)).u64[0] & 0x8000000000000000) == 0)goto LABEL_15; {
  v144  = v24;
  }
  v145  = v23;
  v146  = v19;
  v147  = v21;
  v148  = v22;
  v149  = v20;
  v150  = v17;
  v152  = v18;
  float64x2_t v33 = *(float64x2_t *)a3;
  float64x2_t v34 = *(float64x2_t *)(a3 + 16);
  float64x2_t v35 = *(float64x2_t *)(a3 + 32);
  int8x16_t v36 = *(int8x16_t *)(a3 + 48);
  float64x2_t v37 = *(float64x2_t *)(a3 + 64);
  float64x2_t v38 = *(float64x2_t *)(a3 + 80);
  v39.f64[0]  = *(float64_t *)(a3 + 80);
  v39.f64[1]  = *(float64_t *)(a3 + 64);
  v40.f64[0]  = *(float64_t *)(a3 + 48);
  v40.f64[1]  = *(float64_t *)(a3 + 32);
  if (vmulq_f64(v34, vmlaq_laneq_f64(vmulq_f64(v37, vnegq_f64((float64x2_t)vdupq_laneq_s64((int64x2_t)v35, 1))), v35, v37, 1)).f64[0]+ vaddvq_f64(vmulq_f64(*(float64x2_t *)a3, vmlaq_f64(vmulq_f64((float64x2_t)vextq_s8((int8x16_t)v37, (int8x16_t)v38, 8uLL), vnegq_f64(v40)), v39, (float64x2_t)vextq_s8((int8x16_t)v35, v36, 8uLL)))) == 0.0)
  {
    float64x2_t v42 = v31;
    float64x2_t v41 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v43 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v44 = v31;
    float64x2_t v45 = (float64x2_t)xmmword_228C1FC60;
    float64x2_t v47 = v31;
    float64x2_t v46 = (float64x2_t)xmmword_228C1FC60;
  }
  else
  {
    v154  = *(float64x2_t *)(a3 + 96);
    v156  = *(float64x2_t *)(a3 + 112);
    v161  = 0u;
    v162  = 0u;
    v159  = 0u;
    v160  = 0u;
    v157  = 0u;
    v158  = 0u;
    v165  = v33;
    v166  = v34;
    v167  = v35;
    v168  = (float64x2_t)v36;
    v169  = v37;
    v170  = v38;
    uint64_t result = __invert_d3();
    float64x2_t v31 = 0u;
    float64x2_t v41 = 0u;
    float64x2_t v42 = 0u;
    float64x2_t v43 = 0u;
    float64x2_t v44 = 0u;
    float64x2_t v45 = 0u;
    float64x2_t v46 = vnegq_f64(vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v154, (float64x2_t)0), (float64x2_t)0, v154, 1), v156, (float64x2_t)0));
    float64x2_t v47 = vnegq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64((float64x2_t)0, v154.f64[0]), (float64x2_t)0, v154, 1), (float64x2_t)0, v156.f64[0]));
  }
  float64x2_t v48 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL);
  v38.f64[0]  = INFINITY;
  int64x2_t v49 = (int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v31), (int8x16_t)vcgezq_f64(v31)), (int8x16_t)vceqq_f64(vabsq_f64(v31), v48));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v49, 1), vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v41), (int8x16_t)vcgezq_f64(v41)), (int8x16_t)vceqq_f64(vabsq_f64(v41), v38)), (int8x16_t)v49)).u64[0] & 0x8000000000000000) == 0)goto LABEL_15; {
  float64x2_t v50 = vabsq_f64(v42);
  }
  int8x16_t v51 = (int8x16_t)vceqq_f64(v50, v48);
  v50.f64[0]  = INFINITY;
  int64x2_t v52 = (int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v42), (int8x16_t)vcgezq_f64(v42)), v51);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v52, 1), vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v43), (int8x16_t)vcgezq_f64(v43)), (int8x16_t)vceqq_f64(vabsq_f64(v43), v50)), (int8x16_t)v52)).u64[0] & 0x8000000000000000) == 0|| (float64x2_t v53 = (float64x2_t)vdupq_n_s64(0x7FF0000000000000uLL), v38.f64[0] = INFINITY, v54 = (int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v44), (int8x16_t)vcgezq_f64(v44)), (int8x16_t)vceqq_f64(vabsq_f64(v44), v53)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v54, 1), vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v45), (int8x16_t)vcgezq_f64(v45)), (int8x16_t)vceqq_f64(vabsq_f64(v45), v38)), (int8x16_t)v54)).u64[0] & 0x8000000000000000) == 0)|| (v55 = vabsq_f64(v47),
        int8x16_t v56 = (int8x16_t)vceqq_f64(v55, v53),
        v55.f64[0]  = INFINITY,
        int64x2_t v57 = (int64x2_t)vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v47), (int8x16_t)vcgezq_f64(v47)), v56),
        (vandq_s8((int8x16_t)vdupq_laneq_s64(v57, 1), vandq_s8(vbicq_s8(vorrq_s8((int8x16_t)vcltzq_f64(v46), (int8x16_t)vcgezq_f64(v46)), (int8x16_t)vceqq_f64(vabsq_f64(v46), v55)), (int8x16_t)v57)).u64[0] & 0x8000000000000000) == 0))
  {
LABEL_15:
    a4[4]  = (float64x2_t)xmmword_228C22140;
    a4[5]  = (float64x2_t)unk_228C22150;
    a4[6]  = (float64x2_t)xmmword_228C22160;
    a4[7]  = (float64x2_t)unk_228C22170;
    *a4  = (float64x2_t)SPAffineTransform3DInvalid_0;
    a4[1]  = (float64x2_t)unk_228C22110;
    a4[2]  = (float64x2_t)xmmword_228C22120;
    a4[3]  = (float64x2_t)unk_228C22130;
    return result;
  }
  a4[6]  = 0u;
  a4[7]  = 0u;
  a4[4]  = 0u;
  a4[5]  = 0u;
  a4[2]  = 0u;
  a4[3]  = 0u;
  *a4  = 0u;
  a4[1]  = 0u;
  float64x2_t v59 = *(float64x2_t *)a2;
  float64x2_t v58 = *(float64x2_t *)(a2 + 16);
  float64x2_t v61 = *(float64x2_t *)(a2 + 32);
  float64x2_t v60 = *(float64x2_t *)(a2 + 48);
  float64x2_t v63 = *(float64x2_t *)(a2 + 64);
  float64x2_t v62 = *(float64x2_t *)(a2 + 80);
  v65  = *(float64x2_t *)(a2 + 96);
  v64  = *(float64x2_t *)(a2 + 112);
  float64x2_t v67 = *(float64x2_t *)MEMORY[0x263EF8988];
  float64x2_t v66 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v69 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v68 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v71 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v70 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v72 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v69, v61), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], *(float64x2_t *)a2)), (int8x16_t)vceqq_f64(v71, v63));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v72, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v68, v60), (int8x16_t)vceqq_f64(v66, v58)), (int8x16_t)vceqq_f64(v70, v62)), (int8x16_t)v72)).u64[0] & 0x8000000000000000) != 0&& (v73  = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v69, v42), (int8x16_t)vceqq_f64(v67, v31)), (int8x16_t)vceqq_f64(v71, v44)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v73, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v68, v43), (int8x16_t)vceqq_f64(v66, v41)), (int8x16_t)vceqq_f64(v70, v45)), (int8x16_t)v73)).u64[0] & 0x8000000000000000) != 0))
  {
    v65  = vaddq_f64(v47, v65);
    v64  = vaddq_f64(v46, v64);
  }
  else
  {
    v74  = vceqzq_f64(v65);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v74, 1), vandq_s8((int8x16_t)vceqzq_f64(v64), (int8x16_t)v74)).u64[0] & 0x8000000000000000) != 0
      && (v75.f64[0]  = v47.f64[0],
          *(void *)&v75.f64[1]  = vextq_s8((int8x16_t)v47, (int8x16_t)v47, 8uLL).u64[0],
          v76  = vceqzq_f64(v75),
          (vandq_s8((int8x16_t)vdupq_laneq_s64(v76, 1), vandq_s8((int8x16_t)vceqzq_f64(v46), (int8x16_t)v76)).u64[0] & 0x8000000000000000) != 0))
    {
      v117  = 0;
      v157  = v31;
      v158  = v41;
      v159  = v42;
      v160  = v43;
      v161  = v44;
      v162  = v45;
      v165  = 0u;
      v166  = 0u;
      v167  = 0u;
      v168  = 0u;
      v169  = 0u;
      v170  = 0u;
      *(void *)&v59.f64[1]  = vextq_s8((int8x16_t)v59, (int8x16_t)v59, 8uLL).u64[0];
      *(void *)&v61.f64[1]  = vextq_s8((int8x16_t)v61, (int8x16_t)v61, 8uLL).u64[0];
      *(void *)&v63.f64[1]  = vextq_s8((int8x16_t)v63, (int8x16_t)v63, 8uLL).u64[0];
      do
      {
        v119  = *(float64x2_t *)((char *)&v157 + v117);
        v118  = *(float64x2_t *)((char *)&v157 + v117 + 16);
        v120  = (float64x2_t *)((char *)&v165 + v117);
        *v120  = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v59, v119.f64[0]), v61, v119, 1), v63, v118.f64[0]);
        v120[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v58, v119), v60, v119, 1), v118, v62);
        v117 += 32;
      }
      while (v117 != 96);
      float64x2_t v59 = v165;
      float64x2_t v58 = v166;
      float64x2_t v61 = v167;
      float64x2_t v60 = v168;
      float64x2_t v63 = v169;
      float64x2_t v62 = v170;
      *a4  = v165;
      a4[1]  = v58;
    }
    else
    {
      v77  = 0;
      v58.f64[1]  = 0.0;
      v60.f64[1]  = 0.0;
      v62.f64[1]  = 0.0;
      v64.f64[1]  = 1.0;
      v46.f64[1]  = 1.0;
      v157  = v31;
      v158  = (float64x2_t)*(unint64_t *)&v41.f64[0];
      v159  = v42;
      v160  = (float64x2_t)*(unint64_t *)&v43.f64[0];
      v161  = v44;
      v162  = (float64x2_t)*(unint64_t *)&v45.f64[0];
      v163  = v47;
      v164  = v46;
      v165  = 0u;
      v166  = 0u;
      v167  = 0u;
      v168  = 0u;
      v169  = 0u;
      v170  = 0u;
      v171  = 0u;
      v172  = 0u;
      do
      {
        v79  = *(float64x2_t *)((char *)&v157 + v77);
        v78  = *(float64x2_t *)((char *)&v157 + v77 + 16);
        v80  = (float64x2_t *)((char *)&v165 + v77);
        *v80  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v59, v79.f64[0]), v61, v79, 1), v63, v78.f64[0]), v65, v78, 1);
        v80[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v58, v79.f64[0]), v60, v79, 1), v62, v78.f64[0]), v64, v78, 1);
        v77 += 32;
      }
      while (v77 != 128);
      float64x2_t v59 = v165;
      float64x2_t v58 = v166;
      float64x2_t v61 = v167;
      float64x2_t v60 = v168;
      float64x2_t v63 = v169;
      float64x2_t v62 = v170;
      v65  = v171;
      v64  = v172;
      *a4  = v165;
      a4[1]  = v58;
      a4[2]  = v61;
      a4[3]  = v60;
    }
  }
  v81  = v6[1];
  v83  = v6[2];
  v82  = v6[3];
  v85  = v6[4];
  v84  = v6[5];
  v87  = v6[6];
  v86  = v6[7];
  v88  = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v69, v61), (int8x16_t)vceqq_f64(v67, v59)), (int8x16_t)vceqq_f64(v71, v63));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v88, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v68, v60), (int8x16_t)vceqq_f64(v66, v58)), (int8x16_t)vceqq_f64(v70, v62)), (int8x16_t)v88)).u64[0] & 0x8000000000000000) != 0&& (v89  = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v69, v83), (int8x16_t)vceqq_f64(v67, *v6)), (int8x16_t)vceqq_f64(v71, v85)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v89, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v68, v82), (int8x16_t)vceqq_f64(v66, v81)), (int8x16_t)vceqq_f64(v70, v84)), (int8x16_t)v89)).u64[0] & 0x8000000000000000) != 0))
  {
    v65  = vaddq_f64(v65, v87);
    v64  = vaddq_f64(v64, v86);
  }
  else
  {
    v90  = vceqzq_f64(v65);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v90, 1), vandq_s8((int8x16_t)vceqzq_f64(v64), (int8x16_t)v90)).u64[0] & 0x8000000000000000) != 0
      && (v91  = vceqzq_f64(v87),
          (vandq_s8((int8x16_t)vdupq_laneq_s64(v91, 1), vandq_s8((int8x16_t)vceqzq_f64(v86), (int8x16_t)v91)).u64[0] & 0x8000000000000000) != 0))
    {
      v121  = 0;
      v157  = *v6;
      v158  = v81;
      v159  = v83;
      v160  = v82;
      v161  = v85;
      v162  = v84;
      v165  = 0u;
      v166  = 0u;
      v167  = 0u;
      v168  = 0u;
      v169  = 0u;
      v170  = 0u;
      *(void *)&v59.f64[1]  = vextq_s8((int8x16_t)v59, (int8x16_t)v59, 8uLL).u64[0];
      *(void *)&v61.f64[1]  = vextq_s8((int8x16_t)v61, (int8x16_t)v61, 8uLL).u64[0];
      *(void *)&v63.f64[1]  = vextq_s8((int8x16_t)v63, (int8x16_t)v63, 8uLL).u64[0];
      do
      {
        v123  = *(float64x2_t *)((char *)&v157 + v121);
        v122  = *(float64x2_t *)((char *)&v157 + v121 + 16);
        v124  = (float64x2_t *)((char *)&v165 + v121);
        *v124  = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v59, v123.f64[0]), v61, v123, 1), v63, v122.f64[0]);
        v124[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v58, v123), v60, v123, 1), v122, v62);
        v121 += 32;
      }
      while (v121 != 96);
      float64x2_t v59 = v165;
      float64x2_t v58 = v166;
      float64x2_t v61 = v167;
      float64x2_t v60 = v168;
      float64x2_t v63 = v169;
      float64x2_t v62 = v170;
    }
    else
    {
      v92  = 0;
      v58.f64[1]  = 0.0;
      v60.f64[1]  = 0.0;
      v62.f64[1]  = 0.0;
      v64.f64[1]  = 1.0;
      v86.f64[1]  = 1.0;
      v157  = *v6;
      v158  = (float64x2_t)*(unint64_t *)&v81.f64[0];
      v159  = v83;
      v160  = (float64x2_t)*(unint64_t *)&v82.f64[0];
      v161  = v85;
      v162  = (float64x2_t)*(unint64_t *)&v84.f64[0];
      v163  = v87;
      v164  = v86;
      v165  = 0u;
      v166  = 0u;
      v167  = 0u;
      v168  = 0u;
      v169  = 0u;
      v170  = 0u;
      v171  = 0u;
      v172  = 0u;
      do
      {
        v94  = *(float64x2_t *)((char *)&v157 + v92);
        v93  = *(float64x2_t *)((char *)&v157 + v92 + 16);
        v95  = (float64x2_t *)((char *)&v165 + v92);
        *v95  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v59, v94.f64[0]), v61, v94, 1), v63, v93.f64[0]), v65, v93, 1);
        v95[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v58, v94.f64[0]), v60, v94, 1), v62, v93.f64[0]), v64, v93, 1);
        v92 += 32;
      }
      while (v92 != 128);
      float64x2_t v59 = v165;
      float64x2_t v58 = v166;
      float64x2_t v61 = v167;
      float64x2_t v60 = v168;
      float64x2_t v63 = v169;
      float64x2_t v62 = v170;
      v65  = v171;
      v64  = v172;
    }
  }
  *a4  = v59;
  a4[1]  = v58;
  a4[2]  = v61;
  a4[3]  = v60;
  a4[4]  = v63;
  a4[5]  = v62;
  a4[6]  = v65;
  a4[7]  = v64;
  v96  = *(float64x2_t *)(a3 + 16);
  v98  = *(float64x2_t *)(a3 + 32);
  v97  = *(float64x2_t *)(a3 + 48);
  v100  = *(float64x2_t *)(a3 + 64);
  v99  = *(float64x2_t *)(a3 + 80);
  v102  = *(float64x2_t *)(a3 + 96);
  v101  = *(float64x2_t *)(a3 + 112);
  v103  = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v69, v61), (int8x16_t)vceqq_f64(v67, v59)), (int8x16_t)vceqq_f64(v71, v63));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v103, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v68, v60), (int8x16_t)vceqq_f64(v66, v58)), (int8x16_t)vceqq_f64(v70, v62)), (int8x16_t)v103)).u64[0] & 0x8000000000000000) != 0&& (v104  = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v69, v98), (int8x16_t)vceqq_f64(v67, *(float64x2_t *)a3)), (int8x16_t)vceqq_f64(v71, v100)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v104, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v68, v97), (int8x16_t)vceqq_f64(v66, v96)), (int8x16_t)vceqq_f64(v70, v99)), (int8x16_t)v104)).u64[0] & 0x8000000000000000) != 0))
  {
    v65  = vaddq_f64(v65, v102);
    v64  = vaddq_f64(v64, v101);
  }
  else
  {
    v105  = vceqzq_f64(v65);
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v105, 1), vandq_s8((int8x16_t)vceqzq_f64(v64), (int8x16_t)v105)).u64[0] & 0x8000000000000000) == 0
      || (v106  = vceqzq_f64(v102),
          (vandq_s8((int8x16_t)vdupq_laneq_s64(v106, 1), vandq_s8((int8x16_t)vceqzq_f64(v101), (int8x16_t)v106)).u64[0] & 0x8000000000000000) == 0))
    {
      v107  = 0;
      v58.f64[1]  = 0.0;
      v60.f64[1]  = 0.0;
      v62.f64[1]  = 0.0;
      v64.f64[1]  = 1.0;
      v101.f64[1]  = 1.0;
      v157  = *(float64x2_t *)a3;
      v158  = (float64x2_t)*(unint64_t *)&v96.f64[0];
      v159  = v98;
      v160  = (float64x2_t)*(unint64_t *)&v97.f64[0];
      v161  = v100;
      v162  = (float64x2_t)*(unint64_t *)&v99.f64[0];
      v163  = v102;
      v164  = v101;
      v165  = 0u;
      v166  = 0u;
      v167  = 0u;
      v168  = 0u;
      v169  = 0u;
      v170  = 0u;
      v171  = 0u;
      v172  = 0u;
      v108  = v146;
      do
      {
        v110  = *(float64x2_t *)((char *)&v157 + v107);
        v109  = *(float64x2_t *)((char *)&v157 + v107 + 16);
        v111  = (float64x2_t *)((char *)&v165 + v107);
        *v111  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v59, v110.f64[0]), v61, v110, 1), v63, v109.f64[0]), v65, v109, 1);
        v111[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v58, v110.f64[0]), v60, v110, 1), v62, v109.f64[0]), v64, v109, 1);
        v107 += 32;
      }
      while (v107 != 128);
      float64x2_t v59 = v165;
      float64x2_t v58 = v166;
      float64x2_t v61 = v167;
      float64x2_t v60 = v168;
      float64x2_t v63 = v169;
      float64x2_t v62 = v170;
      v65  = v171;
      v64  = v172;
      v113  = v150;
      v112  = v152;
      v115  = v148;
      v114  = v149;
      v116  = v147;
      goto LABEL_50;
    }
    v125  = 0;
    v157  = *(float64x2_t *)a3;
    v158  = v96;
    v159  = v98;
    v160  = v97;
    v161  = v100;
    v162  = v99;
    v165  = 0u;
    v166  = 0u;
    v167  = 0u;
    v168  = 0u;
    v169  = 0u;
    v170  = 0u;
    *(void *)&v59.f64[1]  = vextq_s8((int8x16_t)v59, (int8x16_t)v59, 8uLL).u64[0];
    *(void *)&v61.f64[1]  = vextq_s8((int8x16_t)v61, (int8x16_t)v61, 8uLL).u64[0];
    *(void *)&v63.f64[1]  = vextq_s8((int8x16_t)v63, (int8x16_t)v63, 8uLL).u64[0];
    do
    {
      v127  = *(float64x2_t *)((char *)&v157 + v125);
      v126  = *(float64x2_t *)((char *)&v157 + v125 + 16);
      v128  = (float64x2_t *)((char *)&v165 + v125);
      *v128  = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v59, v127.f64[0]), v61, v127, 1), v63, v126.f64[0]);
      v128[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v58, v127), v60, v127, 1), v126, v62);
      v125 += 32;
    }
    while (v125 != 96);
    float64x2_t v59 = v165;
    float64x2_t v58 = v166;
    float64x2_t v61 = v167;
    float64x2_t v60 = v168;
    float64x2_t v63 = v169;
    float64x2_t v62 = v170;
  }
  v113  = v150;
  v112  = v152;
  v115  = v148;
  v114  = v149;
  v108  = v146;
  v116  = v147;
LABEL_50:
  v129  = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v69, v61), (int8x16_t)vceqq_f64(v71, v63)), (int8x16_t)vceqq_f64(v67, v59));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v129, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v68, v60), (int8x16_t)vceqq_f64(v70, v62)), (int8x16_t)vceqq_f64(v66, v58)), (int8x16_t)v129)).u64[0] & 0x8000000000000000) != 0&& (v130  = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v68, v116), (int8x16_t)vceqq_f64(v66, v108)), (int8x16_t)vceqq_f64(v70, v115)), v131  = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v69, v112), (int8x16_t)vceqq_f64(v67, v113)), (int8x16_t)vceqq_f64(v71, v114)), (vandq_s8((int8x16_t)vdupq_laneq_s64(v131, 1), vandq_s8(v130, (int8x16_t)v131)).u64[0] & 0x8000000000000000) != 0))
  {
    v65  = vaddq_f64(v144, v65);
    v64  = vaddq_f64(v145, v64);
  }
  else
  {
    v132  = vceqzq_f64(v65);
    v133.f64[0]  = v145.f64[0];
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v132, 1), vandq_s8((int8x16_t)vceqzq_f64(v64), (int8x16_t)v132)).u64[0] & 0x8000000000000000) != 0
      && (v134.f64[0]  = v144.f64[0],
          *(void *)&v134.f64[1]  = vextq_s8((int8x16_t)v144, (int8x16_t)v144, 8uLL).u64[0],
          v135  = vceqzq_f64(v134),
          (vandq_s8((int8x16_t)vdupq_laneq_s64(v135, 1), vandq_s8((int8x16_t)vceqzq_f64(v145), (int8x16_t)v135)).u64[0] & 0x8000000000000000) != 0))
    {
      v140  = 0;
      v157  = v113;
      v158  = v108;
      v159  = v112;
      v160  = v116;
      v161  = v114;
      v162  = v115;
      v165  = 0u;
      v166  = 0u;
      v167  = 0u;
      v168  = 0u;
      v169  = 0u;
      v170  = 0u;
      *(void *)&v59.f64[1]  = vextq_s8((int8x16_t)v59, (int8x16_t)v59, 8uLL).u64[0];
      *(void *)&v61.f64[1]  = vextq_s8((int8x16_t)v61, (int8x16_t)v61, 8uLL).u64[0];
      *(void *)&v63.f64[1]  = vextq_s8((int8x16_t)v63, (int8x16_t)v63, 8uLL).u64[0];
      do
      {
        v142  = *(float64x2_t *)((char *)&v157 + v140);
        v141  = *(float64x2_t *)((char *)&v157 + v140 + 16);
        v143  = (float64x2_t *)((char *)&v165 + v140);
        *v143  = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v59, v142.f64[0]), v61, v142, 1), v63, v141.f64[0]);
        v143[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v58, v142), v60, v142, 1), v141, v62);
        v140 += 32;
      }
      while (v140 != 96);
      float64x2_t v59 = v165;
      float64x2_t v58 = v166;
      float64x2_t v61 = v167;
      float64x2_t v60 = v168;
      float64x2_t v63 = v169;
      float64x2_t v62 = v170;
    }
    else
    {
      v136  = 0;
      v58.f64[1]  = 0.0;
      v60.f64[1]  = 0.0;
      v62.f64[1]  = 0.0;
      v64.f64[1]  = 1.0;
      v133.f64[1]  = 1.0;
      v157  = v113;
      v158  = (float64x2_t)*(unint64_t *)&v108.f64[0];
      v159  = v112;
      v160  = (float64x2_t)*(unint64_t *)&v116.f64[0];
      v161  = v114;
      v162  = (float64x2_t)*(unint64_t *)&v115.f64[0];
      v163  = v144;
      v164  = v133;
      v165  = 0u;
      v166  = 0u;
      v167  = 0u;
      v168  = 0u;
      v169  = 0u;
      v170  = 0u;
      v171  = 0u;
      v172  = 0u;
      do
      {
        v138  = *(float64x2_t *)((char *)&v157 + v136);
        v137  = *(float64x2_t *)((char *)&v157 + v136 + 16);
        v139  = (float64x2_t *)((char *)&v165 + v136);
        *v139  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v59, v138.f64[0]), v61, v138, 1), v63, v137.f64[0]), v65, v137, 1);
        v139[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v58, v138.f64[0]), v60, v138, 1), v62, v137.f64[0]), v64, v137, 1);
        v136 += 32;
      }
      while (v136 != 128);
      float64x2_t v59 = v165;
      float64x2_t v58 = v166;
      float64x2_t v61 = v167;
      float64x2_t v60 = v168;
      float64x2_t v63 = v169;
      float64x2_t v62 = v170;
      v65  = v171;
      v64  = v172;
    }
  }
  *a4  = v59;
  a4[1]  = v58;
  a4[2]  = v61;
  a4[3]  = v60;
  a4[4]  = v63;
  a4[5]  = v62;
  a4[6]  = v65;
  a4[7]  = v64;
  return result;
}

uint64_t sub_228C1CEF4()
{
  return MEMORY[0x270FA0238](v0, 144, 15);
}

unint64_t lazy protocol witness table accessor for type SPAffineTransform3D and conformance SPAffineTransform3D()
{
  unint64_t result = lazy protocol witness table cache variable for type SPAffineTransform3D and conformance SPAffineTransform3D;
  if (!lazy protocol witness table cache variable for type SPAffineTransform3D and conformance SPAffineTransform3D)
  {
    type metadata accessor for SPAffineTransform3D(255);
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPAffineTransform3D and conformance SPAffineTransform3D);
  }
  return result;
}

double sub_228C1CF60@<D0>(__n128 *a1@<X0>, __n128 *a2@<X8>)
{
  __n128 v2 = a1[1];
  __n128 v3 = a1[2];
  __n128 v4 = a1[3];
  __n128 v5 = a1[4];
  __n128 v6 = a1[5];
  __n128 v7 = a1[6];
  __n128 v8 = a1[7];
  v10[0]  = *a1;
  v10[1]  = v2;
  v10[2]  = v3;
  v10[3]  = v4;
  v10[4]  = v5;
  v10[5]  = v6;
  v10[6]  = v7;
  v10[7]  = v8;
  *(void *)&double result = SPAffineTransform3DGetTranslation(v10, a2).n128_u64[0];
  return result;
}

double sub_228C1CFA4(double *a1, SPAffineTransform3D *a2)
{
  long long v2 = *(_OWORD *)a1;
  double v3 = a1[3];
  v5.double z = a1[2];
  v5.vector.f64[3]  = v3;
  *(_OWORD *)&v5.x  = v2;
  *(void *)&double result = SPAffineTransform3DSetTranslation(a2, &v5).n128_u64[0];
  return result;
}

unsigned char *storeEnumTagSinglePayload for SPAffineTransform3D.CodingKeys(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 3 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 3) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFD) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFC)
  {
    unsigned int v6 = ((a2 - 253) >> 8) + 1;
    *double result = a2 + 3;
    switch(v5)
    {
      case 1:
        result[1]  = v6;
        break;
      case 2:
        *(_WORD *)(result + 1)  = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x228C1D0ACLL);
      case 4:
        *(_DWORD *)(result + 1)  = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1]  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1)  = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1)  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *double result = a2 + 3;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for SPAffineTransform3D.CodingKeys()
{
  return &type metadata for SPAffineTransform3D.CodingKeys;
}

float64x2_t *SPAffineTransform3DRotateByQuaternion@<X0>(float64x2_t *result@<X0>, float64x2_t *a2@<X1>, float64x2_t *a3@<X8>)
{
  _Q1  = *a2;
  _Q0  = a2[1];
  _D2  = a2->f64[1];
  __asm { FMLS            D16, D0, V0.D[0] }
  _D4  = a2[1].f64[1];
  __asm { FMLA            D16, D4, V0.D[1] }
  double v15 = vmlad_n_f64(vmuld_lane_f64(_Q0.f64[0], _Q0, 1), _D2, a2->f64[0]);
  v16.f64[0]  = vmuld_lane_f64(_D2, _Q0, 1);
  double v17 = vmlad_n_f64(-(_D2 * _D4), _Q0.f64[0], a2->f64[0]);
  v3.f64[0]  = v17 + v17;
  _Q16.f64[1]  = v15 + v15;
  double v19 = vmlad_n_f64(-(_Q0.f64[0] * _D4), _D2, a2->f64[0]);
  v24.f64[0]  = v19 + v19;
  __asm
  {
    FMLA            D3, D2, V1.D[1]
    FMLA            D3, D4, V0.D[1]
    FMLS            D3, D1, V1.D[0]
    FMLA            D5, D0, V1.D[1]
  }
  v24.f64[1]  = _D3;
  __asm
  {
    FMLS            D19, D1, V1.D[0]
    FMLS            D19, D2, V1.D[1]
  }
  _Q1.f64[0]  = a2[1].f64[0];
  v5.f64[0]  = _D5 + _D5;
  v16.f64[1]  = -(a2->f64[0] * _D4);
  float64x2_t v26 = vmlaq_f64(v16, (float64x2_t)vzip1q_s64(*(int64x2_t *)a2, (int64x2_t)_Q0), _Q1);
  float64x2_t v27 = vaddq_f64(v26, v26);
  float64x2_t v29 = *result;
  float64x2_t v28 = result[1];
  float64x2_t v31 = result[2];
  float64x2_t v30 = result[3];
  float64x2_t v33 = result[4];
  float64x2_t v32 = result[5];
  float64x2_t v35 = result[6];
  float64x2_t v34 = result[7];
  float64x2_t v36 = *(float64x2_t *)(MEMORY[0x263EF8988] + 16);
  float64x2_t v38 = *(float64x2_t *)(MEMORY[0x263EF8988] + 32);
  float64x2_t v37 = *(float64x2_t *)(MEMORY[0x263EF8988] + 48);
  float64x2_t v40 = *(float64x2_t *)(MEMORY[0x263EF8988] + 64);
  float64x2_t v39 = *(float64x2_t *)(MEMORY[0x263EF8988] + 80);
  int64x2_t v41 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v38, v31), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], *result)), (int8x16_t)vceqq_f64(v40, v33));
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v41, 1), vandq_s8(vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v37, v30), (int8x16_t)vceqq_f64(v36, v28)), (int8x16_t)vceqq_f64(v39, v32)), (int8x16_t)v41)).u64[0] & 0x8000000000000000) != 0)
  {
    int8x16_t v42 = vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v39, _Q19), (int8x16_t)vceqq_f64(v37, v5)), (int8x16_t)vceqq_f64(v36, v3));
    int64x2_t v43 = (int64x2_t)vandq_s8(vandq_s8((int8x16_t)vceqq_f64(v40, v27), (int8x16_t)vceqq_f64(v38, v24)), (int8x16_t)vceqq_f64(*MEMORY[0x263EF8988], _Q16));
    if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v43, 1), vandq_s8(v42, (int8x16_t)v43)).u64[0] & 0x8000000000000000) != 0)
    {
      float64x2_t v34 = vaddq_f64(v34, (float64x2_t)0);
      float64x2_t v35 = vaddq_f64(v35, (float64x2_t)0);
      *a3  = v29;
      a3[1]  = v28;
      a3[2]  = v31;
      a3[3]  = v30;
      a3[4]  = v33;
      a3[5]  = v32;
LABEL_11:
      a3[6]  = v35;
      a3[7]  = v34;
      return result;
    }
  }
  int64x2_t v44 = vceqzq_f64(v35);
  if ((vandq_s8((int8x16_t)vdupq_laneq_s64(v44, 1), vandq_s8((int8x16_t)vceqzq_f64(v34), (int8x16_t)v44)).u64[0] & 0x8000000000000000) != 0)
  {
    uint64_t v56 = 0;
    v65  = _Q16;
    float64x2_t v66 = v3;
    float64x2_t v67 = v24;
    float64x2_t v68 = v5;
    float64x2_t v69 = v27;
    float64x2_t v70 = _Q19;
    v74  = 0u;
    v75  = 0u;
    v76  = 0u;
    v77  = 0u;
    v78  = 0u;
    v79  = 0u;
    *(void *)&v29.f64[1]  = vextq_s8((int8x16_t)v29, (int8x16_t)v29, 8uLL).u64[0];
    *(void *)&v31.f64[1]  = vextq_s8((int8x16_t)v31, (int8x16_t)v31, 8uLL).u64[0];
    *(void *)&v33.f64[1]  = vextq_s8((int8x16_t)v33, (int8x16_t)v33, 8uLL).u64[0];
    do
    {
      float64x2_t v58 = *(float64x2_t *)((char *)&v65 + v56);
      float64x2_t v57 = *(float64x2_t *)((char *)&v65 + v56 + 16);
      float64x2_t v59 = (float64x2_t *)((char *)&v74 + v56);
      *float64x2_t v59 = vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v29, v58.f64[0]), v31, v58, 1), v33, v57.f64[0]);
      v59[1]  = vmlaq_f64(vmlaq_laneq_f64(vmulq_f64(v28, v58), v30, v58, 1), v57, v32);
      v56 += 32;
    }
    while (v56 != 96);
    float64x2_t v60 = v75;
    float64x2_t v61 = v76;
    float64x2_t v62 = v77;
    float64x2_t v63 = v78;
    v64  = v79;
    *a3  = v74;
    a3[1]  = v60;
    a3[2]  = v61;
    a3[3]  = v62;
    a3[4]  = v63;
    a3[5]  = v64;
    goto LABEL_11;
  }
  uint64_t v45 = 0;
  v28.f64[1]  = 0.0;
  v30.f64[1]  = 0.0;
  v32.f64[1]  = 0.0;
  v34.f64[1]  = 1.0;
  v65  = _Q16;
  float64x2_t v66 = (float64x2_t)*(unint64_t *)&v3.f64[0];
  float64x2_t v67 = v24;
  float64x2_t v68 = (float64x2_t)*(unint64_t *)&v5.f64[0];
  float64x2_t v69 = v27;
  float64x2_t v70 = (float64x2_t)*(unint64_t *)&_Q19.f64[0];
  uint64_t v71 = 0;
  uint64_t v72 = 0;
  v73  = xmmword_228C1F7A0;
  v74  = 0u;
  v75  = 0u;
  v76  = 0u;
  v77  = 0u;
  v78  = 0u;
  v79  = 0u;
  v80  = 0u;
  v81  = 0u;
  do
  {
    float64x2_t v47 = *(float64x2_t *)((char *)&v65 + v45);
    float64x2_t v46 = *(float64x2_t *)((char *)&v65 + v45 + 16);
    float64x2_t v48 = (float64x2_t *)((char *)&v74 + v45);
    *float64x2_t v48 = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v29, v47.f64[0]), v31, v47, 1), v33, v46.f64[0]), v35, v46, 1);
    v48[1]  = vmlaq_laneq_f64(vmlaq_n_f64(vmlaq_laneq_f64(vmulq_n_f64(v28, v47.f64[0]), v30, v47, 1), v32, v46.f64[0]), v34, v46, 1);
    v45 += 32;
  }
  while (v45 != 128);
  float64x2_t v49 = v75;
  float64x2_t v50 = v76;
  float64x2_t v51 = v77;
  float64x2_t v52 = v78;
  float64x2_t v53 = v79;
  float64x2_t v54 = v80;
  float64x2_t v55 = v81;
  *a3  = v74;
  a3[1]  = v49;
  a3[2]  = v50;
  a3[3]  = v51;
  a3[4]  = v52;
  a3[5]  = v53;
  a3[6]  = v54;
  a3[7]  = v55;
  return result;
}

uint64_t lazy protocol witness table accessor for type SIMD3<Double> and conformance SIMD3<A>(unint64_t *a1)
{
  uint64_t result = *a1;
  if (!result)
  {
    __swift_instantiateConcreteTypeFromMangledNameAbstract(&demangling cache variable for type metadata for SIMD3<Double>);
    uint64_t result = swift_getWitnessTable();
    atomic_store(result, a1);
  }
  return result;
}

uint64_t specialized == infix<A>(_:_:)(unsigned __int8 a1, unsigned __int8 a2)
{
  int v2 = a1;
  if (a1)
  {
    if (a1 == 1) {
      uint64_t v3 = 0x746867696568;
    }
    else {
      uint64_t v3 = 0x6874706564;
    }
    if (v2 == 1) {
      unint64_t v4 = 0xE600000000000000;
    }
    else {
      unint64_t v4 = 0xE500000000000000;
    }
    int v5 = a2;
    if (a2)
    {
LABEL_9:
      if (v5 == 1) {
        uint64_t v6 = 0x746867696568;
      }
      else {
        uint64_t v6 = 0x6874706564;
      }
      if (v5 == 1) {
        unint64_t v7 = 0xE600000000000000;
      }
      else {
        unint64_t v7 = 0xE500000000000000;
      }
      if (v3 != v6) {
        goto LABEL_21;
      }
      goto LABEL_19;
    }
  }
  else
  {
    unint64_t v4 = 0xE500000000000000;
    uint64_t v3 = 0x6874646977;
    int v5 = a2;
    if (a2) {
      goto LABEL_9;
    }
  }
  unint64_t v7 = 0xE500000000000000;
  if (v3 != 0x6874646977)
  {
LABEL_21:
    char v8 = _stringCompareWithSmolCheck(_:_:expecting:)();
    goto LABEL_22;
  }
LABEL_19:
  if (v4 != v7) {
    goto LABEL_21;
  }
  char v8 = 1;
LABEL_22:
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  return v8 & 1;
}

{
  int v2;
  uint64_t v3;
  unint64_t v4;
  int v5;
  uint64_t v6;
  unint64_t v7;
  char v8;

  int v2 = a1;
  if (a1)
  {
    if (a1 == 1) {
      uint64_t v3 = 0x74616E696C636E69;
    }
    else {
      uint64_t v3 = 0x6874756D697A61;
    }
    if (v2 == 1) {
      unint64_t v4 = 0xEB000000006E6F69;
    }
    else {
      unint64_t v4 = 0xE700000000000000;
    }
    int v5 = a2;
    if (a2)
    {
LABEL_9:
      if (v5 == 1) {
        uint64_t v6 = 0x74616E696C636E69;
      }
      else {
        uint64_t v6 = 0x6874756D697A61;
      }
      if (v5 == 1) {
        unint64_t v7 = 0xEB000000006E6F69;
      }
      else {
        unint64_t v7 = 0xE700000000000000;
      }
      if (v3 != v6) {
        goto LABEL_21;
      }
      goto LABEL_19;
    }
  }
  else
  {
    unint64_t v4 = 0xE600000000000000;
    uint64_t v3 = 0x737569646172;
    int v5 = a2;
    if (a2) {
      goto LABEL_9;
    }
  }
  unint64_t v7 = 0xE600000000000000;
  if (v3 != 0x737569646172)
  {
LABEL_21:
    char v8 = _stringCompareWithSmolCheck(_:_:expecting:)();
    goto LABEL_22;
  }
LABEL_19:
  if (v4 != v7) {
    goto LABEL_21;
  }
  char v8 = 1;
LABEL_22:
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  return v8 & 1;
}

{
  char v2;

  if ((unint64_t)a1 << 48 == (unint64_t)a2 << 48) {
    int v2 = 1;
  }
  else {
    int v2 = _stringCompareWithSmolCheck(_:_:expecting:)();
  }
  swift_bridgeObjectRelease_n();
  return v2 & 1;
}

{
  int v2;
  uint64_t v3;
  unint64_t v4;
  int v5;
  uint64_t v6;
  unint64_t v7;
  char v8;

  int v2 = a1;
  if (a1)
  {
    if (a1 == 1) {
      uint64_t v3 = 0x6E6F697461746F72;
    }
    else {
      uint64_t v3 = 0x656C616373;
    }
    if (v2 == 1) {
      unint64_t v4 = 0xE800000000000000;
    }
    else {
      unint64_t v4 = 0xE500000000000000;
    }
    int v5 = a2;
    if (a2)
    {
LABEL_9:
      if (v5 == 1) {
        uint64_t v6 = 0x6E6F697461746F72;
      }
      else {
        uint64_t v6 = 0x656C616373;
      }
      if (v5 == 1) {
        unint64_t v7 = 0xE800000000000000;
      }
      else {
        unint64_t v7 = 0xE500000000000000;
      }
      if (v3 != v6) {
        goto LABEL_21;
      }
      goto LABEL_19;
    }
  }
  else
  {
    unint64_t v4 = 0xE800000000000000;
    uint64_t v3 = 0x6E6F697469736F70;
    int v5 = a2;
    if (a2) {
      goto LABEL_9;
    }
  }
  unint64_t v7 = 0xE800000000000000;
  if (v3 != 0x6E6F697469736F70)
  {
LABEL_21:
    char v8 = _stringCompareWithSmolCheck(_:_:expecting:)();
    goto LABEL_22;
  }
LABEL_19:
  if (v4 != v7) {
    goto LABEL_21;
  }
  char v8 = 1;
LABEL_22:
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  return v8 & 1;
}

uint64_t specialized == infix<A>(_:_:)(char a1, char a2)
{
  BOOL v2 = (a1 & 1) == 0;
  if (a1) {
    uint64_t v3 = 1702521203;
  }
  else {
    uint64_t v3 = 0x6E696769726FLL;
  }
  if (v2) {
    unint64_t v4 = 0xE600000000000000;
  }
  else {
    unint64_t v4 = 0xE400000000000000;
  }
  if (a2) {
    uint64_t v5 = 1702521203;
  }
  else {
    uint64_t v5 = 0x6E696769726FLL;
  }
  if (a2) {
    unint64_t v6 = 0xE400000000000000;
  }
  else {
    unint64_t v6 = 0xE600000000000000;
  }
  if (v3 == v5 && v4 == v6) {
    char v7 = 1;
  }
  else {
    char v7 = _stringCompareWithSmolCheck(_:_:expecting:)();
  }
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  return v7 & 1;
}

{
  uint64_t v2;
  uint64_t v3;
  char v4;

  if (a1) {
    BOOL v2 = 0x6E6F697461746F72;
  }
  else {
    BOOL v2 = 0x6E6F697469736F70;
  }
  if (a2) {
    uint64_t v3 = 0x6E6F697461746F72;
  }
  else {
    uint64_t v3 = 0x6E6F697469736F70;
  }
  if (v2 == v3) {
    unint64_t v4 = 1;
  }
  else {
    unint64_t v4 = _stringCompareWithSmolCheck(_:_:expecting:)();
  }
  swift_bridgeObjectRelease_n();
  return v4 & 1;
}

{
  BOOL v2;
  uint64_t v3;
  unint64_t v4;
  uint64_t v5;
  unint64_t v6;
  char v7;

  BOOL v2 = (a1 & 1) == 0;
  if (a1) {
    uint64_t v3 = 0x6F69746365726964;
  }
  else {
    uint64_t v3 = 0x6E696769726FLL;
  }
  if (v2) {
    unint64_t v4 = 0xE600000000000000;
  }
  else {
    unint64_t v4 = 0xE90000000000006ELL;
  }
  if (a2) {
    uint64_t v5 = 0x6F69746365726964;
  }
  else {
    uint64_t v5 = 0x6E696769726FLL;
  }
  if (a2) {
    unint64_t v6 = 0xE90000000000006ELL;
  }
  else {
    unint64_t v6 = 0xE600000000000000;
  }
  if (v3 == v5 && v4 == v6) {
    char v7 = 1;
  }
  else {
    char v7 = _stringCompareWithSmolCheck(_:_:expecting:)();
  }
  swift_bridgeObjectRelease();
  swift_bridgeObjectRelease();
  return v7 & 1;
}

{
  char v2;

  if (a1 == a2) {
    BOOL v2 = 1;
  }
  else {
    BOOL v2 = _stringCompareWithSmolCheck(_:_:expecting:)();
  }
  swift_bridgeObjectRelease_n();
  return v2 & 1;
}

uint64_t static SPAxis.x.getter()
{
  return 1;
}

uint64_t static SPAxis.y.getter()
{
  return 2;
}

uint64_t static SPAxis.z.getter()
{
  return 4;
}

double SPRotationAxis3D.init(_:)(simd_quatd a1)
{
  simd_quatd v3 = a1;
  SPRotation3DMakeWithQuaternion(a1, (uint64_t)&v3, &v2);
  return *(double *)&v2;
}

void __swiftcall SPRotationAxis3D.init(x:y:z:)(SPRotationAxis3D *__return_ptr retstr, Swift::Double x, Swift::Double y, Swift::Double z)
{
  SPPoint3DMake(x, y, z, (double *)&v4);
}

double SPRotationAxis3D.y.getter(double a1, double a2)
{
  return a2;
}

double SPRotationAxis3D.z.getter(double a1, double a2, double a3)
{
  return a3;
}

void one-time initialization function for x()
{
}

double static SPRotationAxis3D.x.getter()
{
  if (one-time initialization token for x != -1) {
    swift_once();
  }
  return *(double *)&static SPRotationAxis3D.x;
}

void one-time initialization function for y()
{
}

double static SPRotationAxis3D.y.getter()
{
  if (one-time initialization token for y != -1) {
    swift_once();
  }
  return *(double *)&static SPRotationAxis3D.y;
}

void one-time initialization function for z()
{
}

double static SPRotationAxis3D.z.getter()
{
  if (one-time initialization token for z != -1) {
    swift_once();
  }
  return *(double *)&static SPRotationAxis3D.z;
}

void one-time initialization function for xy()
{
}

double static SPRotationAxis3D.xy.getter()
{
  if (one-time initialization token for xy != -1) {
    swift_once();
  }
  return *(double *)&static SPRotationAxis3D.xy;
}

void one-time initialization function for yz()
{
}

double static SPRotationAxis3D.yz.getter()
{
  if (one-time initialization token for yz != -1) {
    swift_once();
  }
  return *(double *)&static SPRotationAxis3D.yz;
}

void one-time initialization function for xz()
{
}

double static SPRotationAxis3D.xz.getter()
{
  if (one-time initialization token for xz != -1) {
    swift_once();
  }
  return *(double *)&static SPRotationAxis3D.xz;
}

void one-time initialization function for xyz()
{
}

double static SPRotationAxis3D.xyz.getter()
{
  if (one-time initialization token for xyz != -1) {
    swift_once();
  }
  return *(double *)&static SPRotationAxis3D.xyz;
}

void SPRotationAxis3D.x.setter(double a1)
{
  double *v1 = a1;
}

double (*SPRotationAxis3D.x.modify(void *a1))(uint64_t a1)
{
  a1[1]  = v1;
  *a1  = *v1;
  return SPRotationAxis3D.x.modify;
}

double SPRotationAxis3D.x.modify(uint64_t a1)
{
  double result = *(double *)a1;
  **(void **)(a1 + 8)  = *(void *)a1;
  return result;
}

void SPRotationAxis3D.y.setter(double a1)
{
  *(double *)(v1 + 8)  = a1;
}

double (*SPRotationAxis3D.y.modify(void *a1))(uint64_t a1)
{
  a1[1]  = v1;
  *a1  = *(void *)(v1 + 8);
  return SPRotationAxis3D.y.modify;
}

double SPRotationAxis3D.y.modify(uint64_t a1)
{
  double result = *(double *)a1;
  *(void *)(*(void *)(a1 + 8) + 8)  = *(void *)a1;
  return result;
}

void SPRotationAxis3D.z.setter(double a1)
{
  *(double *)(v1 + 16)  = a1;
}

double (*SPRotationAxis3D.z.modify(void *a1))(uint64_t a1)
{
  a1[1]  = v1;
  *a1  = *(void *)(v1 + 16);
  return SPRotationAxis3D.z.modify;
}

double SPRotationAxis3D.z.modify(uint64_t a1)
{
  double result = *(double *)a1;
  *(void *)(*(void *)(a1 + 8) + 16)  = *(void *)a1;
  return result;
}

void __swiftcall SPRotationAxis3D.init(_:)(SPRotationAxis3D *__return_ptr retstr, SPVector3D *a2)
{
  double v5 = v2.vector.f64[0];
  double v6 = v2.vector.f64[2];
  uint64_t v7 = v3;
  SPRotation3DMakeWithQuaternion(v2, (uint64_t)&v5, &v4);
}

BOOL static SPRotationAxis3D.== infix(_:_:)(double a1, double a2, double a3, double a4, double a5, double a6)
{
  BOOL v6 = a1 == a4;
  if (a2 != a5) {
    BOOL v6 = 0;
  }
  return a3 == a6 && v6;
}

void SPRotationAxis3D.hash(into:)(__n128 a1, double a2, double a3)
{
  a1.n128_f64[1]  = a2;
  specialized SIMD.hash(into:)(a1, a3);
}

Swift::Int SPRotationAxis3D.hashValue.getter(double a1, double a2, double a3)
{
  Hasher.init(_seed:)();
  v3.n128_f64[0]  = a1;
  v3.n128_f64[1]  = a2;
  specialized SIMD.hash(into:)(v3, a3);
  return Hasher._finalize()();
}

uint64_t protocol witness for static Equatable.== infix(_:_:) in conformance SPRotationAxis3D.CodingKeys(char *a1, char *a2)
{
  return specialized == infix<A>(_:_:)(*a1, *a2) & 1;
}

unint64_t protocol witness for RawRepresentable.init(rawValue:) in conformance SPRotationAxis3D.CodingKeys@<X0>(Swift::String *a1@<X0>, unsigned char *a2@<X8>)
{
  unint64_t result = specialized SPRotationAxis3D.CodingKeys.init(rawValue:)(*a1);
  *a2  = result;
  return result;
}

unint64_t protocol witness for CodingKey.init(stringValue:) in conformance SPRotationAxis3D.CodingKeys@<X0>(Swift::String a1@<X1:X0>, unsigned char *a2@<X8>)
{
  unint64_t result = specialized SPRotationAxis3D.CodingKeys.init(rawValue:)(a1);
  *a2  = result;
  return result;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPRotationAxis3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys();

  return MEMORY[0x270FA00B0](a1, v2);
}

uint64_t protocol witness for CustomDebugStringConvertible.debugDescription.getter in conformance SPRotationAxis3D.CodingKeys(uint64_t a1)
{
  unint64_t v2 = lazy protocol witness table accessor for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys();

  return MEMORY[0x270FA00B8](a1, v2);
}

uint64_t SPRotationAxis3D.encode(to:)(void *a1)
{
  uint64_t v3 = __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for KeyedEncodingContainer<SPRotationAxis3D.CodingKeys>);
  uint64_t v4 = *(void *)(v3 - 8);
  MEMORY[0x270FA5388](v3);
  BOOL v6 = &v8[-((v5 + 15) & 0xFFFFFFFFFFFFFFF0)];
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys();
  dispatch thunk of Encoder.container<A>(keyedBy:)();
  v8[15]  = 0;
  KeyedEncodingContainer.encode(_:forKey:)();
  if (!v1)
  {
    v8[14]  = 1;
    KeyedEncodingContainer.encode(_:forKey:)();
    v8[13]  = 2;
    KeyedEncodingContainer.encode(_:forKey:)();
  }
  return (*(uint64_t (**)(unsigned char *, uint64_t))(v4 + 8))(v6, v3);
}

double SPRotationAxis3D.init(from:)(void *a1)
{
  return specialized SPRotationAxis3D.init(from:)(a1);
}

void protocol witness for Decodable.init(from:) in conformance SPRotationAxis3D(void *a1@<X0>, uint64_t a2@<X8>)
{
  double v4 = specialized SPRotationAxis3D.init(from:)(a1);
  if (!v2)
  {
    *(void *)(a2 + 24)  = 0;
    *(double *)a2  = v4;
    *(void *)(a2 + 8)  = v5;
    *(void *)(a2 + 16)  = v6;
  }
}

uint64_t protocol witness for Encodable.encode(to:) in conformance SPRotationAxis3D(void *a1)
{
  return SPRotationAxis3D.encode(to:)(a1);
}

uint64_t SPRotationAxis3D.description.getter()
{
  _StringGuts.grow(_:)(21);
  v0._countAndFlagsBits  = 540702760;
  v0._object  = (void *)0xE400000000000000;
  String.append(_:)(v0);
  Double.write<A>(to:)();
  v1._countAndFlagsBits  = 0x203A79202CLL;
  v1._object  = (void *)0xE500000000000000;
  String.append(_:)(v1);
  Double.write<A>(to:)();
  v2._countAndFlagsBits  = 0x203A7A202CLL;
  v2._object  = (void *)0xE500000000000000;
  String.append(_:)(v2);
  Double.write<A>(to:)();
  v3._countAndFlagsBits  = 41;
  v3._object  = (void *)0xE100000000000000;
  String.append(_:)(v3);
  return 0;
}

uint64_t protocol witness for CustomStringConvertible.description.getter in conformance SPRotationAxis3D()
{
  return SPRotationAxis3D.description.getter();
}

uint64_t SPRotationAxis3D.customMirror.getter(double a1, double a2, double a3)
{
  uint64_t v6 = type metadata accessor for Mirror.AncestorRepresentation();
  uint64_t v7 = *(void *)(v6 - 8);
  MEMORY[0x270FA5388](v6);
  float64x2_t v9 = (char *)v19 - ((v8 + 15) & 0xFFFFFFFFFFFFFFF0);
  uint64_t v10 = __swift_instantiateConcreteTypeFromMangledName((uint64_t *)&demangling cache variable for type metadata for Mirror.DisplayStyle?);
  MEMORY[0x270FA5388](v10 - 8);
  float64x2_t v12 = (char *)v19 - ((v11 + 15) & 0xFFFFFFFFFFFFFFF0);
  *(double *)double v19 = a1;
  *(double *)&v19[1]  = a2;
  *(double *)&v19[2]  = a3;
  __swift_instantiateConcreteTypeFromMangledName(&demangling cache variable for type metadata for _ContiguousArrayStorage<(String, Any)>);
  uint64_t v13 = swift_allocObject();
  *(_OWORD *)(v13 + 16)  = xmmword_228C1FC50;
  *(void *)(v13 + 32)  = 120;
  *(void *)(v13 + 40)  = 0xE100000000000000;
  uint64_t v14 = MEMORY[0x263F8D538];
  *(double *)(v13 + 48)  = a1;
  *(void *)(v13 + 72)  = v14;
  *(void *)(v13 + 80)  = 121;
  *(void *)(v13 + 88)  = 0xE100000000000000;
  *(double *)(v13 + 96)  = a2;
  *(void *)(v13 + 120)  = v14;
  *(void *)(v13 + 128)  = 122;
  *(void *)(v13 + 136)  = 0xE100000000000000;
  *(void *)(v13 + 168)  = v14;
  *(double *)(v13 + 144)  = a3;
  uint64_t v15 = *MEMORY[0x263F8E808];
  uint64_t v16 = type metadata accessor for Mirror.DisplayStyle();
  uint64_t v17 = *(void *)(v16 - 8);
  (*(void (**)(char *, uint64_t, uint64_t))(v17 + 104))(v12, v15, v16);
  (*(void (**)(char *, void, uint64_t, uint64_t))(v17 + 56))(v12, 0, 1, v16);
  (*(void (**)(char *, void, uint64_t))(v7 + 104))(v9, *MEMORY[0x263F8E830], v6);
  type metadata accessor for SPRotationAxis3D(0);
  return Mirror.init<A>(_:children:displayStyle:ancestorRepresentation:)();
}

uint64_t protocol witness for CustomReflectable.customMirror.getter in conformance SPRotationAxis3D()
{
  return SPRotationAxis3D.customMirror.getter(*v0, v0[1], v0[2]);
}

unint64_t lazy protocol witness table accessor for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys()
{
  unint64_t result = lazy protocol witness table cache variable for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys);
  }
  return result;
}

{
  unint64_t result;

  unint64_t result = lazy protocol witness table cache variable for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys;
  if (!lazy protocol witness table cache variable for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys)
  {
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys);
  }
  return result;
}

unint64_t specialized SPRotationAxis3D.CodingKeys.init(rawValue:)(Swift::String string)
{
  object  = string._object;
  v2._countAndFlagsBits  = string._countAndFlagsBits;
  v2._object  = object;
  unint64_t v3 = _findStringSwitchCase(cases:string:)((Swift::OpaquePointer)&outlined read-only object #0 of SPRotationAxis3D.CodingKeys.init(rawValue:), v2);
  swift_bridgeObjectRelease();
  if (v3 >= 3) {
    return 3;
  }
  else {
    return v3;
  }
}

double specialized SPRotationAxis3D.init(from:)(void *a1)
{
  uint64_t v4 = __swift_instantiateConcreteTypeFromMangledName((uint64_t *)&demangling cache variable for type metadata for KeyedDecodingContainer<SPRotationAxis3D.CodingKeys>);
  uint64_t v5 = *(void *)(v4 - 8);
  MEMORY[0x270FA5388](v4);
  uint64_t v7 = (char *)&v14 - ((v6 + 15) & 0xFFFFFFFFFFFFFFF0);
  __swift_project_boxed_opaque_existential_1(a1, a1[3]);
  lazy protocol witness table accessor for type SPRotationAxis3D.CodingKeys and conformance SPRotationAxis3D.CodingKeys();
  dispatch thunk of Decoder.container<A>(keyedBy:)();
  if (!v1)
  {
    LOBYTE(v14)  = 0;
    KeyedDecodingContainer.decode(_:forKey:)();
    double v9 = v8;
    LOBYTE(v14)  = 1;
    KeyedDecodingContainer.decode(_:forKey:)();
    double v11 = v10;
    LOBYTE(v14)  = 2;
    KeyedDecodingContainer.decode(_:forKey:)();
    SPPoint3DMake(v9, v11, v13, &v14);
    double v2 = v14;
    (*(void (**)(char *, uint64_t))(v5 + 8))(v7, v4);
  }
  __swift_destroy_boxed_opaque_existential_1(a1);
  return v2;
}

unint64_t lazy protocol witness table accessor for type SPRotationAxis3D and conformance SPRotationAxis3D()
{
  unint64_t result = lazy protocol witness table cache variable for type SPRotationAxis3D and conformance SPRotationAxis3D;
  if (!lazy protocol witness table cache variable for type SPRotationAxis3D and conformance SPRotationAxis3D)
  {
    type metadata accessor for SPRotationAxis3D(255);
    unint64_t result = swift_getWitnessTable();
    atomic_store(result, (unint64_t *)&lazy protocol witness table cache variable for type SPRotationAxis3D and conformance SPRotationAxis3D);
  }
  return result;
}

double sub_228C1F07C@<D0>(double *a1@<X0>, void *a2@<X8>)
{
  double result = *a1;
  *a2  = *(void *)a1;
  return result;
}

double sub_228C1F088(double *a1, void *a2)
{
  double result = *a1;
  *a2  = *(void *)a1;
  return result;
}

double sub_228C1F094@<D0>(uint64_t a1@<X0>, double *a2@<X8>)
{
  double result = *(double *)(a1 + 8);
  *a2  = result;
  return result;
}

double sub_228C1F0A0(double *a1, uint64_t a2)
{
  double result = *a1;
  *(double *)(a2 + 8)  = *a1;
  return result;
}

double sub_228C1F0AC@<D0>(uint64_t a1@<X0>, double *a2@<X8>)
{
  double result = *(double *)(a1 + 16);
  *a2  = result;
  return result;
}

double sub_228C1F0B8(double *a1, uint64_t a2)
{
  double result = *a1;
  *(double *)(a2 + 16)  = *a1;
  return result;
}

unsigned char *storeEnumTagSinglePayload for SPRotationAxis3D.CodingKeys(unsigned char *result, unsigned int a2, unsigned int a3)
{
  if (a3 + 2 >= 0xFFFF00) {
    int v3 = 4;
  }
  else {
    int v3 = 2;
  }
  if ((a3 + 2) >> 8 < 0xFF) {
    unsigned int v4 = 1;
  }
  else {
    unsigned int v4 = v3;
  }
  if (a3 >= 0xFE) {
    uint64_t v5 = v4;
  }
  else {
    uint64_t v5 = 0;
  }
  if (a2 > 0xFD)
  {
    unsigned int v6 = ((a2 - 254) >> 8) + 1;
    *double result = a2 + 2;
    switch(v5)
    {
      case 1:
        result[1]  = v6;
        break;
      case 2:
        *(_WORD *)(result + 1)  = v6;
        break;
      case 3:
LABEL_23:
        __break(1u);
        JUMPOUT(0x228C1F190);
      case 4:
        *(_DWORD *)(result + 1)  = v6;
        break;
      default:
        return result;
    }
  }
  else
  {
    switch(v5)
    {
      case 1:
        result[1]  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      case 2:
        *(_WORD *)(result + 1)  = 0;
        goto LABEL_17;
      case 3:
        goto LABEL_23;
      case 4:
        *(_DWORD *)(result + 1)  = 0;
        if (!a2) {
          return result;
        }
        goto LABEL_18;
      default:
LABEL_17:
        if (a2) {
LABEL_18:
        }
          *double result = a2 + 2;
        break;
    }
  }
  return result;
}

ValueMetadata *type metadata accessor for SPRotationAxis3D.CodingKeys()
{
  return &type metadata for SPRotationAxis3D.CodingKeys;
}

uint64_t BinaryFloatingPoint.init<A>(_:)()
{
  return MEMORY[0x270F9D010]();
}

uint64_t String.hash(into:)()
{
  return MEMORY[0x270F9D708]();
}

Swift::Void __swiftcall String.append(_:)(Swift::String a1)
{
}

uint64_t static Array._allocateBufferUninitialized(minimumCapacity:)()
{
  return MEMORY[0x270F9DC20]();
}

uint64_t Double.description.getter()
{
  return MEMORY[0x270F9DD60]();
}

uint64_t Double.write<A>(to:)()
{
  return MEMORY[0x270F9DDA0]();
}

uint64_t type metadata accessor for Optional()
{
  return MEMORY[0x270F9E3C0]();
}

Swift::Void __swiftcall _StringGuts.grow(_:)(Swift::Int a1)
{
}

uint64_t _print_unlocked<A, B>(_:_:)()
{
  return MEMORY[0x270F9ED70]();
}

uint64_t _assertionFailure(_:_:file:line:flags:)()
{
  return MEMORY[0x270F9EFF0]();
}

Swift::Int __swiftcall _findStringSwitchCase(cases:string:)(Swift::OpaquePointer cases, Swift::String string)
{
  return MEMORY[0x270F9F230](cases._rawValue, string._countAndFlagsBits, string._object);
}

uint64_t KeyedDecodingContainer.decode(_:forKey:)()
{
  return MEMORY[0x270F9F2F8]();
}

uint64_t KeyedDecodingContainer.decode<A>(_:forKey:)()
{
  return MEMORY[0x270F9F318]();
}

uint64_t KeyedEncodingContainer.encode(_:forKey:)()
{
  return MEMORY[0x270F9F438]();
}

uint64_t KeyedEncodingContainer.encode<A>(_:forKey:)()
{
  return MEMORY[0x270F9F458]();
}

uint64_t _stringCompareWithSmolCheck(_:_:expecting:)()
{
  return MEMORY[0x270F9F7D0]();
}

uint64_t dispatch thunk of SingleValueDecodingContainer.decode(_:)()
{
  return MEMORY[0x270F9F7F8]();
}

uint64_t dispatch thunk of SingleValueEncodingContainer.encode(_:)()
{
  return MEMORY[0x270F9F888]();
}

uint64_t Hasher.init(_seed:)()
{
  return MEMORY[0x270F9FC48]();
}

Swift::Void __swiftcall Hasher._combine(_:)(Swift::UInt a1)
{
}

Swift::Void __swiftcall Hasher._combine(_:)(Swift::UInt64 a1)
{
}

Swift::Int __swiftcall Hasher._finalize()()
{
  return MEMORY[0x270F9FC90]();
}

uint64_t type metadata accessor for Mirror.DisplayStyle()
{
  return MEMORY[0x270F9FCB8]();
}

uint64_t type metadata accessor for Mirror.AncestorRepresentation()
{
  return MEMORY[0x270F9FCC8]();
}

uint64_t Mirror.init<A>(_:children:displayStyle:ancestorRepresentation:)()
{
  return MEMORY[0x270F9FCF0]();
}

uint64_t dispatch thunk of Decoder.singleValueContainer()()
{
  return MEMORY[0x270F9FD60]();
}

uint64_t dispatch thunk of Decoder.container<A>(keyedBy:)()
{
  return MEMORY[0x270F9FD70]();
}

uint64_t dispatch thunk of Encoder.singleValueContainer()()
{
  return MEMORY[0x270F9FD88]();
}

uint64_t dispatch thunk of Encoder.container<A>(keyedBy:)()
{
  return MEMORY[0x270F9FD98]();
}

uint64_t __invert_d3()
{
  return MEMORY[0x270ED7E00]();
}

uint64_t __invert_d4()
{
  return MEMORY[0x270ED7E08]();
}

__double2 __sincos_stret(double a1)
{
  MEMORY[0x270ED7E88](a1);
  result.__cosval  = v2;
  result.__sinval  = v1;
  return result;
}

long double acos(long double __x)
{
  MEMORY[0x270ED8588](__x);
  return result;
}

long double acosh(long double __x)
{
  MEMORY[0x270ED85A0](__x);
  return result;
}

long double asin(long double __x)
{
  MEMORY[0x270ED8610](__x);
  return result;
}

long double asinh(long double __x)
{
  MEMORY[0x270ED8620](__x);
  return result;
}

long double atan(long double __x)
{
  MEMORY[0x270ED86A0](__x);
  return result;
}

long double atan2(long double __y, long double __x)
{
  MEMORY[0x270ED86A8](__y, __x);
  return result;
}

long double atanh(long double __x)
{
  MEMORY[0x270ED86C8](__x);
  return result;
}

long double cos(long double __x)
{
  MEMORY[0x270ED9128](__x);
  return result;
}

long double cosh(long double __x)
{
  MEMORY[0x270ED9138](__x);
  return result;
}

long double exp(long double __x)
{
  MEMORY[0x270ED9858](__x);
  return result;
}

long double fmod(long double __x, long double __y)
{
  MEMORY[0x270ED99D0](__x, __y);
  return result;
}

void free(void *a1)
{
}

long double log(long double __x)
{
  MEMORY[0x270EDA0A8](__x);
  return result;
}

void *__cdecl malloc(size_t __size)
{
  return (void *)MEMORY[0x270EDA328](__size);
}

size_t malloc_size(const void *ptr)
{
  return MEMORY[0x270EDA378](ptr);
}

void *__cdecl memcpy(void *__dst, const void *__src, size_t __n)
{
  return (void *)MEMORY[0x270EDA470](__dst, __src, __n);
}

void *__cdecl memmove(void *__dst, const void *__src, size_t __len)
{
  return (void *)MEMORY[0x270EDA488](__dst, __src, __len);
}

long double remainder(long double __x, long double __y)
{
  MEMORY[0x270EDB1D8](__x, __y);
  return result;
}

long double sin(long double __x)
{
  MEMORY[0x270EDB4E8](__x);
  return result;
}

long double sinh(long double __x)
{
  MEMORY[0x270EDB4F8](__x);
  return result;
}

uint64_t swift_allocObject()
{
  return MEMORY[0x270FA0198]();
}

uint64_t swift_bridgeObjectRelease()
{
  return MEMORY[0x270FA01E8]();
}

uint64_t swift_bridgeObjectRelease_n()
{
  return MEMORY[0x270FA01F0]();
}

uint64_t swift_deallocClassInstance()
{
  return MEMORY[0x270FA0228]();
}

uint64_t swift_getForeignTypeMetadata()
{
  return MEMORY[0x270FA0370]();
}

uint64_t swift_getTypeByMangledNameInContext2()
{
  return MEMORY[0x270FA0440]();
}

uint64_t swift_getTypeByMangledNameInContextInMetadataState2()
{
  return MEMORY[0x270FA0448]();
}

uint64_t swift_getWitnessTable()
{
  return MEMORY[0x270FA0450]();
}

uint64_t swift_isUniquelyReferenced_nonNull_native()
{
  return MEMORY[0x270FA04C8]();
}

uint64_t swift_makeBoxUnique()
{
  return MEMORY[0x270FA04D8]();
}

uint64_t swift_once()
{
  return MEMORY[0x270FA04F0]();
}

uint64_t swift_retain()
{
  return MEMORY[0x270FA0530]();
}

uint64_t swift_setDeallocating()
{
  return MEMORY[0x270FA0550]();
}

long double tan(long double __x)
{
  MEMORY[0x270EDB810](__x);
  return result;
}

long double tanh(long double __x)
{
  MEMORY[0x270EDB820](__x);
  return result;
}